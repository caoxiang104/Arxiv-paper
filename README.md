# 162、在聚焦二十面体网格上应用VertexShuffle实现360度视频超分辨率
- [ ] Applying VertexShuffle Toward 360-Degree Video Super-Resolution on Focused-Icosahedral-Mesh 
时间：2021年06月21日                         第一作者：Na Li                        [链接](https://arxiv.org/abs/2106.11253).                     
## 摘要：随着360度图像/视频、增强现实（AR）和虚拟现实（VR）的出现，人们对球形信号的分析和处理的需求越来越大。然而，大量的研究工作都集中在从球面信号投影出来的平面信号上，导致了像素浪费、失真等问题。球形CNN的最新进展为直接分析球形信号提供了可能。然而，他们关注的是全网格，这使得在实际应用中由于对带宽的要求非常大而难以处理。为了解决360度视频流的带宽浪费问题和节省计算量，我们利用聚焦二十面体网格来表示一个小区域，并构造矩阵将球形内容旋转到聚焦网格区域。我们还提出了一种新的顶点洗牌操作，与UGSCNN中引入的MeshConv转置操作相比，该操作可以显著提高性能和效率。我们进一步将所提出的方法应用于超分辨率模型，这是第一个提出一个直接操作360度数据的球形像素网格表示的球形超分辨率模型。为了评估我们的模型，我们还收集了一组高分辨率的360度视频来生成一个球形图像数据集。我们的实验表明，与使用简单MeshConv转置操作的基线球面超分辨率模型相比，我们提出的球面超分辨率模型在性能和推理时间方面都取得了显著的优势。综上所述，我们的模型在360度输入上取得了很好的超分辨率性能，在对网格上的16x顶点进行超分辨率处理时，平均达到32.79db的PSNR。
<details>	<summary>英文摘要</summary>	With the emerging of 360-degree image/video, augmented reality (AR) and virtual reality (VR), the demand for analysing and processing spherical signals get tremendous increase. However, plenty of effort paid on planar signals that projected from spherical signals, which leading to some problems, e.g. waste of pixels, distortion. Recent advances in spherical CNN have opened up the possibility of directly analysing spherical signals. However, they pay attention to the full mesh which makes it infeasible to deal with situations in real-world application due to the extremely large bandwidth requirement. To address the bandwidth waste problem associated with 360-degree video streaming and save computation, we exploit Focused Icosahedral Mesh to represent a small area and construct matrices to rotate spherical content to the focused mesh area. We also proposed a novel VertexShuffle operation that can significantly improve both the performance and the efficiency compared to the original MeshConv Transpose operation introduced in UGSCNN. We further apply our proposed methods on super resolution model, which is the first to propose a spherical super-resolution model that directly operates on a mesh representation of spherical pixels of 360-degree data. To evaluate our model, we also collect a set of high-resolution 360-degree videos to generate a spherical image dataset. Our experiments indicate that our proposed spherical super-resolution model achieves significant benefits in terms of both performance and inference time compared to the baseline spherical super-resolution model that uses the simple MeshConv Transpose operation. In summary, our model achieves great super-resolution performance on 360-degree inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices on the mesh. </details>
<details>	<summary>注释</summary>	This paper introduce a new mesh representation and a new upsampling method on a mesh </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 161、基于深度度量学习的生成建模对手流形匹配
- [ ] Adversarial Manifold Matching via Deep Metric Learning for Generative Modeling 
时间：2021年06月20日                         第一作者：Mengyu Dai                        [链接](https://arxiv.org/abs/2106.10777).                     
## 摘要：我们提出了一种生成模型的流形匹配方法，它包括一个分布生成器（或数据生成器）和一个度量生成器。在我们的框架中，我们将真实的数据集看作嵌入高维欧氏空间的流形。分布生成器的目标是生成样本，这些样本遵循围绕真实数据流形压缩的某种分布。它是通过使用两组点的几何形状描述符（例如质心和直径）和学习的距离度量来匹配两组点来实现的；度量生成器利用真实数据和生成的样本来学习距离度量，该距离度量接近真实数据流形上的某个固有测地距离。生成的距离度量进一步用于流形匹配。在训练过程中，两个网络同时学习。我们将该方法应用于无监督学习和有监督学习任务中：在无条件图像生成任务中，与已有的生成模型相比，该方法取得了较好的效果；在超分辨率任务中，我们将该框架融入到基于感知的模型中，通过生成具有更自然纹理的样本来提高视觉质量。理论分析和实际数据实验都证明了该框架的可行性和有效性。
<details>	<summary>英文摘要</summary>	We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and $p$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Both theoretical analysis and real data experiments guarantee the feasibility and effectiveness of the proposed framework. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 160、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年06月19日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
## 摘要：超分辨率（SR）是一个一对多的任务，有多种可能的解决方案。然而，以往的研究并不关注这一特征。对于一对多管道，生成器应该能够生成重建的多个估计，并且不会因为生成相似且同样真实的图像而受到惩罚。为了实现这一点，我们建议在剩余密集块（RRDB）中的每一个残差之后加入加权像素噪声，以使生成器能够生成各种图像。我们修改了严格的内容丢失，只要内容一致，就不会惩罚重建图像中的随机变化。此外，我们观察到，在DIV2K和DIV8K数据集中，有一些没有焦点的区域提供了毫无帮助的指导方针。我们使用[10]的方法过滤训练数据中的模糊区域。最后，我们修改鉴别器以接收低解析度影像作为参考影像与目标影像，以提供更好的回馈给产生器。使用我们提出的方法，我们能够提高ESRGAN在x4知觉SR中的性能，并在x16知觉极限SR中获得最先进的LPIPS分数。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is a one-to-many task with multiple possible solutions. However, previous works were not concerned about this characteristic. For a one-to-many pipeline, the generator should be able to generate multiple estimates of the reconstruction, and not be penalized for generating similar and equally realistic images. To achieve this, we propose adding weighted pixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable the generator to generate various images. We modify the strict content loss to not penalize the stochastic variation in reconstructed images as long as it has consistent content. Additionally, we observe that there are out-of-focus regions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We filter blurry regions in the training data using the method of [10]. Finally, we modify the discriminator to receive the low-resolution image as a reference image along with the target image to provide better feedback to the generator. Using our proposed methods, we were able to improve the performance of ESRGAN in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual extreme SR. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 159、基于主观评价的真实图像增强方法
- [ ] Debiased Subjective Assessment of Real-World Image Enhancement 
时间：2021年06月18日                         第一作者：Cao Peibei. Wang Zhangyang                       [链接](https://arxiv.org/abs/2106.10080).                     
## 摘要：在实际图像增强中，获取地面真实数据往往是一个挑战（如果不是不可能的话），这就妨碍了采用距离度量进行客观质量评估。因此，人们常常求助于主观质量评估，这是评估图像增强最直接和可靠的方法。传统的主观测试需要人工预选一小部分视觉样本，这可能会产生三种偏差：1）由于所选样本在图像空间的分布极为稀疏，导致采样偏差；2） 由于所选样本的潜在过拟合导致的算法偏差；3） 由于进一步潜在的樱桃采摘试验结果而产生的主观偏见。这最终使得现实世界中的图像增强领域更像是一门艺术，而不是一门科学。在这里，我们采取步骤，通过自动采样一组自适应和多样性的图像进行后续测试，来削弱传统的主观评估。这是通过将样本选择转化为增强子之间的差异和所选输入图像之间的多样性的联合最大化来实现的。仔细的视觉检查得到的增强图像提供了一个增强算法的排名。我们展示了我们的主观评估方法使用三个流行的和实际要求的图像增强任务：去杂波，超分辨率和弱光增强。
<details>	<summary>英文摘要</summary>	In real-world image enhancement, it is often challenging (if not impossible) to acquire ground-truth data, preventing the adoption of distance metrics for objective quality assessment. As a result, one often resorts to subjective quality assessment, the most straightforward and reliable means of evaluating image enhancement. Conventional subjective testing requires manually pre-selecting a small set of visual examples, which may suffer from three sources of biases: 1) sampling bias due to the extremely sparse distribution of the selected samples in the image space; 2) algorithmic bias due to potential overfitting the selected samples; 3) subjective bias due to further potential cherry-picking test results. This eventually makes the field of real-world image enhancement more of an art than a science. Here we take steps towards debiasing conventional subjective assessment by automatically sampling a set of adaptive and diverse images for subsequent testing. This is achieved by casting sample selection into a joint maximization of the discrepancy between the enhancers and the diversity among the selected input images. Careful visual inspection on the resulting enhanced images provides a debiased ranking of the enhancement algorithms. We demonstrate our subjective assessment method using three popular and practically demanding image enhancement tasks: dehazing, super-resolution, and low-light enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月21日</details>

# 158、从模糊中恢复形状：快速移动对象的纹理三维形状和运动
- [ ] Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects 
时间：2021年06月16日                         第一作者：Denys Rozumnyi                       [链接](https://arxiv.org/abs/2106.08762).                     
## 摘要：我们提出了一个新的任务，联合重建三维形状，纹理和运动的物体从一个单一的运动模糊图像。虽然以前的方法只在二维图像域解决去模糊问题，但我们提出的在三维域中对所有对象属性的严格建模能够正确描述任意对象的运动。这将导致更好的图像分解和更清晰的去模糊结果。我们将观察到的运动模糊物体的外观建模为背景和具有恒定平移和旋转的三维物体的组合。我们的方法通过使用合适的正则化器进行可微渲染来最小化重建输入图像的损失。这使得能够以高保真度估计模糊对象的纹理三维网格。在快速运动物体去模糊的几个基准上，我们的方法明显优于其他方法。定性结果表明，重建的三维网格生成了高质量的时间超分辨率和新颖的图像。
<details>	<summary>英文摘要</summary>	We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to significantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high fidelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object. </details>
<details>	<summary>注释</summary>	15 pages, 8 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年06月17日</details>

# 157、受感知启发的压缩视频超分辨率
- [ ] Perceptually-inspired super-resolution of compressed videos 
时间：2021年06月15日                         第一作者：Di Ma                       [链接](https://arxiv.org/abs/2106.08147).                     
## 摘要：空间分辨率自适应是视频压缩中常用的一种提高编码效率的技术。这种方法对输入视频的低分辨率版本进行编码，并在解码过程中重建原始分辨率。为了进一步提高重建质量，最近的工作采用了基于卷积神经网络（CNNs）的先进的超分辨率方法来代替传统的上采样滤波器。这些方法通常被训练来最小化基于像素的损失，例如均方误差（MSE），尽管这种类型的损失度量与主观观点没有很好的相关性。本文提出了一种基于感知启发的超分辨率方法（M-SRGAN），该方法利用一种改进的CNN模型对压缩视频进行空间上采样，该模型是在具有感知损失函数的压缩内容上用生成对抗网络（GAN）训练的。该方法与HEVC-hm16.20相结合，并在JVET通用测试条件（UHD测试序列）上使用随机访问配置进行了评估。结果表明，与原来的hm16.20相比，感知质量有了明显的改善，基于感知质量度量VMAF的平均比特率节省了35.6%（Bj{\o}ntegaard Delta度量）。
<details>	<summary>英文摘要</summary>	Spatial resolution adaptation is a technique which has often been employed in video compression to enhance coding efficiency. This approach encodes a lower resolution version of the input video and reconstructs the original resolution during decoding. Instead of using conventional up-sampling filters, recent work has employed advanced super-resolution methods based on convolutional neural networks (CNNs) to further improve reconstruction quality. These approaches are usually trained to minimise pixel-based losses such as Mean-Squared Error (MSE), despite the fact that this type of loss metric does not correlate well with subjective opinions. In this paper, a perceptually-inspired super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of compressed video using a modified CNN model, which has been trained using a generative adversarial network (GAN) on compressed content with perceptual loss functions. The proposed method was integrated with HEVC HM 16.20, and has been evaluated on the JVET Common Test Conditions (UHD test sequences) using the Random Access configuration. The results show evident perceptual quality improvement over the original HM 16.20, with an average bitrate saving of 35.6% (Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF. </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 156、SinIR：单图像重建的高效通用图像处理
- [ ] SinIR: Efficient General Image Manipulation with Single Image Reconstruction 
时间：2021年06月14日                         第一作者：Jihyeong Yoo                        [链接](https://arxiv.org/abs/2106.07140).                     
## 摘要：我们提出了一个基于SinIR的高效重建框架，它训练在单个自然图像上进行一般的图像处理，包括超分辨率、编辑、协调、绘制到图像、照片真实感风格转换和艺术风格转换。我们通过级联多尺度学习在单个图像上训练我们的模型，每个尺度上的每个网络负责图像重建。与GAN目标相比，该重建目标大大降低了训练的复杂度和运行时间。然而，重建目标也加剧了产出质量。因此，为了解决这个问题，我们进一步利用简单的随机像素洗牌，这也提供了控制操作，受去噪自动编码器的启发。通过定量评估，我们发现SinIR在各种图像处理任务上都有很强的竞争力。此外，使用更简单的训练目标（即重建），SinIR的训练速度是SinGAN（500 X 500图像）的33.5倍，后者可以解决类似的任务。我们的代码可在github.com/YooJiHyeong/SinIR上公开获取。
<details>	<summary>英文摘要</summary>	We propose SinIR, an efficient reconstruction-based framework trained on a single natural image for general image manipulation, including super-resolution, editing, harmonization, paint-to-image, photo-realistic style transfer, and artistic style transfer. We train our model on a single image with cascaded multi-scale learning, where each network at each scale is responsible for image reconstruction. This reconstruction objective greatly reduces the complexity and running time of training, compared to the GAN objective. However, the reconstruction objective also exacerbates the output quality. Therefore, to solve this problem, we further utilize simple random pixel shuffling, which also gives control over manipulation, inspired by the Denoising Autoencoder. With quantitative evaluation, we show that SinIR has competitive performance on various image manipulation tasks. Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our code is publicly available at github.com/YooJiHyeong/SinIR. </details>
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 155、基于群的双向递归小波神经网络在视频超分辨率中的应用
- [ ] Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video Super-Resolution 
时间：2021年06月14日                         第一作者：Young-Ju Choi                       [链接](https://arxiv.org/abs/2106.07190).                     
## 摘要：视频超分辨率（VSR）的目标是从低分辨率（LR）帧中估计出高分辨率（HR）帧。VSR的关键挑战在于有效地利用帧内空间相关性和连续帧间的时间相关性。然而，以往的方法大多对不同类型的空间特征进行统一处理，从分离的模块中提取空间和时间特征。这导致缺乏获得有意义的信息和加强细节。在VSR中，有三种时态建模框架：二维卷积神经网络（CNN）、三维CNN和递归神经网络（RNN）。其中，基于RNN的方法适用于序列数据。因此，利用相邻帧的隐藏状态可以大大提高SR性能。然而，在递归结构中的每一个时间步，基于RNN的以往工作都限制性地利用相邻特征。由于每个时间步的可访问运动范围很窄，因此对于动态或大运动，恢复丢失的细节仍然存在限制。本文提出了一种基于群的双向递归小波神经网络（GBR-WNN）来有效地挖掘VSR的序列数据和时空信息。提出了一种基于组的双向RNN（GBR）时序建模框架，该框架建立在具有图片组（GOP）的结构良好的过程之上。我们提出了一个时间小波注意（TWA）模块，其中注意被用于空间和时间特征。实验结果表明，与现有方法相比，该方法在定量和定性评价方面都取得了较好的效果。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame from a low-resolution (LR) frames. The key challenge for VSR lies in the effective exploitation of spatial correlation in an intra-frame and temporal dependency between consecutive frames. However, most of the previous methods treat different types of the spatial features identically and extract spatial and temporal features from the separated modules. It leads to lack of obtaining meaningful information and enhancing the fine details. In VSR, there are three types of temporal modeling frameworks: 2D convolutional neural networks (CNN), 3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach is suitable for sequential data. Thus the SR performance can be greatly improved by using the hidden states of adjacent frames. However, at each of time step in a recurrent structure, the RNN-based previous works utilize the neighboring features restrictively. Since the range of accessible motion per time step is narrow, there are still limitations to restore the missing details for dynamic or large motion. In this paper, we propose a group-based bi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the sequential data and spatio-temporal information effectively for VSR. The proposed group-based bi-directional RNN (GBR) temporal modeling framework is built on the well-structured process with the group of pictures (GOP). We propose a temporal wavelet attention (TWA) module, in which attention is adopted for both spatial and temporal features. Experimental results demonstrate that the proposed method achieves superior performance compared with state-of-the-art methods in both of quantitative and qualitative evaluations. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 154、单幅图像超分辨率的反馈金字塔注意网络
- [ ] Feedback Pyramid Attention Networks for Single Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06966).                     
## 摘要：近年来，基于卷积神经网络（CNN）的图像超分辨率（SR）方法取得了显著的性能改进。然而，大多数基于CNN的方法主要集中在前馈结构设计上，而忽略了人类视觉系统中普遍存在的反馈机制。在本文中，我们提出了反馈金字塔注意网络（FPAN）来充分利用特征之间的相互依赖性。具体地说，本文提出了一种新的反馈连接结构，利用高层信息增强低层特征表达。在我们的方法中，每一层在第一阶段的输出也被用作下一阶段对应层的输入，以重新更新先前的低层滤波器。此外，我们还引入了金字塔非局部结构来模拟不同尺度下的全局上下文信息，提高了网络的区分性。在各种数据集上的大量实验结果证明了我们的FPAN与最先进的SR方法相比的优越性。
<details>	<summary>英文摘要</summary>	Recently, convolutional neural network (CNN) based image super-resolution (SR) methods have achieved significant performance improvement. However, most CNN-based methods mainly focus on feed-forward architecture design and neglect to explore the feedback mechanism, which usually exists in the human visual system. In this paper, we propose feedback pyramid attention networks (FPAN) to fully exploit the mutual dependencies of features. Specifically, a novel feedback connection structure is developed to enhance low-level feature expression with high-level information. In our method, the output of each layer in the first stage is also used as the input of the corresponding layer in the next state to re-update the previous low-level filters. Moreover, we introduce a pyramid non-local structure to model global contextual information in different scales and improve the discriminative representation of the network. Extensive experimental results on various datasets demonstrate the superiority of our FPAN in comparison with the state-of-the-art SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 153、用于超分辨率轻量化图像的金字塔密集注意网络
- [ ] Pyramidal Dense Attention Networks for Lightweight Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06996).                     
## 摘要：近年来，深度卷积神经网络方法在图像超分辨率（SR）方面取得了很好的效果，但由于存储开销大，不易应用于嵌入式设备。为了解决这一问题，本文提出了一种用于轻量化图像超分辨率的金字塔密集注意网络（PDAN）。在我们的方法中，所提出的金字塔密集学习可以逐渐增加金字塔密集块内部密集连接层的宽度，从而有效地提取深层特征。同时，引入了组数随卷积层数线性增长的自适应组卷积，以减少参数爆炸。此外，我们还提出了一种新的联合注意方法，以有效地捕捉空间维度和通道维度之间的跨维度交互，从而提供丰富的区分性特征表示。大量的实验结果表明，与目前最先进的轻量级SR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural network methods have achieved an excellent performance in image superresolution (SR), but they can not be easily applied to embedded devices due to large memory cost. To solve this problem, we propose a pyramidal dense attention network (PDAN) for lightweight image super-resolution in this paper. In our method, the proposed pyramidal dense learning can gradually increase the width of the densely connected layer inside a pyramidal dense block to extract deep features efficiently. Meanwhile, the adaptive group convolution that the number of groups grows linearly with dense convolutional layers is introduced to relieve the parameter explosion. Besides, we also present a novel joint attention to capture cross-dimension interaction between the spatial dimensions and channel dimension in an efficient way for providing rich discriminative feature representations. Extensive experimental results show that our method achieves superior performance in comparison with the state-of-the-art lightweight SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 152、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年06月13日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
<details>	<summary>注释</summary>	10 pages, 3 figures Journal-ref: International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2021) </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 151、用于关节MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年06月12日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
## 摘要：磁共振成像（MRI）的核心问题是加速度和图像质量之间的权衡。图像重建和超分辨率是磁共振成像（MRI）的两项关键技术。当前的方法被设计为分别执行这些任务，而忽略了它们之间的相关性。在这项工作中，我们提出了一个端到端的任务变压器网络（T$^2$Net）用于关节MRI重建和超分辨率，它允许在多个任务之间共享表示和特征传输，从而从高度欠采样和退化的MRI数据中获得更高质量、超分辨率和无运动伪影的图像。我们的框架结合了重建和超分辨率，分为两个子分支，其特征表示为查询和键。具体来说，我们鼓励两个任务之间的联合特征学习，从而传递准确的任务信息。我们首先使用两个独立的CNN分支来提取特定于任务的特征。然后，设计了一个任务转换器模块来嵌入和合成两个任务之间的相关性。实验结果表明，我们的多任务模型在定量和定性上都明显优于先进的序贯方法。
<details>	<summary>英文摘要</summary>	The core problem of Magnetic Resonance Imaging (MRI) is the trade off between acceleration and image quality. Image reconstruction and super-resolution are two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are designed to perform these tasks separately, ignoring the correlations between them. In this work, we propose an end-to-end task transformer network (T$^2$Net) for joint MRI reconstruction and super-resolution, which allows representations and feature transmission to be shared between multiple task to achieve higher-quality, super-resolved and motion-artifacts-free images from highly undersampled and degenerated MRI data. Our framework combines both reconstruction and super-resolution, divided into two sub-branches, whose features are expressed as queries and keys. Specifically, we encourage joint feature learning between the two tasks, thereby transferring accurate task information. We first use two separate CNN branches to extract task-specific features. Then, a task transformer module is designed to embed and synthesize the relevance between the two tasks. Experimental results show that our multi-task model significantly outperforms advanced sequential methods, both quantitatively and qualitatively. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 150、视频超分辨率转换器
- [ ] Video Super-Resolution Transformer 
时间：2021年06月12日                         第一作者：Jiezhang Cao                       [链接](https://arxiv.org/abs/2106.06847).                     
## 摘要：视频超分辨率（VSR）是一个时空序列预测问题，其目的是从相应的低分辨率视频中恢复出高分辨率的视频。近年来，Transformer以其对序列间建模的并行计算能力得到了广泛的应用。因此，应用视觉变换器来求解VSR似乎是很简单的。然而，由于以下两个原因，具有全连接自关注层和令牌前馈层的变压器的典型块设计不适合VSR。首先，完全连通的自注意层由于依赖于线性层来计算注意图而忽略了对数据局部性的利用。其次，令牌前馈层缺乏特征对齐，这对于VSR很重要，因为该层独立地处理每个输入令牌嵌入，它们之间没有任何交互。本文对VSR用变压器进行了首次尝试。具体来说，为了解决第一个问题，我们提出了一个时空卷积自我注意层的理论理解，以利用局部信息。对于第二个问题，我们设计了一个基于双向光流的前馈层来发现不同视频帧之间的相关性并对齐特征。在多个基准数据集上的大量实验证明了该方法的有效性。代码将在https://github.com/caojiezhang/VSR-Transformer.
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 149、用于GAN自动设计的自适应超分辨结构框架
- [ ] A self-adapting super-resolution structures framework for automatic design of GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.06011).                     
## 摘要：随着深度学习的发展，单一的超分辨率图像重建网络模型变得越来越复杂。模型超参数的微小变化对模型性能的影响较大。在现有的工作中，专家们已经逐渐探索出一套基于经验值或进行暴力搜索的最优模型参数。本文介绍了一种新的超分辨率图像重建生成对抗性网络框架，并用贝叶斯优化方法对发生器和鉴别器的超参数进行了优化。发生器采用自校正卷积法，鉴别器采用卷积法。定义了网络层数、神经元数等超参数。该方法采用贝叶斯优化作为GAN的优化策略。不仅可以自动找到最优超参数解，而且可以构造超分辨率图像重建网络，减少人工工作量。实验表明，贝叶斯优化算法比其他两种优化算法能更早地搜索到最优解。
<details>	<summary>英文摘要</summary>	With the development of deep learning, the single super-resolution image reconstruction network models are becoming more and more complex. Small changes in hyperparameters of the models have a greater impact on model performance. In the existing works, experts have gradually explored a set of optimal model parameters based on empirical values or performing brute-force search. In this paper, we introduce a new super-resolution image reconstruction generative adversarial network framework, and a Bayesian optimization method used to optimizing the hyperparameters of the generator and discriminator. The generator is made by self-calibrated convolution, and discriminator is made by convolution lays. We have defined the hyperparameters such as the number of network layers and the number of neurons. Our method adopts Bayesian optimization as a optimization policy of GAN in our model. Not only can find the optimal hyperparameter solution automatically, but also can construct a super-resolution image reconstruction network, reducing the manual workload. Experiments show that Bayesian optimization can search the optimal solution earlier than the other two optimization algorithms. </details>
<details>	<summary>注释</summary>	9 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月14日</details>

# 148、基于自标定卷积GAN的超分辨率图像重建
- [ ] Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.05545).                     
## 摘要：随着深度学习在计算机视觉中的有效应用，超分辨率图像重建的研究取得了突破性进展。然而，许多研究指出，神经网络对图像特征提取的不足可能会导致新重建图像的恶化。另一方面，由于过度平滑，生成的图片有时过于人工。为了解决上述问题，我们提出了一种新的自校正卷积生成对抗网络。该发生器由特征提取和图像重建两部分组成。特征提取使用自校正卷积，卷积包含四个部分，每个部分都有特定的功能。它不仅可以扩大感受野的范围，而且可以获得长距离的空间依赖性和通道间依赖性。然后进行图像重建，最后重建超分辨率图像。在SSIM评估方法下，我们对set5、set14和BSD100等不同的数据集进行了深入的实验。实验结果证明了该网络的有效性。
<details>	<summary>英文摘要</summary>	With the effective application of deep learning in computer vision, breakthroughs have been made in the research of super-resolution images reconstruction. However, many researches have pointed out that the insufficiency of the neural network extraction on image features may bring the deteriorating of newly reconstructed image. On the other hand, the generated pictures are sometimes too artificial because of over-smoothing. In order to solve the above problems, we propose a novel self-calibrated convolutional generative adversarial networks. The generator consists of feature extraction and image reconstruction. Feature extraction uses self-calibrated convolutions, which contains four portions, and each portion has specific functions. It can not only expand the range of receptive fields, but also obtain long-range spatial and inter-channel dependencies. Then image reconstruction is performed, and finally a super-resolution image is reconstructed. We have conducted thorough experiments on different datasets including set5, set14 and BSD100 under the SSIM evaluation method. The experimental results prove the effectiveness of the proposed network. </details>
<details>	<summary>注释</summary>	8 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月11日</details>

# 147、基于参考图像超分辨率的变分自动编码器
- [ ] Variational AutoEncoder for Reference based Image Super-Resolution 
时间：2021年06月08日                         第一作者：Zhi-Song Liu                        [链接](https://arxiv.org/abs/2106.04090).                     
## 摘要：本文提出了一种新的基于参考的变分自编码器（RefVAE）图像超分辨率方法。现有的超分辨率方法主要集中在单幅图像的超分辨率上，对于较大的上采样因子（如8$\times$）效果不佳。提出了一种基于参考的图像超分辨率方法，任意一幅图像都可以作为参考进行超分辨率处理。即使使用随机映射或低分辨率图像本身，该方法也能将参考图像中的知识转化为超分辨率图像。根据不同的参考文献，该方法可以从隐藏的超分辨率空间生成不同版本的超分辨率图像。除了使用不同的数据集对PSNR和SSIM进行一些标准评估外，我们还参加了NTIR2021 SR空间挑战赛，并提供了我们方法的随机性评估结果。与其他最先进的方法相比，我们的方法获得了更高的多样性分数。
<details>	<summary>英文摘要</summary>	In this paper, we propose a novel reference based image super-resolution approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8$\times$. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space. Besides using different datasets for some standard evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space challenge and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores. </details>
<details>	<summary>注释</summary>	10 pages, 6 figures Journal-ref: 2021 IEEE Conference on Computer Vision and Pattern Recognition Workshop </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 146、NTIRE 2021突发超分辨率挑战：方法和结果
- [ ] NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results 
时间：2021年06月07日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2106.03839).                     
## 摘要：本文回顾了NTIRE2021对突发超分辨率的挑战。给定一个原始噪声脉冲作为输入，挑战中的任务是生成一个分辨率高出4倍的干净RGB图像。挑战包含两个轨道；轨道1评估综合生成的数据，轨道2使用移动摄像机的真实世界爆发。在最后的测试阶段，6个团队使用不同的解决方案提交了结果。性能最好的方法为突发超分辨率任务设置了一个新的最新技术。
<details>	<summary>英文摘要</summary>	This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a RAW noisy burst as input, the task in the challenge was to generate a clean RGB image with 4 times higher resolution. The challenge contained two tracks; Track 1 evaluating on synthetically generated data, and Track 2 using real-world bursts from mobile camera. In the final testing phase, 6 teams submitted results using a diverse set of solutions. The top-performing methods set a new state-of-the-art for the burst super-resolution task. </details>
<details>	<summary>注释</summary>	NTIRE 2021 Burst Super-Resolution challenge report </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 145、基于深度神经网络的图像和视频流增强技术：综述和未来发展方向
- [ ] Deep Neural Network-based Enhancement for Image and Video Streaming Systems: A Survey and Future Directions 
时间：2021年06月07日                         第一作者：Royson Lee                       [链接](https://arxiv.org/abs/2106.03727).                     
## 摘要：支持互联网的智能手机和超宽显示器正在将各种视频应用程序从点播电影和360度视频转变为视频会议和流媒体直播。然而，在不稳定的网络条件下，在具有不同功能的设备上可靠地交付可视内容仍然是一个开放的问题。近年来，在诸如超分辨率和图像增强等任务的深度学习领域取得的进展使得从低质量图像生成高质量图像的性能达到了前所未有的水平，我们称之为神经增强。在本文中，我们调查了最先进的内容交付系统，这些系统将神经增强作为实现快速响应时间和高视觉质量的关键组件。我们首先介绍现有内容交付系统的组件和体系结构，强调它们面临的挑战，并鼓励使用神经增强模型作为对策。然后，我们将讨论这些模型的部署挑战，并分析现有系统及其设计决策，以有效地克服这些技术挑战。此外，我们强调了针对不同用例的跨系统的关键趋势和通用方法。最后，基于深度学习研究的最新见解，我们提出了未来有希望的方向，以进一步提高内容交付系统的体验质量。
<details>	<summary>英文摘要</summary>	Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360{\deg} videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the components and architecture of existing content delivery systems, highlighting their challenges and motivating the use of neural enhancement models as a countermeasure. We then cover the deployment challenges of these models and analyze existing systems and their design decisions in efficiently overcoming these technical challenges. Additionally, we underline the key trends and common approaches across systems that target diverse use-cases. Finally, we present promising future directions based on the latest insights from deep learning research to further boost the quality of experience of content delivery systems. </details>
<details>	<summary>注释</summary>	Accepted for publication at the ACM Computing Surveys (CSUR) journal, 2021. arXiv admin note: text overlap with arXiv:2010.05838 </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 144、深度学习使体积荧光显微镜无参考各向同性超分辨率成为可能
- [ ] Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy 
时间：2021年06月07日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 143、学习超分辨空间的噪声条件流模型
- [ ] Noise Conditional Flow Model for Learning the Super-Resolution Space 
时间：2021年06月06日                         第一作者：Younggeun Kim                       [链接](https://arxiv.org/abs/2106.04428).                     
## 摘要：从根本上说，超分辨率是一个病态的问题，因为低分辨率图像可以从许多高分辨率图像。最近对超分辨率的研究不能产生多样化的超分辨率图像。尽管SRFlow试图通过预测给定低分辨率图像的多幅高分辨率图像来解释超分辨率的不适定性，但仍有提高多样性和视觉质量的空间。本文提出了噪声条件流超分辨率模型NCSR，通过噪声条件层提高图像的视觉质量和多样性。为了了解更多不同的数据分布，我们在训练数据中加入噪声。然而，低质量的图像是由于添加噪声造成的。我们提出了噪声条件层来克服这一现象。噪声条件层使得我们的模型生成的图像更加多样化，视觉质量也更高。此外，我们还证明了该层可以克服数据分布不匹配的问题，这是在规范化流模型时出现的一个问题。基于这些优点，NCSR在多样性和视觉质量方面优于基线，并且比基于GAN的传统模型获得更好的视觉质量。在NTIRE 2021挑战赛中，我们也获得了优异的成绩。
<details>	<summary>英文摘要</summary>	Fundamentally, super-resolution is ill-posed problem because a low-resolution image can be obtained from many high-resolution images. Recent studies for super-resolution cannot create diverse super-resolution images. Although SRFlow tried to account for ill-posed nature of the super-resolution by predicting multiple high-resolution images given a low-resolution image, there is room to improve the diversity and visual quality. In this paper, we propose Noise Conditional flow model for Super-Resolution, NCSR, which increases the visual quality and diversity of images through noise conditional layer. To learn more diverse data distribution, we add noise to training data. However, low-quality images are resulted from adding noise. We propose the noise conditional layer to overcome this phenomenon. The noise conditional layer makes our model generate more diverse images with higher visual quality than other works. Furthermore, we show that this layer can overcome data distribution mismatch, a problem that arises in normalizing flow models. With these benefits, NCSR outperforms baseline in diversity and visual quality and achieves better visual quality than traditional GAN-based models. We also get outperformed scores at NTIRE 2021 challenge. </details>
<details>	<summary>注释</summary>	Final CVPR2021 workshop version </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 142、基于参考的超分辨率图像匹配加速与空间自适应
- [ ] MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution 
时间：2021年06月04日                         第一作者：Liying Lu                       [链接](https://arxiv.org/abs/2106.02299).                     
## 摘要：基于参考的图像超分辨率（RefSR）在利用外部参考图像（Ref）恢复高频细节方面取得了成功。在这个任务中，纹理细节从参考图像传输到低分辨率（LR）图像根据其点或面片的对应关系。因此，高质量的匹配是至关重要的。它还要求计算效率高。此外，现有的RefSR方法往往忽略了LR图像和Ref图像在分布上可能存在的较大差异，影响了信息利用的有效性。在本文中，我们提出了RefSR的MASA网络，其中设计了两个新的模块来解决这些问题。该匹配与提取模块通过粗到细的匹配方案显著降低了计算量。空间自适应模块学习LR和Ref图像之间的分布差异，并以空间自适应的方式将Ref特征的分布重新映射到LR特征的分布。该方案使得网络对不同的参考图像具有很强的鲁棒性。大量的定量和定性实验验证了该模型的有效性。
<details>	<summary>英文摘要</summary>	Reference-based image super-resolution (RefSR) has shown promising success in recovering high-frequency details by utilizing an external reference image (Ref). In this task, texture details are transferred from the Ref image to the low-resolution (LR) image according to their point- or patch-wise correspondence. Therefore, high-quality correspondence matching is critical. It is also desired to be computationally efficient. Besides, existing RefSR methods tend to ignore the potential large disparity in distributions between the LR and Ref images, which hurts the effectiveness of the information utilization. In this paper, we propose the MASA network for RefSR, where two novel modules are designed to address these problems. The proposed Match & Extraction Module significantly reduces the computational cost by a coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module learns the difference of distribution between the LR and Ref images, and remaps the distribution of Ref features to that of LR features in a spatially adaptive way. This scheme makes the network robust to handle different reference images. Extensive quantitative and qualitative experiments validate the effectiveness of our proposed model. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 141、SOUP-GAN：基于生成对抗网络的超分辨率MRI
- [ ] SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks 
时间：2021年06月04日                         第一作者：Kuan Zhang                       [链接](https://arxiv.org/abs/2106.02599).                     
## 摘要：在临床和科研应用中，对高分辨率医学图像的需求越来越大。为了获得更好的患者舒适度、更低的检查成本、更低的剂量和更少的运动诱发伪影，图像质量不可避免地要与采集时间进行权衡。对于许多基于图像的任务，通常使用增加垂直平面上的视分辨率来生成多平面重新格式化或三维图像。单图像超分辨率（SR）是一种很有前途的基于无监督学习的HR图像提供技术，可以提高2D图像的分辨率，但是关于3D SR的报道很少。此外，文献中提出的感知损失比使用像素损失函数更好地捕捉文本细节和边缘，通过比较预先训练的二维网络（如VGG）在高维特征空间中的语义距离。然而，目前尚不清楚如何将其推广到三维医学图像中，随之而来的影响仍不清楚。在本文中，我们提出了一个称为SOUP-GAN的框架：使用感知调谐生成对抗网络（GAN）优化的超分辨率，以产生具有抗混叠和去模糊功能的更薄切片（例如，Z平面上的高分辨率）医学图像。通过定性和定量的比较，该方法优于其它常规的分辨率增强方法和以往的SR方法。具体地说，我们检验了该模型在各种SR比值和成像模式下的泛化。通过解决这些限制，我们的模型显示了作为一种新的三维SR插值技术的前景，在临床和研究环境中提供了潜在的应用。
<details>	<summary>英文摘要</summary>	There is a growing demand for high-resolution (HR) medical images in both the clinical and research applications. Image quality is inevitably traded off with the acquisition time for better patient comfort, lower examination costs, dose, and fewer motion-induced artifacts. For many image-based tasks, increasing the apparent resolution in the perpendicular plane to produce multi-planar reformats or 3D images is commonly used. Single image super-resolution (SR) is a promising technique to provide HR images based on unsupervised learning to increase resolution of a 2D image, but there are few reports on 3D SR. Further, perceptual loss is proposed in the literature to better capture the textual details and edges than using pixel-wise loss functions, by comparing the semantic distances in the high-dimensional feature space of a pre-trained 2D network (e.g., VGG). However, it is not clear how one should generalize it to 3D medical images, and the attendant implications are still unclear. In this paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using Perceptual-tuned Generative Adversarial Network (GAN), in order to produce thinner slice (e.g., high resolution in the 'Z' plane) medical images with anti-aliasing and deblurring. The proposed method outperforms other conventional resolution-enhancement methods and previous SR work on medical images upon both qualitative and quantitative comparisons. Specifically, we examine the model in terms of its generalization for various SR ratios and imaging modalities. By addressing those limitations, our model shows promise as a novel 3D SR interpolation technique, providing potential applications in both clinical and research settings. </details>
<details>	<summary>注释</summary>	10 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 140、基于C2匹配的鲁棒参考超分辨方法
- [ ] Robust Reference-based Super-Resolution via C2-Matching 
时间：2021年06月03日                         第一作者：Yuming Jiang                       [链接](https://arxiv.org/abs/2106.01863).                     
## 摘要：基于参考的超分辨率（Ref-SR）通过引入额外的高分辨率（HR）参考图像来增强低分辨率（LR）输入图像是一种很有前途的方法。现有的Ref-SR方法大多依靠隐式对应匹配从参考图像中借用HR纹理来补偿输入图像中的信息丢失。然而，由于输入图像和参考图像之间存在两个间隙：变换间隙（例如缩放和旋转）和分辨率间隙（例如HR和LR），因此执行局部传输是困难的。为了应对这些挑战，我们在这项工作中提出了C2匹配，它产生了显式的鲁棒匹配交叉变换和分辨率。1） 对于变换间隙，我们提出了一种对比对应网络，该网络利用输入图像的增广视图学习变换鲁棒对应。2） 对于分辨率差距，我们采用了师生相关提取的方法，从比较容易的HR-HR匹配中提取知识，指导比较模糊的LR-HR匹配。3） 最后，我们设计了一个动态聚合模块来解决潜在的失调问题。此外，为了真实地评估Ref-SR在现实环境下的性能，我们模拟实际使用场景，提供了web引用SR（WR-SR）数据集。大量的实验表明，我们提出的C2匹配明显优于国家的艺术超过1dB的标准CUFED5基准。值得注意的是，它在WR-SR数据集上具有很强的通用性，并且在大规模变换和旋转变换中具有很强的鲁棒性。
<details>	<summary>英文摘要</summary>	Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g. scale and rotation) and the resolution gap (e.g. HR and LR). To tackle these challenges, we propose C2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C2-Matching significantly outperforms state of the arts by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation transformations. </details>
<details>	<summary>注释</summary>	To appear in CVPR2021. The source code is available at https://github.com/yumingj/C2-Matching </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 139、互增强立体图像超分辨率和视差估计的反馈网络
- [ ] Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation 
时间：2021年06月02日                         第一作者：Qinyan Dai                       [链接](https://arxiv.org/abs/2106.00985).                     
## 摘要：在立体背景下，图像超分辨率（SR）和视差估计问题是相互关联的，每个问题的结果可以帮助解决另一个问题。有效地利用不同视图之间的对应关系有利于SR性能的提高，而细节更丰富的高分辨率特征有利于对应关系的估计。基于这一动机，我们提出了一种立体图像超分辨率和视差估计反馈网络（ssrdefnet），它在统一的框架下同时处理立体图像的超分辨率和视差估计，并将它们相互作用以进一步提高它们的性能。具体而言，SSRDE-FNet由两个用于左视图和右视图的双重递归子网络组成。除了在低分辨率（LR）空间中利用交叉视图信息外，还利用SR过程产生的HR表示来进行更高精度的HR视差估计，通过该方法可以聚集HR特征以产生更精细的SR结果。然后，提出的HR视差信息反馈（HRDIF）机制将HR视差所携带的信息反馈给前一层，进一步细化SR图像重建。大量实验证明了SSRDE-FNet的有效性和先进性。
<details>	<summary>英文摘要</summary>	Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 138、有效感知图像超分辨率的傅里叶空间损失
- [ ] Fourier Space Losses for Efficient Perceptual Image Super-Resolution 
时间：2021年06月01日                         第一作者：Dario Fuoli                       [链接](https://arxiv.org/abs/2106.00783).                     
## 摘要：许多超分辨率（SR）模型只针对高性能进行优化，因此由于模型复杂度大而缺乏效率。由于大模型在实际应用中往往不实用，我们研究并提出了新的损失函数，以使SR从更有效的模型中获得更高的感知质量。一个给定的低复杂度发电网络的代表功率只有通过对最优参数集的有力指导才能得到充分利用。我们证明了仅应用我们提出的损失函数就可以提高最近引入的高效发电机结构的性能。特别地，我们使用傅立叶空间监督损失来改进从地面真实图像中恢复丢失的高频（HF）内容，并设计了一种直接在傅立叶域工作的鉴别器结构，以更好地匹配目标HF分布。我们发现，我们的损失直接强调在傅立叶空间的频率显着提高了感知图像质量，同时保持了较高的恢复质量相比，以前提出的损失函数的任务。由于两种表示在训练期间提供互补信息，因此通过利用空间域和频域损失的组合来进一步改进性能。除此之外，经过训练的生成器与最新的感知SR方法RankSRGAN和SRFlow相比，分别快2.4倍和48倍。
<details>	<summary>英文摘要</summary>	Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4x and 48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 137、CTSpine1K：一个用于ct脊柱分割的大规模数据集
- [ ] CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography 
时间：2021年05月31日                         第一作者：Yang Deng                       [链接](https://arxiv.org/abs/2105.14711).                     
## 摘要：脊柱相关疾病发病率高，造成巨大的社会成本负担。脊柱成像是无创可视化和评估脊柱病理学的重要工具。CT图像中椎体的分割是脊柱疾病临床诊断和手术计划的定量医学图像分析的基础。目前公开获得的脊柱注释数据集规模较小。由于缺乏大规模的带注释脊柱图像数据集，目前主流的基于深度学习的数据驱动分割方法受到很大的限制。在本文中，我们介绍了一个大型脊柱CT数据集CTSpine1K，该数据集来自多个来源，用于椎体分割，包含1005个CT体积，超过11100个标记的椎体属于不同的脊柱状况。基于此数据集，我们进行了多个脊椎分割实验来建立第一个基准。我们相信，这个大规模的数据集将有助于进一步研究许多与脊柱相关的图像分析任务，包括但不限于椎骨分割、标记、双平面x线片的三维脊柱重建、图像超分辨率和增强。
<details>	<summary>英文摘要</summary>	Spine-related diseases have high morbidity and cause a huge burden of social cost. Spine imaging is an essential tool for noninvasively visualizing and assessing spinal pathology. Segmenting vertebrae in computed tomography (CT) images is the basis of quantitative medical image analysis for clinical diagnosis and surgery planning of spine diseases. Current publicly available annotated datasets on spinal vertebrae are small in size. Due to the lack of a large-scale annotated spine image dataset, the mainstream deep learning-based segmentation methods, which are data-driven, are heavily restricted. In this paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated from multiple sources for vertebra segmentation, which contains 1,005 CT volumes with over 11,100 labeled vertebrae belonging to different spinal conditions. Based on this dataset, we conduct several spinal vertebrae segmentation experiments to set the first benchmark. We believe that this large-scale dataset will facilitate further research in many spine-related image analysis tasks, including but not limited to vertebrae segmentation, labeling, 3D spine reconstruction from biplanar radiographs, image super-resolution, and enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 136、SNIPS：随机求解含噪反问题
- [ ] SNIPS: Solving Noisy Inverse Problems Stochastically 
时间：2021年05月31日                         第一作者：Bahjat Kawar                       [链接](https://arxiv.org/abs/2105.14951).                     
## 摘要：在这项工作中，我们介绍了一种新的随机算法称为SNIPS，它从任何线性逆问题的后验分布中提取样本，假设观测值被加性高斯白噪声污染。我们的解决方案结合了朗之万动力学和牛顿法的思想，并利用预先训练的最小均方误差（MMSE）高斯去噪器。所提出的方法依赖于后验分数函数的复杂推导，该后验分数函数包括退化算子的奇异值分解（SVD），以便获得所需采样的易于处理的迭代算法。由于其随机性，该算法可以为同一噪声观测产生多个高感知质量的样本。我们证明了所提出的模式的能力，图像去模糊，超分辨率，压缩传感。我们发现，所产生的样本是尖锐的，详细的，并与给定的测量一致，其多样性暴露了所解决的反问题固有的不确定性。
<details>	<summary>英文摘要</summary>	In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 135、光谱之外：通过再合成检测假货
- [ ] Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis 
时间：2021年05月29日                         第一作者：Yang He                        [链接](https://arxiv.org/abs/2105.14376).                     
## 摘要：在过去的几年里，深度生成模型的快速发展已经导致了高度真实的媒体，称为深度赝品，通常从真实的眼睛到人类的眼睛是无法区分的。这些进步使得评估视觉数据的真实性变得越来越困难，并对视觉内容的可信性构成了错误信息的威胁。尽管最近的工作已经显示出这种深度假货的强检测精度，但成功在很大程度上依赖于识别生成图像中的频率伪影，这将不会产生一种可持续的检测方法，因为生成模型继续发展并缩小与真实图像的差距。为了克服这一问题，我们提出了一种新的伪检测方法，该方法通过对检测图像的重新合成和提取视觉线索进行检测。再合成过程是灵活的，允许我们纳入一系列的视觉任务-我们采用超分辨率，去噪和彩色作为再合成。我们在CelebA HQ、FFHQ和LSUN数据集上的多个生成器的各种检测场景中展示了改进的有效性、交叉通用性和抗干扰的鲁棒性。源代码位于https://github.com/SSAW14/BeyondtheSpectrum.
<details>	<summary>英文摘要</summary>	The rapid advances in deep generative models over the past years have led to highly {realistic media, known as deepfakes,} that are commonly indistinguishable from real to human eyes. These advances make assessing the authenticity of visual data increasingly difficult and pose a misinformation threat to the trustworthiness of visual content in general. Although recent work has shown strong detection accuracy of such deepfakes, the success largely relies on identifying frequency artifacts in the generated images, which will not yield a sustainable detection approach as generative models continue evolving and closing the gap to real images. In order to overcome this issue, we propose a novel fake detection that is designed to re-synthesize testing images and extract visual cues for detection. The re-synthesis procedure is flexible, allowing us to incorporate a series of visual tasks - we adopt super-resolution, denoising and colorization as the re-synthesis. We demonstrate the improved effectiveness, cross-GAN generalization, and robustness against perturbations of our approach in a variety of detection scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN datasets. Source code is available at https://github.com/SSAW14/BeyondtheSpectrum. </details>
<details>	<summary>注释</summary>	To appear in IJCAI2021. Source code at https://github.com/SSAW14/BeyondtheSpectrum </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 134、动态时空学习满足静态图像理解的盲运动去模糊超分辨率方法
- [ ] Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding 
时间：2021年05月27日                         第一作者：Wenjia Niu                       [链接](https://arxiv.org/abs/2105.13077).                     
## 摘要：单帧超分辨率（SR）和多帧超分辨率（SR）是超分辨率低分辨率图像的两种方法。单个图像SR通常独立地处理每个图像，但忽略连续帧中隐含的时间信息。多帧SR能够通过捕获运动信息来建模时间相关性。然而，它依赖于在现实世界中并不总是可用的相邻帧。同时，轻微的相机抖动容易导致长距离拍摄低分辨率图像的运动模糊。针对这些问题，提出了一种盲运动去模糊超分辨率网络BMDSRNet，用于从单个静态运动模糊图像中学习动态时空信息。运动模糊图像是相机曝光过程中随时间的积累，而BMDSRNet学习反向过程，并基于精心设计的重建损失函数，使用三个流来学习双向时空信息，以恢复干净的高分辨率图像。大量实验表明，该算法的性能优于现有的图像去模糊和随机共振处理方法。
<details>	<summary>英文摘要</summary>	Single-image super-resolution (SR) and multi-frame SR are two ways to super resolve low-resolution images. Single-Image SR generally handles each image independently, but ignores the temporal information implied in continuing frames. Multi-frame SR is able to model the temporal dependency via capturing motion information. However, it relies on neighbouring frames which are not always available in the real world. Meanwhile, slight camera shake easily causes heavy motion blur on long-distance-shot low-resolution images. To address these problems, a Blind Motion Deblurring Super-Reslution Networks, BMDSRNet, is proposed to learn dynamic spatio-temporal information from single static motion-blurred images. Motion-blurred images are the accumulation over time during the exposure of cameras, while the proposed BMDSRNet learns the reverse process and uses three-streams to learn Bidirectional spatio-temporal information based on well designed reconstruction loss functions to recover clean high-resolution images. Extensive experiments demonstrate that the proposed BMDSRNet outperforms recent state-of-the-art methods, and has the ability to simultaneously deal with image deblurring and SR. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 133、CogView：通过变压器控制文本到图像的生成
- [ ] CogView: Mastering Text-to-Image Generation via Transformers 
时间：2021年05月26日                         第一作者：Ming Ding                       [链接](https://arxiv.org/abs/2105.13290).                     
## 摘要：文本到图像的生成在一般领域一直是一个开放的问题，它需要生成模型和跨模态理解。我们提出了CogView，一个带有VQ-VAE标记器的40亿参数变压器来解决这个问题。我们还展示了各种下游任务的微调策略，例如风格学习、超分辨率、文本图像排序和时装设计，以及稳定预训练的方法，例如消除NaN损失。CogView（zero-shot）在COCO上实现了一种新的最先进的FID，优于以前基于GAN的模型和最近的类似工作DALL-E。
<details>	<summary>英文摘要</summary>	Text-to-Image generation in the general domain has long been an open problem, which requires both generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 132、低分辨率信息也很重要：学习多分辨率表示以重新识别人
- [ ] Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification 
时间：2021年05月26日                         第一作者：Guoqing Zhang                       [链接](https://arxiv.org/abs/2105.12684).                     
## 摘要：人员再识别（re-ID）是视频监控和取证领域的一项重要任务，其目的是对非重叠摄像机拍摄到的人员图像进行匹配。在不受约束的场景中，人物图像通常会遇到分辨率不匹配的问题，即\emph{Cross resolution person Re ID}。为了克服这一问题，现有的大多数方法都是通过超分辨率（SR）将低分辨率（LR）图像恢复到高分辨率（HR）。然而，它们只关注于HR特征的提取，而忽略了原始LR图像的有效信息。在这项工作中，我们探讨了分辨率对特征提取的影响，并提出了一种新的交叉分辨率人物识别方法，称为多分辨率表示法。该方法由分辨率重建网络（RRN）和双特征融合网络（DFFN）组成。RRN使用一个输入图像构造一个HR版本和一个LR版本，其中包含一个编码器和两个解码器，而DFFN采用双分支结构从多分辨率图像生成人物表示。在五个基准上的综合实验验证了所提出的MRJL方法相对于现有方法的优越性。
<details>	<summary>英文摘要</summary>	As a prevailing task in video surveillance and forensics field, person re-identification (re-ID) aims to match person images captured from non-overlapped cameras. In unconstrained scenarios, person images often suffer from the resolution mismatch problem, i.e., \emph{Cross-Resolution Person Re-ID}. To overcome this problem, most existing methods restore low resolution (LR) images to high resolution (HR) by super-resolution (SR). However, they only focus on the HR feature extraction and ignore the valid information from original LR images. In this work, we explore the influence of resolutions on feature extraction and develop a novel method for cross-resolution person re-ID called \emph{\textbf{M}ulti-Resolution \textbf{R}epresentations \textbf{J}oint \textbf{L}earning} (\textbf{MRJL}). Our method consists of a Resolution Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN uses an input image to construct a HR version and a LR version with an encoder and two decoders, while the DFFN adopts a dual-branch structure to generate person representations from multi-resolution images. Comprehensive experiments on five benchmarks verify the superiority of the proposed MRJL over the relevent state-of-the-art methods. </details>
<details>	<summary>注释</summary>	accepted by IJCAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 131、多时相图像超分辨率的排列不变性和不确定性
- [ ] Permutation invariance and uncertainty in multitemporal image super-resolution 
时间：2021年05月26日                         第一作者：Diego Valsesia                       [链接](https://arxiv.org/abs/2105.12409).                     
## 摘要：最近的进展表明，从低分辨率图像的多时相采集开始，深层神经网络在超分辨率遥感图像方面是多么的有效。然而，现有的模型忽略了时间排列的问题，即输入图像的时间顺序不携带任何与超分辨率任务相关的信息，并且导致这种模型对于可用于训练的、通常稀缺的地面真实数据是低效的。因此，模型不应该学习依赖于时间顺序的特征提取器。在本文中，我们展示了如何建立一个对时间排列完全不变的模型来显著地提高性能和数据效率。此外，我们还研究了如何量化超分辨率图像的不确定性，以便最终用户了解产品的局部质量。我们展示了不确定性如何与序列中的时间变化相关，以及量化它如何进一步提高模型性能。在Proba-V挑战数据集上的实验表明，在不需要自我感知的情况下，比现有技术有了显著的改进，并且提高了数据效率，仅用25%的训练数据就达到了挑战胜利者的表现。
<details>	<summary>英文摘要</summary>	Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of low-resolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data. </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 130、基于对比自蒸馏的紧凑型单幅图像超分辨率研究
- [ ] Towards Compact Single Image Super-Resolution via Contrastive Self-distillation 
时间：2021年05月25日                         第一作者：Yanbo Wang                       [链接](https://arxiv.org/abs/2105.11683).                     
## 摘要：卷积神经网络（CNNs）是一种非常成功的超分辨率（SR）网络，但其结构复杂，内存开销大，计算开销大，极大地限制了其在资源有限的设备上的实际应用。在本文中，我们提出了一个新的对比自蒸馏（CSD）框架来同时压缩和加速各种现成的SR模型。特别地，信道分裂超分辨率网络可以首先从目标教师网络构造为紧凑的学生网络。然后，我们提出了一种新的对比度损失方法，通过显式知识转移来提高SR图像和PSNR/SSIM的质量。大量实验表明，该方案有效地压缩和加速了EDSR、RCAN和CARN等标准SR模型。代码位于https://github.com/Booooooooooo/CSD.
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) are highly successful for super-resolution (SR) but often require sophisticated architectures with heavy memory cost and computational overhead, significantly restricts their practical deployments on resource-limited devices. In this paper, we proposed a novel contrastive self-distillation (CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models. In particular, a channel-splitting super-resolution network can first be constructed from a target teacher network as a compact student network. Then, we propose a novel contrastive loss to improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer. Extensive experiments demonstrate that the proposed CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN. Code is available at https://github.com/Booooooooooo/CSD. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI-21 </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 129、高频感知图像增强
- [ ] High-Frequency aware Perceptual Image Enhancement 
时间：2021年05月25日                         第一作者：Hyungmin Roh                        [链接](https://arxiv.org/abs/2105.11711).                     
## 摘要：本文介绍了一种适用于多尺度分析的深度神经网络，并提出了一种有效的模型不可知方法，帮助该网络从高频域提取信息，重建出更清晰的图像。该模型可应用于多尺度图像增强问题，包括去噪、去模糊和单图像超分辨率。在SIDD、Flickr2K、DIV2K和REDS数据集上的实验表明，我们的方法在每个任务上都达到了最先进的性能。此外，我们还证明了我们的模型可以克服现有面向PSNR的方法中普遍存在的过度平滑问题，并通过对抗性训练生成更自然的高分辨率图像。
<details>	<summary>英文摘要</summary>	In this paper, we introduce a novel deep neural network suitable for multi-scale analysis and propose efficient model-agnostic methods that help the network extract information from high-frequency domains to reconstruct clearer images. Our model can be applied to multi-scale image enhancement problems including denoising, deblurring and single image super-resolution. Experiments on SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves state-of-the-art performance on each task. Furthermore, we show that our model can overcome the over-smoothing problem commonly observed in existing PSNR-oriented methods and generate more natural high-resolution images by applying adversarial training. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 128、基于快速RCNN检测模型的无人机RGB图像玉米密度估计：空间分辨率的影响
- [ ] Estimates of maize plant density from UAV RGB images using Faster-RCNN detection model: impact of the spatial resolution 
时间：2021年05月25日                         第一作者：Kaaviya Velumani                       [链接](https://arxiv.org/abs/2105.11857).                     
## 摘要：在给定的环境条件和管理措施下，早期植株密度是决定基因型命运的重要性状。使用从无人机上拍摄的RGB图像可以取代传统的现场视觉计数，从而提高吞吐量、精确度和工厂定位。然而，需要高分辨率（HR）图像来检测早期存在的小植物。研究了图像地面采样距离（GSD）对快速RCNN在3-5叶期玉米植株检测性能的影响。在HR（GSD=0.3cm）收集的6个对比部位的数据用于模型训练。另外两个具有高分辨率和低分辨率（GSD=0.6cm）图像的位置用于模型评估。结果表明，当本地HR图像同时用于训练和验证时，更快的RCNN可以获得非常好的植物检测和计数性能（rRMSE=0.08）。同样地，当模型在通过对本地训练HR图像下采样获得的合成低分辨率（LR）图像上训练并应用于合成LR验证图像时，观察到良好的性能（rRMSE=0.11）。相反，当模型在给定的空间分辨率上训练并应用到另一个空间分辨率上时，会获得较差的性能。混合HR和LR图像的训练允许在本地HR（rRMSE=0.06）和合成LR（rRMSE=0.10）图像上获得非常好的性能。然而，在本地LR图像（rRMSE=0.48）上仍然观察到非常低的性能，主要是由于本地LR图像的质量差。最后，提出了一种基于生成对抗网络（generativediscountarial network，GAN）的改进的超分辨率方法，该方法引入了来自本地HR图像的额外纹理信息，并应用于本地LR验证图像。结果表明，与双三次上采样方法相比，该方法有一些显著的改进（rRMSE=0.22）。
<details>	<summary>英文摘要</summary>	Early-stage plant density is an essential trait that determines the fate of a genotype under given environmental conditions and management practices. The use of RGB images taken from UAVs may replace traditional visual counting in fields with improved throughput, accuracy and access to plant localization. However, high-resolution (HR) images are required to detect small plants present at early stages. This study explores the impact of image ground sampling distance (GSD) on the performances of maize plant detection at 3-5 leaves stage using Faster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used for model training. Two additional sites with images acquired both at high and low (GSD=0.6cm) resolution were used for model evaluation. Results show that Faster-RCNN achieved very good plant detection and counting (rRMSE=0.08) performances when native HR images are used both for training and validation. Similarly, good performances were observed (rRMSE=0.11) when the model is trained over synthetic low-resolution (LR) images obtained by down-sampling the native training HR images, and applied to the synthetic LR validation images. Conversely, poor performances are obtained when the model is trained on a given spatial resolution and applied to another spatial resolution. Training on a mix of HR and LR images allows to get very good performances on the native HR (rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low performances are still observed over the native LR images (rRMSE=0.48), mainly due to the poor quality of the native LR images. Finally, an advanced super-resolution method based on GAN (generative adversarial network) that introduces additional textural information derived from the native HR images was applied to the native LR validation images. Results show some significant improvement (rRMSE=0.22) compared to bicubic up-sampling approach. </details>
<details>	<summary>注释</summary>	16 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 127、野外非配对深度增强与超分辨率研究进展
- [ ] Towards Unpaired Depth Enhancement and Super-Resolution in the Wild 
时间：2021年05月25日                         第一作者：Aleks                       [链接](https://arxiv.org/abs/2105.12038).                     
## 摘要：商品传感器获取的深度图通常质量和分辨率较低；这些地图需要增强才能在许多应用中使用。最先进的深度图超分辨率数据驱动方法依赖于同一场景的低分辨率和高分辨率深度图的注册对。获取真实世界的成对数据需要专门的设置。另一种替代方法是，通过子采样、添加噪声和其他人工退化方法从高分辨率地图生成低分辨率地图，这种方法不能完全捕捉现实世界中低分辨率图像的特征。因此，在这种人工配对数据上训练的有监督学习方法在现实世界的低分辨率输入上可能表现不好。我们考虑一种基于未配对数据学习的深度图增强方法。虽然已经提出了许多非配对图像到图像的转换技术，但大多数技术并不直接适用于深度图。我们提出了一种同时进行深度增强和超分辨率的非配对学习方法，该方法基于可学习退化模型和表面法线估计作为特征来生成更精确的深度图。我们证明了我们的方法优于现有的非配对方法，并在我们开发的新的非配对学习基准上与配对方法相当。
<details>	<summary>英文摘要</summary>	Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth map enhancement based on learning from unpaired data. While many techniques for unpaired image-to-image translation have been proposed, most are not directly applicable to depth maps. We propose an unpaired learning method for simultaneous depth enhancement and super-resolution, which is based on a learnable degradation model and surface normal estimates as features to produce more accurate depth maps. We demonstrate that our method outperforms existing unpaired methods and performs on par with paired methods on a new benchmark for unpaired learning that we developed. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 126、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月23日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 125、MIASSR：一种医学图像任意尺度超分辨率处理方法
- [ ] MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution 
时间：2021年05月22日                         第一作者：Jin Zhu                       [链接](https://arxiv.org/abs/2105.10738).                     
## 摘要：单幅图像超分辨率（Single-image super-resolution，SISR）的目标是从一幅低分辨率图像中获得高分辨率的输出。目前，基于深度学习的SISR方法在医学图像处理中得到了广泛的讨论，因为它们可以在不需要额外扫描的情况下获得高质量、高空间分辨率的图像。然而，大多数现有的方法是针对特定规模的SR任务而设计的，无法在放大规模上推广。本文提出了一种医学图像任意尺度超分辨率（MIASSR）方法，该方法将元学习与生成对抗网络（GANs）相结合，实现了（1，4）中任意尺度的医学图像超分辨率。与单模态磁共振（MR）脑图像（OASIS-brains）和多模态MR脑图像（BraTS）上最先进的SISR算法相比，MIASSR以最小的模型尺寸获得了相当的保真度性能和最佳的感知质量。我们还利用转移学习使MIASSR能够处理新医学模式的SR任务，如心脏MR图像（ACDC）和胸部CT图像（COVID-CT）。我们工作的源代码也是公开的。因此，MIASSR有可能成为临床图像分析任务（如重建、图像质量增强和分割）中一个新的基础前/后处理步骤。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalise over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation. </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 124、用卷积鉴别器组合变压器发生器
- [ ] Combining Transformer Generators with Convolutional Discriminators 
时间：2021年05月21日                         第一作者：Ricard Durall                       [链接](https://arxiv.org/abs/2105.10189).                     
## 摘要：变压器模型最近引起了计算机视觉研究人员的极大兴趣，并已成功地应用于一些传统的卷积神经网络解决的问题。同时，在过去的几年中，使用生成性对抗网络（GANs）的图像合成有了很大的改进。最近提出的TransGAN是第一个只使用基于变压器的结构的GAN，并且与卷积GAN相比取得了竞争性的结果。然而，由于变压器是数据饥渴的架构，TransGAN需要数据扩充、训练期间的辅助超分辨率任务以及引导自我注意机制之前的掩蔽。在本文中，我们研究了基于变压器的发生器和卷积鉴别器的组合，成功地消除了上述设计选择的需要。我们通过对著名的CNN鉴别器进行基准测试来评估我们的方法，烧蚀基于变压器的发电机的大小，并表明将两种结构元素结合到一个混合模型中可以得到更好的结果。此外，我们研究了生成图像的频谱特性，并观察到我们的模型保留了基于注意的生成器的优点。
<details>	<summary>英文摘要</summary>	Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator. </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 123、线性组合像素自适应回归网络用于单图像超分辨率及更高分辨率
- [ ] LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond 
时间：2021年05月21日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2105.10422).                     
## 摘要：单图像超分辨率（SISR）是将低分辨率（LR）图像上采样为高分辨率（HR）图像的一个基本问题。在过去的几年里，在深度学习方法的推动下取得了令人瞩目的进步。然而，现有方法面临的一个关键挑战是如何找到一个深度模型复杂性和由此产生的SISR质量的最佳点。本文提出了一种线性组合像素自适应回归网络（LAPAR），将LR到HR的直接映射学习转化为多个预定义滤波器基字典上的线性系数回归任务。这样的参数表示使得我们的模型具有高度的轻量级和易于优化，同时在SISR基准上获得最先进的结果。此外，基于同样的思想，将LAPAR算法扩展到图像去噪和JPEG图像去块等恢复任务中，同样获得了很好的性能。代码可在https://github.com/dvlab-research/Simple-SR.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) deals with a fundamental problem of upsampling a low-resolution (LR) image to its high-resolution (HR) version. Last few years have witnessed impressive progress propelled by deep learning methods. However, one critical challenge faced by existing methods is to strike a sweet spot of deep model complexity and resulting SISR quality. This paper addresses this pain point by proposing a linearly-assembled pixel-adaptive regression network (LAPAR), which casts the direct LR to HR mapping learning into a linear coefficient regression task over a dictionary of multiple predefined filter bases. Such a parametric representation renders our model highly lightweight and easy to optimize while achieving state-of-the-art results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended to tackle other restoration tasks, e.g., image denoising and JPEG image deblocking, and again, yields strong performance. The code is available at https://github.com/dvlab-research/Simple-SR. </details>
<details>	<summary>注释</summary>	NeurIPS2020 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 122、用于图像去模糊和超分辨率的特征空间图卷积网络
- [ ] Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution 
时间：2021年05月21日                         第一作者：Boyan Xu                        [链接](https://arxiv.org/abs/2105.10465).                     
## 摘要：图卷积网络（GCNs）在处理非欧几里德结构的数据方面取得了巨大的成功。它们的成功直接归功于将图形结构有效地与社交媒体和知识数据库中的数据相匹配。对于图像处理应用，图形结构和gcn的使用还没有得到充分的探讨。本文提出了一种新的加图卷积的编解码网络，通过将特征映射转换为预生成图的顶点来综合构造图结构数据。通过这样做，我们莫名其妙地应用图拉普拉斯正则化的特征地图，使他们更结构化。实验结果表明，该算法能显著提高图像恢复的性能，包括去模糊和超分辨率。我们相信它为更多应用中基于GCN的方法提供了机会。
<details>	<summary>英文摘要</summary>	Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications. </details>
<details>	<summary>注释</summary>	Accepted by IJCNN 2021 (Oral) Journal-ref: International Joint Conference on Neural Networks (IJCNN) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 121、快速图像超分辨率的内容自适应表示学习
- [ ] Content-adaptive Representation Learning for Fast Image Super-resolution 
时间：2021年05月20日                         第一作者：Yukai Shi                       [链接](https://arxiv.org/abs/2105.09645).                     
## 摘要：深度卷积网络在图像恢复和增强中受到了广泛的关注。通常，通过构造更多的卷积块来提高恢复质量。然而，这些方法大多学习一个特定的模型来处理所有的图像，忽略了难度的多样性。换言之，图像中的高频区域在压缩过程中往往丢失更多信息，而低频区域往往丢失更少信息。本文针对图像检索中的效率问题，提出了一种基于分片滚动网络（PRN）的内容自适应恢复算法。与现有的忽略难度多样性的研究不同，本文采用不同阶段的神经网络进行图像恢复。此外，我们还提出了一种滚动策略，使得每一阶段的参数都更加灵活。大量的实验表明，我们的模型不仅显示了显著的加速，而且保持了最先进的性能。
<details>	<summary>英文摘要</summary>	Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance. </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 120、基于锚网的移动图像超分辨率平面网
- [ ] Anchor-based Plain Net for Mobile Image Super-Resolution 
时间：2021年05月20日                         第一作者：Zongcai Du                       [链接](https://arxiv.org/abs/2105.09750).                     
## 摘要：随着实际应用的快速发展，对图像超分辨率（SR）的精度和效率提出了更高的要求。现有的方法虽然取得了显著的成功，但大多需要大量的计算资源和大量的RAM，不能很好地应用于移动设备。本文旨在设计高效的8位量化体系结构，并将其部署到移动设备上。首先，我们通过分解轻量级SR架构来进行关于元节点延迟的实验，这决定了我们可以利用的可移植操作。在此基础上，深入探讨了什么样的体系结构有利于8位量化，并提出了基于锚的平面网（ABPN）。最后，我们采用量化感知训练策略来进一步提升绩效。该模型在满足实际需要的同时，PSNR性能比8位量化FSRCNN提高了近2dB。代码可在https://github.com/NJU- Jet/SR\移动\量化。
<details>	<summary>英文摘要</summary>	Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization. </details>
<details>	<summary>注释</summary>	accepted by CVPR2021 MAI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 119、XCycles反投影声学超分辨率
- [ ] XCycles Backprojection Acoustic Super-Resolution 
时间：2021年05月19日                         第一作者：Feras Almasri                       [链接](https://arxiv.org/abs/2105.09128).                     
## 摘要：利用深度神经网络（DNNs）进行可见光图像超分辨率（SR）的研究已引起计算机视觉界的广泛关注，并取得了令人瞩目的成果。声成像传感器等非可见光传感器的发展引起了人们的广泛关注，因为它们可以使人们直观地看到可见光谱以外的声波强度。然而，由于获取声学数据的局限性，需要新的方法来提高声学图像的分辨率。目前，还没有为SR问题设计的声学成像数据集。本文提出了一种新的用于声像超分辨率问题的反投影模型体系结构，并与声地图成像VUB-ULB数据集（AMIVU）相结合。数据集提供了不同分辨率的大型模拟和真实捕获图像。与前馈模型方法相比，本文提出的XCycles反投影模型（XCBP）充分利用了每个周期的迭代校正过程，在低分辨率和高分辨率空间中重建编码特征的残差校正。在数据集上对所提出的方法进行了评估，结果表明，与经典的插值算子和最新的前馈模型相比，该方法具有更高的性能。这也大大减少了数据采集过程中产生的次采样误差。
<details>	<summary>英文摘要</summary>	The computer vision community has paid much attention to the development of visible image super-resolution (SR) using deep neural networks (DNNs) and has achieved impressive results. The advancement of non-visible light sensors, such as acoustic imaging sensors, has attracted much attention, as they allow people to visualize the intensity of sound waves beyond the visible spectrum. However, because of the limitations imposed on acquiring acoustic data, new methods for improving the resolution of the acoustic images are necessary. At this time, there is no acoustic imaging dataset designed for the SR problem. This work proposed a novel backprojection model architecture for the acoustic image super-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset (AMIVU). The dataset provides large simulated and real captured images at different resolutions. The proposed XCycles BackProjection model (XCBP), in contrast to the feedforward model approach, fully uses the iterative correction procedure in each cycle to reconstruct the residual error correction for the encoded features in both low- and high-resolution space. The proposed approach was evaluated on the dataset and showed high outperformance compared to the classical interpolation operators and to the recent feedforward state-of-the-art models. It also contributed to a drastically reduced sub-sampling error produced during the data acquisition. </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 118、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年05月19日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
## 摘要：超分辨率（SR）对提高磁共振成像（MRI）的图像质量起着至关重要的作用。磁共振成像产生多对比度图像，可以提供软组织的清晰显示。然而，目前的超分辨率方法仅采用单一对比度，或者采用简单的多对比度融合机制，忽略了不同对比度之间丰富的关系，这对提高磁共振成像的分辨率是有价值的，它明确地建模了不同阶段的多对比度图像之间的依赖关系以指导图像SR。特别地，我们的MINet首先从多个卷积阶段学习不同对比度图像的分层特征表示。随后，我们引入了一个多级集成模块来挖掘多对比度图像表示之间的综合关系。具体来说，该模块将每个表示与所有其他特征相匹配，这些特征根据其相似性进行集成，以获得丰富的表示。在fastMRI和真实临床数据集上的大量实验表明：1）我们的MINet在各种指标方面优于最先进的多对比度SR方法；2）我们的多阶段集成模块能够挖掘不同阶段多对比度特征之间的复杂交互作用，从而提高目标图像质量。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) plays a crucial role in improving the image quality of magnetic resonance imaging (MRI). MRI produces multi-contrast images and can provide a clear display of soft tissues. However, current super-resolution methods only employ a single contrast, or use a simple multi-contrast fusion mechanism, ignoring the rich relations among different contrasts, which are valuable for improving SR. In this work, we propose a multi-stage integration network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the dependencies between multi-contrast images at different stages to guide image SR. In particular, our MINet first learns a hierarchical feature representation from multiple convolutional stages for each of different-contrast image. Subsequently, we introduce a multi-stage integration module to mine the comprehensive relations between the representations of the multi-contrast images. Specifically, the module matches each representation with all other features, which are integrated in terms of their similarities to obtain an enriched representation. Extensive experiments on fastMRI and real-world clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art multi-contrast SR methods in terms of various metrics and 2) our multi-stage integration module is able to excavate complex interactions among multi-contrast features at different stages, leading to improved target-image quality. </details>
<details>	<summary>注释</summary>	10 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 117、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年05月19日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 116、道路网序列中小目标检测的改进方法
- [ ] Improved detection of small objects in road network sequences 
时间：2021年05月18日                         第一作者：Iv\'an Garc\'ia                       [链接](https://arxiv.org/abs/2105.08416).                     
## 摘要：当前道路网络中现有的大量IP摄像机为利用捕获的数据、分析视频和检测任何重大事件提供了机会。为此，有必要检测移动的车辆，直到几年前，这项任务一直是使用经典的人工视觉技术来完成的。如今，深度学习网络已经取得了显著的进步。尽管如此，目标检测仍然被认为是计算机视觉领域的主要开放性问题之一。目前的情况是不断发展的，新的模型和技术正在出现，试图改善这一领域。特别地，在检测小目标方面出现了新的问题和缺点，这些小目标主要对应于道路场景中出现的车辆。所有这一切意味着，新的解决方案，试图提高低检出率的小元素是必不可少的。在众多新兴的研究领域中，本文主要研究小目标的检测。特别是，我们的建议旨在从视频监控摄像头捕获的图像中检测车辆。本文提出了一种基于卷积神经网络（CNN）检测的超分辨过程检测小尺度目标的新方法。将神经网络与提高图像分辨率的过程相结合，提高目标检测性能。通过对一组包含不同尺度元素的交通图像的测试，验证了该方法的有效性，表明该方法在多种情况下都取得了良好的效果。
<details>	<summary>英文摘要</summary>	The vast number of existing IP cameras in current road networks is an opportunity to take advantage of the captured data and analyze the video and detect any significant events. For this purpose, it is necessary to detect moving vehicles, a task that was carried out using classical artificial vision techniques until a few years ago. Nowadays, significant improvements have been obtained by deep learning networks. Still, object detection is considered one of the leading open issues within computer vision. The current scenario is constantly evolving, and new models and techniques are appearing trying to improve this field. In particular, new problems and drawbacks appear regarding detecting small objects, which correspond mainly to the vehicles that appear in the road scenes. All this means that new solutions that try to improve the low detection rate of small elements are essential. Among the different emerging research lines, this work focuses on the detection of small objects. In particular, our proposal aims to vehicle detection from images captured by video surveillance cameras. In this work, we propose a new procedure for detecting small-scale objects by applying super-resolution processes based on detections performed by convolutional neural networks \emph{(CNN)}. The neural network is integrated with processes that are in charge of increasing the resolution of the images to improve the object detection performance. This solution has been tested for a set of traffic images containing elements of different scales to test the efficiency according to the detections obtained by the model, thus demonstrating that our proposal achieves good results in a wide range of situations. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 115、超网络在固定触发器计数下的过参数化实现了快速的神经图像增强
- [ ] Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast Neural Image Enhancement 
时间：2021年05月18日                         第一作者：Lorenz K. Muller                       [链接](https://arxiv.org/abs/2105.08470).                     
## 摘要：深度卷积神经网络可以增强小型移动相机传感器拍摄的图像，擅长去噪、去噪和超分辨率等任务。然而，在移动设备上的实际应用中，这些网络往往需要太多的触发器，减少了卷积层的触发器，也减少了其参数计数。鉴于最近的研究发现，过度参数化的神经网络通常是泛化效果最好的网络，这是有问题的。在本文中，我们建议使用超网络来打破固定比例的触发器参数的标准卷积。这使我们能够在苏黎世原始到DSLR（ZRR）数据集上以低于10倍的浮点计数超过SSIM和MS-SSIM中以前最先进的体系结构。在ZRR上，我们进一步观察到在大图像极限下，在固定的FLOP计数下与“双下降”行为一致的泛化曲线。最后，我们证明了同样的技术可以应用于一个现有的网络（VDN），以降低其计算成本，同时保持对智能手机图像去噪数据集（SIDD）的保真度。附录中给出了关键功能的代码。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks can enhance images taken with small mobile camera sensors and excel at tasks like demoisaicing, denoising and super-resolution. However, for practical use on mobile devices these networks often require too many FLOPs and reducing the FLOPs of a convolution layer, also reduces its parameter count. This is problematic in view of the recent finding that heavily over-parameterized neural networks are often the ones that generalize best. In this paper we propose to use HyperNetworks to break the fixed ratio of FLOPs to parameters of standard convolutions. This allows us to exceed previous state-of-the-art architectures in SSIM and MS-SSIM on the Zurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we further observe generalization curves consistent with 'double-descent' behavior at fixed FLOP-count, in the large image limit. Finally we demonstrate the same technique can be applied to an existing network (VDN) to reduce its computational cost while maintaining fidelity on the Smartphone Image Denoising Dataset (SIDD). Code for key functions is given in the appendix. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 114、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年05月18日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 113、深度学习智能手机上的实时视频超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Video Super-Resolution on Smartphones with Deep Learning, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.08826).                     
## 摘要：随着视频通信和流媒体服务的兴起，视频超分辨率已经成为移动领域的一个重要问题。虽然已经有许多解决方案被提出来完成这项任务，但是大多数解决方案的计算成本太高，无法在硬件资源有限的便携式设备上运行。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的视频超分辨率解决方案，可以在移动gpu上实现实时性能。向参与者提供了REDS数据集，并训练他们的模型进行有效的4X视频放大。所有模型的运行时间都在OPPO Find X2智能手机上进行了评估，Snapdragon 865 SoC能够加速Adreno GPU上的浮点网络。所提出的解决方案完全兼容任何移动GPU，可以将视频放大到高清分辨率，分辨率高达80帧/秒，同时显示高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial text overlap with arXiv:2105.07825. substantial text overlap with arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.08630 </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 112、移动NPU上的实时量化图像超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.07825).                     
## 摘要：图像超分辨率是计算机视觉领域的一个热点问题，在移动设备中有着重要的应用。虽然已经有许多解决方案被提出用于这项任务，但它们通常甚至没有针对常见的智能手机AI硬件进行优化，更不用说通常只支持INT8推理的更受约束的智能电视平台了。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的图像超分辨率解决方案，可以在移动或边缘NPU上展示实时性能。为此，参与者被提供了DIV2K数据集和经过训练的量化模型来进行有效的3X图像放大。所有模型的运行时间均在Synaptics VS680智能家居板上进行评估，该智能家居板配有一个能够加速量化神经网络的专用NPU。所提出的解决方案与所有主要的移动人工智能加速器完全兼容，能够在40-60毫秒的时间内重建全高清图像，同时获得高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/ </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 111、用于高保真材料标签传输的无监督超分辨率卫星图像
- [ ] Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer 
时间：2021年05月16日                         第一作者：Arthita Ghosh                       [链接](https://arxiv.org/abs/2105.07322).                     
## 摘要：遥感图像中的城市物质识别是一个高度相关但极具挑战性的问题，尤其是在低分辨率卫星图像上，由于人类注释的获取非常困难。为此，我们提出了一种基于对抗学习的无监督领域自适应方法。我们的目标是从少量的高分辨率数据（源域）中获取信息，并利用这些数据对低分辨率图像（目标域）进行超分辨率处理。这可能有助于语义以及材料标签从带丰富注释的源到目标域的转移。
<details>	<summary>英文摘要</summary>	Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain. </details>
<details>	<summary>注释</summary>	Published in the proceedings of the 2019 IEEE International Geoscience and Remote Sensing Symposium Journal-ref: IGARSS (2019), 5144-5147 DOI: 10.1109/IGARSS.2019.8900639 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 110、图像超分辨率质量评估：结构保真度与统计自然度
- [ ] Image Super-Resolution Quality Assessment: Structural Fidelity Versus Statistical Naturalness 
时间：2021年05月15日                         第一作者：Wei Zhou                       [链接](https://arxiv.org/abs/2105.07139).                     
## 摘要：单图像超分辨率（SISR）算法用低分辨率（LR）重建高分辨率（HR）图像。发展图像质量评价（IQA）方法不仅可以评价和比较SISR算法，而且可以指导其未来的发展。在本文中，我们评估了在结构保真度与统计自然度的二维（2D）空间中SISR生成图像的质量。这使得我们能够观察不同SISR算法在2D空间中的行为。具体地说，SISR方法传统上是为了获得高的结构保真度而设计的，但往往牺牲了统计的自然性，而最近基于生成性对抗网络（GAN）的算法倾向于创建更自然的结果，但在结构保真度上损失很大。此外，这样的2D评估可以容易地与标量质量预测融合。有趣的是，我们发现一个简单的线性组合一个简单的局部结构保真度和一个全球性的统计自然度措施产生令人惊讶的准确预测SISR图像质量时，测试使用公共科目评分SISR图像数据集。建议的SFSN模型的代码可在\url上公开获取{https://github.com/weizhou-geek/SFSN}.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) algorithms reconstruct high-resolution (HR) images with their low-resolution (LR) counterparts. It is desirable to develop image quality assessment (IQA) methods that can not only evaluate and compare SISR algorithms, but also guide their future development. In this paper, we assess the quality of SISR generated images in a two-dimensional (2D) space of structural fidelity versus statistical naturalness. This allows us to observe the behaviors of different SISR algorithms as a tradeoff in the 2D space. Specifically, SISR methods are traditionally designed to achieve high structural fidelity but often sacrifice statistical naturalness, while recent generative adversarial network (GAN) based algorithms tend to create more natural-looking results but lose significantly on structural fidelity. Furthermore, such a 2D evaluation can be easily fused to a scalar quality prediction. Interestingly, we find that a simple linear combination of a straightforward local structural fidelity and a global statistical naturalness measures produce surprisingly accurate predictions of SISR image quality when tested using public subject-rated SISR image datasets. Code of the proposed SFSN model is publicly available at \url{https://github.com/weizhou-geek/SFSN}. </details>
<details>	<summary>注释</summary>	Accepted by QoMEX 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 109、低分辨率扫描病理图像的多尺度超分辨率生成
- [ ] Multi-scale super-resolution generation of low-resolution scanned pathological images 
时间：2021年05月15日                         第一作者：Yanhua Gao (1)                       [链接](https://arxiv.org/abs/2105.07200).                     
## 摘要：数字化病理切片易于存储和管理，便于浏览和传输。然而，由于数字化过程中的高分辨率扫描（例如40倍放大率），每张幻灯片图像的文件大小超过1G字节，最终导致巨大的存储容量和非常缓慢的网络传输。我们设计了一种低分辨率（5X）幻灯片扫描策略，并提出了一种超分辨率方法来恢复诊断时的图像细节。该方法基于多尺度生成对抗网络，依次生成10X、20X和40X三幅高分辨率图像。在三种图像分辨率下比较了生成图像和真实图像的感知损失、发生器损失，并用鉴别器来评价最高分辨率生成图像和真实图像的差异。一个由10万张人体组织病理图像组成的数据集被用来训练和测试网络。生成的图像具有较高的峰值信噪比（PSNR）和结构相似性指数（SSIM）。10X-40X图像的峰值信噪比分别为24.16、22.27和20.44，SSIM分别为0.845、0.680和0.512，优于DBPN、ESPCN、RDN、EDSR和MDSR等超分辨率网络。此外，视觉检测表明，我们的网络生成的高分辨率图像具有足够的诊断细节，色彩再现性好，接近真实图像，而其他五个网络严重模糊，局部变形或遗漏重要细节。而且，基于生成的图像和真实图像的病理诊断没有显著差异。提出的多尺度网络能够生成高分辨率的病理图像，为数字病理学提供了一种低成本的存储（约15MB/5X图像），更快的图像共享方法。
<details>	<summary>英文摘要</summary>	Digital pathology slide is easy to store and manage, convenient to browse and transmit. However, because of the high-resolution scan for example 40 times magnification(40X) during the digitization, the file size of each whole slide image exceeds 1Gigabyte, which eventually leads to huge storage capacity and very slow network transmission. We design a strategy to scan slides with low resolution (5X) and a super-resolution method is proposed to restore the image details when in diagnosis. The method is based on a multi-scale generative adversarial network, which sequentially generate three high-resolution images such as 10X, 20X and 40X. The perceived loss, generator loss of the generated images and real images are compared on three image resolutions, and a discriminator is used to evaluate the difference of highest-resolution generated image and real image. A dataset consisting of 100,000 pathological images from 10 types of human tissues is performed for training and testing the network. The generated images have high peak-signal-to-noise-ratio (PSNR) and structural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16, 22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than other super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR. Moreover, visual inspections show that the generated high-resolution images by our network have enough details for diagnosis, good color reproduction and close to real images, while other five networks are severely blurred, local deformation or miss important details. Moreover, no significant differences can be found on pathological diagnosis based on the generated and real images. The proposed multi-scale network can generate good high-resolution pathological images, and will provide a low-cost storage (about 15MB/image on 5X), faster image sharing method for digital pathology. </details>
<details>	<summary>注释</summary>	27 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 108、盲超分辨的端到端交替优化
- [ ] End-to-end Alternating Optimization for Blind Super Resolution 
时间：2021年05月14日                         第一作者：Zhengxiong Luo                       [链接](https://arxiv.org/abs/2105.06878).                     
## 摘要：以前的方法将盲超分辨率（SR）问题分解为两个连续的步骤：\textit{i}从给定的低分辨率（LR）图像中估计模糊核和\textit{ii}基于估计核恢复SR图像。这两个步骤的解决方案涉及两个独立训练的模型，这两个模型可能不太兼容。第一步的微小估计误差可能导致第二步的性能严重下降。另一方面，第一步只能利用有限的LR图像信息，难以预测出高精度的模糊核。针对这两个问题，本文采用交替优化算法，在单一模型下估计模糊核，恢复SR图像，而不是单独考虑这两个步骤。具体来说，我们设计了两个卷积神经模块，即\textit{Restorer}和\textit{Estimator}\textit{Restorer}基于预测核对SR图像进行恢复，而\textit{Estimator}借助恢复的SR图像对模糊核进行估计。我们反复交替这两个模块，展开这个过程，形成一个端到端的培训网络。通过这种方式，\textit{Estimator}利用来自LR和SR图像的信息，这使得模糊核的估计更容易。更重要的是，用估计核来训练恢复核，而不是用真核来训练恢复核，因此恢复核对估计核的估计误差有更大的容忍度。在合成数据集和真实图像上的大量实验表明，我们的模型在很大程度上优于最先进的方法，并以更高的速度产生更直观的结果。源代码位于\url{https://github.com/greatlog/DAN.git}.
<details>	<summary>英文摘要</summary>	Previous methods decompose the blind super-resolution (SR) problem into two sequential steps: \textit{i}) estimating the blur kernel from given low-resolution (LR) image and \textit{ii}) restoring the SR image based on the estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. A small estimation error of the first step could cause a severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from the LR image, which makes it difficult to predict a highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate the blur kernel and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the predicted kernel, and \textit{Estimator} estimates the blur kernel with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of the blur kernel easier. More importantly, \textit{Restorer} is trained with the kernel estimated by \textit{Estimator}, instead of the ground-truth kernel, thus \textit{Restorer} could be more tolerant to the estimation error of \textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at a much higher speed. The source code is available at \url{https://github.com/greatlog/DAN.git}. </details>
<details>	<summary>注释</summary>	Submited to PAMI. arXiv admin note: substantial text overlap with arXiv:2010.02631 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 107、合成X射线图像超分辨率的频域约束
- [ ] A Frequency Domain Constraint for Synthetic X-ray Image Super Resolution 
时间：2021年05月14日                         第一作者：Qing Ma                       [链接](https://arxiv.org/abs/2105.06887).                     
## 摘要：合成的X射线图像有助于图像引导系统和虚拟现实仿真。然而，由于CT扫描分辨率有限、计算资源要求高或算法复杂等原因，很难实时生成高质量的任意视点合成X射线图像。我们的目标是通过对低分辨率图像进行上采样，实时生成高分辨率的合成X射线图像。基于参考的超分辨率（RefSR）近年来得到了广泛的研究，并被证明比传统的单图像超分辨率（SISR）更强大。RefSR可以利用参考图像产生精细的细节，但仍不可避免地产生一些伪影和噪声。本文提出了一种基于频域的纹理变换器超分辨率（TTSR-FD）。我们引入频域损耗作为约束条件，进一步提高了RefSR结果的质量，细节清晰，无明显伪影。这使得实时合成X射线图像引导程序VR仿真系统成为可能。据我们所知，这是第一篇利用频域作为超分辨率领域损失函数的论文。我们在合成X射线图像数据集上评估了TTSR-FD，并取得了最新的结果。
<details>	<summary>英文摘要</summary>	Synthetic X-ray images can be helpful for image guiding systems and VR simulations. However, it is difficult to produce high-quality arbitrary view synthetic X-ray images in real-time due to limited CT scanning resolution, high computation resource demand or algorithm complexity. Our goal is to generate high-resolution synthetic X-ray images in real-time by upsampling low-resolution im-ages. Reference-based Super Resolution (RefSR) has been well studied in recent years and has been proven to be more powerful than traditional Single Image Su-per-Resolution (SISR). RefSR can produce fine details by utilizing the reference image but it still inevitably generates some artifacts and noise. In this paper, we propose texture transformer super-resolution with frequency domain (TTSR-FD). We introduce frequency domain loss as a constraint to further improve the quality of the RefSR results with fine details and without obvious artifacts. This makes a real-time synthetic X-ray image-guided procedure VR simulation system possible. To the best of our knowledge, this is the first paper utilizing the frequency domain as part of the loss functions in the field of super-resolution. We evaluated TTSR-FD on our synthetic X-ray image dataset and achieved state-of-the-art results. </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 106、用于视频超分辨率的流引导可变形对准网络FDAN
- [ ] FDAN: Flow-guided Deformable Alignment Network for Video Super-Resolution 
时间：2021年05月12日                         第一作者：Jiayi Lin                       [链接](https://arxiv.org/abs/2105.05640).                     
## 摘要：大多数视频超分辨率（VSR）方法通过对齐相邻帧并挖掘这些帧上的信息来增强视频参考帧。近年来，可变形对齐技术以其能自适应地将相邻帧和参考帧对齐的优异性能，引起了VSR界的广泛关注。然而，我们在实验中发现，可变形对齐方法仍然会因局部损失驱动的偏移预测而受到快速运动的影响，并且缺乏明确的运动约束。因此，我们提出了一个基于匹配的流量估计（MFE）模块来进行全局语义特征匹配，并将光流估计为每个位置的粗偏移量。提出了一种流引导可变形模块（FDM），将光流集成到可变形卷积中。FDM首先利用光流对相邻帧进行扭曲。然后，利用扭曲的相邻帧和参考帧对每个粗偏移量预测一组精细偏移量。一般来说，我们提出了一种端到端的深度网络，称为流引导可变形对齐网络（FDAN），它在两个基准数据集上达到了最先进的性能，同时在计算和内存消耗方面仍然具有竞争力。
<details>	<summary>英文摘要</summary>	Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption. </details>
<details>	<summary>邮件日期</summary>	2021年05月13日</details>

# 105、增强的深金字塔模糊图像恢复网络EDPN
- [ ] EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration 
时间：2021年05月11日                         第一作者：Ruikang Xu                       [链接](https://arxiv.org/abs/2105.04872).                     
## 摘要：随着深度神经网络的发展，图像去模糊技术有了很大的发展。然而，在实际应用中，模糊图像常常会受到诸如缩小尺度和压缩之类的额外退化。为了解决这些问题，我们提出了一种改进的深度金字塔网络（EDPN），通过充分利用退化图像的自相似性和跨尺度相似性，从多个退化图像中恢复模糊图像。，金字塔递进转移（PPT）模块和金字塔自我注意（PSA）模块作为该网络的主要组成部分。PPT模块以多幅复制的模糊图像作为输入，以渐进的方式传输同一退化图像的自相似性和跨尺度相似性信息。然后，PSA模块利用自我和空间注意机制将上述特征融合起来进行后续恢复。实验结果表明，该方法在模糊图像超分辨率和模糊图像去块方面的性能明显优于现有的方法。在NTIRE 2021图像去模糊挑战中，EDPN在第1轨（低分辨率）中获得最佳PSNR/SSIM/LPIPS分数，在第2轨（JPEG伪影）中获得最佳SSIM/LPIPS分数。
<details>	<summary>英文摘要</summary>	Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image.Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts). </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Ruikang and Zeyu contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年05月12日</details>

# 104、超低分辨率印刷文本图像的端到端光学字符识别方法
- [ ] An end-to-end Optical Character Recognition approach for ultra-low-resolution printed text images 
时间：2021年05月10日                         第一作者：Julian D. Gilbey                       [链接](https://arxiv.org/abs/2105.04515).                     
## 摘要：一些历史和较新的打印文档以非常低的分辨率（如60 dpi）进行扫描或存储。尽管这样的扫描相对容易让人阅读，但对于光学字符识别（OCR）系统来说仍然是一个巨大的挑战。目前的技术水平是使用超分辨率重建原始高分辨率图像的近似值，并将其输入标准OCR系统。我们新的端到端方法绕过了超分辨率步骤，产生了更好的OCR结果。这种方法的灵感来自我们对人类视觉系统的理解，并建立在已建立的用于执行OCR的神经网络的基础上。我们的实验表明，在60 dpi扫描的英文文本图像上进行OCR是可能的，这是一个显著低于最新技术的分辨率，并且在一组1000页左右的60 dpi文本中，在广泛的字体范围内，我们实现了99.7%的平均字符级准确率（CLA）和98.9%的单词级准确率（WLA）。对于75dpi图像，同一文本样本的平均CLA为99.9%，平均WLA为99.4%。我们公开我们的代码和数据（包括一组低分辨率图像及其基本事实），作为该领域未来工作的基准。
<details>	<summary>英文摘要</summary>	Some historical and more recent printed documents have been scanned or stored at very low resolutions, such as 60 dpi. Though such scans are relatively easy for humans to read, they still present significant challenges for optical character recognition (OCR) systems. The current state-of-the art is to use super-resolution to reconstruct an approximation of the original high-resolution image and to feed this into a standard OCR system. Our novel end-to-end method bypasses the super-resolution step and produces better OCR results. This approach is inspired from our understanding of the human visual system, and builds on established neural networks for performing OCR. Our experiments have shown that it is possible to perform OCR on 60 dpi scanned images of English text, which is a significantly lower resolution than the state-of-the-art, and we achieved a mean character level accuracy (CLA) of 99.7% and word level accuracy (WLA) of 98.9% across a set of about 1000 pages of 60 dpi text in a wide range of fonts. For 75 dpi images, the mean CLA was 99.9% and the mean WLA was 99.4% on the same sample of texts. We make our code and data (including a set of low-resolution images with their ground truths) publicly available as a benchmark for future work in this field. </details>
<details>	<summary>注释</summary>	8 pages MSC-class: 68T10 ACM-class: I.7.5 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 103、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年05月10日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 102、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年05月10日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 101、基于分层可微神经搜索的轻量级图像超分辨率
- [ ] Lightweight Image Super-Resolution with Hierarchical and Differentiable Neural Architecture Search 
时间：2021年05月09日                         第一作者：Han Huang                       [链接](https://arxiv.org/abs/2105.03939).                     
## 摘要：单图像超分辨率（SISR）任务在深度神经网络中取得了显著的性能。然而，在基于CNN的SISR任务处理方法中，大量的参数需要大量的计算。虽然最近提出了几种有效的SISR模型，但大多数都是手工制作的，因此缺乏灵活性。在这项工作中，我们提出了一种新的可微神经结构搜索（NAS）方法，在细胞和网络两个层面上搜索轻量级SISR模型。具体来说，单元级搜索空间是基于信息提取机制设计的，关注轻量级操作的组合，旨在构建更轻量级、更精确的SR结构。网络级搜索空间的设计考虑了小区间的特征联系，目的是找出哪些信息流对小区最有利，从而提高小区的性能。与现有的基于强化学习（RL）或进化算法（EA）的SISR任务NAS方法不同，我们的搜索管道是完全可微的，并且轻量级SISR模型可以在单个GPU上同时在单元级和网络级进行有效搜索。实验表明，我们的方法在PSNR、SSIM和模型复杂度方面可以在基准数据集上达到最先进的性能，而$\乘以2$的任务只需要68G多次加法，而$\乘以4$的任务只需要18G多次加法。代码将在\url处提供{https://github.com/DawnHH/DLSR-PyTorch}.
<details>	<summary>英文摘要</summary>	Single Image Super-Resolution (SISR) tasks have achieved significant performance with deep neural networks. However, the large number of parameters in CNN-based methods for SISR tasks require heavy computations. Although several efficient SISR models have been recently proposed, most are handcrafted and thus lack flexibility. In this work, we propose a novel differentiable Neural Architecture Search (NAS) approach on both the cell-level and network-level to search for lightweight SISR models. Specifically, the cell-level search space is designed based on an information distillation mechanism, focusing on the combinations of lightweight operations and aiming to build a more lightweight and accurate SR structure. The network-level search space is designed to consider the feature connections among the cells and aims to find which information flow benefits the cell most to boost the performance. Unlike the existing Reinforcement Learning (RL) or Evolutionary Algorithm (EA) based NAS methods for SISR tasks, our search pipeline is fully differentiable, and the lightweight SISR models can be efficiently searched on both the cell-level and network-level jointly on a single GPU. Experiments show that our methods can achieve state-of-the-art performance on the benchmark datasets in terms of PSNR, SSIM, and model complexity with merely 68G Multi-Adds for $\times 2$ and 18G Multi-Adds for $\times 4$ SR tasks. Code will be available at \url{https://github.com/DawnHH/DLSR-PyTorch}. </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 100、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月08日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
## 摘要：近年来，高时间分辨率卫星在各种实际应用中引起了广泛的关注。然而，由于带宽和硬件成本的限制，这类卫星的空间分辨率相当低，很大程度上限制了它们在需要空间明确信息的情况下的潜力。为了提高图像分辨率，人们提出了许多基于训练低-高分辨率对的方法来解决超分辨率问题。然而，尽管它们取得了成功，但在具有高时间分辨率的卫星中通常很难获得低/高空间分辨率对，这使得在SR中使用这种方法不切实际。在本文中，我们提出了一种新的无监督学习框架，称为MIP，它可以在没有低/高分辨率图像对的情况下完成SR任务。首先，将随机噪声映射输入到设计的生成对抗网络（GAN）中进行重构。然后，该方法将参考图像转换为潜空间作为偏移图像的先验信息。最后，利用隐式方法对输入噪声进行更新，进一步传递参考图像的纹理和结构信息。在Draper数据集上的大量实验结果表明，MIP在数量和质量上都比最先进的方法有显著的改进。提议的MIP是开源的http://github.com/jiaming-wang/MIP.
<details>	<summary>英文摘要</summary>	Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. Despite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called "MIP", which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at http://github.com/jiaming-wang/MIP. </details>
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 99、用消噪器中的先验隐式法求解线性反问题
- [ ] Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser 
时间：2021年05月07日                         第一作者：Zahra Kadkhodaie                        [链接](https://arxiv.org/abs/2007.13640).                     
<details>	<summary>注释</summary>	19 pages, 12 figures. Changes: more detailed description of relationships to previous literature, including empirical comparisons for super-resolution, debarring, and compressive sensing </details>
<details>	<summary>邮件日期</summary>	2021年05月10日</details>

# 98、基于局部推理和全局参数联合估计的实时视频超分辨率
- [ ] Real-Time Video Super-Resolution by Joint Local Inference and Global Parameter Estimation 
时间：2021年05月06日                         第一作者：Noam Elron                       [链接](https://arxiv.org/abs/2105.02794).                     
## 摘要：视频超分辨率（SR）是基于深度学习的技术，但在现实世界的视频中表现不佳（见图1）。原因是训练图像对通常是通过缩小高分辨率图像的比例来产生低分辨率的对应图像。因此，深度模型被训练为撤消缩小尺度，而不是推广到超分辨率的真实世界图像。最近的一些出版物提出了改进基于学习的随机共振泛化的技术，但都不适合实时应用。我们提出了一种新的方法来合成训练数据，通过模拟两个不同尺度的数码相机图像捕获过程。我们的方法产生图像对，其中两幅图像都具有自然图像的特性。使用这些数据训练SR模型可以更好地推广到真实世界的图像和视频。此外，深视频SR模型的特点是每像素运算量大，这使得其无法实时应用。我们提出了一种有效的CNN架构，使得视频SR能够在低功耗边缘设备上实时应用。我们将SR任务分为两个子任务：一个控制流，用于估计输入视频的全局属性，并调整执行实际处理的CNN的权重和偏差。由于CNN过程是根据输入的统计信息定制的，因此它的容量保持较低，同时保持了有效性。另外，由于视频统计发展缓慢，控制流以远低于视频帧速率的速率操作。这减少了多达两个数量级的总体计算负载。这种将算法的自适应性与像素处理解耦的框架，可以应用于一大类实时视频增强应用中，如视频去噪、局部色调映射、稳定等。
<details>	<summary>英文摘要</summary>	The state of the art in video super-resolution (SR) are techniques based on deep learning, but they perform poorly on real-world videos (see Figure 1). The reason is that training image-pairs are commonly created by downscaling a high-resolution image to produce a low-resolution counterpart. Deep models are therefore trained to undo downscaling and do not generalize to super-resolving real-world images. Several recent publications present techniques for improving the generalization of learning-based SR, but are all ill-suited for real-time application. We present a novel approach to synthesizing training data by simulating two digital-camera image-capture processes at different scales. Our method produces image-pairs in which both images have properties of natural images. Training an SR model using this data leads to far better generalization to real-world images and videos. In addition, deep video-SR models are characterized by a high operations-per-pixel count, which prohibits their application in real-time. We present an efficient CNN architecture, which enables real-time application of video SR on low-power edge-devices. We split the SR task into two sub-tasks: a control-flow which estimates global properties of the input video and adapts the weights and biases of a processing-CNN that performs the actual processing. Since the process-CNN is tailored to the statistics of the input, its capacity kept low, while retaining effectivity. Also, since video-statistics evolve slowly, the control-flow operates at a much lower rate than the video frame-rate. This reduces the overall computational load by as much as two orders of magnitude. This framework of decoupling the adaptivity of the algorithm from the pixel processing, can be applied in a large family of real-time video enhancement applications, e.g., video denoising, local tone-mapping, stabilization, etc. </details>
<details>	<summary>注释</summary>	Technical report; accompanying a poster appearing in ICCP 2021 Journal-ref: ICCP 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月07日</details>

# 97、FC$^2$N：用于单图像超分辨率的全通道连接网络
- [ ] FC$^2$N: Fully Channel-Concatenated Network for Single Image Super-Resolution 
时间：2021年05月05日                         第一作者：Xiaole Zhao                       [链接](https://arxiv.org/abs/1907.03221).                     
<details>	<summary>注释</summary>	17 pages, 8 figures and 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 96、COMISR：压缩信息视频超分辨率
- [ ] COMISR: Compression-Informed Video Super-Resolution 
时间：2021年05月04日                         第一作者：Yinxiao Li                       [链接](https://arxiv.org/abs/2105.01237).                     
## 摘要：大多数视频超分辨率方法的重点是从低分辨率视频中恢复高分辨率的视频帧，而不考虑压缩。然而，网络或移动设备上的大多数视频都是压缩的，当带宽有限时，压缩会很严重。本文提出了一种新的基于压缩信息的视频超分辨率模型，在不引入压缩伪影的情况下恢复高分辨率的视频内容。该模型由三个视频超分辨率模块组成：双向递归扭曲、细节保持流估计和拉普拉斯增强。所有这三个模块都用于处理压缩特性，例如输入帧中帧内的位置和输出帧中的平滑度。为了进行全面的性能评估，我们在标准数据集上进行了广泛的实验，包括许多真实的视频用例。结果表明，该方法不仅能从广泛使用的基准数据集中恢复未压缩帧上的高分辨率内容，而且在基于大量量化指标的超分辨率压缩视频中也取得了最新的性能。我们还通过模拟YouTube上的流媒体来评估该方法的有效性和鲁棒性。
<details>	<summary>英文摘要</summary>	Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compression-informed video super-resolution model to restore high-resolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. </details>
<details>	<summary>注释</summary>	14 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 95、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年05月03日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 94、基于对抗图神经网络的脑图超分辨及其在脑功能连接中的应用
- [ ] Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity 
时间：2021年05月02日                         第一作者：Megi Isallari                        [链接](https://arxiv.org/abs/2105.00425).                     
## 摘要：近年来，随着以不同分辨率获取的神经影像数据的激增，脑图像分析得到了长足的发展。近年来，脑图像超分辨率的研究得到了迅速的发展，但由于非欧氏图数据的复杂性，脑图像超分辨率的研究还很薄弱。在本文中，我们提出了有史以来第一个深度图超分辨率（GSR）框架，该框架试图从具有N个节点的低分辨率（LR）图（其中N<N'）自动生成具有N'节点的高分辨率（HR）脑图（即解剖感兴趣区域（roi））。首先，我们将GSR问题形式化为一个节点特征嵌入学习任务。一旦学习了HR节点的嵌入，就可以通过基于一种新的图形U-Net结构的聚合规则来获得大脑roi之间的成对连接强度。图U-Net是一种典型的以节点为中心的结构，其中图的嵌入主要依赖于节点的属性，我们提出了一种以节点为中心的结构，其中节点特征的嵌入基于图的拓扑结构。其次，受图谱理论的启发，我们打破了U-Net结构的对称性，用一个GSR层和两个图卷积网络层对低分辨率的脑图结构和节点内容进行了超分辨，进一步了解了HR图中的节点嵌入。第三，为了处理基本真值和预测的HR脑图之间的域转移，我们结合了对抗正则化来对齐它们各自的分布。我们提出的AGSR网络框架在从低分辨率脑功能图预测高分辨率脑功能图方面优于其变体。我们的AGSR网络代码可在GitHub上获得https://github.com/basiralab/AGSR-Net.
<details>	<summary>英文摘要</summary>	Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2009.11080 </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 93、基于无监督深度学习的磁共振弥散加权成像超分辨率和运动伪影同步去除
- [ ] Simultaneous super-resolution and motion artifact removal in diffusion-weighted MRI using unsupervised deep learning 
时间：2021年05月01日                         第一作者：Hyungjin Chung                       [链接](https://arxiv.org/abs/2105.00240).                     
## 摘要：磁共振弥散加权成像由于其预后的能力，现在常规进行，但扫描质量往往不令人满意，这可能会阻碍临床应用。为了克服这些局限性，本文提出了一种完全无监督的质量增强方案，该方案在提高分辨率的同时去除了运动伪影。这一过程是通过使用具有随机退化块的最优传输驱动cycleGAN训练网络来实现的，cycleGAN学习去除混叠伪影并提高分辨率，然后在测试阶段使用训练好的网络，利用bootstrap子采样和聚集来抑制运动伪影。进一步证明了在推理阶段通过控制bootstrap子采样率可以控制伪影校正量和分辨率之间的折衷。据我们所知，提出的方法是第一个解决超分辨率和运动伪影校正，同时在磁共振背景下使用无监督学习。我们通过将该方法应用于模拟研究的定量评价和活体扩散加权MR扫描，证明了该方法的有效性，这表明该方法优于目前最先进的方法。该方法具有灵活性，可以应用于其他类型MR扫描的各种质量增强方案，也可以直接应用于表观扩散系数图的质量增强。
<details>	<summary>英文摘要</summary>	Diffusion-weighted MRI is nowadays performed routinely due to its prognostic ability, yet the quality of the scans are often unsatisfactory which can subsequently hamper the clinical utility. To overcome the limitations, here we propose a fully unsupervised quality enhancement scheme, which boosts the resolution and removes the motion artifact simultaneously. This process is done by first training the network using optimal transport driven cycleGAN with stochastic degradation block which learns to remove aliasing artifacts and enhance the resolution, then using the trained network in the test stage by utilizing bootstrap subsampling and aggregation for motion artifact suppression. We further show that we can control the trade-off between the amount of artifact correction and resolution by controlling the bootstrap subsampling ratio at the inference stage. To the best of our knowledge, the proposed method is the first to tackle super-resolution and motion artifact correction simultaneously in the context of MRI using unsupervised learning. We demonstrate the efficiency of our method by applying it to both quantitative evaluation using simulation study, and to in vivo diffusion-weighted MR scans, which shows that our method is superior to the current state-of-the-art methods. The proposed method is flexible in that it can be applied to various quality enhancement schemes in other types of MR scans, and also directly to the quality enhancement of apparent diffusion coefficient maps. </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 92、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年04月30日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
## 摘要：超分辨率（Super-Resolution，SR）是一项基本的计算机视觉任务，其目标是从给定的低分辨率图像中获得高分辨率的清晰图像。本文回顾了NTIRE 2021在视频超分辨率方面的挑战。我们给出了两条赛道的评估结果以及建议的解决方案。track1的目标是开发传统的视频SR方法，重点是恢复质量。轨道2假设一个具有较低帧速率的更具挑战性的环境，投射时空SR问题。在每项比赛中，分别有247名和223名参赛者报名。在最后的测试阶段，14个团队在每条赛道上进行比赛，以实现视频SR任务的最先进性能。
<details>	<summary>英文摘要</summary>	Super-Resolution (SR) is a fundamental computer vision task that aims to obtain a high-resolution clean image from the given low-resolution counterpart. This paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We present evaluation results from two competition tracks as well as the proposed solutions. Track 1 aims to develop conventional video SR methods focusing on the restoration quality. Track 2 assumes a more challenging environment with lower frame rates, casting spatio-temporal SR problem. In each competition, 247 and 223 participants have registered, respectively. During the final testing phase, 14 teams competed in each track to achieve state-of-the-art performance on video SR tasks. </details>
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 91、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年04月30日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是从给定的低分辨率（low-resolution，LR）图像重建出高分辨率（high-resolution，HR）图像，由于一幅LR图像对应于多幅HR图像，因此这是一个病态问题。近年来，基于学习的SISR方法在性能上明显优于传统的SISR方法，而面向PSNR、GAN驱动和基于流的SISR方法存在过平滑、模式崩溃和模型占用大等问题。为了解决这些问题，我们提出了一种新的单图像超分辨率扩散概率模型（SRDiff），这是第一个基于扩散的SISR模型。SRDiff利用数据似然的变分界变量进行优化，通过马尔可夫链将高斯噪声逐步转化为LR输入条件下的超分辨率（SR）图像，可以提供多样化和真实的SR预测。另外，在整个框架中引入残差预测，加快了收敛速度。我们在面部和一般基准测试（CelebA和DIV2K数据集）上的大量实验表明：1）SRDiff只需一个LR输入，就可以生成丰富细节的不同SR结果，具有最先进的性能；2） SRDiff易于训练，占地面积小；SRDiff可以进行灵活的图像处理，包括潜在空间插值和内容融合。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion. </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 90、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月30日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
<details>	<summary>注释</summary>	This paper is accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 89、BasicVSR++：通过增强传播和对齐提高视频超分辨率
- [ ] BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment 
时间：2021年04月27日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2104.13371).                     
## 摘要：递归结构是视频超分辨率处理的常用框架选择。最先进的方法BasicVSR采用双向传播和特征对齐，有效地利用整个输入视频中的信息。在这项研究中，我们提出二阶网格传播和流动导向的变形对准来重新设计BasicVSR。我们表明，通过增强传播和对齐的循环框架，可以更有效地利用跨错位视频帧的时空信息。在类似的计算约束下，新的组件可以提高性能。特别是，我们的模型BasicVSR++在参数数目相近的情况下，PSNR比BasicVSR高出0.82db。除了视频超分辨率，BasicVSR++还可以很好地推广到其他视频恢复任务，如压缩视频增强。2021年，BasicVSR++在视频超分辨率和压缩视频增强挑战中获得三个冠军和一个亚军。代码和模型将发布给MMEditing。
<details>	<summary>英文摘要</summary>	A recurrent structure is a popular framework choice for the task of video super-resolution. The state-of-the-art method BasicVSR adopts bidirectional propagation with feature alignment to effectively exploit information from the entire input video. In this study, we redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment. We show that by empowering the recurrent framework with the enhanced propagation and alignment, one can exploit spatiotemporal information across misaligned video frames more effectively. The new components lead to an improved performance under a similar computational constraint. In particular, our model BasicVSR++ surpasses BasicVSR by 0.82 dB in PSNR with similar number of parameters. In addition to video super-resolution, BasicVSR++ generalizes well to other video restoration tasks such as compressed video enhancement. In NTIRE 2021, BasicVSR++ obtains three champions and one runner-up in the Video Super-Resolution and Compressed Video Enhancement Challenges. Codes and models will be released to MMEditing. </details>
<details>	<summary>注释</summary>	3 champions and 1 runner-up in NTIRE 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月28日</details>

# 88、好的艺术家复制，伟大的艺术家窃取：模型提取攻击图像翻译生成对抗网络
- [ ] Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks 
时间：2021年04月26日                         第一作者：Sebastian Szyller                       [链接](https://arxiv.org/abs/2104.12623).                     
## 摘要：机器学习模型通常通过推理api提供给潜在的客户机用户。当恶意客户端使用从查询中收集到的信息到受害者模型$F_V$的推理API来构建具有类似功能的代理模型$F_a$时，就会发生模型提取攻击。最近的研究表明，针对图像分类和NLP模型的模型提取攻击是成功的。在本文中，我们展示了第一个针对真实世界生成性对抗网络（GAN）图像翻译模型的模型提取攻击。我们提出了一个对图像翻译模型进行模型提取攻击的框架，并证明了对手可以成功地提取功能代理模型。敌方不需要知道$F\V$的体系结构或任何其他超出其预期图像翻译任务的信息，并且使用从与$F\V$的训练数据相同的域中提取的数据来查询$F\V$的推理接口。我们使用两种流行的图像翻译类型的三个不同实例来评估我们的攻击的有效性：（1）自拍到动画，（2）莫奈到照片（图像样式转换），和（3）超分辨率（超分辨率）。使用GANs的标准性能指标，我们证明了我们的攻击在这三种情况下都是有效的，$F_V$和$F_A$与目标之间的差异在以下范围内：自拍到动画：FID$13.36-68.66$，莫奈到照片：FID$3.57-4.40$，超分辨率：SSIM:$0.06-0.08$，PSNR:$1.43-4.46$。此外，我们进行了一项大规模（125名参与者）的用户研究，从自拍到动画，从莫奈到照片，以表明人类对受害者和代理模型产生的图像的感知是等效的，在科恩的$d=0.3$的等效范围内。
<details>	<summary>英文摘要</summary>	Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ that has comparable functionality. Recent research has shown successful model extraction attacks against image classification, and NLP models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting model extraction attacks against image translation models, and show that the adversary can successfully extract functional surrogate models. The adversary is not required to know $F_V$'s architecture or any other information about it beyond its intended image translation task, and queries $F_V$'s inference interface using data drawn from the same domain as the training data for $F_V$. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective in each of the three cases -- the differences between $F_V$ and $F_A$, compared to the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$, Monet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and PSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by the victim and surrogate models can be considered equivalent, within an equivalence bound of Cohen's $d=0.3$. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 87、意向性深度过度适应学习（IDOL）：一种新的适应性放射治疗深度学习策略
- [ ] Intentional Deep Overfit Learning (IDOL): A Novel Deep Learning Strategy for Adaptive Radiation Therapy 
时间：2021年04月23日                         第一作者：Jaehee Chun (3)                       [链接](https://arxiv.org/abs/2104.11401).                     
## 摘要：在这项研究中，我们提出了一个针对患者特定表现的定制DL框架，该框架利用了一个模型的行为，该模型故意过度拟合患者特定的训练数据集，该数据集是从ART工作流中可用的先验信息扩充而来的——我们称之为故意深度过度拟合学习（IDOL）。在放射治疗的任何任务中实施IDOL框架包括两个训练阶段：1）训练一个具有N个患者的不同训练数据集的广义模型，就像传统的DL方法一样，2）有意地将该一般模型过度拟合到特定于感兴趣的患者（N+1）的小训练数据集，该数据集是通过对可用的特定于任务和患者的先验信息的扰动和增强而生成的，以建立个性化的偶像模型。IDOL框架本身是任务无关的，因此广泛适用于ART工作流的许多组件，其中三个我们在这里用作概念证明：用于传统ART的重新规划CT的自动轮廓任务，用于MRI引导ART的MRI超分辨率（SR）任务，以及仅用于MRI ART的合成CT（sCT）重建任务。在重新规划CT自动轮廓任务中，采用Dice相似系数测量的精度由一般模型的0.847提高到采用IDOL模型的0.935。在MRI-SR的情况下，使用IDOL框架比传统模型的平均绝对误差（MAE）提高了40%。最后，在sCT重建任务中，利用IDOL框架将MAE从68降到22hu。
<details>	<summary>英文摘要</summary>	In this study, we propose a tailored DL framework for patient-specific performance that leverages the behavior of a model intentionally overfitted to a patient-specific training dataset augmented from the prior information available in an ART workflow - an approach we term Intentional Deep Overfit Learning (IDOL). Implementing the IDOL framework in any task in radiotherapy consists of two training stages: 1) training a generalized model with a diverse training dataset of N patients, just as in the conventional DL approach, and 2) intentionally overfitting this general model to a small training dataset-specific the patient of interest (N+1) generated through perturbations and augmentations of the available task- and patient-specific prior information to establish a personalized IDOL model. The IDOL framework itself is task-agnostic and is thus widely applicable to many components of the ART workflow, three of which we use as a proof of concept here: the auto-contouring task on re-planning CTs for traditional ART, the MRI super-resolution (SR) task for MRI-guided ART, and the synthetic CT (sCT) reconstruction task for MRI-only ART. In the re-planning CT auto-contouring task, the accuracy measured by the Dice similarity coefficient improves from 0.847 with the general model to 0.935 by adopting the IDOL model. In the case of MRI SR, the mean absolute error (MAE) is improved by 40% using the IDOL framework over the conventional model. Finally, in the sCT reconstruction task, the MAE is reduced from 68 to 22 HU by utilizing the IDOL framework. </details>
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 86、基于三层神经网络结构的高效单幅图像超分辨率搜索
- [ ] Trilevel Neural Architecture Search for Efficient Single Image Super-Resolution 
时间：2021年04月23日                         第一作者：Yan Wu                       [链接](https://arxiv.org/abs/2101.06658).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 85、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年04月23日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 84、SRWarp：任意变换下的广义图像超分辨率
- [ ] SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation 
时间：2021年04月21日                         第一作者：Sanghyun Son                        [链接](https://arxiv.org/abs/2104.10325).                     
## 摘要：深部CNNs在图像处理和应用方面取得了巨大的成功，包括单图像超分辨率（SR）。然而，传统方法仍然求助于一些预定的整数比例因子，例如x2或x4。因此，当需要任意的目标分辨率时，它们很难被应用。最近的方法将范围扩展到实值上采样因子，甚至使用不同的纵横比来处理限制。在本文中，我们提出SRWarp框架来进一步将SR任务推广到任意图像变换。我们将传统的图像扭曲任务，特别是当输入被放大时，解释为一个空间变化的SR问题。我们还提出了一些新的公式，包括自适应翘曲层和多尺度混合，重建视觉上良好的结果在转换过程中。与以前的方法相比，我们没有将SR模型限制在规则的网格上，而是允许多种可能的变形，以实现灵活多样的图像编辑。大量的实验和烧蚀研究证明了该方法的必要性，并证明了该方法在各种变换下的优越性。
<details>	<summary>英文摘要</summary>	Deep CNNs have achieved significant successes in image processing and its applications, including single image super-resolution (SR). However, conventional methods still resort to some predetermined integer scaling factors, e.g., x2 or x4. Thus, they are difficult to be applied when arbitrary target resolutions are required. Recent approaches extend the scope to real-valued upsampling factors, even with varying aspect ratios to handle the limitation. In this paper, we propose the SRWarp framework to further generalize the SR tasks toward an arbitrary image transformation. We interpret the traditional image warping task, specifically when the input is enlarged, as a spatially-varying SR problem. We also propose several novel formulations, including the adaptive warping layer and multiscale blending, to reconstruct visually favorable results in the transformation process. Compared with previous methods, we do not constrain the SR model on a regular grid but allow numerous possible deformations for flexible and diverse image editing. Extensive experiments and ablation studies justify the necessity and demonstrate the advantage of the proposed SRWarp method under various transformations. </details>
<details>	<summary>注释</summary>	Accepted to CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 83、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月21日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
## 摘要：时空视频超分辨率（STVSR）旨在提高低分辨率、低帧速率视频的时空分辨率。近年来，基于可变形卷积的STVSR方法取得了很好的性能，但它们只能在训练阶段推断出预先定义的中间帧。此外，这些方法低估了相邻帧之间的短期运动线索。本文提出了一种时间调制网络（TMNet）来插值任意中间帧，实现精确的高分辨率重建。具体地说，我们提出了一种时间调制块（TMB）来调制可变形卷积核以实现可控的特征插值。为了更好地利用视频中的时间信息，我们提出了一种局部时间特征比较（LFC）模块和双向可变形ConvLSTM来提取视频中的短期和长期运动线索。在三个基准数据集上的实验表明，我们的TMNet算法优于以前的STVSR算法。代码可在https://github.com/CS-GangXu/TMNet.
<details>	<summary>英文摘要</summary>	Space-time video super-resolution (STVSR) aims to increase the spatial and temporal resolutions of low-resolution and low-frame-rate videos. Recently, deformable convolution based methods have achieved promising STVSR performance, but they could only infer the intermediate frame pre-defined in the training stage. Besides, these methods undervalued the short-term motion cues among adjacent frames. In this paper, we propose a Temporal Modulation Network (TMNet) to interpolate arbitrary intermediate frame(s) with accurate high-resolution reconstruction. Specifically, we propose a Temporal Modulation Block (TMB) to modulate deformable convolution kernels for controllable feature interpolation. To well exploit the temporal information, we propose a Locally-temporal Feature Comparison (LFC) module, along with the Bi-directional Deformable ConvLSTM, to extract short-term and long-term motion cues in videos. Experiments on three benchmark datasets demonstrate that our TMNet outperforms previous STVSR methods. The code is available at https://github.com/CS-GangXu/TMNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 82、单图像超分辨率的两级注意网络
- [ ] A Two-Stage Attentive Network for Single Image Super-Resolution 
时间：2021年04月21日                         第一作者：Jiqing Zhang                       [链接](https://arxiv.org/abs/2104.10488).                     
## 摘要：近年来，深度卷积神经网络（CNNs）在单幅图像超分辨率（SISR）中得到了广泛的应用，并取得了显著的进展。然而，现有的基于CNNs的SISR方法大多在特征提取阶段没有充分挖掘背景信息，对最终的高分辨率图像重建步骤关注较少，从而影响了理想的SR性能。为了解决上述两个问题，本文提出了一种从粗到精的两级注意网络（TSAN）方法。具体来说，我们设计了一个新的多上下文注意块（MCAB），使网络关注更多的信息上下文特征。此外，我们提出了一个必要的精细注意块（RAB），它可以在HR空间中寻找有用的线索来重建精细的HR图像。对四个基准数据集的广泛评估显示了我们提出的TSAN在定量指标和视觉效果方面的有效性。代码位于https://github.com/Jee-King/TSAN.
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at https://github.com/Jee-King/TSAN. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 81、TWIST-GAN：面向小波变换和传输GAN的时空单图像超分辨率研究
- [ ] TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatio-Temporal Single Image Super Resolution 
时间：2021年04月20日                         第一作者：Fayaz Ali Dharejo                       [链接](https://arxiv.org/abs/2104.10268).                     
## 摘要：单图像超分辨率（Single-Image Super-resolution，SISR）是从低空间分辨率的遥感图像中提取高分辨率、高空间分辨率的图像。近年来，针对具有挑战性的单图像超分辨率（SISR）问题，深入学习和生成对抗网络（GANs）取得了突破性进展。然而，生成的图像仍然存在一些不需要的伪影，例如缺少纹理特征表示和高频信息。提出了一种基于频域的时空遥感单幅图像超分辨率重建技术，结合不同频段的生成性对抗网络（GANs）对HR图像进行重建。介绍了一种结合小波变换（WT）特性和传递生成对抗网络的新方法。利用小波变换将LR图像分解为多个频段，而传递生成对抗网络则通过一种新的结构来预测高频分量。最后，通过小波变换的逆变换得到超分辨率的重建图像。该模型首先在一个外部DIV2 Kdataset上进行训练，并用UC-Merceed Landsat遥感数据集和Set14进行验证，每个图像大小为256x256。然后，为了减小计算量的差异，提高纹理信息的质量，利用传递的GANs对时空遥感图像进行处理。对研究结果进行了定性和定性的比较，并与现有的研究方法进行了比较。此外，我们在训练期间节省了大约43%的GPU内存，并通过消除批处理规范化层加快了简化版本的执行。
<details>	<summary>英文摘要</summary>	Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from aremotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks(GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, thegenerated image still suffers from undesirable artifacts such as, the absence of texture-feature representationand high-frequency information. We propose a frequency domain-based spatio-temporal remote sensingsingle image super-resolution technique to reconstruct the HR image combined with generative adversarialnetworks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporatingWavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image hasbeen split into various frequency bands by using the WT, whereas, the transfer generative adversarial networkpredicts high-frequency components via a proposed architecture. Finally, the inverse transfer of waveletsproduces a reconstructed image with super-resolution. The model is first trained on an external DIV2 Kdataset and validated with the UC Merceed Landsat remote sensing dataset and Set14 with each image sizeof 256x256. Following that, transferred GANs are used to process spatio-temporal remote sensing images inorder to minimize computation cost differences and improve texture information. The findings are comparedqualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of theGPU memory during training and accelerated the execution of our simplified version by eliminating batchnormalization layers. </details>
<details>	<summary>注释</summary>	Accepted: ACM TIST (10-03-2021) </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 80、立体图像超分辨率的对称视差注意
- [ ] Symmetric Parallax Attention for Stereo Image Super-Resolution 
时间：2021年04月20日                         第一作者：Yingqian Wang                       [链接](https://arxiv.org/abs/2011.03802).                     
<details>	<summary>注释</summary>	Accepted to NTIRE workshop at CVPR 2021. The first two authors contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 79、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月20日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 78、核不可知的真实图像超分辨率
- [ ] Kernel Agnostic Real-world Image Super-resolution 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2104.09008).                     
## 摘要：近年来，深度神经网络模型在各个研究领域取得了令人瞩目的成果。随着深度超分辨（SR）技术的发展，它引起了越来越多的关注。许多现有的方法都试图从直接下采样的低分辨率图像中恢复高分辨率图像，或者由于其简单性而假设高斯退化核具有加性噪声。然而，在真实场景中，即使失真图像在视觉上与清晰图像相似，也可能涉及高度复杂的核和非加性噪声。在这种情况下，现有的SR模型很难处理真实世界的图像。本文提出了一种新的核不可知SR框架来处理现实世界中的图像SR问题。这个框架可以无缝地挂接到多个主流模型上。在该框架中，退化核和噪声被自适应地建模而不是显式地指定。此外，我们还从正交的角度提出了一个迭代监督过程和频率参与目标，以进一步提高性能。实验验证了该框架在多个真实数据集上的有效性。
<details>	<summary>英文摘要</summary>	Recently, deep neural network models have achieved impressive results in various research fields. Come with it, an increasing number of attentions have been attracted by deep super-resolution (SR) approaches. Many existing methods attempt to restore high-resolution images from directly down-sampled low-resolution images or with the assumption of Gaussian degradation kernels with additive noises for their simplicities. However, in real-world scenarios, highly complex kernels and non-additive noises may be involved, even though the distorted images are visually similar to the clear ones. Existing SR models are facing difficulties to deal with real-world images under such circumstances. In this paper, we introduce a new kernel agnostic SR framework to deal with real-world image SR problem. The framework can be hanged seamlessly to multiple mainstream models. In the proposed framework, the degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and frequency-attended objective from orthogonal perspectives to further boost the performance. The experiments validate the effectiveness of the proposed framework on multiple real-world datasets. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 77、基于稠密搜索空间的神经网络超分辨率搜索：DeCoNAS
- [ ] Neural Architecture Search for Image Super-Resolution Using Densely Constructed Search Space: DeCoNAS 
时间：2021年04月19日                         第一作者：Joon Young Ahn                        [链接](https://arxiv.org/abs/2104.09048).                     
## 摘要：近年来，深度卷积神经网络在单幅图像超分辨率（SISR）和其他许多视觉任务中取得了巨大的成功。通过深化网络和开发更复杂的网络结构，它们的性能也在提高。然而，为给定的问题找到一个最优的结构是一项困难的任务，即使是对人类专家来说也是如此。为此，人们引入了神经结构搜索（NAS）方法，使结构的构建过程自动化。在本文中，我们将NAS扩展到超分辨率域，找到了一个轻量级的密集连接网络decoasnet。我们使用分层搜索策略来寻找与局部和全局特征的最佳连接。在这个过程中，我们定义了一个基于复杂度的惩罚来解决图像的超分辨率问题，这可以看作是一个多目标问题。实验结果表明，我们的decoasnet比现有的基于NAS的设计和手工设计的轻量级超分辨率网络具有更好的性能。
<details>	<summary>英文摘要</summary>	The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 76、基于无监督深度学习的三维荧光显微镜轴向-横向超分辨分析
- [ ] Axial-to-lateral super-resolution for 3D fluorescence microscopy using unsupervised deep learning 
时间：2021年04月19日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
## 摘要：与横向分辨率相比，荧光显微镜的体积成像通常受到轴向分辨率较低的各向异性空间分辨率的限制。为了解决这一问题，本文提出了一种基于深度学习的无监督超分辨技术来增强体荧光显微镜中的各向异性图像。与现有的需要匹配高分辨率目标体图像的深度学习方法相比，我们的方法大大减少了投入实践的工作量，因为网络的训练只需要一个3D图像堆栈，而不需要图像形成过程的先验知识、训练数据的配准或单独的训练获取目标数据。这是基于最优传输驱动循环一致生成对抗网络实现的，该网络从横向图像平面的高分辨率二维图像和其他平面的低分辨率二维图像之间的不成对匹配中学习。利用荧光共焦显微镜和光片显微镜，我们证明了训练的网络不仅提高了轴向分辨率超过衍射极限，而且增强了成像平面之间被抑制的视觉细节，消除了成像伪影。
<details>	<summary>英文摘要</summary>	Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution from inferior axial resolution compared to the lateral resolution. To address this problem, here we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target volume images, our method greatly reduces the effort to put into practice as the training of a network requires as little as a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in lateral image plane and low-resolution 2D images in the other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution beyond the diffraction limit, but also enhances suppressed visual details between the imaging planes and removes imaging artifacts. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 75、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年04月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
## 摘要：在过去的十年中，卷积神经网络在单幅图像超分辨率（SISR）方面取得了显著的进展。在SISR的最新进展中，注意机制是高性能SR模型的关键。然而，很少有作品真正讨论注意力为什么起作用以及它是如何起作用的。在这项工作中，我们试图量化和可视化的静态注意机制，并表明并非所有的注意模块都是同样有益的。然后，我们提出注意网络中的注意（A$^2$N）来获得高精度的图像SR。具体来说，我们的A$^2$N由一个非注意分支和一个耦合注意分支组成。提出了一种基于输入特征的动态注意权值提取模块，该模块能够有效地抑制不必要的注意调整。这允许注意模块专门化有益的例子，而无需其他惩罚，从而大大提高了注意网络的容量，而参数开销很小。实验表明，与现有的轻量级网络相比，该模型具有更好的折衷性能。在局部属性图上的实验也证明了注意力结构（attention-in-attention，A$^2$）可以从更广的范围内提取特征。
<details>	<summary>英文摘要</summary>	Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high performance SR models. However, few works really discuss why attention works and how it works. In this work, we attempt to quantify and visualize the static attention mechanisms and show that not all attention modules are equally beneficial. We then propose attention in attention network (A$^2$N) for highly accurate image SR. Specifically, our A$^2$N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. Experiments have demonstrated that our model could achieve superior trade-off performances comparing with state-of-the-art lightweight networks. Experiments on local attribution maps also prove attention in attention (A$^2$) structure can extract features from a wider range. </details>
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes will be available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 74、全量化图像超分辨率网络
- [ ] Fully Quantized Image Super-Resolution Networks 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2011.14265).                     
<details>	<summary>注释</summary>	Results updated </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 73、VSpSR：基于变分稀疏表示的可探索超分辨率
- [ ] VSpSR: Explorable Super-Resolution via Variational Sparse Representation 
时间：2021年04月17日                         第一作者：Hangqi Zhou                       [链接](https://arxiv.org/abs/2104.08575).                     
## 摘要：超分辨率（SR）是一个不适定问题，它意味着无限多幅高分辨率（HR）图像可以退化为同一幅低分辨率（LR）图像。为了研究一对多随机SR映射，我们隐式地表示了自然图像的非局部自相似性，并通过神经网络建立了一个变分稀疏的超分辨率框架（VSpSR）。由于HR图像的每一小块都可以很好地用原子在超完备字典中的稀疏表示来逼近，因此我们设计了一个双分支模块VSpM来探索SR空间。具体地说，VSpM的一个分支从LR输入中提取面片级基，另一个分支根据稀疏系数推断像素级的变分分布。通过重复采样系数，我们可以得到无限的稀疏表示，从而产生不同的HR图像。根据NTIRE 2021学习SR空间挑战赛的初步结果，我们团队（FUDANMIC21）的发布分数排名第7位。VSpSR的实现发布于https://zmiclab.github.io/。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 72、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月16日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
## 摘要：最新的视频编码标准，称为多功能视频编码（VVC），在编码链的不同层次上包含了几种新颖而精细的编码工具。与以前的高效视频编码（HEVC）标准相比，这些工具带来了显著的编码增益。然而，编码器仍然可以引入可见的编码伪影，主要由应用于将比特率调整到可用带宽的编码决策引起。因此，通常将预处理和后处理技术添加到编码管道以提高解码视频的质量。由于近年来在深度学习方面的进步，与传统方法相比，这些方法最近显示出了突出的效果。通常，多个神经网络被独立地训练来执行不同的任务，因此忽略了模型之间存在的冗余。在本文中，我们研究了一种基于学习的解决方案作为后处理步骤，以提高解码的VVC视频质量。我们的方法依赖于多任务学习来执行质量增强和超分辨率使用一个单一的共享网络优化多个退化水平。与传统的专用体系结构相比，该方案在减少编码伪影和超分辨率方面具有良好的性能，且网络参数较少。
<details>	<summary>英文摘要</summary>	The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 71、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年04月15日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年04月19日</details>

# 70、缩放SlowMo：一种高效的单级时空视频超分辨率框架
- [ ] Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution 
时间：2021年04月15日                         第一作者：Xiaoyu Xiang                       [链接](https://arxiv.org/abs/2104.07473).                     
## 摘要：本文提出了一种基于低分辨率（LR）和低帧速率（LFR）视频序列的高分辨率（HR）慢动作视频超分辨率算法。一种简单的方法是将其分解为两个子任务：视频帧插值（VFI）和视频超分辨率（VSR）。然而，在这个问题中，时间插值和空间上缩放是相互关联的。两阶段方法不能充分利用这一自然属性。另外，现有的VFI或VSR深度网络为了获得高质量的真实感视频帧，通常需要一个较大的帧重建模块，这使得两阶段的方法具有较大的模型，因而相对耗时。为了克服这一问题，我们提出了一种单级时空视频超分辨率框架，该框架可以直接从输入的LR和LFR视频中重建HR慢动作视频序列。我们不象VFI模型那样重建丢失的LR中间帧，而是通过特征时间插值模块对捕获局部时间上下文的丢失LR帧的LR帧特征进行时间插值。在广泛使用的基准测试上的大量实验表明，该框架不仅在干净和有噪声的LR帧上实现了更好的定性和定量性能，而且比最新的两级网络快数倍。源代码在中发布https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.
<details>	<summary>英文摘要</summary>	In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na\"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 . </details>
<details>	<summary>注释</summary>	Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 69、BAM：一种轻量级高效的单图像超分辨率均衡注意机制
- [ ] BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution 
时间：2021年04月15日                         第一作者：Fanyi Wang                       [链接](https://arxiv.org/abs/2104.07566).                     
## 摘要：单图像超分辨率（SISR）是计算机视觉领域中最具挑战性的问题之一。在基于深度卷积神经网络的方法中，注意机制显示出巨大的潜力。然而，由于网络结构的多样性，SISR任务缺乏一种通用的注意机制。本文提出了一种轻量级、高效的平衡注意机制（BAM），该机制可广泛适用于不同的SISR网络。它由Avgpool通道注意模块（ACAM）和Maxpool空间注意模块（MSAM）组成。这两个模块是并联的，以尽量减少误差积累和串扰。为了减少冗余信息对注意力产生的不良影响，我们仅将Avgpool应用于通道注意，因为Maxpool可以在空间维度上提取特征图中的虚幻极值点，我们只将Maxpool应用于空间注意，因为通道维度上的有用特征通常以最大值的形式存在于SISR任务中。为了验证BAM的有效性和鲁棒性，我们将其应用于12个最先进的SISR网络，其中8个没有注意，因此我们插入了BAM，4个有注意，因此我们用BAM替换了原有的注意模块。我们在Set5、Set14和BSD100基准数据集上进行了实验，其标度因子为x2、x3和x4。实验结果表明，BAM可以普遍提高网络性能。此外，我们还进行了烧蚀实验来证明BAM的极简性。结果表明，BAM的并行结构能够更好地平衡信道和空间注意，从而优于传统卷积块注意模块（CBAM）的串行结构。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) is one of the most challenging problems in the field of computer vision. Among the deep convolutional neural network based methods, attention mechanism has shown the enormous potential. However, due to the diverse network architectures, there is a lack of a universal attention mechanism for the SISR task. In this paper, we propose a lightweight and efficient Balanced Attention Mechanism (BAM), which can be generally applicable for different SISR networks. It consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM). These two modules are connected in parallel to minimize the error accumulation and the crosstalk. To reduce the undesirable effect of redundant information on the attention generation, we only apply Avgpool for channel attention because Maxpool could pick up the illusive extreme points in the feature map across the spatial dimensions, and we only apply Maxpool for spatial attention because the useful features along the channel dimension usually exist in the form of maximum values for SISR task. To verify the efficiency and robustness of BAM, we apply it to 12 state-of-the-art SISR networks, among which eight were without attention thus we plug BAM in and four were with attention thus we replace its original attention module with BAM. We experiment on Set5, Set14 and BSD100 benchmark datasets with the scale factor of x2 , x3 and x4 . The results demonstrate that BAM can generally improve the network performance. Moreover, we conduct the ablation experiments to prove the minimalism of BAM. Our results show that the parallel structure of BAM can better balance channel and spatial attentions, thus outperforming the series structure of prior Convolutional Block Attention Module (CBAM). </details>
<details>	<summary>注释</summary>	13 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 68、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年04月15日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
## 摘要：我们提出了SR3，一种通过重复细化实现图像超分辨率的方法。SR3采用去噪扩散概率模型生成条件图像，并通过随机去噪过程进行超分辨率处理。推理从纯高斯噪声开始，并使用在不同噪声水平下进行去噪训练的U网络模型迭代地细化噪声输出。SR3在不同放大倍数的超分辨率任务、人脸和自然图像上表现出很强的性能。我们在CelebA HQ上对标准8X人脸超分辨率任务进行了人体评估，并与SOTA-GAN方法进行了比较。SR3实现了接近50%的傻瓜率，这表明照片逼真的输出，而GANs不超过34%的傻瓜率。我们进一步证明了SR3在级联图像生成中的有效性，其中生成模型与超分辨率模型相链接，在ImageNet上产生了11.3的竞争性FID分数。
<details>	<summary>英文摘要</summary>	We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 67、用于制导深度图超分辨率的离散余弦变换网络
- [ ] Discrete Cosine Transform Network for Guided Depth Map Super-Resolution 
时间：2021年04月14日                         第一作者：Zixiang Zhao                       [链接](https://arxiv.org/abs/2104.06977).                     
## 摘要：制导深度超分辨率（GDSR）是多模图像处理中的一个热点问题。目标是使用高分辨率（HR）RGB图像提供关于边缘和对象轮廓的额外信息，以便低分辨率深度贴图可以向上采样到HR贴图。针对现有方法中存在的RGB纹理过度传输、跨模态特征提取困难、模块工作机制不明确等问题，提出了一种改进的离散余弦变换网络（DCTNet）。首先，将成对的RGB/深度图像输入到半耦合特征提取模块。共享卷积核分别提取跨模态的公共特征，私有核分别提取各自的独特特征。然后将RGB特征输入到边缘注意机制中，以突出显示对上采样有用的边缘。随后，在离散余弦变换（DCT）模块中，采用DCT来解决图像域GDSR的优化问题。将该方法推广到多通道RGB/depth特征上采样，提高了DCTNet的合理性，比传统方法更灵活有效。最终的深度预测由重建模块输出。大量的定性和定量实验证明了该方法的有效性，它可以生成准确的HR深度图，超过了现有的方法。同时，通过烧蚀实验验证了模块设计的合理性。
<details>	<summary>英文摘要</summary>	Guided depth super-resolution (GDSR) is a hot topic in multi-modal image processing. The goal is to use high-resolution (HR) RGB images to provide extra information on edges and object contours, so that low-resolution depth maps can be upsampled to HR ones. To solve the issues of RGB texture over-transferred, cross-modal feature extraction difficulty and unclear working mechanism of modules in existing methods, we propose an advanced Discrete Cosine Transform Network (DCTNet), which is composed of four components. Firstly, the paired RGB/depth images are input into the semi-coupled feature extraction module. The shared convolution kernels extract the cross-modal common features, and the private kernels extract their unique features, respectively. Then the RGB features are input into the edge attention mechanism to highlight the edges useful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT) module, where DCT is employed to solve the optimization problem designed for image domain GDSR. The solution is then extended to implement the multi-channel RGB/depth features upsampling, which increases the rationality of DCTNet, and is more flexible and effective than conventional methods. The final depth prediction is output by the reconstruction module. Numerous qualitative and quantitative experiments demonstrate the effectiveness of our method, which can generate accurate and HR depth maps, surpassing state-of-the-art methods. Meanwhile, the rationality of modules is also proved by ablation experiments. </details>
<details>	<summary>邮件日期</summary>	2021年04月15日</details>

# 66、SRR-Net：一种高分辨率MR图像的超分辨率重建方法
- [ ] SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging 
时间：2021年04月13日                         第一作者：Wenqi Huang                       [链接](https://arxiv.org/abs/2104.05901).                     
## 摘要：提高磁共振成像（MRI）的图像分辨率和采集速度是一个具有挑战性的问题。主要有两种策略来处理速度-分辨率的折衷：（1）$k$空间欠采样和高分辨率采集；（2）低分辨率图像重建和图像超分辨率流水线。然而，这些方法要么在某些高加速因子下性能有限，要么存在两级结构的误差积累。本文将MR重建和图像超分辨率的思想结合起来，直接从低分辨率的$k$空间采样数据中恢复HR图像。特别地，将SR重建问题描述为一个变分问题，并提出了一种从求解算法中展开的可学习网络。为了提高细节细化性能，引入了鉴别器。在体HR多线圈脑数据的实验结果表明，该SRR网络能够恢复高分辨率的脑图像，具有良好的视觉质量和感知质量。
<details>	<summary>英文摘要</summary>	Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 65、走向快速准确的真实世界深度超分辨率：基准数据集和基线
- [ ] Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline 
时间：2021年04月13日                         第一作者：Lingzhi He                       [链接](https://arxiv.org/abs/2104.06174).                     
## 摘要：商业深度传感器获取的深度图分辨率较低，难以用于各种计算机视觉任务。因此，深度图超分辨率（SR）是一项实用而有价值的工作，它可以将深度图提升到高分辨率（HR）空间。然而，由于缺乏真实世界的成对低分辨率（LR）和HR深度图，现有的方法大多采用降采样来获得成对的训练样本。为此，我们首先构建了一个名为RGB-D-D的大规模数据集，它可以极大地促进深度图SR的研究，甚至可以促进更多与深度相关的实际任务。数据集中的“D-D”表示从手机和Lucid Helios分别捕获的成对LR和HR深度图，范围从室内场景到具有挑战性的室外场景。此外，我们提供了一个快速的深度图超分辨率（FDSR）基线，其中高频分量从RGB图像自适应分解来指导深度图超分辨率。在现有公共数据集上的大量实验证明了我们的网络与现有方法相比的有效性和效率。此外，对于真实的LR深度图，我们的算法可以生成更精确的HR深度图，边界更清晰，并且在一定程度上修正了深度值误差。
<details>	<summary>英文摘要</summary>	Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which upscales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use downsampling to obtain paired training samples. To this end, we first construct a large-scale dataset named "RGB-D-D", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The "D-D" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 64、混叠是你的盟友：从原始图像突发端到端的超分辨率
- [ ] Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年04月13日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
## 摘要：本演示解决了从空间和时间上稍微不同的视点捕获的多个低分辨率快照重建高分辨率图像的问题。解决这个问题的关键挑战包括（i）以亚像素精度对齐输入图片，（ii）处理原始（噪声）图像以最大程度地忠实于本地相机数据，以及（iii）设计/学习非常适合该任务的图像先验（正则化器）。基于Wronski等人的见解，我们采用一种混合算法来解决这三个难题。在这种情况下，混叠是一个盟友，参数可以端到端地学习，同时保留了反问题经典方法的可解释性。我们的方法在合成和真实图像突发上的有效性得到了证明，在几个基准上建立了一个新的技术状态，并在智能手机和prosumer相机捕获的真实原始突发上提供了极好的定性结果。
<details>	<summary>英文摘要</summary>	This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 63、利用低分辨率流和掩模上采样实现高效的时空视频超分辨率
- [ ] Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling 
时间：2021年04月12日                         第一作者：Saikat Dutta                       [链接](https://arxiv.org/abs/2104.05778).                     
## 摘要：针对低分辨率、低帧速率的慢动作视频，提出了一种高效的时空超分辨率解决方案。一个简单的解决方案是连续运行视频超分辨率和视频帧插值模型。然而，这类解的内存效率低，推理时间长，不能充分利用时空关系的性质。为此，我们首先利用二次模型在LR空间中进行插值。使用最先进的视频超分辨率方法对输入LR帧进行超分辨率。利用双线性上采样技术，在HR空间重用用于合成LR插值帧的流程图和混合模板。这导致HR中间帧的粗略估计，该中间帧通常包含沿运动边界的伪影。通过残差学习，利用细化网络提高HR中间帧的质量。我们的模型是轻量级的，在REDS-STSR验证集中的性能比目前最先进的模型要好。
<details>	<summary>英文摘要</summary>	This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set. </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Project page: https://github.com/saikatdutta/FMU_STSR </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 62、基于深度学习的超分辨率网络边缘感知图像压缩
- [ ] Edge-Aware Image Compression using Deep Learning-based Super-resolution Network 
时间：2021年04月11日                         第一作者：Dipti Mishra                       [链接](https://arxiv.org/abs/2104.04926).                     
## 摘要：我们提出了一种基于学习的压缩方案，在预处理和后处理的深度cnn之间封装一个标准的编解码器。具体地说，我们通过引入：（a）一个边缘感知损失函数来防止在以前的工作中经常出现的模糊&（b）一个用于后处理的超分辨率卷积神经网络（CNN）以及一个相应的预处理网络，来展示对使用压缩-解压缩网络的先前方法的改进在低速率下提高率失真性能。该算法在从低分辨率到高分辨率的各种数据集上进行评估，即Set 5、Set 7、Classic 5、Set 14、Live 1、Kodak、General 100、CLIC 2019。与JPEG、JPEG2000、BPG和最近的CNN方法相比，该算法在低码率和高码率下的峰值信噪比分别提高了20.75%、8.47%、3.22%、3.23%和24.59%、14.46%、10.14%和8.57%。同样，在低比特率和高比特率下，MS-SSIM的这种改进分别约为71.43%、50%、36.36%、23.08%、64.70%和64.47%、61.29%、47.06%、51.52%和16.28%。使用CLIC 2019数据集，在低比特率和高比特率下，PSNR分别约为16.67%、10.53%、6.78%和24.62%、17.39%和14.08%，优于JPEG2000、BPG和最近的CNN方法。同样，与相同的方法相比，MS-SSIM在低比特率和高比特率下的性能分别约为72%、45.45%、39.13%、18.52%和71.43%、50%、41.18%和17.07%。其他数据集也实现了类似的改进。
<details>	<summary>英文摘要</summary>	We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%, 61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%, 10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%, and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively, compared to the same approaches. A similar type of improvement is achieved with other datasets also. </details>
<details>	<summary>注释</summary>	13 pages, 9 figures, 16 tables </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 61、CoPE：使用多项式展开的条件图像生成
- [ ] CoPE: Conditional image generation using Polynomial Expansions 
时间：2021年04月11日                         第一作者：Grigorios G Chrysos                       [链接](https://arxiv.org/abs/2104.05077).                     
## 摘要：生成建模已经发展成为机器学习的一个重要领域。深度多项式神经网络（PNNs）在无监督图像生成中取得了令人印象深刻的结果，其任务是将输入向量（即噪声）映射到合成图像。然而，PNNs的成功还没有在超分辨率等条件生成任务中得到推广。现有的pnn主要集中在单变量多项式展开上，对于两个变量的输入，即噪声变量和条件变量，pnn表现不好。在这项工作中，我们引入了一个通用的框架，称为CoPE，它可以对两个输入变量进行多项式展开，并捕捉它们的自相关和互相关。我们展示了CoPE如何被简单地扩充以接受任意数量的输入变量。CoPE分为五个任务（类条件生成、反问题、边缘到图像的转换、图像到图像的转换、属性引导生成），涉及八个数据集。全面评估表明，CoPE可用于处理各种条件生成任务。
<details>	<summary>英文摘要</summary>	Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 60、作物类型语义切分的语境自对比预训练
- [ ] Context-self contrastive pretraining for crop type semantic segmentation 
时间：2021年04月09日                         第一作者：Michail Tarasiou                       [链接](https://arxiv.org/abs/2104.04310).                     
## 摘要：本文提出了一种基于对比学习的全监督预训练方案，特别适合于密集分类任务。提出的上下文自对比丢失（CSCL）算法利用训练样本中每个位置与其局部上下文之间的相似性度量，学习一个使语义边界弹出的嵌入空间。对于卫星图像中作物类型的语义分割，我们发现包裹边界的性能是一个关键的瓶颈，并解释了CSCL如何解决该问题的根本原因，从而提高该任务的最新性能。此外，利用Sentinel-2（S2）卫星任务的图像，我们编制了据我们所知最大的卫星图像时间序列数据集，这些数据集由作物类型和包裹标识密集标注，我们与数据生成管道一起公开。利用这些数据，我们发现CSCL，即使在最小的预训练下，也可以改善所有的基线，并提出了一个超分辨率的语义分割过程，以获得更细粒度的作物类。该方法在二维和三维立体图像的语义分割任务中得到了进一步的验证，结果表明，该方法在竞争性基线下的性能得到了一致的提高。
<details>	<summary>英文摘要</summary>	In this paper we propose a fully-supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in an training sample and its local context. For crop type semantic segmentation from satellite images we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, dataset of satellite image timeseries densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The proposed method is further validated on the task of semantic segmentation on 2D and 3D volumetric images showing consistent performance improvements upon competitive baselines. </details>
<details>	<summary>注释</summary>	11 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 59、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月09日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 58、加法器：迈向节能图像超分辨率
- [ ] AdderSR: Towards Energy Efficient Image Super-Resolution 
时间：2021年04月09日                         第一作者：Dehua Song                       [链接](https://arxiv.org/abs/2009.08891).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 57、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月08日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
## 摘要：虽然单图像超分辨率（single-image super-resolution，SISR）方法在单次退化方面取得了很大的成功，但在实际应用中仍存在性能下降和多重退化的问题。近年来，人们对多重降解的盲模型和非盲模型进行了研究。然而，由于训练数据和测试数据之间的分布变化，这些方法通常会显著降低性能。为此，我们首次提出了一个条件元网络框架（CMDSR），帮助SR框架学习如何适应输入分布的变化。我们利用所提出的条件网在任务级提取退化先验，以适应基本SR网络（BaseNet）的参数。具体地说，我们框架的ConditionNet首先从一个支持集学习退化先验知识，该支持集由来自同一任务的一系列退化图像块组成。然后自适应基网根据条件特征快速地改变其参数。此外，为了更好地提取退化先验信息，我们提出了一种任务对比损失的方法来减小任务内部的距离，增加任务级特征之间的跨任务距离。在不预先定义退化映射的情况下，我们的盲框架可以进行单参数更新，从而产生可观的SR结果。大量的实验证明了CMDSR在各种盲甚至非盲方法中的有效性。灵活的基本网结构也表明CMDSR可以作为一个大系列SISR模型的通用框架。
<details>	<summary>英文摘要</summary>	Although single-image super-resolution (SISR) methods have achieved great success on single degradation, they still suffer performance drop with multiple degrading effects in real scenarios. Recently, some blind and non-blind models for multiple degradations have been explored. However, those methods usually degrade significantly for distribution shifts between the training and test data. Towards this end, we propose a conditional meta-network framework (named CMDSR) for the first time, which helps SR framework learn how to adapt to changes in input distribution. We extract degradation prior at task-level with the proposed ConditionNet, which will be used to adapt the parameters of the basic SR network (BaseNet). Specifically, the ConditionNet of our framework first learns the degradation prior from a support set, which is composed of a series of degraded image patches from the same task. Then the adaptive BaseNet rapidly shifts its parameters according to the conditional features. Moreover, in order to better extract degradation prior, we propose a task contrastive loss to decrease the inner-task distance and increase the cross-task distance between task-level features. Without predefining degradation maps, our blind framework can conduct one single parameter update to yield considerable SR results. Extensive experiments demonstrate the effectiveness of CMDSR over various blind, even non-blind methods. The flexible BaseNet structure also reveals that CMDSR can be a general framework for large series of SISR models. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 56、太阳电池检测的联合超分辨与校正
- [ ] Joint Super-Resolution and Rectification for Solar Cell Inspection 
时间：2021年04月07日                         第一作者：Mathis Hoffmann                       [链接](https://arxiv.org/abs/2011.05003).                     
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 55、BasicVSR：寻找视频超分辨率及更高分辨率的关键组件
- [ ] BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond 
时间：2021年04月07日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2012.02181).                     
<details>	<summary>注释</summary>	CVPR 2021 camera-ready </details>
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 54、基于内容自适应超分辨率的高效视频压缩
- [ ] Efficient Video Compression via Content-Adaptive Super-Resolution 
时间：2021年04月06日                         第一作者：Mehrdad Khani                       [链接](https://arxiv.org/abs/2104.02322).                     
## 摘要：视频压缩是互联网视频传输的重要组成部分。最近的研究表明，深度学习技术可以与人类设计的算法相媲美或优于人类设计的算法，但这些方法的计算效率和功耗明显低于现有的编解码器。本文提出了一种新的方法，通过一个小的、内容自适应的超分辨率模型来增强现有的编解码器，从而显著提高视频质量。我们的方法，SRVC，将视频编码成两个比特流：（i）内容流，通过使用现有的编解码器压缩下采样的低分辨率视频产生，（ii）模型流，对为视频的短片段定制的轻量级超分辨率神经网络的周期性更新进行编码。SRVC通过将解压缩后的低分辨率视频帧通过（时变）超分辨率模型来重构高分辨率视频帧，从而对视频进行解码。我们的结果表明，为了获得相同的PSNR，SRVC在慢模式下需要H.265每像素16%的比特，而在DVC（一种新的基于深度学习的视频压缩方案）中需要2%的每像素比特。SRVC在nvidiav100gpu上以每秒90帧的速度运行。
<details>	<summary>英文摘要</summary>	Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 53、超分辨率的测试时间调整：你只需要在更多的图像上过度调整
- [ ] Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a Few More Images 
时间：2021年04月06日                         第一作者：Mohammad Saeed Rad                       [链接](https://arxiv.org/abs/2104.02663).                     
## 摘要：现有的基于参考（RF）的超分辨率（SR）模型试图在高分辨率RF图像与低分辨率（LR）输入匹配的假设下提高SR的感知质量。由于射频图像在内容、颜色、对比度等方面应与测试图像相似，这妨碍了其在实际场景中的适用性。其他提高图像感知质量的方法，包括感知损失和对抗损失，往往通过显著降低PSNR/SSIM来显著降低对地面真实感的保真度。针对这两个问题，我们提出了一种简单而通用的方法，通过进一步微调训练数据集中具有与初始HR预测相似激活模式的图像子集上的SR网络，来提高预先训练的SR网络对给定LR输入的HR预测的感知质量，关于特征提取器的过滤器。特别地，我们从感知质量和PSNR/SSIM值方面展示了微调对这些图像的影响。与感知驱动的方法相反，我们证明了微调网络产生的HR预测具有更高的感知质量和相对于初始HR预测的PSNR/SSIM的最小变化。此外，我们还提出了有关随机共振网络滤波器的新的数值实验，通过滤波器相关性，我们表明，与基线网络或随机图像上微调的网络相比，我们方法中微调网络的滤波器更接近“理想”滤波器。
<details>	<summary>英文摘要</summary>	Existing reference (RF)-based super-resolution (SR) models try to improve perceptual quality in SR under the assumption of the availability of high-resolution RF images paired with low-resolution (LR) inputs at testing. As the RF images should be similar in terms of content, colors, contrast, etc. to the test image, this hinders the applicability in a real scenario. Other approaches to increase the perceptual quality of images, including perceptual loss and adversarial losses, tend to dramatically decrease fidelity to the ground-truth through significant decreases in PSNR/SSIM. Addressing both issues, we propose a simple yet universal approach to improve the perceptual quality of the HR prediction from a pre-trained SR network on a given LR input by further fine-tuning the SR network on a subset of images from the training dataset with similar patterns of activation as the initial HR prediction, with respect to the filters of a feature extractor. In particular, we show the effects of fine-tuning on these images in terms of the perceptual quality and PSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate that the fine-tuned network produces a HR prediction with both greater perceptual quality and minimal changes to the PSNR/SSIM with respect to the initial HR prediction. Further, we present novel numerical experiments concerning the filters of SR networks, where we show through filter correlation, that the filters of the fine-tuned network from our method are closer to "ideal" filters, than those of the baseline network or a network fine-tuned on random images. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 52、深脉冲超分辨率
- [ ] Deep Burst Super-Resolution 
时间：2021年04月06日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2101.10997).                     
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 51、基于注意的层次多模态融合高分辨率深度图成像
- [ ] High-resolution Depth Maps Imaging via Attention-based Hierarchical Multi-modal Fusion 
时间：2021年04月04日                         第一作者：Zhiwei Zhong                       [链接](https://arxiv.org/abs/2104.01530).                     
## 摘要：深度图记录了场景中视点和物体之间的距离，在许多实际应用中起着至关重要的作用。然而，消费者级RGB-D相机拍摄的深度图空间分辨率较低。引导深度图超分辨率（DSR）是解决这一问题的一种常用方法，它试图从输入的低分辨率（LR）深度及其作为引导的耦合HR-RGB图像恢复高分辨率（HR）深度图。如何正确地选择和传播一致性结构，正确地处理不一致性结构是指导DSR最具挑战性的问题。本文提出了一种新的基于注意的分层多模态融合（AHMF）网络。具体来说，为了有效地从LR深度和HR引导中提取和组合相关信息，我们提出了一种分层卷积层的多模式基于注意的融合（MMAF）策略，包括一个特征增强块，用于选择有价值的特征；一个特征重新校准块，用于统一具有不同外观特征的模式的相似性度量。在此基础上，提出了一种双向分层特征协作（BHFC）模型，充分利用多尺度特征间的低层空间信息和高层结构信息。实验结果表明，该方法在重建精度、运行速度和存储效率等方面均优于现有方法。
<details>	<summary>英文摘要</summary>	Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance. The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 50、具有光谱混合和异构数据集的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution with Spectral Mixup and Heterogeneous Datasets 
时间：2021年04月03日                         第一作者：Ke Li                       [链接](https://arxiv.org/abs/2101.07589).                     
<details>	<summary>注释</summary>	16 pages, 14 tables, 5 figures; Code available at https://github.com/kli8996/HSISR </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 49、用于学习失调光学变焦的平方变形对准网络
- [ ] SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom 
时间：2021年04月02日                         第一作者：Kangfu Mei                       [链接](https://arxiv.org/abs/2104.00848).                     
## 摘要：基于深度神经网络（DNN）的超分辨率算法大大提高了生成图像的质量。然而，由于学习失调光学变焦的困难，这些算法在处理真实世界的超分辨率问题时往往会产生明显的伪影。为了解决这一问题，本文提出了一种平方变形对准网络（SDAN）。我们的网络学习卷积核的每点平方偏移量，然后根据偏移量在校正的卷积窗口中对齐特征。因此，通过提取对齐的特征，可以最大限度地减少不对齐。与普通可变形卷积网络（DCN）中的逐点偏移不同，本文提出的平方偏移不仅加快了偏移学习，而且在参数较少的情况下提高了生成质量。此外，我们进一步提出一个有效的交叉堆积注意层来提高学习偏移量的准确性。它利用打包和解包操作来扩大偏移量学习的接受域，增强低分辨率图像与参考图像之间空间联系的提取能力。综合实验表明，该方法在计算效率和真实感细节方面均优于其他先进方法。
<details>	<summary>英文摘要</summary>	Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details. </details>
<details>	<summary>注释</summary>	ICME21. Code is available at https://github.com/MKFMIKU/SDAN </details>
<details>	<summary>邮件日期</summary>	2021年04月05日</details>

# 48、盲超分辨的无监督退化表示学习
- [ ] Unsupervised Degradation Representation Learning for Blind Super-Resolution 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2104.00416).                     
## 摘要：大多数现有的基于CNN的超分辨率（SR）方法都是在假设退化是固定的和已知的（例如双三次下采样）的基础上发展起来的。然而，当实际性能下降与假设不同时，这些方法的性能会严重下降。在实际应用中，为了处理各种未知的退化，以往的方法都是依靠退化估计来重建SR图像。然而，退化估计方法通常是耗时的，并且可能由于较大的估计误差而导致SR失效。本文提出了一种无监督退化表示学习方案，用于盲随机共振，无需显式退化估计。具体来说，我们学习抽象表示来区分表示空间中的各种退化，而不是像素空间中的显式估计。此外，我们还提出了一种基于学习表示的退化感知SR（DASR）网络，该网络能够灵活地适应各种退化。实验结果表明，我们的退化表征学习方法可以提取有区别的表征来获得精确的退化信息。在合成图像和真实图像上的实验表明，我们的网络对于盲SR任务达到了最先进的性能。代码位于：https://github.com/LongguangWang/DASR。
<details>	<summary>英文摘要</summary>	Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To handle various unknown degradations in real-world applications, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation estimation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this paper, we propose an unsupervised degradation representation learning scheme for blind SR without explicit degradation estimation. Specifically, we learn abstract representations to distinguish various degradations in the representation space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 47、探索图像超分辨率中的稀疏性实现高效推理
- [ ] Exploring Sparsity in Image Super-Resolution for Efficient Inference 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2006.09603).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 46、通过学习匹配内部斑块分布来估计MR切片轮廓
- [ ] MR Slice Profile Estimation by Learning to Match Internal Patch Distributions 
时间：2021年03月31日                         第一作者：Shuo Han                       [链接](https://arxiv.org/abs/2104.00100).                     
## 摘要：为了超分辨多层面二维磁共振（MR）图像的通平面方向，在训练监督算法时，可以将其切片选择剖面作为高分辨率（HR）到低分辨率（LR）的退化模型来生成成对数据。现有的超分辨率算法对切片选择轮廓进行了假设，因为给定的图像不容易知道它。在这项工作中，我们通过学习匹配其内部补丁分布来估计给定特定图像的切片选择剖面。具体来说，我们假设在应用正确的切片选择轮廓之后，沿着HR平面内方向的图像面片分布应该与沿着LR平面内方向的分布相匹配。因此，我们将切片选择轮廓的估计作为生成对抗网络（GAN）中学习生成器的一部分。这样，就可以在没有任何外部数据的情况下学习切片选择轮廓。我们的算法通过各向同性MR图像的模拟进行了测试，并将其与一种通过平面的超分辨率算法结合起来，以证明其优越性，同时也被用作测量图像分辨率的工具。我们的密码是https://github.com/shuohan/espreso2。
<details>	<summary>英文摘要</summary>	To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, accepted by Information Processing in Medical Imaging (IPMI) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 45、视频探索通过视频特定的自动编码器
- [ ] Video Exploration via Video-Specific Autoencoders 
时间：2021年03月31日                         第一作者：Kevin Wang                        [链接](https://arxiv.org/abs/2103.17261).                     
## 摘要：我们提出了简单的视频特定的自动编码器，使人类可控的视频探索。这包括各种各样的分析任务，例如（但不限于）空间和时间超分辨率、空间和时间编辑、对象移除、视频纹理、平均视频探索以及视频内部和跨视频的对应估计。以前的工作已经独立地研究了这些问题，并提出了不同的公式。在这项工作中，我们观察到一个简单的自动编码器训练（从头开始）对多个帧的特定视频，使一个人能够执行各种各样的视频处理和编辑任务。我们的任务是通过两个关键的观察来实现的：（1）由自动编码器学习的潜在代码捕获视频的空间和时间特性；（2）自动编码器可以将样本输入投射到视频特定的流形上。例如：（1）内插潜在代码实现了时间超分辨率和用户可控的视频纹理；（2）流形重投影实现了空间超分辨率、对象移除和去噪，而无需对任何任务进行训练。重要的是，通过主成分分析的潜在代码的二维可视化可以作为用户可视化和直观控制视频编辑的工具。最后，我们将我们的方法与现有技术进行定量对比，发现在没有任何监督和任务特定知识的情况下，我们的方法可以与专门为任务训练的监督方法进行比较。
<details>	<summary>英文摘要</summary>	We present simple video-specific autoencoders that enables human-controllable video exploration. This includes a wide variety of analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, object removal, video textures, average video exploration, and correspondence estimation within and across videos. Prior work has independently looked at each of these problems and proposed different formulations. In this work, we observe that a simple autoencoder trained (from scratch) on multiple frames of a specific video enables one to perform a large variety of video processing and editing tasks. Our tasks are enabled by two key observations: (1) latent codes learned by the autoencoder capture spatial and temporal properties of that video and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. For e.g. (1) interpolating latent codes enables temporal super-resolution and user-controllable video textures; (2) manifold reprojection enables spatial super-resolution, object removal, and denoising without training for any of the tasks. Importantly, a two-dimensional visualization of latent codes via principal component analysis acts as a tool for users to both visualize and intuitively control video edits. Finally, we quantitatively contrast our approach with the prior art and found that without any supervision and task-specific knowledge, our approach can perform comparably to supervised approaches specifically trained for a task. </details>
<details>	<summary>注释</summary>	Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/ </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 44、用于人脸超分辨率的边缘和身份保持网络
- [ ] Edge and Identity Preserving Network for Face Super-Resolution 
时间：2021年03月31日                         第一作者：Jonghyun Kim                       [链接](https://arxiv.org/abs/2008.11977).                     
<details>	<summary>注释</summary>	Neurocomputing'2021 DOI: 10.1016/j.neucom.2021.03.048 </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 43、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月31日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	The first two authors contributed equally to this work. Accepted to CVPR 2021 (camera-ready version) </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 42、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月30日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 41、传递学习：探索盲超分辨退化的传递性
- [ ] Transitive Learning: Exploring the Transitivity of Degradations for Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2103.15290).                     
## 摘要：现有的盲超分辨率（SR）方法由于对数据或模型的迭代估计和校正的依赖性，通常耗时且效率较低。针对这一问题，本文提出了一种基于端到端网络的盲随机共振传递学习方法。首先，我们分析并论证了退化的传递性，包括广泛使用的加法退化和卷积退化。在此基础上，提出了一种新的传递学习方法，通过自适应地推导传递变换函数来求解未知退化问题，而不需要任何迭代操作。具体而言，端到端TLSR网络由传递度（DoT）估计网络、齐次特征提取网络和传递学习模块组成。对盲SR任务的定量和定性评价表明，与现有的盲SR方法相比，本文提出的TLSR具有更好的性能和更少的时间消耗。代码可在https://github.com/YuanfeiHuang/TLSR。
<details>	<summary>英文摘要</summary>	Being extremely dependent on the iterative estimation and correction of data or models, the existing blind super-resolution (SR) methods are generally time-consuming and less effective. To address it, this paper proposes a transitive learning method for blind SR using an end-to-end network without any additional iterations in inference. To begin with, we analyze and demonstrate the transitivity of degradations, including the widely used additive and convolutive degradations. We then propose a novel Transitive Learning method for blind Super-Resolution on transitive degradations (TLSR), by adaptively inferring a transitive transformation function to solve the unknown degradations without any iterative operations in inference. Specifically, the end-to-end TLSR network consists of a degree of transitivity (DoT) estimation network, a homogeneous feature extraction network, and a transitive learning module. Quantitative and qualitative evaluations on blind SR tasks demonstrate that the proposed TLSR achieves superior performance and consumes less time against the state-of-the-art blind SR methods. The code is available at https://github.com/YuanfeiHuang/TLSR. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 40、高细节图像超分辨率的最佳伙伴
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年03月29日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
## 摘要：我们考虑了单图像超分辨率（SISR）问题，即基于低分辨率（LR）输入生成高分辨率（HR）图像。近年来，生成性对抗网络（generativediscountarialnetworks，GANs）开始流行于制造幻觉。沿着这条路线的大多数方法依赖于预定义的单个LR-single-HR映射，这对于SISR任务来说不够灵活。而且，甘生成的虚假细节往往会破坏整个图像的真实感。我们通过为丰富的细节SISR提出bestbuddygan（bebygan）来解决这些问题。放松了不可变的一对一约束，使得估计出的补丁在训练过程中动态地寻求最佳监督，有利于产生更合理的细节。此外，我们提出了一种区域感知的对抗式学习策略，使我们的模型能够自适应地生成纹理区域的细节。大量的实验证明了我们方法的有效性。同时还构建了一个超高分辨率4K数据集，为今后的超分辨率研究提供了便利。
<details>	<summary>英文摘要</summary>	We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the immutable one-to-one constraint, we allow the estimated patches to dynamically seek the best supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 39、全知视频超分辨率
- [ ] Omniscient Video Super-Resolution 
时间：2021年03月29日                         第一作者：Peng Yi                        [链接](https://arxiv.org/abs/2103.15683).                     
## 摘要：最新的视频超分辨率（SR）方法要么采用迭代的方式来处理来自时间滑动窗口的低分辨率（LR）帧，要么利用先前估计的SR输出来帮助循环地重构当前帧。一些研究试图将这两种结构结合起来形成一个混合框架，但未能充分发挥其作用。在本文中，我们提出了一个全知的框架，不仅可以利用前面的SR输出，还可以利用现在和将来的SR输出。全知框架更具通用性，因为迭代框架、循环框架和混合框架可以看作它的特例。提出的全知框架使得生成器比其他框架下的生成器表现得更好。在公共数据集上的大量实验表明，该方法在客观度量、主观视觉效果和复杂度方面均优于现有的方法。我们的代码将会公开。
<details>	<summary>英文摘要</summary>	Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity. Our code will be made public. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 38、交叉MPI：利用多平面图像实现图像超分辨率的交叉尺度立体成像
- [ ] Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images 
时间：2021年03月29日                         第一作者：Yuemei Zhou                       [链接](https://arxiv.org/abs/2011.14631).                     
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 37、基于隐式场景表示的现场场景标注与理解
- [ ] In-Place Scene Labelling and Understanding with Implicit Scene Representation 
时间：2021年03月29日                         第一作者：Shuaifeng Zhi                       [链接](https://arxiv.org/abs/2103.15875).                     
## 摘要：语义标注与几何和辐射重建高度相关，因为形状和外观相似的场景实体更可能来自相似的类。最近的隐式神经重建技术是有吸引力的，因为他们不需要事先训练数据，但同样的完全自我监督的方法是不可能的语义，因为标签是人类定义的属性。我们扩展了神经辐射场（NeRF）技术，将语义与外观和几何信息进行联合编码，从而使用少量的特定场景的就地标注就可以得到完整、准确的二维语义标注。NeRF内在的多视图一致性和平滑性使得稀疏标签能够有效地传播，从而有利于语义。我们展示了这种方法的好处，当标签是稀疏或非常嘈杂的房间规模的场景。在视觉语义映射系统中，我们展示了它在各种有趣的应用中的优势，如高效的场景标注工具、新颖的语义视图合成、标签去噪、超分辨率、标签插值和多视图语义标签融合。
<details>	<summary>英文摘要</summary>	Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems. </details>
<details>	<summary>注释</summary>	Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/ </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 36、基于流的核先验算法及其在盲超分辨中的应用
- [ ] Flow-based Kernel Prior with Application to Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2103.15977).                     
## 摘要：核估计通常是盲图像超分辨率（SR）的关键问题之一。最近，Double-DIP提出通过网络结构对核进行建模，KernelGAN则采用深度线性网络和一些正则化损失来约束核空间。然而，他们没有充分利用一般的SR核假设，即各向异性高斯核对图像SR是足够的。为了解决这个问题，本文提出了一种基于归一化流的核先验（FKP）核建模方法。通过学习各向异性高斯核分布和可处理的潜在分布之间的可逆映射，FKP可以很容易地代替双倾角和KernelGAN的核建模模块。具体地说，FKP在隐空间而不是网络参数空间中对核进行优化，从而生成合理的核初始化，遍历学习到的核流形，提高优化的稳定性。在合成图像和真实图像上的大量实验表明，所提出的FKP算法可以在较少的参数、运行时间和内存使用的情况下显著提高核估计精度，从而得到最新的盲SR结果。
<details>	<summary>英文摘要</summary>	Kernel estimation is generally one of the key problems for blind image super-resolution (SR). Recently, Double-DIP proposes to model the kernel via a network architecture prior, while KernelGAN employs the deep linear network and several regularization losses to constrain the kernel space. However, they fail to fully exploit the general SR kernel assumption that anisotropic Gaussian kernels are sufficient for image SR. To address this issue, this paper proposes a normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an invertible mapping between the anisotropic Gaussian kernel distribution and a tractable latent distribution, FKP can be easily used to replace the kernel modeling modules of Double-DIP and KernelGAN. Specifically, FKP optimizes the kernel in the latent space rather than the network parameter space, which allows it to generate reasonable kernel initialization, traverse the learned kernel manifold and improve the optimization stability. Extensive experiments on synthetic and real-world images demonstrate that the proposed FKP can significantly improve the kernel estimation accuracy with less parameters, runtime and memory usage, leading to state-of-the-art blind SR results. </details>
<details>	<summary>注释</summary>	Accepted by CVPR2021. Code: https://github.com/JingyunLiang/FKP </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 35、去噪去噪超分辨率流水线的再思考
- [ ] Rethinking the Pipeline of Demosaicing, Denoising and Super-Resolution 
时间：2021年03月29日                         第一作者：Guocheng Qian                        [链接](https://arxiv.org/abs/1905.02538).                     
<details>	<summary>注释</summary>	Code is available at: https://github.com/guochengqian/TENet </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 34、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年03月28日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 33、D2C-SR：一种从散度到收敛的图像超分辨率方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Image Super-Resolution 
时间：2021年03月26日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
## 摘要：本文提出了一种新的图像超分辨率（SR）框架D2C-SR。作为一个不适定问题，超分辨率相关任务的关键挑战是对于给定的低分辨率输入可以有多个预测。大多数经典的方法和早期的基于深度学习的方法忽略了这一基本事实，将这个问题建模为一个确定性的过程，这往往导致不满意的结果。受最近SRFlow等工作的启发，我们采用半概率的方法解决了这一问题，并提出了一种两阶段流水线：一个发散阶段用于学习离散形式的高分辨率输出的分布，然后一个收敛阶段用于将学习到的预测融合成最终的输出。更具体地说，我们提出了一种基于树结构的深度网络，其中每个分支被设计来学习可能的高分辨率预测。在发散阶段，对每个分支分别进行训练以拟合地面真值，并用三重损失来增强发散分支的输出。随后，我们添加一个保险丝模块来组合多个预测，因为第一阶段的输出可能是次优的。可以训练引信模块以端到端的方式将w.r.t收敛到最终的高分辨率图像。我们对几个基准进行了评估，包括一个新提出的具有8倍放大因子的数据集。我们的实验表明，D2C-SR可以在峰值信噪比和SSIM上实现最先进的性能，并且计算量显著减少。
<details>	<summary>英文摘要</summary>	In this paper, we present D2C-SR, a novel framework for the task of image super-resolution(SR). As an ill-posed problem, the key challenge for super-resolution related tasks is there can be multiple predictions for a given low-resolution input. Most classical methods and early deep learning based approaches ignored this fundamental fact and modeled this problem as a deterministic processing which often lead to unsatisfactory results. Inspired by recent works like SRFlow, we tackle this problem in a semi-probabilistic manner and propose a two-stage pipeline: a divergence stage is used to learn the distribution of underlying high-resolution outputs in a discrete form, and a convergence stage is followed to fuse the learned predictions into a final output. More specifically, we propose a tree-based structure deep network, where each branch is designed to learn a possible high-resolution prediction. At the divergence stage, each branch is trained separately to fit ground truth, and a triple loss is used to enforce the outputs from different branches divergent. Subsequently, we add a fuse module to combine the multiple predictions as the outputs from the first stage can be sub-optimal. The fuse module can be trained to converge w.r.t the final high-resolution image in an end-to-end manner. We conduct evaluations on several benchmarks, including a new proposed dataset with 8x upscaling factor. Our experiments demonstrate that D2C-SR can achieve state-of-the-art performance on PSNR and SSIM, with a significantly less computational cost. </details>
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 32、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月25日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
## 摘要：近五年来，深度卷积神经网络（CNNs）被广泛应用于低层视觉。根据不同应用的特点，设计合适的CNN结构。然而，定制的体系结构通过对所有像素点的等价处理来聚集不同的特征，从而提高了应用的性能，忽略了局部功率像素点的影响，导致训练效率低下。在本文中，我们提出了一个非对称CNN（ACNet）包括一个非对称块（AB），一个mem？图像超分辨率增强块（MEB）和高频特征增强块（HFFEB）。该算法利用一维非对称卷积，在水平方向和垂直方向上增强平方卷积核，以增强局部显著特征对SISR的影响。MEB通过残差学习（RL）技术融合AB的所有分层低频特征，以解决长期依赖问题，并转换得到的低频fea？转换成高频特性。HFFEB利用低频和高频特征来获得更健壮的超分辨率特征，并解决过多的特征增强问题。广告？另外，它还负责重建高分辨率（HR）图像。大量实验表明，该网络能有效地解决单图像超分辨率（SISR）、盲SISR和盲噪声的盲SISR问题。ACNet的代码如所示https://github.com/hellloxiaotian/ACNet。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks (CNNs) have been widely applied for low-level vision over the past five years. According to nature of different applications, designing appropriate CNN architectures is developed. However, customized architectures gather different features via treating all pixel points as equal to improve the performance of given application, which ignores the effects of local power pixel points and results in low training efficiency. In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric block (AB), a mem?ory enhancement block (MEB) and a high-frequency feature enhancement block (HFFEB) for image super-resolution. The AB utilizes one-dimensional asymmetric convolutions to intensify the square convolution kernels in horizontal and vertical directions for promoting the influences of local salient features for SISR. The MEB fuses all hierarchical low-frequency features from the AB via residual learning (RL) technique to resolve the long-term dependency problem and transforms obtained low-frequency fea?tures into high-frequency features. The HFFEB exploits low- and high-frequency features to obtain more robust super-resolution features and address excessive feature enhancement problem. Ad?ditionally, it also takes charge of reconstructing a high-resolution (HR) image. Extensive experiments show that our ACNet can effectively address single image super-resolution (SISR), blind SISR and blind SISR of blind noise problems. The code of the ACNet is shown at https://github.com/hellloxiaotian/ACNet. </details>
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 31、JDSR-GAN：构建联合协作的蒙面超分辨率学习网络
- [ ] JDSR-GAN: Constructing A Joint and Collaborative Learning Network for Masked Face Super-Resolution 
时间：2021年03月25日                         第一作者：Guangwei Gao                       [链接](https://arxiv.org/abs/2103.13676).                     
## 摘要：随着预防COVID-19病毒的重要性日益增强，在大多数视频监控场景中获得的人脸图像都是低分辨率的。然而，以往的人脸超分辨率算法大多不能在一个模型中同时处理两个任务。本文将遮罩遮挡视为图像噪声，构建了一个联合协作学习网络JDSR-GAN，用于遮罩人脸的超分辨率识别。给定一幅以掩模为输入的低质量人脸图像，由去噪模块和超分辨率模块组成的发生器的作用是获取高质量的高分辨率人脸图像。鉴别器利用了一些精心设计的损失函数来保证恢复的人脸图像的质量。此外，我们将身份信息和注意机制融入到我们的网络中，以实现可行的相关特征表达和信息性特征学习。通过联合进行去噪和人脸超分辨率处理，这两个任务可以相互补充，获得良好的性能。大量的定性和定量结果表明，我们提出的JDSR-GAN方法优于一些分别执行前两个任务的可比较方法。
<details>	<summary>英文摘要</summary>	With the growing importance of preventing the COVID-19 virus, face images obtained in most video surveillance scenarios are low resolution with mask simultaneously. However, most of the previous face super-resolution solutions can not handle both tasks in one model. In this work, we treat the mask occlusion as image noise and construct a joint and collaborative learning network, called JDSR-GAN, for the masked face super-resolution task. Given a low-quality face image with the mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some comparable methods which perform the previous two tasks separately. </details>
<details>	<summary>注释</summary>	24 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 30、噪声数据的多帧超分辨率分析
- [ ] Multi-frame Super-resolution from Noisy Data 
时间：2021年03月25日                         第一作者：Kireeti Bodduna                        [链接](https://arxiv.org/abs/2103.13778).                     
## 摘要：由于问题的病态性，从低分辨率数据中获得高分辨率图像在算法上具有挑战性。到目前为止，这类问题几乎没有得到解决，现有的一些方法使用了简单的正则化方法。我们证明了两种基于各向异性扩散思想的自适应正则化方法的有效性：除了评估经典的边缘增强各向异性扩散正则化方法外，我们还引入了一种新的非局部正则化方法。这被称为部门扩散。我们将其与经典超分辨率观测模型的所有六种变体结合起来，这些变体是由其三种扭曲、模糊和下采样算子的排列产生的。令人惊讶的是，在实际相关的噪声场景中进行的评估产生的排名与我们之前工作（SSVM 2017）中在无噪声环境中的排名不同。
<details>	<summary>英文摘要</summary>	Obtaining high resolution images from low resolution data with clipped noise is algorithmically challenging due to the ill-posed nature of the problem. So far such problems have hardly been tackled, and the few existing approaches use simplistic regularisers. We show the usefulness of two adaptive regularisers based on anisotropic diffusion ideas: Apart from evaluating the classical edge-enhancing anisotropic diffusion regulariser, we introduce a novel non-local one with one-sided differences and superior performance. It is termed sector diffusion. We combine it with all six variants of the classical super-resolution observational model that arise from permutations of its three operators for warping, blurring, and downsampling. Surprisingly, the evaluation in a practically relevant noisy scenario produces a different ranking than the one in the noise-free setting in our previous work (SSVM 2017). </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 29、一种实用的深盲图像超分辨率退化模型的设计
- [ ] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution 
时间：2021年03月25日                         第一作者：Kai Zhang                       [链接](https://arxiv.org/abs/2103.14006).                     
## 摘要：人们普遍认为，如果假设的退化模型与真实图像中的退化模型相背离，单图像超分辨率（SISR）方法将无法取得很好的效果。虽然有几种退化模型考虑了模糊等其他因素，但它们仍然不能有效地覆盖真实图像的各种退化。针对这一问题，本文提出了一种更为复杂但实用的退化模型，该模型由随机混洗模糊、下采样和噪声退化三部分组成。具体地说，模糊由两个具有各向同性和各向异性高斯核的卷积来逼近；下采样从最近点、双线性和双三次插值中随机选择；噪声由不同噪声级的高斯噪声叠加而成，采用不同质量因子的JPEG压缩，通过逆前向摄像机图像信号处理（ISP）流水线模型和原始图像噪声模型生成处理后的摄像机传感器噪声。为了验证新退化模型的有效性，我们训练了一个深度盲ESRGAN超分解器，并将其应用于不同退化程度的合成图像和真实图像的超分辨。实验结果表明，新的退化模型有助于提高深超旋转变压器的实用性，为实际SISR应用提供了一种强有力的替代方案。
<details>	<summary>英文摘要</summary>	It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ESRGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications. </details>
<details>	<summary>注释</summary>	Code: https://github.com/cszn/BSRGAN </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 28、基于物理激励下采样核的内窥镜零炮超分辨
- [ ] Zero-shot super-resolution with a physically-motivated downsampling kernel for endomicroscopy 
时间：2021年03月25日                         第一作者：Agnieszka Barbara Szczotka                       [链接](https://arxiv.org/abs/2103.14015).                     
## 摘要：随着卷积神经网络（CNNs）的发展，超分辨率（SR）方法得到了长足的发展。CNNs已被成功应用于提高内镜成像质量。然而，内窥镜下SR研究的固有局限性仍然是缺乏地面真实高分辨率（HR）图像，通常用于监督训练和基于参考的图像质量评估（IQA）。因此，替代方法，如无监督SR正在探索中。为了解决非参考图像质量改善的需要，我们设计了一种新的零炮超分辨率（ZSSR）方法，该方法仅依赖于内窥镜数据，不需要地面真实图像，而是以自我监督的方式进行处理。我们根据内窥镜的特殊性定制了建议的管道，引入了两种方法：一种物理激励的Voronoi降尺度核，用于解释内窥镜基于不规则纤维的采样模式和真实的噪声模式。我们还利用视频序列来开发一个图像序列，以提高自监督零拍图像的质量。我们进行了烧蚀研究，以评估我们在缩小核尺度和噪声模拟方面的贡献。我们在合成数据和原始数据上验证了我们的方法。合成实验用基于参考的IQA进行评估，而我们对原始图像的结果则在由专家和非专家观察者进行的用户研究中进行评估。结果表明，ZSSR重建的图像质量优于基线方法。与有监督的单幅图像SR相比，ZSSR具有很强的竞争力，尤其是作为专家们首选的重建技术。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) methods have seen significant advances thanks to the development of convolutional neural networks (CNNs). CNNs have been successfully employed to improve the quality of endomicroscopy imaging. Yet, the inherent limitation of research on SR in endomicroscopy remains the lack of ground truth high-resolution (HR) images, commonly used for both supervised training and reference-based image quality assessment (IQA). Therefore, alternative methods, such as unsupervised SR are being explored. To address the need for non-reference image quality improvement, we designed a novel zero-shot super-resolution (ZSSR) approach that relies only on the endomicroscopy data to be processed in a self-supervised manner without the need for ground-truth HR images. We tailored the proposed pipeline to the idiosyncrasies of endomicroscopy by introducing both: a physically-motivated Voronoi downscaling kernel accounting for the endomicroscope's irregular fibre-based sampling pattern, and realistic noise patterns. We also took advantage of video sequences to exploit a sequence of images for self-supervised zero-shot image quality improvement. We run ablation studies to assess our contribution in regards to the downscaling kernel and noise simulation. We validate our methodology on both synthetic and original data. Synthetic experiments were assessed with reference-based IQA, while our results for original images were evaluated in a user study conducted with both expert and non-expert observers. The results demonstrated superior performance in image quality of ZSSR reconstructions in comparison to the baseline method. The ZSSR is also competitive when compared to supervised single-image SR, especially being the preferred reconstruction technique by experts. </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 27、基于跨任务知识转移的单深度超分辨率场景结构引导学习
- [ ] Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for Single Depth Super-Resolution 
时间：2021年03月24日                         第一作者：Baoli Sun                       [链接](https://arxiv.org/abs/2103.12955).                     
## 摘要：现有的颜色引导深度超分辨率（DSR）方法需要成对的RGB-D数据作为训练样本，利用RGB图像的几何相似性作为结构引导来恢复退化的深度图。然而，在实际测试环境中，成对数据的收集可能有限或昂贵。因此，我们第一次探索在训练阶段学习跨模态知识，在训练阶段RGB和深度模态都可用，但在目标数据集上测试，只有单一的深度模态存在。我们的核心思想是在不改变网络结构的前提下，将场景结构制导知识从RGB模态提取到单个DSR任务。具体地说，我们构造了一个以RGB图像为输入的辅助深度估计（DE）任务来估计深度图，并协同训练DSR任务和DE任务来提高DSR的性能。在此基础上，提出了一个跨任务交互模块来实现双边跨任务知识转移。首先，我们设计了一个跨任务的提炼方案，鼓励DSR和DE网络以师生角色交换的方式相互学习。然后，我们提出了一个结构预测（SP）任务，该任务提供额外的结构正则化，以帮助DSR和DE网络学习更多信息的结构表示，以便进行深度恢复。大量实验表明，与其他DSR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 26、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年03月24日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
## 摘要：近年来，采用深度复杂卷积神经网络结构的单图像超分辨率（SISR）方法取得了良好的效果。然而，这些方法以较高的内存消耗为代价来提高性能，难以应用于存储和计算资源有限的移动设备。为了解决这个问题，我们提出了一个轻量级的多尺度特征交互网络（MSFIN）。对于轻量级SISR，MSFIN扩展了接收域，充分利用了低分辨率观测图像的信息特征，这些特征来自不同的尺度和交互连接。此外，我们还设计了一个轻量级的循环剩余信道注意块（RRCAB），使得网络能够在充分轻量级的同时受益于信道注意机制。在一些基准上的大量实验已经证实，我们提出的MSFIN可以通过更轻量化的模型实现与现有技术相当的性能。
<details>	<summary>英文摘要</summary>	Recently, the single image super-resolution (SISR) approaches with deep and complex convolutional neural network structures have achieved promising performance. However, those methods improve the performance at the cost of higher memory consumption, which is difficult to be applied for some mobile devices with limited storage and computing resources. To solve this problem, we present a lightweight multi-scale feature interaction network (MSFIN). For lightweight SISR, MSFIN expands the receptive field and adequately exploits the informative features of the low-resolution observed images from various scales and interactive connections. In addition, we design a lightweight recurrent residual channel attention block (RRCAB) so that the network can benefit from the channel attention mechanism while being sufficiently lightweight. Extensive experiments on some benchmarks have confirmed that our proposed MSFIN can achieve comparable performance against the state-of-the-arts with a more lightweight model. </details>
<details>	<summary>注释</summary>	Accepted by ICME2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 25、UltraSR：空间编码是基于隐式图像函数的任意尺度超分辨率的关键
- [ ] UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution 
时间：2021年03月23日                         第一作者：Xingqian Xu                       [链接](https://arxiv.org/abs/2103.12716).                     
## 摘要：NeRF和其他相关隐式神经表示方法的成功为连续图像表示开辟了一条新的途径，即不再需要从存储的离散二维阵列中查找像素值，而是可以从连续空间域上的神经网络模型中推断像素值。尽管最近的研究表明，这种新的方法在任意尺度的超分辨率任务中都能取得很好的效果，但由于高频纹理的错误预测，放大后的图像往往会出现结构性失真。在这项工作中，我们提出了一种简单而有效的基于隐式图像函数的新网络设计方法UltraSR，其中空间坐标和周期编码与隐式神经表示深度结合。我们通过大量的实验和研究表明，空间编码确实是下一阶段高精度隐式图像函数的关键。与以前最先进的方法相比，我们的UltraSR在DIV2K基准上设置了所有超分辨率标度下的最新性能。UltraSR在其他标准基准数据集上也取得了优异的性能，在几乎所有的实验中都优于以前的工作。我们的代码将在https://github.com/SHI-Labs/UltraSR-arbitral-Scale-Super-Resolution。
<details>	<summary>英文摘要</summary>	The recent success of NeRF and other related implicit neural representation methods has opened a new path for continuous image representation, where pixel values no longer need to be looked up from stored discrete 2D arrays but can be inferred from neural network models on a continuous spatial domain. Although the recent work LIIF has demonstrated that such novel approach can achieve good performance on the arbitrary-scale super-resolution task, their upscaled images frequently show structural distortion due to the faulty prediction on high-frequency textures. In this work, we propose UltraSR, a simple yet effective new network design based on implicit image functions in which spatial coordinates and periodic encoding are deeply integrated with the implicit neural representation. We show that spatial encoding is indeed a missing key towards the next-stage high-accuracy implicit image function through extensive experiments and ablation studies. Our UltraSR sets new state-of-the-art performance on the DIV2K benchmark under all super-resolution scales comparing to previous state-of-the-art methods. UltraSR also achieves superior performance on other standard benchmark datasets in which it outperforms prior works in almost all experiments. Our code will be released at https://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution. </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 24、双子网多级通信上采样大运动视频超分辨率
- [ ] Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling 
时间：2021年03月22日                         第一作者：Hongying Liu                       [链接](https://arxiv.org/abs/2103.11744).                     
## 摘要：视频超分辨率（VSR）的目标是恢复低分辨率（LR）的视频，并将其提高到更高的分辨率（HR）。由于视频任务的特点，在VSR算法中，对帧间运动信息的关注、总结和利用是非常重要的。特别是当视频包含大运动时，传统的方法容易产生非相干的结果或伪影。提出了一种新的双子网多级通信上采样深度神经网络（DSMC），用于大运动视频的超分辨率处理。设计了一个新的三维卷积U形残差密集网络（U3D-RDN）模块，用于精细隐式运动估计和运动补偿（MEMC）以及粗空间特征提取。提出了一种新的多级通信上采样（MSCU）模块，充分利用上采样的中间结果指导VSR。此外，本文还设计了一种新的双子网来辅助DSMC的训练，它的双子网损失有助于减少解空间和提高泛化能力。实验结果表明，与现有的方法相比，该方法在大运动视频上具有更好的性能。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月23日</details>

# 23、一种新的单图像超分辨率公共Alsat-2B数据集
- [ ] A new public Alsat-2B dataset for single-image super-resolution 
时间：2021年03月21日                         第一作者：Achraf Djerida                       [链接](https://arxiv.org/abs/2103.12547).                     
## 摘要：目前，当有可靠的训练数据集可用时，深度学习方法是图像超分辨率的主要解决方案。然而，对于遥感基准来说，获取高空间分辨率的图像是非常昂贵的。大多数超分辨率方法都采用下采样技术来模拟低分辨率和高分辨率的空间对，并构造训练样本。为了解决这一问题，本文提出了一种新的低分辨率和高分辨率（分别为10m和2.5m）的公共遥感数据集（Alsat2B），用于单幅图像的超分辨率处理。通过平移锐化得到高分辨率图像。此外，基于通用准则对数据集上一些超分辨率方法的性能进行了评估。结果表明，该方法是有前途的，并突出了数据集的挑战，这表明需要先进的方法来掌握低分辨率和高分辨率斑块之间的关系。
<details>	<summary>英文摘要</summary>	Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches. </details>
<details>	<summary>注释</summary>	This paper has been Accepted for publication in the International Geoscience and Remote Sensing Symposium (IGARSS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 22、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年03月19日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
## 摘要：高光谱图像具有较窄的光谱波段，能够获取丰富的光谱信息，适合于许多计算机视觉任务。HSI的一个基本限制是它的低空间分辨率，最近一些关于超分辨率（SR）的工作被提出来解决这个问题。然而，由于HSI摄像机的多样性，不同的摄像机捕捉到的图像具有不同的光谱响应函数和总通道数。现有的HSI数据集通常很小，因此不足以建模。提出了一种基于元学习的超分辨率（MLSR）模型，该模型可以在任意多个输入波段的峰值波长处获取HSI图像，并生成任意多个输出波段的峰值波长的超分辨率HSI。我们通过对NTIRE2020和ICVL数据集的波段进行采样，人工创建子数据集，模拟交叉数据集设置，并对其进行谱内插和外推的HSI SR。我们为所有子数据集训练单个MLSR模型，并为每个子数据集训练专用的基线模型。结果表明，与现有的HSI-SR方法相比，该模型具有相同的水平或更好的性能。
<details>	<summary>英文摘要</summary>	Hyperspectral images (HSIs) with narrow spectral bands can capture rich spectral information, making them suitable for many computer vision tasks. One of the fundamental limitations of HSI is its low spatial resolution, and several recent works on super-resolution(SR) have been proposed to tackle this challenge. However, due to HSI cameras' diversity, different cameras capture images with different spectral response functions and the number of total channels. The existing HSI datasets are usually small and consequently insufficient for modeling. We propose a Meta-Learning-Based Super-Resolution(MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate super-resolved HSIs with an arbitrary number of output bands' peak wavelengths. We artificially create sub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate the cross-dataset settings and perform HSI SR with spectral interpolation and extrapolation on them. We train a single MLSR model for all sub-datasets and train dedicated baseline models for each sub-dataset. The results show the proposed model has the same level or better performance compared to the-state-of-the-art HSI SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 21、视频超分辨率的自监督自适应算法
- [ ] Self-Supervised Adaptation for Video Super-Resolution 
时间：2021年03月18日                         第一作者：Jinsu Yoo                        [链接](https://arxiv.org/abs/2103.10081).                     
## 摘要：近年来，单图像超分辨率（single-image super-resolution，SISR）网络通过利用输入数据中的信息和大量外部数据集，使网络参数适应特定的输入图像，取得了良好的效果。然而，这些自监督SISR方法在视频处理中的扩展还有待研究。因此，我们提出了一种新的学习算法，使得传统的视频超分辨率（VSR）网络能够在不使用地面真实数据集的情况下调整参数来测试视频帧。通过利用空间和时间上的许多自相似块，我们提高了完全预训练VSR网络的性能，并产生了时间一致的视频帧。此外，我们提出了一种测试时知识提取技术，以较少的硬件资源加快了自适应速度。在我们的实验中，我们证明了我们的新学习算法可以微调最先进的VSR网络，并在大量的基准数据集上显著提高性能。
<details>	<summary>英文摘要</summary>	Recent single-image super-resolution (SISR) networks, which can adapt their network parameters to specific input images, have shown promising results by exploiting the information available within the input data as well as large external datasets. However, the extension of these self-supervised SISR approaches to video handling has yet to be studied. Thus, we present a new learning algorithm that allows conventional video super-resolution (VSR) networks to adapt their parameters to test video frames without using the ground-truth datasets. By utilizing many self-similar patches across space and time, we improve the performance of fully pre-trained VSR networks and produce temporally consistent video frames. Moreover, we present a test-time knowledge distillation technique that accelerates the adaptation speed with less hardware resources. In our experiments, we demonstrate that our novel learning algorithm can fine-tune state-of-the-art VSR networks and substantially elevate performance on numerous benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年03月19日</details>

# 20、结构化输出依赖建模的一般感知损失
- [ ] Generic Perceptual Loss for Modeling Structured Output Dependencies 
时间：2021年03月18日                         第一作者：Yifan Liu                       [链接](https://arxiv.org/abs/2103.10571).                     
## 摘要：感知损失作为一个有效的损失项被广泛应用于图像合成任务中，包括图像超分辨率、风格转换等。人们认为，成功的关键在于从经过大量图像预训练的CNNs中提取高层次的感知特征表示。这里我们揭示了，重要的是网络结构，而不是训练的权重。在没有任何学习的情况下，深层网络的结构就足以利用多层cnn捕获多个层次的变量统计之间的依赖关系。这种洞察消除了预先训练和特定网络结构（通常是VGG）的要求，这些都是先前假设的感知损失，从而实现了更广泛的应用。为此，我们证明了一个随机加权的深度CNN可以用来模拟输出的结构化依赖关系。在语义分割、深度估计和实例分割等稠密的逐像素预测任务中，与单独使用逐像素丢失的基线相比，扩展的随机感知丢失方法得到了更好的结果。我们希望这种简单的，扩展的知觉损失可以作为一种通用的结构化输出损失，适用于大多数结构化输出学习任务。
<details>	<summary>英文摘要</summary>	The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution, and style transfer. It was believed that the success lies in the high-level perceptual feature representations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the network structure instead of the trained weights. Without any learning, the structure of a deep network is sufficient to capture the dependencies between multiple levels of variable statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously assumed for the perceptual loss, thus enabling a significantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks. </details>
<details>	<summary>注释</summary>	Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 19、视频流预测辅助帧超分辨率
- [ ] Prediction-assistant Frame Super-Resolution for Video Streaming 
时间：2021年03月17日                         第一作者：Wang Shen                       [链接](https://arxiv.org/abs/2103.09455).                     
## 摘要：在在线视频游戏、现场直播等实时应用中，视频帧的传输延迟是至关重要的，新帧的接收截止时间必须赶上帧的渲染时间。否则，系统会缓冲一段时间，用户会遇到冻结屏幕，导致用户体验不尽如人意。一种有效的方法是在较差的带宽条件下传输较低质量的帧，例如使用可伸缩视频编码。在本文中，我们提出在两种情况下使用有损帧来提高视频质量。首先，当当前帧在渲染截止时间之前太晚而无法接收时（即丢失），我们建议使用先前接收到的高分辨率图像来预测未来的帧。第二，当当前接收到的帧的质量较低（即有损）时，我们建议使用先前接收到的高分辨率帧来增强低质量的当前帧。对于第一种情况，我们提出了一个小而有效的视频帧预测网络。对于第二种情况，我们将视频预测网络改进为视频增强网络，将当前帧和前一帧关联起来，以恢复高质量的图像。大量的实验结果表明，我们的方法在有损视频流环境中的性能优于现有的算法。
<details>	<summary>英文摘要</summary>	Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment. </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 18、ShipSRDet：一种基于超分辨特征表示的端到端遥感舰船探测器
- [ ] ShipSRDet: An End-to-End Remote Sensing Ship Detector Using Super-Resolved Feature Representation 
时间：2021年03月17日                         第一作者：Shitian He                       [链接](https://arxiv.org/abs/2103.09699).                     
## 摘要：高分辨率遥感图像可以为船舶检测提供丰富的外观信息。虽然已有的一些方法采用图像超分辨率（SR）来提高检测性能，但它们将图像超分辨率和船舶检测视为两个独立的过程，忽略了这两个相关任务之间的内在一致性。在本文中，我们探讨了图像SR对船舶检测的潜在好处，并提出了一种端到端网络ShipSRDet。在我们的方法中，我们不仅将超分辨图像提供给检测器，而且将SR网络的中间特征与检测网络的中间特征结合起来。这样，SR网络提取的信息性特征表示就可以充分用于船舶检测。在HRSC数据集上的实验结果验证了该方法的有效性。我们的ShipSRDet可以从输入图像中恢复丢失的细节，并取得了良好的船舶检测性能。
<details>	<summary>英文摘要</summary>	High-resolution remote sensing images can provide abundant appearance information for ship detection. Although several existing methods use image super-resolution (SR) approaches to improve the detection performance, they consider image SR and ship detection as two separate processes and overlook the internal coherence between these two correlated tasks. In this paper, we explore the potential benefits introduced by image SR to ship detection, and propose an end-to-end network named ShipSRDet. In our method, we not only feed the super-resolved images to the detector but also integrate the intermediate features of the SR network with those of the detection network. In this way, the informative feature representation extracted by the SR network can be fully used for ship detection. Experimental results on the HRSC dataset validate the effectiveness of our method. Our ShipSRDet can recover the missing details from the input image and achieves promising ship detection performance. </details>
<details>	<summary>注释</summary>	Accepted to IGARSS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 17、基于单镜头样本的超分辨跨域人脸模型
- [ ] Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar 
时间：2021年03月16日                         第一作者：Peike Li                       [链接](https://arxiv.org/abs/2103.08863).                     
## 摘要：传统的人脸超分辨率方法通常假设检测低分辨率（LR）图像与训练图像位于同一个域。由于光照条件和成像硬件的不同，在许多实际场景中，训练图像和测试图像之间不可避免地会出现域间隙。忽略这些域间隙将导致较低的人脸超分辨率（FSR）性能。然而，如何将训练好的FSR模型有效地转移到目标域中还没有被研究。为了解决这个问题，我们开发了一个基于域感知金字塔的人脸超分辨率网络，命名为DAP-FSR网络。我们的DAP-FSR是第一次尝试利用目标域中的一对高分辨率（HR）和LR样本从目标域超分辨LR人脸。具体来说，我们的DAP-FSR首先使用编码器来提取输入LR人脸的多尺度潜在表示。考虑到只有一个目标域的例子可用，我们建议通过混合目标域面和源域面的潜在表示来扩充目标域数据，然后将混合表示提供给我们的DAP-FSR解码器。解码器将生成与目标域图像样式相似的新人脸图像。生成的HR面依次用于优化我们的解码器以减少域间隙。通过迭代更新潜在的表示和我们的解码器，我们的DAP-FSR将适应目标域，从而实现真实和高质量的上采样HR人脸。在三个新构建的基准上的大量实验验证了我们的DAP-FSR的有效性和优越的性能。
<details>	<summary>英文摘要</summary>	Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art. </details>
<details>	<summary>邮件日期</summary>	2021年03月17日</details>

# 16、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年03月15日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
## 摘要：基于深度学习的方法，特别是卷积神经网络（CNNs）已经成功地应用于单幅图像超分辨率（SISR）领域。为了获得更好的逼真度和视觉质量，现有的网络大多采用繁重的设计和大量的计算。然而，现代移动设备的计算资源有限，难以承受昂贵的成本。为此，本文提出了一种基于离散余弦变换（DCT）域的频率感知动态网络，将输入信号按其系数分为多个部分。在实际应用中，高频部分采用昂贵的运算，低频部分采用廉价的运算，以减轻计算负担。由于像素或图像块属于低频区域，包含的纹理细节相对较少，因此这种动态网络不会影响生成的超分辨率图像的质量。此外，我们将预测器嵌入到所提出的动态网路中，以端到端微调手工制作的频率感知遮罩。在基准SISR模型和数据集上进行的大量实验表明，频率感知动态网络可以应用于各种SISR神经结构，在视觉质量和计算复杂度之间获得更好的折衷。例如，我们可以在保持最先进的SISR性能的同时，将EDSR模型的失败率降低大约50\%$。
<details>	<summary>英文摘要</summary>	Deep learning based methods, especially convolutional neural networks (CNNs) have been successfully applied in the field of single image super-resolution (SISR). To obtain better fidelity and visual quality, most of existing networks are of heavy design with massive computation. However, the computation resources of modern mobile devices are limited, which cannot easily support the expensive cost. To this end, this paper explores a novel frequency-aware dynamic network for dividing the input into multiple parts according to its coefficients in the discrete cosine transform (DCT) domain. In practice, the high-frequency part will be processed using expensive operations and the lower-frequency part is assigned with cheap operations to relieve the computation burden. Since pixels or image patches belong to low-frequency areas contain relatively few textural details, this dynamic network will not affect the quality of resulting super-resolution images. In addition, we embed predictors into the proposed dynamic network to end-to-end fine-tune the handcrafted frequency-aware masks. Extensive experiments conducted on benchmark SISR models and datasets show that the frequency-aware dynamic network can be employed for various SISR neural architectures to obtain the better tradeoff between visual quality and computational complexity. For instance, we can reduce the FLOPs of EDSR model by approximate $50\%$ while preserving state-of-the-art SISR performance. </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 15、低分辨率图像和视频中的三维人体姿势、形状和纹理
- [ ] 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos 
时间：2021年03月11日                         第一作者：Xiangyu Xu                       [链接](https://arxiv.org/abs/2103.06498).                     
## 摘要：基于单眼图像的三维人体姿态和形状估计一直是计算机视觉领域的一个研究热点。现有的深度学习方法依赖于高分辨率的输入，然而，在视频监控和体育广播等许多场景中并不总是可用的。处理低分辨率图像的两种常用方法是对输入应用超分辨率技术，这可能会导致不愉快的伪影，或者只是针对每个分辨率训练一个模型，这在许多实际应用中是不切实际的。针对上述问题，本文提出了一种新的算法RSC-Net，该算法由分辨率感知网络、自监督损失和对比学习机制组成。该方法能够在单个模型上学习不同分辨率下的三维人体姿态和形状。自我监督缺失加强了输出的尺度一致性，而对比学习方案加强了深层特征的尺度一致性。我们表明，这两个新的损失提供鲁棒性学习时，在弱监督的方式。此外，我们扩展了RSC网络来处理低分辨率的视频，并将其应用于从低分辨率输入中重建具有纹理的三维行人。大量的实验表明，RSC网络在处理低分辨率图像时，可以取得比现有方法更好的效果。
<details>	<summary>英文摘要</summary>	3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2007.13666 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 14、一种基于学习的轴向超分辨率视图外推方法
- [ ] A learning-based view extrapolation method for axial super-resolution 
时间：2021年03月11日                         第一作者：Zhaolin Xiao                       [链接](https://arxiv.org/abs/2103.06510).                     
## 摘要：轴向光场分辨率是指通过重新聚焦来区分不同深度特征的能力。轴向再聚焦精度相当于两个可分辨的再聚焦平面在轴向上的最小距离。高再聚焦精度对于一些光场应用（如显微镜）来说是必不可少的。在这篇论文中，我们提出了一种基于学习的方法来外推新的观点从轴向体积剪切极平面图像（EPIs）。与经典成像中的扩展数值孔径（NA）一样，外推光场可以获得具有较浅景深（DOF）的重聚焦图像，从而获得更精确的重聚焦结果。最重要的是，该方法不需要精确的深度估计。对合成光场和真实光场的实验结果表明，该方法不仅适用于全光相机（尤其是1.0型全光相机）拍摄的基线较小的光场，而且适用于基线较大的光场。
<details>	<summary>英文摘要</summary>	Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines. </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 13、使用真实退化图像的超分辨率卫星硬件
- [ ] Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images 
时间：2021年03月10日                         第一作者：Jack White                       [链接](https://arxiv.org/abs/2103.06270).                     
## 摘要：现代深超分辨率（SR）网络已成为图像重建和增强的重要技术。然而，这些网络通常是在缺乏真实图像中典型的图像降质噪声的基准图像数据上训练和测试的。在这篇论文中，我们通过评估SR在重建真实退化卫星图像中的性能，来测试在真实遥感有效载荷中使用深度SR的可行性。我们证明了一种称为增强深超分辨率网络（EDSR）的先进SR技术，在没有特定领域的预训练的情况下，只要地面分辨率足够远，就可以在地面采样距离较短的图像上恢复编码的像素数据。然而，这种恢复因所选地理类型而异。我们的结果表明，定制训练有可能进一步改善高架图像的重建，新的卫星硬件应优先考虑光学性能，而不是最小化像素大小，因为深SR可以克服后者的不足，但不能克服前者。
<details>	<summary>英文摘要</summary>	Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former. </details>
<details>	<summary>注释</summary>	6 pages, 6 figures, for supplementary results, see https://smpetrie.github.io/superres/ ACM-class: I.4.3 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 12、高光谱图像超分辨率空间光谱反馈网络
- [ ] Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral Imagery 
时间：2021年03月07日                         第一作者：Enhai Liu                       [链接](https://arxiv.org/abs/2103.04354).                     
## 摘要：近年来，基于深度学习的单灰度/RGB图像超分辨率（SR）方法取得了很大的成功。然而，单幅高光谱图像的超分辨率处理存在两大障碍，限制了技术的发展。一是高光谱图像中高维复杂的光谱模式，使得空间信息和波段间的光谱信息难以同时获取。另一方面，高光谱训练样本的数目非常小，在训练深度神经网络时容易导致过度拟合。为了解决这些问题，本文提出了一种新的空间谱反馈网络（SSFN），利用全局谱带的高阶信息来细化局部谱带间的低阶表示。它不仅可以缓解高光谱数据高维性给特征提取带来的困难，而且可以使训练过程更加稳定。具体地说，我们在具有有限展开的RNN中使用隐藏状态来实现这种反馈方式。为了充分利用空间和光谱先验知识，设计了空间光谱反馈块（SSFB）来处理反馈连接并生成强大的高层表示。提出的SSFN具有早期预测能力，可以逐步重建最终的高分辨率高光谱图像。在三个基准数据集上的大量实验结果表明，与现有的方法相比，所提出的SSFN具有更好的性能。源代码位于https://github.com/tangzhenjie/SSFN。
<details>	<summary>英文摘要</summary>	Recently, single gray/RGB image super-resolution (SR) methods based on deep learning have achieved great success. However, there are two obstacles to limit technical development in the single hyperspectral image super-resolution. One is the high-dimensional and complex spectral patterns in hyperspectral image, which make it difficult to explore spatial information and spectral information among bands simultaneously. The other is that the number of available hyperspectral training samples is extremely small, which can easily lead to overfitting when training a deep neural network. To address these issues, in this paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to refine low-level representations among local spectral bands with high-level information from global spectral bands. It will not only alleviate the difficulty in feature extraction due to high dimensional of hyperspectral data, but also make the training process more stable. Specifically, we use hidden states in an RNN with finite unfoldings to achieve such feedback manner. To exploit the spatial and spectral prior, a Spatial-Spectral Feedback Block (SSFB) is designed to handle the feedback connections and generate powerful high-level representations. The proposed SSFN comes with a early predictions and can reconstruct the final high-resolution hyperspectral image step by step. Extensive experimental results on three benchmark datasets demonstrate that the proposed SSFN achieves superior performance in comparison with the state-of-the-art methods. The source code is available at https://github.com/tangzhenjie/SSFN. </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 11、基于深度学习的小数据集超分辨荧光显微镜
- [ ] Deep learning-based super-resolution fluorescence microscopy on small datasets 
时间：2021年03月07日                         第一作者：Varun Mannam                       [链接](https://arxiv.org/abs/2103.04989).                     
## 摘要：荧光显微术以微米级的分辨率显示生物有机体，使现代生物学有了巨大的发展。然而，由于衍射极限的限制，亚微米/纳米级的特征很难分辨。虽然各种超分辨率技术是为了达到纳米级的分辨率而发展起来的，但它们往往需要昂贵的光学装置或专门的荧光团。近年来，深度学习显示出减少技术障碍和从衍射限制图像获得超分辨率的潜力。为了得到准确的结果，传统的深度学习技术需要成千上万的图像作为训练数据集。由于荧光团的光漂白、光毒性和生物体内发生的动态过程，从生物样品中获取大量数据通常是不可行的。因此，利用小数据集实现基于深度学习的超分辨率具有挑战性。我们用一种新的基于卷积神经网络的方法来解决这一限制，这种方法成功地用小数据集训练并获得超分辨率图像。我们从15个不同的视场共采集了750张图像作为训练数据集来演示该技术。在每个视场中，采用超分辨率径向起伏方法生成单个目标图像。正如预期的那样，这个小数据集无法使用传统的超分辨率体系结构生成可用的模型。然而，使用新的方法，可以训练一个网络来从这个小数据集获得超分辨率图像。这种深度学习模型可应用于其他生物医学成像模式，如MRI和X射线成像，在这些模式中获取大型训练数据集是一项挑战。
<details>	<summary>英文摘要</summary>	Fluorescence microscopy has enabled a dramatic development in modern biology by visualizing biological organisms with micrometer scale resolution. However, due to the diffraction limit, sub-micron/nanometer features are difficult to resolve. While various super-resolution techniques are developed to achieve nanometer-scale resolution, they often either require expensive optical setup or specialized fluorophores. In recent years, deep learning has shown the potentials to reduce the technical barrier and obtain super-resolution from diffraction-limited images. For accurate results, conventional deep learning techniques require thousands of images as a training dataset. Obtaining large datasets from biological samples is not often feasible due to the photobleaching of fluorophores, phototoxicity, and dynamic processes occurring within the organism. Therefore, achieving deep learning-based super-resolution using small datasets is challenging. We address this limitation with a new convolutional neural network-based approach that is successfully trained with small datasets and achieves super-resolution images. We captured 750 images in total from 15 different field-of-views as the training dataset to demonstrate the technique. In each FOV, a single target image is generated using the super-resolution radial fluctuation method. As expected, this small dataset failed to produce a usable model using traditional super-resolution architecture. However, using the new approach, a network can be trained to achieve super-resolution images from this small dataset. This deep learning model can be applied to other biomedical imaging modalities such as MRI and X-ray imaging, where obtaining large training datasets is challenging. </details>
<details>	<summary>注释</summary>	SPIE Proceedings Volume 11650, Single Molecule Spectroscopy and Superresolution Imaging XIV; 116500O (2021) DOI: 10.1117/12.2578519 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 10、ClassSR：一种利用数据特征加速超分辨率网络的通用框架
- [ ] ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic 
时间：2021年03月06日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2103.04039).                     
## 摘要：我们的目标是在大图像（2K-8K）上加速超分辨率（SR）网络。在实际应用中，大图像通常被分解成小的子图像。在此基础上，我们发现不同的图像区域具有不同的恢复难度，可以由不同容量的网络进行处理。直观地说，平滑区域比复杂纹理更容易超级求解。为了利用这一特性，我们可以采用适当的SR网络对分解后的不同子图像进行处理。在此基础上，我们提出了一种新的解决方案管道——ClassSR，它将分类和SR结合在一个统一的框架中。特别地，它首先使用一个类模块将子图像按恢复难度分为不同的类，然后应用一个SR模块对不同的类进行SR。类模块是一个传统的分类网络，而SR模块是一个由待加速SR网络及其简化版本组成的网络容器。我们进一步引入了一种新的两损失分类方法——类别损失和平均损失来产生分类结果。联合训练后，大部分子图像将通过较小的网络，从而大大降低了计算量。实验表明，我们的ClassSR可以帮助大多数现有方法（如FSRCNN、CARN、SRResNet、RCAN）在DIV8K数据集上节省高达50%的失败率。这个通用框架也可以应用于其他低层次的视觉任务。
<details>	<summary>英文摘要</summary>	We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks. </details>
<details>	<summary>注释</summary>	CVPR2021 paper + supplementary file </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 9、用稀疏表示生成图像
- [ ] Generating Images with Sparse Representations 
时间：2021年03月05日                         第一作者：Charlie Nash                       [链接](https://arxiv.org/abs/2103.03841).                     
## 摘要：图像的高维性对基于似然的生成模型的结构和采样效率提出了挑战。以前的方法，例如VQ-VAE，使用深度自动编码器来获得紧凑的表示，作为基于似然模型的输入更为实用。我们提出了另一种方法，受JPEG等常见图像压缩方法的启发，将图像转换为量化的离散余弦变换（DCT）块，这些块稀疏地表示为DCT通道、空间位置和DCT系数三元组的序列。我们提出了一种基于变换器的自回归结构，该结构被训练成序列预测下一个元素在这些序列中的条件分布，并有效地扩展到高分辨率图像。在一系列的图像数据集上，我们证明了我们的方法可以生成高质量、多样的图像，并且样本度量分数可以与最先进的方法相竞争。此外，我们还表明，简单的修改，我们的方法产生有效的图像着色和超分辨率模型。
<details>	<summary>英文摘要</summary>	The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 8、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月05日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	Accepted to CVPR 2021. The first two authors contributed equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 7、真实世界的单图像超分辨率：简要回顾
- [ ] Real-World Single Image Super-Resolution: A Brief Review 
时间：2021年03月03日                         第一作者：Honggang Chen                       [链接](https://arxiv.org/abs/2103.02368).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是近几十年来图像处理领域的一个研究热点，其目的是通过低分辨率（low-resolution，LR）观测重建高分辨率（high-resolution，HR）图像。特别是基于深度学习的超分辨率（SR）方法已经引起了人们的广泛关注，极大地提高了合成数据的重建性能。最近的研究表明，对合成数据的模拟结果通常高估了对真实世界图像的超分辨能力。在这样的背景下，越来越多的研究者致力于研究真实感图像的随机共振方法。本文对现实世界中的单幅图像超分辨率（RSISR）技术进行了综述。更具体地说，本综述涵盖了RSISR的关键公共可用数据集和评估指标，以及四大类RSISR方法，即基于退化建模的RSISR、基于图像对的RSISR、基于领域翻译的RSISR和基于自学习的RSISR。在基准数据集上比较了代表性RSISR方法的重建质量和计算效率。此外，我们还讨论了RSISR面临的挑战和未来的研究课题。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR), which aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) observation, has been an active research topic in the area of image processing in recent decades. Particularly, deep learning-based super-resolution (SR) approaches have drawn much attention and have greatly improved the reconstruction performance on synthetic data. Recent studies show that simulation results on synthetic data usually overestimate the capacity to super-resolve real-world images. In this context, more and more researchers devote themselves to develop SR approaches for realistic images. This article aims to make a comprehensive review on real-world single image super-resolution (RSISR). More specifically, this review covers the critical publically available datasets and assessment metrics for RSISR, and four major categories of RSISR methods, namely the degradation modeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR, and self-learning-based RSISR. Comparisons are also made among representative RSISR methods on benchmark datasets, in terms of both reconstruction quality and computational efficiency. Besides, we discuss challenges and promising research topics on RSISR. </details>
<details>	<summary>注释</summary>	18 pages, 12 figure, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 6、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年02月25日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
## 摘要：近年来，大量的研究活动致力于视频增强，同时提高时间帧速率和空间分辨率。然而，现有的方法要么不能揭示时空信息之间的内在联系，要么在最终的时空分辨率的选择上缺乏灵活性。在这项工作中，我们提出了一个无约束的时空视频超分辨率网络，它可以有效地利用时空相关性来提高性能。此外，通过使用光流技术和广义像素混洗操作，它在调整时间帧速率和空间分辨率方面具有完全的自由度。实验结果表明，该方法不仅优于现有的算法，而且所需参数少，运行时间短。
<details>	<summary>英文摘要</summary>	Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 5、ShuffleUNet：基于深度学习的磁共振弥散加权成像的超分辨率
- [ ] ShuffleUNet: Super resolution of diffusion-weighted MRIs using deep learning 
时间：2021年02月25日                         第一作者：Soumick Chatterjee                       [链接](https://arxiv.org/abs/2102.12898).                     
## 摘要：弥散加权磁共振成像（DW-MRI）可用于表征神经组织的微观结构，例如通过纤维追踪以非侵入性方式描绘脑白质连接。高空间分辨率的磁共振成像（MRI）将在以更好的方式显示这些纤维束方面发挥重要作用。然而，获得这种分辨率的图像是以较长的扫描时间为代价的。由于患者的心理和身体状况，较长的扫描时间可能与运动伪影的增加有关。单图像超分辨率（Single-Image Super-Resolution，SISR）是一种通过深度学习从单个低分辨率（low-Resolution，LR）输入图像中获取高分辨率细节的技术，是本研究的重点。与插值技术或稀疏编码算法相比，深度学习算法从大数据集中提取先验知识，并从低分辨率图像中产生更优的MRI图像。本研究提出一种基于深度学习的超分辨技术，并将其应用于DW-MRI。从IXI数据集得到的图像被用作地面真值，并被人工降采样以模拟低分辨率图像。所提出的方法在统计学上比基线有了显著的改进，实现了0.913美元-0.045美元的SSIM。
<details>	<summary>英文摘要</summary>	Diffusion-weighted magnetic resonance imaging (DW-MRI) can be used to characterise the microstructure of the nervous tissue, e.g. to delineate brain white matter connections in a non-invasive manner via fibre tracking. Magnetic Resonance Imaging (MRI) in high spatial resolution would play an important role in visualising such fibre tracts in a superior manner. However, obtaining an image of such resolution comes at the expense of longer scan time. Longer scan time can be associated with the increase of motion artefacts, due to the patient's psychological and physical conditions. Single Image Super-Resolution (SISR), a technique aimed to obtain high-resolution (HR) details from one single low-resolution (LR) input image, achieved with Deep Learning, is the focus of this study. Compared to interpolation techniques or sparse-coding algorithms, deep learning extracts prior knowledge from big datasets and produces superior MRI images from the low-resolution counterparts. In this research, a deep learning based super-resolution technique is proposed and has been applied for DW-MRI. Images from the IXI dataset have been used as the ground-truth and were artificially downsampled to simulate the low-resolution images. The proposed method has shown statistically significant improvement over the baselines and achieved an SSIM of $0.913\pm0.045$. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 4、用于视频超分辨率的深展开网络
- [ ] Deep Unrolled Network for Video Super-Resolution 
时间：2021年02月23日                         第一作者：Benjamin Naoto Chiche                       [链接](https://arxiv.org/abs/2102.11720).                     
## 摘要：视频超分辨率（VSR）的目的是从相应的低分辨率（LR）图像中重建一系列高分辨率（HR）图像。传统上，VSR问题的求解是基于迭代算法，该算法可以利用图像形成的先验知识和运动的假设。然而，这些经典的方法很难将自然图像中的复杂统计信息结合起来。此外，VSR最近受益于深度学习（DL）算法带来的改进。这些技术可以有效地从大量的图像集合中学习空间模式。然而，他们没有融入一些有关图像形成模型的知识，这限制了他们的灵活性。为解决反问题而开发的展开优化算法允许将先验信息纳入深度学习体系结构。它们主要用于单个图像恢复任务。采用展开的神经网络结构可以带来以下好处。首先，这可能会提高超分辨率任务的性能。这样，神经网络就具有更好的可解释性。最后，这允许灵活地学习单个模型，以非盲目地处理多个退化。本文提出了一种新的基于展开优化技术的VSR神经网络，并对其性能进行了讨论。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to reconstruct a sequence of high-resolution (HR) images from their corresponding low-resolution (LR) versions. Traditionally, solving a VSR problem has been based on iterative algorithms that can exploit prior knowledge on image formation and assumptions on the motion. However, these classical methods struggle at incorporating complex statistics from natural images. Furthermore, VSR has recently benefited from the improvement brought by deep learning (DL) algorithms. These techniques can efficiently learn spatial patterns from large collections of images. Yet, they fail to incorporate some knowledge about the image formation model, which limits their flexibility. Unrolled optimization algorithms, developed for inverse problems resolution, allow to include prior information into deep learning architectures. They have been used mainly for single image restoration tasks. Adapting an unrolled neural network structure can bring the following benefits. First, this may increase performance of the super-resolution task. Then, this gives neural networks better interpretability. Finally, this allows flexibility in learning a single model to nonblindly deal with multiple degradations. In this paper, we propose a new VSR neural network based on unrolled optimization techniques and discuss its performance. </details>
<details>	<summary>注释</summary>	6 pages. 3 figures. Published in: 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA) DOI: 10.1109/IPTA50016.2020.9286636 </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 3、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月23日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 2、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月21日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
## 摘要：最近COVID-19的爆发促使研究人员在利用人工智能和深度学习的医学成像领域做出贡献。超分辨率（SR）在过去的几年中，利用深度学习方法取得了显著的效果。深度学习方法学习从低分辨率（LR）图像到相应的高分辨率（HR）图像的非线性映射的能力导致了SR在不同研究领域的引人注目的结果。本文提出了一种基于深度学习的切比切夫变换域图像超分辨率结构。这是通过将转换层通过定制的Tchebichef卷积层（$TCL$）集成到提议的体系结构中来实现的。TCL的作用是利用Tchebichef基函数将LR图像从空间域转换到正交变换域。使用称为逆切比雪夫卷积层（ITCL）的另一层实现上述变换的反演，该层将LR图像从变换域转换回空间域。研究表明，利用Tchebichef变换域进行超分辨率重建，利用了图像的高低频特征，简化了超分辨率重建任务。我们进一步引入转移学习方法来提高基于Covid的医学图像的质量。结果表明，我们的结构提高了COVID-19的X线和CT图像质量，提供了更好的图像质量，有助于临床诊断。与使用较少可训练参数的大多数深度学习方法相比，使用所提出的切比雪夫变换域超分辨率（TTDSR）结构获得的实验结果提供了具有竞争力的结果。
<details>	<summary>英文摘要</summary>	The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters. </details>
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 1、用于原子分辨率图像的高精度原子分割、定位、去噪和超分辨率处理的TEMImageNet训练库和atomsenet深度学习模型
- [ ] TEMImageNet Training Library and AtomSegNet Deep-Learning Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-Resolution Processing of Atomic-Resolution Images 
时间：2021年02月20日                         第一作者：Ruoqian Lin                       [链接](https://arxiv.org/abs/2012.09093).                     
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

