# 504、粒子跟踪问题中的动态超分辨率
- [ ] Dynamic super-resolution in particle tracking problems 
时间：2022年04月08日                         第一作者：Ping Liu                       [链接](https://arxiv.org/abs/2204.04092).                     
## 摘要：生物成像中的粒子跟踪涉及重建目标粒子的轨迹、位置或速度。粒子跟踪的标准方法包括两个步骤：首先静态重建每个时间步中的源位置，然后应用跟踪技术获得轨迹和速度。相比之下，动态重建寻求同时从所有帧恢复源位置和速度，这具有一定的优势。在本文中，我们对粒子跟踪问题中通过一般动态重建重建重建源数量、位置和速度的分辨率极限进行了严格的数学分析，并由此证明了动态重建实现超分辨率的可能性。我们发现，当粒子的位置-速度对分离到一定距离（分辨率极限）之外时，粒子的数量和位置-速度对可以稳定地恢复。分辨率极限与成像系统的截止频率、信噪比和光源的稀疏性有关。通过这些估计，我们还得到了稀疏促进动态重建的稳定性结果。此外，我们进一步证明了速度重建具有更好的分辨率极限，随着粒子的移动，分辨率极限不断提高。这一结果是通过观察得出的，即速度恢复的固有截止频率可被视为总观察时间乘以成像系统的截止频率，与每个衍射限制帧的分辨率极限相比，这可能导致更好的分辨率极限。预计这一观察结果将启发新的重建算法，从而在实践中提高粒子跟踪的分辨率。
<details>	<summary>英文摘要</summary>	Particle tracking in biological imaging is concerned with reconstructing the trajectories, locations, or velocities of the targeting particles. The standard approach of particle tracking consists of two steps: first reconstructing statically the source locations in each time step, and second applying tracking techniques to obtain the trajectories and velocities. In contrast, the dynamic reconstruction seeks to simultaneously recover the source locations and velocities from all frames, which enjoys certain advantages. In this paper, we provide a rigorous mathematical analysis for the resolution limit of reconstructing source number, locations, and velocities by general dynamical reconstruction in particle tracking problems, by which we demonstrate the possibility of achieving super-resolution for the dynamic reconstruction. We show that when the location-velocity pairs of the particles are separated beyond certain distances (the resolution limits), the number of particles and the location-velocity pair can be stably recovered. The resolution limits are related to the cut-off frequency of the imaging system, signal-to-noise ratio, and the sparsity of the source. By these estimates, we also derive a stability result for a sparsity-promoting dynamic reconstruction. In addition, we further show that the reconstruction of velocities has a better resolution limit which improves constantly as the particles moving. This result is derived by an observation that the inherent cut-off frequency for the velocity recovery can be viewed as the total observation time multiplies the cut-off frequency of the imaging system, which may lead to a better resolution limit as compared to the one for each diffraction-limited frame. It is anticipated that this observation can inspire new reconstruction algorithms that improve the resolution of particle tracking in practice. </details>
<details>	<summary>邮件日期</summary>	2022年04月11日</details>

# 503、基于深度置换不变网络的超分辨多时相分割
- [ ] Super-resolved multi-temporal segmentation with deep permutation-invariant networks 
时间：2022年04月06日                         第一作者：Diego Valsesia                       [链接](https://arxiv.org/abs/2204.02631).                     
## 摘要：多亏了新的深度学习模型，多时相卫星获取的场景多图像超分辨率最近取得了巨大成功。在本文中，我们超越了传统的高分辨率图像重建，研究了一个超分辨率推理问题，即空间分辨率高于传感平台的语义分割。我们扩展了最近提出的模型，利用时间排列不变性和多分辨率融合模块，能够推断分割任务所需的丰富语义信息。本文提出的模型最近赢得了关于加强哨兵2农业的AI4EO挑战。
<details>	<summary>英文摘要</summary>	Multi-image super-resolution from multi-temporal satellite acquisitions of a scene has recently enjoyed great success thanks to new deep learning models. In this paper, we go beyond classic image reconstruction at a higher resolution by studying a super-resolved inference problem, namely semantic segmentation at a spatial resolution higher than the one of sensing platform. We expand upon recently proposed models exploiting temporal permutation invariance with a multi-resolution fusion module able to infer the rich semantic information needed by the segmentation task. The model presented in this paper has recently won the AI4EO challenge on Enhanced Sentinel 2 Agriculture. </details>
<details>	<summary>注释</summary>	IGARSS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月07日</details>

# 502、具有可变形注意金字塔的快速在线视频超分辨率
- [ ] Fast Online Video Super-Resolution with Deformable Attention Pyramid 
时间：2022年04月06日                         第一作者：Dario Fuoli                       [链接](https://arxiv.org/abs/2202.01731).                     
<details>	<summary>邮件日期</summary>	2022年04月07日</details>

# 501、降水预报随机降尺度的生成性深度学习方法
- [ ] A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts 
时间：2022年04月05日                         第一作者：Lucy Harris                       [链接](https://arxiv.org/abs/2204.02028).                     
## 摘要：尽管不断改进，降水预报仍然不如其他气象变量准确可靠。造成这种情况的一个主要因素是，影响降水分布和强度的几个关键过程发生在全球天气模型的分辨率范围以下。计算机视觉界已经证明，生成性对抗网络（GAN）在超分辨率问题上是成功的，即学习在粗图像中添加精细尺度结构。Leinonen等人（2020年）之前在粗化输入数据的情况下，应用GAN产生重建的高分辨率大气场的集合。在本文中，我们证明了这种方法可以扩展到更具挑战性的问题，即使用高分辨率雷达测量作为“地面真相”，提高来自天气预报模型的相对低分辨率输入的精度和分辨率。神经网络必须学会增加分辨率和结构，同时考虑不可忽略的预测误差。我们表明，GANs和VAE-GANs可以匹配最先进的逐点后处理方法的统计特性，同时创建高分辨率、空间相干的降水图。我们的模型在像素级和合并CRPS分数、功率谱信息和秩直方图（用于评估校准）方面都优于现有的最佳降尺度方法。我们测试了我们的模型，并展示了它们在一系列场景中的表现，包括暴雨。
<details>	<summary>英文摘要</summary>	Despite continuous improvements, precipitation forecasts are still not as accurate and reliable as those of other meteorological variables. A major contributing factor to this is that several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather models. Generative adversarial networks (GANs) have been demonstrated by the computer vision community to be successful at super-resolution problems, i.e., learning to add fine-scale structure to coarse images. Leinonen et al. (2020) previously applied a GAN to produce ensembles of reconstructed high-resolution atmospheric fields, given coarsened input data. In this paper, we demonstrate this approach can be extended to the more challenging problem of increasing the accuracy and resolution of comparatively low-resolution input from a weather forecasting model, using high-resolution radar measurements as a "ground truth". The neural network must learn to add resolution and structure whilst accounting for non-negligible forecast error. We show that GANs and VAE-GANs can match the statistical properties of state-of-the-art pointwise post-processing methods whilst creating high-resolution, spatially coherent precipitation maps. Our model compares favourably to the best existing downscaling methods in both pixel-wise and pooled CRPS scores, power spectrum information and rank histograms (used to assess calibration). We test our models and show that they perform in a range of scenarios, including heavy rainfall. </details>
<details>	<summary>注释</summary>	Submitted to JAMES 4/4/22 </details>
<details>	<summary>邮件日期</summary>	2022年04月06日</details>

# 500、利用遥感时空超分辨率跟踪发展中地区的城市化
- [ ] Tracking Urbanization in Developing Regions with Remote Sensing Spatial-Temporal Super-Resolution 
时间：2022年04月04日                         第一作者：Yutong He                       [链接](https://arxiv.org/abs/2204.01736).                     
## 摘要：随着机器学习和遥感技术的发展，在没有建筑信息的地区，自动跟踪城市发展成为可能。不幸的是，这些解决方案在高分辨率图像上表现最好，因为高分辨率图像的获取成本很高，而且不经常可用，因此很难在长时间跨度和大地理范围内进行缩放。在这项工作中，我们提出了一种利用单个高分辨率图像和公开的低分辨率图像的时间序列生成精确的高分辨率时间序列的管道，用于城市建设中的目标跟踪。与使用单图像超分辨率的基线相比，我们的方法实现了显著的改进，并且有助于在发展中国家扩展建筑施工跟踪的可访问性和可扩展性。
<details>	<summary>英文摘要</summary>	Automated tracking of urban development in areas where construction information is not available became possible with recent advancements in machine learning and remote sensing. Unfortunately, these solutions perform best on high-resolution imagery, which is expensive to acquire and infrequently available, making it difficult to scale over long time spans and across large geographies. In this work, we propose a pipeline that leverages a single high-resolution image and a time series of publicly available low-resolution images to generate accurate high-resolution time series for object tracking in urban construction. Our method achieves significant improvement in comparison to baselines using single image super-resolution, and can assist in extending the accessibility and scalability of building construction tracking across the developing world. </details>
<details>	<summary>注释</summary>	Presented at Workshop on Machine Learning for the Developing World (ML4D) at the 35th Conference on Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2022年04月06日</details>

# 499、基于条件像元合成的卫星图像时空超分辨率
- [ ] Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis 
时间：2022年04月04日                         第一作者：Yutong He                       [链接](https://arxiv.org/abs/2106.11485).                     
<details>	<summary>邮件日期</summary>	2022年04月05日</details>

# 498、基于非局部变分自动编码器的单幅图像内部分布测量
- [ ] Single Image Internal Distribution Measurement Using Non-Local Variational Autoencoder 
时间：2022年04月02日                         第一作者：Yeahia Sarker                       [链接](https://arxiv.org/abs/2204.01711).                     
## 摘要：基于深度学习的超分辨率方法已显示出巨大的潜力，尤其是在单图像超分辨率（SISR）任务中。尽管性能有所提高，但这些方法由于依赖大量数据进行模型训练而受到限制。此外，有监督的SISR解决方案依赖于局部邻域信息，只关注低维图像重建的特征学习过程。此外，由于他们的感受野有限，他们无法利用全球环境。为了应对这些挑战，本文提出了一种新的图像特定解决方案，即非局部变分自动编码器（\texttt{NLVAE}），以从单个低分辨率（LR）图像重建高分辨率（HR）图像，而无需任何事先训练。为了获得各种接收区域和高质量合成图像的最大细节，texttt{NLVAE}作为一种自我监督策略被引入，该策略使用非局部邻域的分离信息重建高分辨率图像。来自七个基准数据集的实验结果证明了\texttt{NLVAE}模型的有效性。此外，通过广泛的定性和定量评估，我们提出的模型优于许多基线和最先进的方法。
<details>	<summary>英文摘要</summary>	Deep learning-based super-resolution methods have shown great promise, especially for single image super-resolution (SISR) tasks. Despite the performance gain, these methods are limited due to their reliance on copious data for model training. In addition, supervised SISR solutions rely on local neighbourhood information focusing only on the feature learning processes for the reconstruction of low-dimensional images. Moreover, they fail to capitalize on global context due to their constrained receptive field. To combat these challenges, this paper proposes a novel image-specific solution, namely non-local variational autoencoder (\texttt{NLVAE}), to reconstruct a high-resolution (HR) image from a single low-resolution (LR) image without the need for any prior training. To harvest maximum details for various receptive regions and high-quality synthetic images, \texttt{NLVAE} is introduced as a self-supervised strategy that reconstructs high-resolution images using disentangled information from the non-local neighbourhood. Experimental results from seven benchmark datasets demonstrate the effectiveness of the \texttt{NLVAE} model. Moreover, our proposed model outperforms a number of baseline and state-of-the-art methods as confirmed through extensive qualitative and quantitative evaluations. </details>
<details>	<summary>注释</summary>	A Preprint Version </details>
<details>	<summary>邮件日期</summary>	2022年04月06日</details>

# 497、MODIS地表温度超分辨率的卷积神经网络建模
- [ ] Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution 
时间：2022年04月01日                         第一作者：Binh Minh Nguyen (IMT Atlantique)                       [链接](https://arxiv.org/abs/2202.10753).                     
<details>	<summary>邮件日期</summary>	2022年04月04日</details>

# 496、基于图像统计深度建模的贝叶斯图像超分辨率
- [ ] Bayesian Image Super-Resolution with Deep Modeling of Image Statistics 
时间：2022年03月31日                         第一作者：Shangqi Gao                        [链接](https://arxiv.org/abs/2204.00623).                     
## 摘要：图像先验的建模统计有助于图像的超分辨率，但基于深度学习的方法的大量工作很少受到关注。在这项工作中，我们提出了一个贝叶斯图像恢复框架，其中自然图像统计建模与平滑度和稀疏性先验相结合。具体地，首先考虑理想图像作为平滑分量和稀疏残差的总和，并对包括模糊、降尺度和噪声损坏在内的真实图像退化进行建模。然后，我们发展了一种变分贝叶斯方法来推断它们的后验概率。最后，我们利用深度神经网络实现了单图像超分辨率（SISR）的变分方法，并提出了一种无监督的训练策略。在三个图像恢复任务（理想SISR、真实SISR和真实SISR）上的实验表明，我们的方法对不同的噪声水平和退化核具有良好的模型泛化能力，并且在无监督的SISR中是有效的。代码和生成的模型通过\url发布{https://zmiclab.github.io/projects.html}.
<details>	<summary>英文摘要</summary>	Modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. In this work, we propose a Bayesian image restoration framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. Concretely, firstly we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. Then, we develop a variational Bayesian approach to infer their posteriors. Finally, we implement the variational approach for single image super-resolution (SISR) using deep neural networks, and propose an unsupervised training strategy. The experiments on three image restoration tasks, \textit{i.e.,} ideal SISR, realistic SISR, and real-world SISR, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised SISR. The code and resulting models are released via \url{https://zmiclab.github.io/projects.html}. </details>
<details>	<summary>注释</summary>	45 pages MSC-class: 62G ACM-class: I.5 Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) DOI: 10.1109/TPAMI.2022.3163307 </details>
<details>	<summary>邮件日期</summary>	2022年04月05日</details>

# 495、MyStyle：个性化的生成优先
- [ ] MyStyle: A Personalized Generative Prior 
时间：2022年03月31日                         第一作者：Yotam Nitzan                       [链接](https://arxiv.org/abs/2203.17272).                     
## 摘要：我们将介绍MyStyle，这是一个个性化的深层次生成性的先验知识，通过几张个人照片进行训练。MyStyle允许重建、增强和编辑特定人物的图像，以便输出忠实于人物的关键面部特征。给定一个小的人像参考集（~100），我们调整预训练的StyleGAN人脸生成器的权重，在潜在空间中形成一个局部的、低维的、个性化的流形。我们表明，这个流形构成了一个个性化区域，它跨越了与个体的不同肖像图像相关联的潜在代码。此外，我们证明了我们获得了一个个性化的生成先验，并提出了一种统一的方法，将其应用于各种不适定的图像增强问题，如修复和超分辨率，以及语义编辑。使用个性化生成先验，我们获得的输出对输入图像表现出高保真度，同时也忠实于参考集中个体的关键面部特征。我们用大量可广泛识别的个体的合理使用图像来展示我们的方法，对于这些个体，我们有对预期结果进行定性评估的先验知识。我们根据少量注射基线评估了我们的方法，并表明我们的个性化先验知识在数量和质量上都优于最先进的替代方法。
<details>	<summary>英文摘要</summary>	We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives. </details>
<details>	<summary>注释</summary>	Project webpage: https://mystyle-personalized-prior.github.io/, Video: https://youtu.be/QvOdQR3tlOc </details>
<details>	<summary>邮件日期</summary>	2022年04月01日</details>

# 494、用于MR图像超分辨率的交叉模态高频变压器
- [ ] Cross-Modality High-Frequency Transformer for MR Image Super-Resolution 
时间：2022年03月29日                         第一作者：Chaowei Fang                       [链接](https://arxiv.org/abs/2203.15314).                     
## 摘要：提高磁共振（MR）图像数据的分辨率对于计算机辅助诊断和脑功能分析至关重要。更高的分辨率有助于捕获更详细的内容，但通常会导致更低的信噪比和更长的扫描时间。为此，磁共振图像超分辨率近年来已成为一个广泛关注的话题。现有的工作建立了基于卷积神经网络（CNN）的传统结构的广泛深度模型。在这项工作中，为了进一步推进这一研究领域，我们早期致力于构建一个基于变压器的MR图像超分辨率框架，并对探索有价值的领域先验知识进行了仔细设计。具体地，我们考虑两个域先验，包括高频结构先验和模态间上下文先验，并建立一种新的变压器体系结构，称为交叉模态高频变压器（COHF T），将这些先验引入超分辨低分辨率（LR）MR图像。在两个数据集上的综合实验表明，Cohf-T实现了新的最先进的性能。
<details>	<summary>英文摘要</summary>	Improving the resolution of magnetic resonance (MR) image data is critical to computer-aided diagnosis and brain function analysis. Higher resolution helps to capture more detailed content, but typically induces to lower signal-to-noise ratio and longer scanning time. To this end, MR image super-resolution has become a widely-interested topic in recent times. Existing works establish extensive deep models with the conventional architectures based on convolutional neural networks (CNN). In this work, to further advance this research field, we make an early effort to build a Transformer-based MR image super-resolution framework, with careful designs on exploring valuable domain prior knowledge. Specifically, we consider two-fold domain priors including the high-frequency structure prior and the inter-modality context prior, and establish a novel Transformer architecture, called Cross-modality high-frequency Transformer (Cohf-T), to introduce such priors into super-resolving the low-resolution (LR) MR images. Comprehensive experiments on two datasets indicate that Cohf-T achieves new state-of-the-art performance. </details>
<details>	<summary>邮件日期</summary>	2022年03月30日</details>

# 493、三维递归卷积自动编码器在扩散磁共振成像中的角度超分辨率
- [ ] Angular Super-Resolution in Diffusion MRI with a 3D Recurrent Convolutional Autoencoder 
时间：2022年03月29日                         第一作者：Matthew Lyon                       [链接](https://arxiv.org/abs/2203.15598).                     
## 摘要：在临床环境中，高分辨率扩散MRI（dMRI）数据通常受到有限扫描时间的限制，从而限制了下游分析技术的使用。在这项工作中，我们开发了一种三维递归卷积神经网络（RCNN），能够在角度（q空间）域超分辨dMRI体积。我们的方法将角度超分辨率的任务描述为使用以目标b向量为条件的3D自动编码器的逐块回归。在网络中，我们使用卷积长短时记忆（ConvLSTM）单元来模拟q空间样本之间的关系。我们将模型性能与基线球谐插值和模型结构的一维变量进行比较。我们表明，在不同的子采样方案和b值中，3D模型的错误率最低。3D RCNN的相对性能在非常低的角度分辨率域中最高。此项目的代码可在https://github.com/m-lyon/dMRI-RCNN.
<details>	<summary>英文摘要</summary>	High resolution diffusion MRI (dMRI) data is often constrained by limited scanning time in clinical settings, thus restricting the use of downstream analysis techniques that would otherwise be available. In this work we develop a 3D recurrent convolutional neural network (RCNN) capable of super-resolving dMRI volumes in the angular (q-space) domain. Our approach formulates the task of angular super-resolution as a patch-wise regression using a 3D autoencoder conditioned on target b-vectors. Within the network we use a convolutional long short term memory (ConvLSTM) cell to model the relationship between q-space samples. We compare model performance against a baseline spherical harmonic interpolation and a 1D variant of the model architecture. We show that the 3D model has the lowest error rates across different subsampling schemes and b-values. The relative performance of the 3D RCNN is greatest in the very low angular resolution domain. Code for this project is available at https://github.com/m-lyon/dMRI-RCNN. </details>
<details>	<summary>注释</summary>	Accepted to published in MIDL'22. Openreview link: https://openreview.net/forum?id=U6HJMtAgW-N </details>
<details>	<summary>邮件日期</summary>	2022年03月30日</details>

# 492、使用多摄像头视频三元组的基于参考的视频超分辨率
- [ ] Reference-based Video Super-Resolution Using Multi-Camera Video Triplets 
时间：2022年03月28日                         第一作者：Junyong Lee                        [链接](https://arxiv.org/abs/2203.14537).                     
## 摘要：我们提出了第一种基于参考的视频超分辨率（RefVSR）方法，该方法利用参考视频获得高保真结果。我们将重点放在三摄像机环境下的RefVSR上，我们的目标是利用广角和长焦视频超分辨率超宽视频。我们介绍了第一个RefVSR网络，该网络将时态参考特征与从低分辨率帧中提取的特征进行反复对齐和传播。为了便于时间参考特征的融合和传播，我们提出了一种传播时间融合模块。为了学习和评估我们的网络，我们展示了第一个RefVSR数据集，由三组超宽、广角和长焦视频组成，这些视频同时取自智能手机的三个摄像头。我们还提出了一种两阶段训练策略，充分利用所提出的数据集中的视频三元组，实现真实世界的4x视频超分辨率。我们对我们的方法进行了广泛的评估，结果显示了4x超分辨率的最新性能。
<details>	<summary>英文摘要</summary>	We propose the first reference-based video super-resolution (RefVSR) approach that utilizes reference videos for high-fidelity results. We focus on RefVSR in a triple-camera setting, where we aim at super-resolving a low-resolution ultra-wide video utilizing wide-angle and telephoto videos. We introduce the first RefVSR network that recurrently aligns and propagates temporal reference features fused with features extracted from low-resolution frames. To facilitate the fusion and propagation of temporal reference features, we propose a propagative temporal fusion module. For learning and evaluation of our network, we present the first RefVSR dataset consisting of triplets of ultra-wide, wide-angle, and telephoto videos concurrently taken from triple cameras of a smartphone. We also propose a two-stage training strategy fully utilizing video triplets in the proposed dataset for real-world 4x video super-resolution. We extensively evaluate our method, and the result shows the state-of-the-art performance in 4x super-resolution. </details>
<details>	<summary>注释</summary>	CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 491、HIME：具有多个样本的高效头照图像超分辨率
- [ ] HIME: Efficient Headshot Image Super-Resolution with Multiple Exemplars 
时间：2022年03月28日                         第一作者：Xiaoyu Xiang                       [链接](https://arxiv.org/abs/2203.14863).                     
## 摘要：利用同一身份的一组高分辨率样本恢复低分辨率头像中丢失的信息是一个很有希望的方向。参考集合中的互补图像可以改善生成的多个不同视图和姿势的头像质量。然而，最好地利用多个范例是一个挑战：无法保证每个范例的质量和一致性。使用低质量和不匹配的图像作为参考会影响输出结果。为了克服这些问题，我们提出了一种高效的多样本网络头照图像超分辨率（HIME）方法。与以前的方法相比，我们的网络可以有效地处理输入和参考之间的不对齐，而不需要面部先验知识，并以端到端的方式学习聚合的参考集表示。此外，为了重建更详细的面部特征，我们提出了一种相关损失，在可控的空间范围内提供了丰富的局部纹理表示。实验结果表明，该框架不仅比现有的范例指导方法具有更少的计算量，而且还具有更好的定性和定量性能。
<details>	<summary>英文摘要</summary>	A promising direction for recovering the lost information in low-resolution headshot images is utilizing a set of high-resolution exemplars from the same identity. Complementary images in the reference set can improve the generated headshot quality across many different views and poses. However, it is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose an efficient Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. Compared with previous methods, our network can effectively handle the misalignment between the input and the reference without requiring facial priors and learn the aggregated reference set representation in an end-to-end manner. Furthermore, to reconstruct more detailed facial features, we propose a correlation loss that provides a rich representation of the local texture in a controllable spatial range. Experimental results demonstrate that the proposed framework not only has significantly fewer computation cost than recent exemplar-guided methods but also achieves better qualitative and quantitative performance. </details>
<details>	<summary>注释</summary>	Technical Report </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 490、RSTT：用于时空视频超分辨率的实时时空转换器
- [ ] RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution 
时间：2022年03月27日                         第一作者：Zhicheng Geng                       [链接](https://arxiv.org/abs/2203.14186).                     
## 摘要：空时视频超分辨率（STVSR）的任务是对低帧速率（LFR）和低分辨率（LR）的视频进行插值，以产生高帧速率（HFR）和高分辨率（HR）的视频。现有的基于卷积神经网络（CNN）的方法在结构复杂、推理速度慢的情况下，取得了令人满意的视觉效果。我们建议通过使用时空转换器来解决这个问题，该转换器自然地将时空超分辨率模块合并到单个模型中。与基于CNN的方法不同，我们没有明确使用分离的构建块进行时间插值和空间超分辨率；相反，我们只使用一个端到端转换器架构。具体来说，编码器根据输入的LFR和LR帧构建可重用的字典，然后在解码器部分使用该字典来合成HFR和HR帧。与最先进的TMNet{xu2021temporal}相比，我们的网络小了60美元（450万对1230万参数），快了80美元（在720美元\times576美元的帧上26.2fps对14.3fps），而不牺牲太多性能。源代码可在https://github.com/llmpass/RSTT.
<details>	<summary>英文摘要</summary>	Space-time video super-resolution (STVSR) is the task of interpolating videos with both Low Frame Rate (LFR) and Low Resolution (LR) to produce High-Frame-Rate (HFR) and also High-Resolution (HR) counterparts. The existing methods based on Convolutional Neural Network~(CNN) succeed in achieving visually satisfied results while suffer from slow inference speed due to their heavy architectures. We propose to resolve this issue by using a spatial-temporal transformer that naturally incorporates the spatial and temporal super resolution modules into a single model. Unlike CNN-based methods, we do not explicitly use separated building blocks for temporal interpolations and spatial super-resolutions; instead, we only use a single end-to-end transformer architecture. Specifically, a reusable dictionary is built by encoders based on the input LFR and LR frames, which is then utilized in the decoder part to synthesize the HFR and HR frames. Compared with the state-of-the-art TMNet \cite{xu2021temporal}, our network is $60\%$ smaller (4.5M vs 12.3M parameters) and $80\%$ faster (26.2fps vs 14.3fps on $720\times576$ frames) without sacrificing much performance. The source code is available at https://github.com/llmpass/RSTT. </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 489、虹膜生物特征识别中的超分辨率及其字典学习评价
- [ ] A Survey of Super-Resolution in Iris Biometrics with Evaluation of Dictionary-Learning 
时间：2022年03月27日                         第一作者：F. Alonso-Fern                       [链接](https://arxiv.org/abs/2203.14203).                     
## 摘要：分辨率的缺乏对基于图像的生物特征识别的性能有负面影响。虽然已经提出了许多通用的超分辨率方法来恢复低分辨率图像，但它们通常旨在增强图像的视觉外观。然而，生物特征图像的视觉增强并不一定与更好的识别性能相关。因此，重建方法需要结合来自目标生物特征模式的特定信息，以有效提高识别率。本文对文献中提出的虹膜超分辨率方法进行了综述。我们还采用了基于局部图像块PCA特征变换的特征块重建方法。虹膜的结构是通过构建一个依赖于补丁位置的字典来利用的。此外，图像面片被单独恢复，具有自己的重建权重。这允许对解决方案进行局部优化，有助于保存局部信息。为了评估该算法，我们对CASIA Interval V3数据库中的高分辨率图像进行了降级。考虑了不同的恢复，分辨率为15x15像素。据我们所知，这是文献中使用的最小分辨率之一。该框架由六个公共虹膜比较器进行补充，用于进行生物特征验证和识别实验。实验结果表明，在很低的分辨率下，该方法明显优于双线性插值和双三次插值。当考虑到只有15x15像素的虹膜图像时，许多比较器的性能达到了令人印象深刻的等错误率，低至5%，最高精度为77-84%。这些结果清楚地证明了在匹配之前使用经过训练的超分辨率技术来提高虹膜图像质量的好处。
<details>	<summary>英文摘要</summary>	The lack of resolution has a negative impact on the performance of image-based biometrics. While many generic super-resolution methods have been proposed to restore low-resolution images, they usually aim to enhance their visual appearance. However, a visual enhancement of biometric images does not necessarily correlate with a better recognition performance. Reconstruction approaches need thus to incorporate specific information from the target biometric modality to effectively improve recognition. This paper presents a comprehensive survey of iris super-resolution approaches proposed in the literature. We have also adapted an Eigen-patches reconstruction method based on PCA Eigen-transformation of local image patches. The structure of the iris is exploited by building a patch-position dependent dictionary. In addition, image patches are restored separately, having their own reconstruction weights. This allows the solution to be locally optimized, helping to preserve local information. To evaluate the algorithm, we degraded high-resolution images from the CASIA Interval V3 database. Different restorations were considered, with 15x15 pixels being the smallest resolution. To the best of our knowledge, this is among the smallest resolutions employed in the literature. The framework is complemented with six public iris comparators, which were used to carry out biometric verification and identification experiments. Experimental results show that the proposed method significantly outperforms both bilinear and bicubic interpolation at very low-resolution. The performance of a number of comparators attains an impressive Equal Error Rate as low as 5%, and a Top-1 accuracy of 77-84% when considering iris images of only 15x15 pixels. These results clearly demonstrate the benefit of using trained super-resolution techniques to improve the quality of iris images prior to matching. </details>
<details>	<summary>注释</summary>	Published at IEEE Access DOI: 10.1109/ACCESS.2018.2889395 </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 488、用于真实世界图像超分辨率的高效自适应网络
- [ ] Efficient and Degradation-Adaptive Network for Real-World Image Super-Resolution 
时间：2022年03月27日                         第一作者：Jie Liang                        [链接](https://arxiv.org/abs/2203.14216).                     
## 摘要：在实际应用中，由于真实图像退化的复杂性和计算资源的有限性，高效的真实图像超分辨率（real-ISR）是一项具有挑战性的任务。最近，通过对图像退化空间进行建模，对真实ISR的研究取得了重大进展；然而，这些方法在很大程度上依赖于重型骨干网络，并且它们在处理不同降级级别的图像时缺乏灵活性。在本文中，我们提出了一种高效且有效的退化自适应超分辨率（DASR）网络，其参数通过估计每个输入图像的退化来自适应指定。具体地说，使用一个微型回归网络来预测输入图像的退化参数，同时使用具有相同拓扑结构的多个卷积专家联合优化，通过专家的非线性混合指定网络参数。多个专家的联合优化和退化自适应管道极大地扩展了模型处理不同级别退化的能力，同时由于只使用一个自适应指定的网络来超分辨率输入图像，因此推理仍然有效。我们的大量实验表明，所提出的DASR不仅在处理不同退化程度的真实图像方面比现有方法更有效，而且易于部署。代码、模型和数据集可在https://github.com/csjliang/DASR.
<details>	<summary>英文摘要</summary>	Efficient and effective real-world image super-resolution (Real-ISR) is a challenging task due to the unknown complex degradation of real-world images and the limited computation resources in practical applications. Recent research on Real-ISR has achieved significant progress by modeling the image degradation space; however, these methods largely rely on heavy backbone networks and they are inflexible to handle images of different degradation levels. In this paper, we propose an efficient and effective degradation-adaptive super-resolution (DASR) network, whose parameters are adaptively specified by estimating the degradation of each input image. Specifically, a tiny regression network is employed to predict the degradation parameters of the input image, while several convolutional experts with the same topology are jointly optimized to specify the network parameters via a non-linear mixture of experts. The joint optimization of multiple experts and the degradation-adaptive pipeline significantly extend the model capacity to handle degradations of various levels, while the inference remains efficient since only one adaptively specified network is used for super-resolving the input image. Our extensive experiments demonstrate that the proposed DASR is not only much more effective than existing methods on handling real-world images with different degradation levels but also efficient for easy deployment. Codes, models and datasets are available at https://github.com/csjliang/DASR. </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 487、引导超分辨率的学习图正则化
- [ ] Learning Graph Regularisation for Guided Super-Resolution 
时间：2022年03月27日                         第一作者：Riccardo de Lutio                        [链接](https://arxiv.org/abs/2203.14297).                     
## 摘要：我们介绍了一种新的引导超分辨率公式。它的核心是一个可微优化层，它在一个学习的亲和图上运行。学习到的图形潜力使利用指南图像中丰富的上下文信息成为可能，而架构内的显式图形优化保证了高分辨率目标与低分辨率源之间的严格保真度。由于决定将源用作约束，而不是仅作为预测的输入，我们的方法不同于用于引导超分辨率的最先进的深度体系结构，该体系结构产生的目标在下采样时仅大致再现源。这不仅在理论上具有吸引力，而且还能产生更清晰、更自然的图像。我们的方法的一个关键特性是，尽管图形连接性仅限于像素点阵，但相关的边缘电位是通过深度特征提取器学习的，并且可以在大的感受野上编码丰富的上下文信息。通过利用稀疏图连通性，可以通过优化层传播梯度，并从数据中学习边缘电位。我们在多个数据集上对我们的方法进行了广泛的评估，在定量重建误差方面始终优于最近的基线，同时也提供了更清晰的视觉输出。此外，我们还证明了我们的方法特别适用于训练期间未发现的新数据集。
<details>	<summary>英文摘要</summary>	We introduce a novel formulation for guided super-resolution. Its core is a differentiable optimisation layer that operates on a learned affinity graph. The learned graph potentials make it possible to leverage rich contextual information from the guide image, while the explicit graph optimisation within the architecture guarantees rigorous fidelity of the high-resolution target to the low-resolution source. With the decision to employ the source as a constraint rather than only as an input to the prediction, our method differs from state-of-the-art deep architectures for guided super-resolution, which produce targets that, when downsampled, will only approximately reproduce the source. This is not only theoretically appealing, but also produces crisper, more natural-looking images. A key property of our method is that, although the graph connectivity is restricted to the pixel lattice, the associated edge potentials are learned with a deep feature extractor and can encode rich context information over large receptive fields. By taking advantage of the sparse graph connectivity, it becomes possible to propagate gradients through the optimisation layer and learn the edge potentials from data. We extensively evaluate our method on several datasets, and consistently outperform recent baselines in terms of quantitative reconstruction errors, while also delivering visually sharper outputs. Moreover, we demonstrate that our method generalises particularly well to new datasets not seen during training. </details>
<details>	<summary>注释</summary>	CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 486、多对比度MRI超分辨率的多尺度上下文匹配和聚合
- [ ] Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution 
时间：2022年03月26日                         第一作者：Guangyuan Li                       [链接](https://arxiv.org/abs/2203.13963).                     
## 摘要：磁共振成像（MRI）可以呈现相同解剖结构的多对比度图像，从而实现多对比度超分辨率（SR）技术。与使用单对比度的SR重建相比，多对比度SR重建通过利用不同成像模式中嵌入的不同但互补的信息，有望获得更高质量的SR图像。然而，现有的方法仍然存在两个缺点：（1）它们忽略了不同尺度的多对比特征包含不同的解剖细节，因此缺乏有效的机制来匹配和融合这些特征以更好地重建；（2）它们在捕捉长程依赖性方面仍然不足，这对于具有复杂解剖结构的区域来说是必不可少的。我们提出了一个新的网络，通过开发一套创新的多尺度上下文匹配和聚合技术来全面解决这些问题；我们称之为麦克默斯。首先，我们驯服了变压器，以模拟参考图像和目标图像中的长期依赖关系。然后，提出了一种新的多尺度上下文匹配方法，从不同尺度的参考特征中获取相应的上下文。此外，我们还引入了一种多尺度聚合机制来逐步交互聚合多尺度匹配特征，以重建目标SR MR图像。大量实验表明，我们的网络性能优于最先进的方法，在临床实践中具有巨大的应用潜力。代码可在https://github.com/XAIMI-Lab/McMRSR.
<details>	<summary>英文摘要</summary>	Magnetic resonance imaging (MRI) can present multi-contrast images of the same anatomical structures, enabling multi-contrast super-resolution (SR) techniques. Compared with SR reconstruction using a single-contrast, multi-contrast SR reconstruction is promising to yield SR images with higher quality by leveraging diverse yet complementary information embedded in different imaging modalities. However, existing methods still have two shortcomings: (1) they neglect that the multi-contrast features at different scales contain different anatomical details and hence lack effective mechanisms to match and fuse these features for better reconstruction; and (2) they are still deficient in capturing long-range dependencies, which are essential for the regions with complicated anatomical structures. We propose a novel network to comprehensively address these problems by developing a set of innovative Transformer-empowered multi-scale contextual matching and aggregation techniques; we call it McMRSR. Firstly, we tame transformers to model long-range dependencies in both reference and target images. Then, a new multi-scale contextual matching method is proposed to capture corresponding contexts from reference features at different scales. Furthermore, we introduce a multi-scale aggregation mechanism to gradually and interactively aggregate multi-scale matched features for reconstructing the target SR MR image. Extensive experiments demonstrate that our network outperforms state-of-the-art approaches and has great potential to be applied in clinical practice. Codes are available at https://github.com/XAIMI-Lab/McMRSR. </details>
<details>	<summary>注释</summary>	CVPR 2022 accepted </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 485、用于盲图像超分辨率的深度约束最小二乘法
- [ ] Deep Constrained Least Squares for Blind Image Super-Resolution 
时间：2022年03月25日                         第一作者：Ziwei Luo                       [链接](https://arxiv.org/abs/2202.07508).                     
<details>	<summary>注释</summary>	CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月28日</details>

# 484、基于正则化反向扩散的MR图像去噪与超分辨率
- [ ] MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion 
时间：2022年03月23日                         第一作者：Hyungjin Chung                       [链接](https://arxiv.org/abs/2203.12621).                     
## 摘要：核磁共振成像（MRI）的患者扫描常常会受到噪声的影响，这会妨碍此类图像的诊断能力。作为一种减轻这种伪影的方法，去噪在医学成像界和普通学科界都得到了广泛的研究。然而，最近基于深度神经网络的方法大多依赖于最小均方误差（MMSE）估计，这往往会产生模糊的输出。此外，这种模型在实际情况下使用时会受到影响：分布外的数据，以及偏离通常参数噪声模型的复杂噪声分布。在这项工作中，我们提出了一种新的基于分数的反向扩散采样去噪方法，它克服了上述所有缺点。我们的网络仅通过冠状位膝关节扫描进行训练，即使在受复杂混合噪声污染的体内肝脏MRI数据分布不均的情况下也表现出色。此外，我们还提出了一种在相同网络条件下提高去噪图像分辨率的方法。通过大量实验，我们表明，我们的方法建立了最先进的性能，同时具有之前的MMSE去噪器所不具备的理想特性：灵活选择去噪程度，并量化不确定性。
<details>	<summary>英文摘要</summary>	Patient scans from MRI often suffer from noise, which hampers the diagnostic capability of such images. As a method to mitigate such artifact, denoising is largely studied both within the medical imaging community and beyond the community as a general subject. However, recent deep neural network-based approaches mostly rely on the minimum mean squared error (MMSE) estimates, which tend to produce a blurred output. Moreover, such models suffer when deployed in real-world sitautions: out-of-distribution data, and complex noise distributions that deviate from the usual parametric noise models. In this work, we propose a new denoising method based on score-based reverse diffusion sampling, which overcomes all the aforementioned drawbacks. Our network, trained only with coronal knee scans, excels even on out-of-distribution in vivo liver MRI data, contaminated with complex mixture of noise. Even more, we propose a method to enhance the resolution of the denoised image with the same network. With extensive experiments, we show that our method establishes state-of-the-art performance, while having desirable properties which prior MMSE denoisers did not have: flexibly choosing the extent of denoising, and quantifying uncertainty. </details>
<details>	<summary>邮件日期</summary>	2022年03月25日</details>

# 483、用于盲图像超分辨率的深度约束最小二乘法
- [ ] Deep Constrained Least Squares for Blind Image Super-Resolution 
时间：2022年03月23日                         第一作者：Ziwei Luo                       [链接](https://arxiv.org/abs/2202.07508).                     
<details>	<summary>注释</summary>	CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月24日</details>

# 482、可扩展单图像超分辨率的自适应补丁
- [ ] Adaptive Patch Exiting for Scalable Single Image Super-Resolution 
时间：2022年03月22日                         第一作者：Shizun Wang                       [链接](https://arxiv.org/abs/2203.11589).                     
## 摘要：由于未来的计算是异构的，可伸缩性是单图像超分辨率的一个关键问题。最近的工作试图训练一个网络，可以部署在不同容量的平台上。然而，它们依赖于像素级稀疏卷积，这对硬件不友好，实际加速比有限。由于图像可以分为不同的块，这些块具有不同的恢复困难，我们提出了一种基于自适应块退出（APE）的可伸缩方法，以实现更实用的加速比。具体来说，我们建议训练一个回归器来预测斑块每层的增量容量。一旦增量容量低于阈值，补丁就可以在特定层退出。我们的方法可以通过改变增量容量的阈值来轻松调整性能和效率之间的权衡。此外，我们还提出了一种新的策略来支持我们方法的网络训练。我们在各种主干、数据集和比例因子上进行了大量实验，以证明我们的方法的优势。代码将被发布。
<details>	<summary>英文摘要</summary>	Since the future of computing is heterogeneous, scalability is a crucial problem for single image super-resolution. Recent works try to train one network, which can be deployed on platforms with different capacities. However, they rely on the pixel-wise sparse convolution, which is not hardware-friendly and achieves limited practical speedup. As image can be divided into patches, which have various restoration difficulties, we present a scalable method based on Adaptive Patch Exiting (APE) to achieve more practical speedup. Specifically, we propose to train a regressor to predict the incremental capacity of each layer for the patch. Once the incremental capacity is below the threshold, the patch can exit at the specific layer. Our method can easily adjust the trade-off between performance and efficiency by changing the threshold of incremental capacity. Furthermore, we propose a novel strategy to enable the network training of our method. We conduct extensive experiments across various backbones, datasets and scaling factors to demonstrate the advantages of our method. Code will be released. </details>
<details>	<summary>邮件日期</summary>	2022年03月23日</details>

# 481、ARM：任何时候的超分辨率方法
- [ ] ARM: Any-Time Super-Resolution Method 
时间：2022年03月21日                         第一作者：Bohong Chen                       [链接](https://arxiv.org/abs/2203.10812).                     
## 摘要：本文提出了一种任意时间超分辨率方法（ARM）来处理过参数化的单幅图像超分辨率（SISR）模型。我们的ARM由三个观察结果驱动：（1）不同大小的SISR网络的不同图像块的性能不同。（2） 在计算开销和重建图像的性能之间存在折衷。（3） 给定一幅输入图像，其边缘信息可以有效地估计其峰值信噪比。随后，我们训练了一个包含不同大小SISR子网的ARM超网，以处理各种复杂度的图像块。为此，我们构建了一个边缘到峰值信噪比查找表，将图像块的边缘分数映射到每个子网的峰值信噪比性能，以及子网的一组计算成本。在推理过程中，为了更好地权衡计算性能，将图像块分别分布到不同的子网。此外，每个SISR子网共享ARM超网的权重，因此不引入额外的参数。多个子网的设置可以很好地使SISR模型的计算成本适应动态可用的硬件资源，允许SISR任务随时处于服务状态。以流行的SISR网络为主干，对不同大小的分辨率数据集进行了大量实验，验证了ARM的有效性和多功能性。源代码位于\url{https://github.com/chenbong/ARM-Net}.
<details>	<summary>英文摘要</summary>	This paper proposes an Any-time super-Resolution Method (ARM) to tackle the over-parameterized single image super-resolution (SISR) models. Our ARM is motivated by three observations: (1) The performance of different image patches varies with SISR networks of different sizes. (2) There is a tradeoff between computation overhead and performance of the reconstructed image. (3) Given an input image, its edge information can be an effective option to estimate its PSNR. Subsequently, we train an ARM supernet containing SISR subnets of different sizes to deal with image patches of various complexity. To that effect, we construct an Edge-to-PSNR lookup table that maps the edge score of an image patch to the PSNR performance for each subnet, together with a set of computation costs for the subnets. In the inference, the image patches are individually distributed to different subnets for a better computation-performance tradeoff. Moreover, each SISR subnet shares weights of the ARM supernet, thus no extra parameters are introduced. The setting of multiple subnets can well adapt the computational cost of SISR model to the dynamically available hardware resources, allowing the SISR task to be in service at any time. Extensive experiments on resolution datasets of different sizes with popular SISR networks as backbones verify the effectiveness and the versatility of our ARM. The source code is available at \url{https://github.com/chenbong/ARM-Net}. </details>
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 480、视频超分辨率光流技术综述
- [ ] Optical Flow for Video Super-Resolution: A Survey 
时间：2022年03月20日                         第一作者：Zhigang Tu                       [链接](https://arxiv.org/abs/2203.10462).                     
## 摘要：视频超分辨率是当前计算机视觉领域最活跃的研究课题之一，它在许多视觉应用中发挥着重要作用。通常，视频超分辨率包含一个重要组件，即运动补偿，用于估计连续视频帧之间的位移，以便进行时间对齐。光流可以在连续帧之间提供密集的亚像素运动，是执行此任务最常用的方法之一。为了更好地理解光流在视频超分辨率中的作用，在这项工作中，我们首次对这一主题进行了全面的综述。本研究涉及以下主要主题：超分辨率的功能（即为什么我们需要超分辨率）；视频超分辨率的概念（即什么是视频超分辨率）；评估指标的描述（即（视频）超分辨率的表现）；介绍了基于光流的视频超分辨率技术；利用光流捕捉视频超分辨率的时间相关性的研究。重点对基于深度学习的视频超分辨率方法进行了深入研究，并对一些有代表性的算法进行了分析和比较。此外，我们还强调了一些有希望的研究方向和有待进一步解决的开放性问题。
<details>	<summary>英文摘要</summary>	Video super-resolution is currently one of the most active research topics in computer vision as it plays an important role in many visual applications. Generally, video super-resolution contains a significant component, i.e., motion compensation, which is used to estimate the displacement between successive video frames for temporal alignment. Optical flow, which can supply dense and sub-pixel motion between consecutive frames, is among the most common ways for this task. To obtain a good understanding of the effect that optical flow acts in video super-resolution, in this work, we conduct a comprehensive review on this subject for the first time. This investigation covers the following major topics: the function of super-resolution (i.e., why we require super-resolution); the concept of video super-resolution (i.e., what is video super-resolution); the description of evaluation metrics (i.e., how (video) superresolution performs); the introduction of optical flow based video super-resolution; the investigation of using optical flow to capture temporal dependency for video super-resolution. Prominently, we give an in-depth study of the deep learning based video super-resolution method, where some representative algorithms are analyzed and compared. Additionally, we highlight some promising research directions and open issues that should be further addressed. </details>
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 479、基于光流复用的时空视频超分辨率双向递归网络
- [ ] Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution 
时间：2022年03月19日                         第一作者：Yuantong Zhang                       [链接](https://arxiv.org/abs/2110.06786).                     
<details>	<summary>注释</summary>	We use bicubic downsampling to facilitate fair comparison </details>
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 478、使用高效超分辨率方法的自拍眼周验证
- [ ] Selfie Periocular Verification using an Efficient Super-Resolution Approach 
时间：2022年03月18日                         第一作者：Juan Tapia                       [链接](https://arxiv.org/abs/2102.08449).                     
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 477、用于空间变形的文本注意网络鲁棒场景文本图像超分辨率
- [ ] A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution 
时间：2022年03月18日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2203.09388).                     
<details>	<summary>注释</summary>	Accepted to CVPR2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月21日</details>

# 476、细节还是伪影：真实图像超分辨率的局部判别学习方法
- [ ] Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution 
时间：2022年03月17日                         第一作者：Jie Liang                        [链接](https://arxiv.org/abs/2203.09195).                     
## 摘要：具有生成性对抗网络（GAN）的单幅图像超分辨率（SISR）由于具有生成丰富细节的潜力，近年来受到了越来越多的关注。然而，GAN的训练是不稳定的，它经常会引入许多感知上不愉快的伪影以及生成的细节。在本文中，我们证明了训练一个基于GAN的SISR模型是可能的，该模型可以稳定地生成感知真实的细节，同时抑制视觉伪影。基于伪影区域的局部统计（例如，残差方差）通常不同于感知友好细节区域的观察，我们开发了一个框架来区分GAN生成的伪影和真实细节，并由此生成伪影图以规范和稳定模型训练过程。我们提出的局部判别学习（LDL）方法简单而有效，可以很容易地插入现成的SISR方法中，并提高其性能。实验表明，LDL优于最先进的基于GAN的SISR方法，不仅在合成数据集和真实数据集上实现了更高的重建精度，而且获得了更好的感知质量。代码和型号可在https://github.com/csjliang/LDL.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) with generative adversarial networks (GAN) has recently attracted increasing attention due to its potentials to generate rich details. However, the training of GAN is unstable, and it often introduces many perceptually unpleasant artifacts along with the generated details. In this paper, we demonstrate that it is possible to train a GAN-based SISR model which can stably generate perceptually realistic details while inhibiting visual artifacts. Based on the observation that the local statistics (e.g., residual variance) of artifact areas are often different from the areas of perceptually friendly details, we develop a framework to discriminate between GAN-generated artifacts and realistic details, and consequently generate an artifact map to regularize and stabilize the model training process. Our proposed locally discriminative learning (LDL) method is simple yet effective, which can be easily plugged in off-the-shelf SISR methods and boost their performance. Experiments demonstrate that LDL outperforms the state-of-the-art GAN based SISR methods, achieving not only higher reconstruction accuracy but also superior perceptual quality on both synthetic and real-world datasets. Codes and models are available at https://github.com/csjliang/LDL. </details>
<details>	<summary>注释</summary>	To appear at CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 475、用于空间变形的文本注意网络鲁棒场景文本图像超分辨率
- [ ] A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution 
时间：2022年03月17日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2203.09388).                     
## 摘要：场景文本图像超分辨率旨在提高低分辨率图像中文本的分辨率和可读性。尽管深度卷积神经网络（CNN）已经取得了显著的改进，但对于空间变形的文本，尤其是旋转和曲线形状的文本，仍然很难重建高分辨率图像。这是因为当前基于CNN的方法采用基于局部性的操作，无法有效处理变形引起的变化。在本文中，我们提出了一个基于CNN的文本注意网络（TATT）来解决这个问题。文本识别模块首先提取文本语义作为文本先验信息。然后，我们设计了一个新的基于转换器的模块，该模块利用全局注意机制，在文本重建过程之前对文本进行语义引导。此外，我们还提出了一种文本结构一致性损失方法，通过对规则文本和变形文本的重建施加结构一致性来改善视觉外观。在基准TextZoom数据集上的实验表明，所提出的TATT不仅在PSNR/SSIM度量方面达到了最先进的性能，而且还显著提高了下游文本识别任务中的识别精度，尤其是对于具有多方向和曲线形状的文本实例。代码可在https://github.com/mjq11302010044/TATT.
<details>	<summary>英文摘要</summary>	Scene text image super-resolution aims to increase the resolution and readability of the text in low-resolution images. Though significant improvement has been achieved by deep convolutional neural networks (CNNs), it remains difficult to reconstruct high-resolution images for spatially deformed texts, especially rotated and curve-shaped ones. This is because the current CNN-based methods adopt locality-based operations, which are not effective to deal with the variation caused by deformations. In this paper, we propose a CNN based Text ATTention network (TATT) to address this problem. The semantics of the text are firstly extracted by a text recognition module as text prior information. Then we design a novel transformer-based module, which leverages global attention mechanism, to exert the semantic guidance of text prior to the text reconstruction process. In addition, we propose a text structure consistency loss to refine the visual appearance by imposing structural consistency on the reconstructions of regular and deformed texts. Experiments on the benchmark TextZoom dataset show that the proposed TATT not only achieves state-of-the-art performance in terms of PSNR/SSIM metrics, but also significantly improves the recognition accuracy in the downstream text recognition task, particularly for text instances with multi-orientation and curved shapes. Code is available at https://github.com/mjq11302010044/TATT. </details>
<details>	<summary>注释</summary>	Accepted to CVPR2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 474、采用深度可变自动编码器的图像超分辨率
- [ ] Image Super-Resolution With Deep Variational Autoencoders 
时间：2022年03月17日                         第一作者：Darius Chira                       [链接](https://arxiv.org/abs/2203.09445).                     
## 摘要：图像超分辨率（SR）技术用于从低分辨率图像生成高分辨率图像。迄今为止，自回归模型和生成性对抗网络（GANs）等深层生成模型已被证明能有效地模拟高分辨率图像。基于变分自动编码器（VAE）的模型经常因其生成性能差而受到批评，但随着VDVAE（甚深VAE）等新技术的发展，现在有强有力的证据表明，在高分辨率图像生成方面，深VAE有可能优于当前最先进的模型。在本文中，我们介绍了VDVAE-SR，这是一种新的模型，旨在利用最新的深度VAE方法，利用预训练VDVAE上的转移学习来提高图像的超分辨率。通过定性和定量评估，我们表明该模型与其他最先进的方法具有竞争力。
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. Models based on Variational Autoencoders (VAEs) have often been criticized for their feeble generative performance, but with new advancements such as VDVAE (very deep VAE), there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon image super-resolution using transfer learning on pretrained VDVAEs. Through qualitative and quantitative evaluations, we show that the proposed model is competitive with other state-of-the-art methods. </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 473、混合像素非缓冲网络实现轻量级图像超分辨率
- [ ] Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution 
时间：2022年03月16日                         第一作者：Bin Sun                       [链接](https://arxiv.org/abs/2203.08921).                     
## 摘要：卷积神经网络（CNN）在图像超分辨率（SR）方面取得了巨大的成功。然而，大多数基于CNN的深层SR模型需要大量计算才能获得高性能。用于多分辨率融合的下采样特征是提高视觉识别性能的有效途径。然而，在SR任务中，这是违反直觉的，它需要将低分辨率输入投射到高分辨率。在本文中，我们通过在SR任务中引入高效的下采样模块，提出了一种新的混合像素非缓冲网络（HPUN）。该网络包含像素非缓冲下采样和自残差深度可分离卷积。具体来说，我们使用像素取消缓冲操作来减少输入特征的采样，并使用分组卷积来减少通道。此外，我们还通过在深度卷积的输出中加入输入特征来提高其性能。在基准数据集上的实验表明，我们的HPUN以较少的参数和计算成本实现并超过了最先进的重建性能。
<details>	<summary>英文摘要</summary>	Convolutional neural network (CNN) has achieved great success on image super-resolution (SR). However, most deep CNN-based SR models take massive computations to obtain high performance. Downsampling features for multi-resolution fusion is an efficient and effective way to improve the performance of visual recognition. Still, it is counter-intuitive in the SR task, which needs to project a low-resolution input to high-resolution. In this paper, we propose a novel Hybrid Pixel-Unshuffled Network (HPUN) by introducing an efficient and effective downsampling module into the SR task. The network contains pixel-unshuffled downsampling and Self-Residual Depthwise Separable Convolutions. Specifically, we utilize pixel-unshuffle operation to downsample the input features and use grouped convolution to reduce the channels. Besides, we enhance the depthwise convolution's performance by adding the input feature to its output. Experiments on benchmark datasets show that our HPUN achieves and surpasses the state-of-the-art reconstruction performance with fewer parameters and computation costs. </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 472、实现超分辨率的真实细节恢复：基准和质量度量
- [ ] Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric 
时间：2022年03月16日                         第一作者：Eugene Lyapustin                       [链接](https://arxiv.org/abs/2203.08923).                     
## 摘要：超分辨率（SR）是近年来广泛研究的课题。SR方法可以提高整体图像和视频质量，并为进一步的内容分析创造新的可能性。但SR主流主要关注于提高生成图像的自然度，尽管可能会失去上下文准确性。这种方法可能会产生错误的数字、字符、人脸或其他结构对象，即使它们产生良好的视觉质量。在手动和自动检测和识别对象时，不正确的细节恢复可能会导致错误。为了分析图像和视频SR模型的细节恢复能力，我们基于自己的视频数据集开发了一个基准测试，其中包含SR模型通常无法正确恢复的复杂模式。我们使用我们的基准评估了32个最近的SR模型，并比较了它们保存场景上下文的能力。我们还对恢复的细节进行了众包比较，并开发了一个客观评估指标，通过与该任务的主观分数的相关性，该指标优于其他质量指标。总之，我们对基准测试结果进行了深入分析，为未来基于SR的工作提供了见解。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) has become a widely researched topic in recent years. SR methods can improve overall image and video quality and create new possibilities for further content analysis. But the SR mainstream focuses primarily on increasing the naturalness of the resulting image despite potentially losing context accuracy. Such methods may produce an incorrect digit, character, face, or other structural object even though they otherwise yield good visual quality. Incorrect detail restoration can cause errors when detecting and identifying objects both manually and automatically. To analyze the detail-restoration capabilities of image and video SR models, we developed a benchmark based on our own video dataset, which contains complex patterns that SR models generally fail to correctly restore. We assessed 32 recent SR models using our benchmark and compared their ability to preserve scene context. We also conducted a crowd-sourced comparison of restored details and developed an objective assessment metric that outperforms other quality metrics by correlation with subjective scores for this task. In conclusion, we provide a deep analysis of benchmark results that yields insights for future SR-based work. </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 471、Panini-Net：基于先验的退化感知特征插值人脸恢复
- [ ] Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration 
时间：2022年03月16日                         第一作者：Yinhuai Wang                       [链接](https://arxiv.org/abs/2203.08444).                     
## 摘要：新兴的高质量面部修复（FR）方法通常使用预先训练的GAN模型（\textit{i.e.}，StyleGAN2）作为GAN先验。然而，这些方法在面对不同的降级级别时，通常难以平衡真实性和保真度。此外，与预先训练的GAN模型相比，视觉质量仍然存在明显差距。在本文中，我们提出了一种新的基于GAN先验的退化感知特征插值网络，称为Panini网络，通过显式学习抽象表示来区分各种退化。具体而言，本文首先提出了一种无监督退化表征学习（UDRL）策略来提取输入退化图像的退化表征（DR）。然后提出了一种退化感知特征插值（DAFI）模块，用于动态融合两种类型的信息特征（输入图像特征和GAN先验特征），并根据DR消融研究灵活适应各种退化。研究揭示了DAFI的工作机制及其可编辑FR的潜力。大量实验表明，我们的Panini Net在多退化人脸恢复和人脸超分辨率方面实现了最先进的性能。源代码可在https://github.com/jianzhangcs/panini.
<details>	<summary>英文摘要</summary>	Emerging high-quality face restoration (FR) methods often utilize pre-trained GAN models (\textit{i.e.}, StyleGAN2) as GAN Prior. However, these methods usually struggle to balance realness and fidelity when facing various degradation levels. Besides, there is still a noticeable visual quality gap compared with pre-trained GAN models. In this paper, we propose a novel GAN Prior based degradation-aware feature interpolation network, dubbed Panini-Net, for FR tasks by explicitly learning the abstract representations to distinguish various degradations. Specifically, an unsupervised degradation representation learning (UDRL) strategy is first developed to extract degradation representations (DR) of the input degraded images. Then, a degradation-aware feature interpolation (DAFI) module is proposed to dynamically fuse the two types of informative features (\textit{i.e.}, features from input images and features from GAN Prior) with flexible adaption to various degradations based on DR. Ablation studies reveal the working mechanism of DAFI and its potential for editable FR. Extensive experiments demonstrate that our Panini-Net achieves state-of-the-art performance for multi-degradation face restoration and face super-resolution. The source code is available at https://github.com/jianzhangcs/panini. </details>
<details>	<summary>注释</summary>	Accepted by AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月17日</details>

# 470、基于噪声和核函数的精细退化盲图像超分辨率建模
- [ ] Blind Image Super-resolution with Elaborate Degradation Modeling on Noise and Kernel 
时间：2022年03月16日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2022 ACM-class: I.4.4 </details>
<details>	<summary>邮件日期</summary>	2022年03月17日</details>

# 469、丰富的CNN Transformer功能聚合网络，实现超分辨率
- [ ] Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution 
时间：2022年03月16日                         第一作者：Jinsu Yoo                       [链接](https://arxiv.org/abs/2203.07682).                     
<details>	<summary>注释</summary>	19 pages, 11 figures, preprint </details>
<details>	<summary>邮件日期</summary>	2022年03月17日</details>

# 468、丰富的CNN Transformer功能聚合网络，实现超分辨率
- [ ] Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution 
时间：2022年03月15日                         第一作者：Jinsu Yoo                       [链接](https://arxiv.org/abs/2203.07682).                     
## 摘要：近年来，视觉转换器和自我关注在各种计算机视觉任务中取得了令人鼓舞的成果。特别是，基于纯变压器的图像恢复架构优于现有的基于CNN的方法，该方法使用具有大量可训练参数的多任务预训练。在本文中，我们介绍了一种用于超分辨率（SR）任务的有效混合体系结构，它利用CNN的局部特征和transformers捕获的远程依赖性来进一步改进SR结果。具体地说，我们的架构由转换器和卷积分支组成，我们通过相互融合两个分支来补充每个表示，从而显著提高了性能。此外，我们还提出了一个跨尺度标记注意模块，该模块允许转换器有效地利用不同尺度标记之间的信息关系。我们提出的方法在大量基准数据集上实现了最先进的SR结果。
<details>	<summary>英文摘要</summary>	Recent vision transformers along with self-attention have achieved promising results on various computer vision tasks. In particular, a pure transformer-based image restoration architecture surpasses the existing CNN-based methods using multi-task pre-training with a large number of trainable parameters. In this paper, we introduce an effective hybrid architecture for super-resolution (SR) tasks, which leverages local features from CNNs and long-range dependencies captured by transformers to further improve the SR results. Specifically, our architecture comprises of transformer and convolution branches, and we substantially elevate the performance by mutually fusing two branches to complement each representation. Furthermore, we propose a cross-scale token attention module, which allows the transformer to efficiently exploit the informative relationships among tokens across different scales. Our proposed method achieves state-of-the-art SR results on numerous benchmark datasets. </details>
<details>	<summary>注释</summary>	19 pages, 11 figures, preprint </details>
<details>	<summary>邮件日期</summary>	2022年03月16日</details>

# 467、用于照片真实感图像超分辨率的高效深度神经网络
- [ ] Efficient Deep Neural Network for Photo-realistic Image Super-Resolution 
时间：2022年03月15日                         第一作者：Namhyuk Ahn                       [链接](https://arxiv.org/abs/1903.02240).                     
<details>	<summary>注释</summary>	Pattern Recognition </details>
<details>	<summary>邮件日期</summary>	2022年03月16日</details>

# 466、从一般到具体：在线更新盲超分辨率
- [ ] From General to Specific: Online Updating for Blind Super-Resolution 
时间：2022年03月15日                         第一作者：Shang Li                       [链接](https://arxiv.org/abs/2107.02398).                     
<details>	<summary>注释</summary>	Accepted by Pattern Recognition Journal-ref: Pattern Recognition, Volume 127, July 2022, 108613 DOI: 10.1016/j.patcog.2022.108613 </details>
<details>	<summary>邮件日期</summary>	2022年03月16日</details>

# 465、时空视频超分辨率变形注意网络
- [ ] STDAN: Deformable Attention Network for Space-Time Video Super-Resolution 
时间：2022年03月14日                         第一作者：Hai Wang                       [链接](https://arxiv.org/abs/2203.06841).                     
## 摘要：时空视频超分辨率（STVSR）的目标是提高低分辨率（LR）和低帧率（LFR）视频的时空分辨率。最近基于深度学习的方法已经取得了显著的改进，但大多数方法仅使用两个相邻的帧，即短期特征，来合成缺失帧嵌入，这会影响对连续输入LR帧的信息流的充分挖掘。此外，现有的STVSR模型几乎没有明确利用时间上下文来辅助高分辨率（HR）帧重建。为了解决这些问题，在本文中，我们为STVSR提出了一个名为STDAN的可变形注意网络。首先，我们设计了一个长短期特征插值（LSTFI）模块，该模块能够通过双向RNN结构从更多相邻的输入帧中挖掘丰富的内容用于插值过程。其次，我们提出了一种时空可变形特征聚合（STDFA）模块，在该模块中，动态视频帧中的时空上下文被自适应地捕获和聚合，以增强SR重建。在多个数据集上的实验结果表明，我们的方法优于最先进的STVSR方法。
<details>	<summary>英文摘要</summary>	The target of space-time video super-resolution (STVSR) is to increase the spatial-temporal resolution of low-resolution (LR) and low frame rate (LFR) videos. Recent approaches based on deep learning have made significant improvements, but most of them only use two adjacent frames, that is, short-term features, to synthesize the missing frame embedding, which suffers from fully exploring the information flow of consecutive input LR frames. In addition, existing STVSR models hardly exploit the temporal contexts explicitly to assist high-resolution (HR) frame reconstruction. To address these issues, in this paper, we propose a deformable attention network called STDAN for STVSR. First, we devise a long-short term feature interpolation (LSTFI) module, which is capable of excavating abundant content from more neighboring input frames for the interpolation process through a bidirectional RNN structure. Second, we put forward a spatial-temporal deformable feature aggregation (STDFA) module, in which spatial and temporal contexts in dynamic video frames are adaptively captured and aggregated to enhance SR reconstruction. Experimental results on several datasets demonstrate that our approach outperforms state-of-the-art STVSR methods. </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 464、用于图像超分辨率的高效远程注意力网络
- [ ] Efficient Long-Range Attention Network for Image Super-resolution 
时间：2022年03月13日                         第一作者：Xindong Zhang                       [链接](https://arxiv.org/abs/2203.06697).                     
## 摘要：最近，基于变换器的方法通过利用自我注意（SA）进行特征提取，在各种视觉任务中取得了令人印象深刻的结果，包括图像超分辨率（SR）。然而，在大多数现有的基于变压器的模型中，SA的计算非常昂贵，而对于SR任务，一些使用的操作可能是冗余的。这限制了SA计算的范围，从而限制了SR性能。在这项工作中，我们为图像SR提出了一种有效的远程注意网络（ELAN）。具体来说，我们首先使用移位卷积（shift-conv）来有效地提取图像的局部结构信息，同时保持与1x1卷积相同的复杂度，然后提出了一个分组多尺度自我注意（GMSA）模块，它使用不同的窗口大小，在非重叠的特征组上计算SA，以利用远程图像依赖性。然后，通过简单地将两个移位conv与GMSA模块级联，构建高效的远程注意块（ELAB），并使用共享注意机制进一步加速。没有铃铛和哨子，我们的ELAN遵循一个相当简单的设计，依次级联Elab。大量实验表明，与基于变压器的SR模型相比，ELAN获得了更好的结果，但复杂度显著降低。源代码可以在https://github.com/xindongzhang/ELAN.
<details>	<summary>英文摘要</summary>	Recently, transformer-based methods have demonstrated impressive results in various vision tasks, including image super-resolution (SR), by exploiting the self-attention (SA) for feature extraction. However, the computation of SA in most existing transformer based models is very expensive, while some employed operations may be redundant for the SR task. This limits the range of SA computation and consequently the SR performance. In this work, we propose an efficient long-range attention network (ELAN) for image SR. Specifically, we first employ shift convolution (shift-conv) to effectively extract the image local structural information while maintaining the same level of complexity as 1x1 convolution, then propose a group-wise multi-scale self-attention (GMSA) module, which calculates SA on non-overlapped groups of features using different window sizes to exploit the long-range image dependency. A highly efficient long-range attention block (ELAB) is then built by simply cascading two shift-conv with a GMSA module, which is further accelerated by using a shared attention mechanism. Without bells and whistles, our ELAN follows a fairly simple design by sequentially cascading the ELABs. Extensive experiments demonstrate that ELAN obtains even better results against the transformer-based SR models but with significantly less complexity. The source code can be found at https://github.com/xindongzhang/ELAN. </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 463、基于畸变关系引导迁移学习的少镜头真实图像超分辨率
- [ ] Few-Shot Real Image Super-resolution via Distortion-Relation Guided Transfer Learning 
时间：2022年03月12日                         第一作者：Xin Li                       [链接](https://arxiv.org/abs/2111.13078).                     
<details>	<summary>注释</summary>	23 pages, first paper for few-shot real image restoration </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 462、多模态Boost：基于小波变换的多注意网络的多模态医学图像超分辨率
- [ ] Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform 
时间：2022年03月12日                         第一作者：Fayaz Ali Dharejo                       [链接](https://arxiv.org/abs/2110.11684).                     
<details>	<summary>注释</summary>	14 pages, 13 Figures, and 3 Tables. Submitted to IEEE/ACM TCBB </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 461、商空间中的流形建模：学习具有图像块可解性的不变映射
- [ ] Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches 
时间：2022年03月10日                         第一作者：Tatsuya Yokota                        [链接](https://arxiv.org/abs/2203.05134).                     
## 摘要：本研究提出了一个基于等价类概念的图像块流形学习框架：商空间流形建模（MMQS）。在MMQS，我们不考虑一组局部补丁的图像，因为它是，而是他们的典范补丁的集合，通过引入等价类的概念，并执行流形学习的典范补丁。规范补丁表示等价类，它们的自动编码器在商空间中构造流形。在此框架基础上，通过引入旋转-翻转等价关系，提出了一种新的基于流形的图像模型。此外，我们还提出了一个图像重建问题，将所提出的图像模型与损坏的观测图像进行拟合，并推导了一个算法来解决该问题。我们的实验表明，所提出的图像模型适用于各种自监督图像重建任务，如图像修复、去模糊、超分辨率和去噪。
<details>	<summary>英文摘要</summary>	This study proposes a framework for manifold learning of image patches using the concept of equivalence classes: manifold modeling in quotient space (MMQS). In MMQS, we do not consider a set of local patches of the image as it is, but rather the set of their canonical patches obtained by introducing the concept of equivalence classes and performing manifold learning on their canonical patches. Canonical patches represent equivalence classes, and their auto-encoder constructs a manifold in the quotient space. Based on this framework, we produce a novel manifold-based image model by introducing rotation-flip-equivalence relations. In addition, we formulate an image reconstruction problem by fitting the proposed image model to a corrupted observed image and derive an algorithm to solve it. Our experiments show that the proposed image model is effective for various self-supervised image reconstruction tasks, such as image inpainting, deblurring, super-resolution, and denoising. </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 460、图像超分辨率的高效非局部对比注意
- [ ] Efficient Non-Local Contrastive Attention for Image Super-Resolution 
时间：2022年03月10日                         第一作者：Bin Xia                       [链接](https://arxiv.org/abs/2201.03794).                     
<details>	<summary>注释</summary>	Code is available at https://github.com/Zj-BinXia/ENLCA Journal-ref: AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 459、基于参考的超分辨率粗到精嵌入补丁匹配和多尺度动态聚合
- [ ] Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution 
时间：2022年03月10日                         第一作者：Bin Xia                       [链接](https://arxiv.org/abs/2201.04358).                     
<details>	<summary>注释</summary>	code is availavle at https://github.com/Zj-BinXia/AMSA Journal-ref: AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 458、超低精度超分辨率网络的动态对偶可训练界
- [ ] Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks 
时间：2022年03月10日                         第一作者：Yunshan Zhong                       [链接](https://arxiv.org/abs/2203.03844).                     
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 457、盲图像超分辨率退化分布的学习
- [ ] Learning the Degradation Distribution for Blind Image Super-Resolution 
时间：2022年03月09日                         第一作者：Zhengxiong Luo                       [链接](https://arxiv.org/abs/2203.04962).                     
## 摘要：合成高分辨率（HR）\&低分辨率（LR）对广泛应用于现有的超分辨率（SR）方法中。为了避免合成图像和测试图像之间的域差距，大多数以前的方法都试图通过确定性模型自适应地学习合成（退化）过程。然而，在真实场景中，一些退化是随机的，不能由图像的内容来确定。这些确定性模型可能无法对降解的随机因素和与含量无关的部分进行建模，这将限制以下SR模型的性能。本文提出了一种概率退化模型（PDM），将退化$\mathbf{D}$作为一个随机变量进行研究，并通过对先验随机变量$\mathbf{z}$到$\mathbf{D}$的映射进行建模来学习其分布。与以往的确定性退化模型相比，PDM可以对更多样的退化进行建模，生成HR-LR对，更好地覆盖测试图像的各种退化，从而防止SR模型过度拟合特定图像。大量实验表明，我们的退化模型可以帮助SR模型在不同的数据集上获得更好的性能。源代码发布于\url{git@github.com：greatlog/UnpairedSR。git}。
<details>	<summary>英文摘要</summary>	Synthetic high-resolution (HR) \& low-resolution (LR) pairs are widely used in existing super-resolution (SR) methods. To avoid the domain gap between synthetic and test images, most previous methods try to adaptively learn the synthesizing (degrading) process via a deterministic model. However, some degradations in real scenarios are stochastic and cannot be determined by the content of the image. These deterministic models may fail to model the random factors and content-independent parts of degradations, which will limit the performance of the following SR models. In this paper, we propose a probabilistic degradation model (PDM), which studies the degradation $\mathbf{D}$ as a random variable, and learns its distribution by modeling the mapping from a priori random variable $\mathbf{z}$ to $\mathbf{D}$. Compared with previous deterministic degradation models, PDM could model more diverse degradations and generate HR-LR pairs that may better cover the various degradations of test images, and thus prevent the SR model from over-fitting to specific ones. Extensive experiments have demonstrated that our degradation model can help the SR model achieve better performance on different datasets. The source codes are released at \url{git@github.com:greatlog/UnpairedSR.git}. </details>
<details>	<summary>注释</summary>	Accepted to CVRP2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 456、用可微光学模型重新思考数据驱动的点扩散函数建模
- [ ] Rethinking data-driven point spread function modeling with a differentiable optical model 
时间：2022年03月09日                         第一作者：Tobias Liaudat                       [链接](https://arxiv.org/abs/2203.04908).                     
## 摘要：在天文学中，即将推出的带有宽视场光学仪器的空间望远镜具有空间变化的点扩散函数（PSF）。某些科学目标要求在没有提供PSF直接测量的目标位置对PSF进行高保真估计。尽管在视场（FOV）的某些位置可以观测到PSF，但它们采样不足、噪声大，并且与仪器通带中的波长有关。PSF建模需要根据这些观测建立一个模型，该模型可以推断出FOV中任何波长和任何位置的超分辨PSF。当前数据驱动的PSF模型可以处理空间变化和超分辨率，但不能捕获颜色变化。我们的模型被称为WaveDiff，它提出了望远镜点扩散函数场数据驱动建模的范式转变。通过在建模框架中加入可微光学正演模型，我们将数据驱动的建模空间从像素变为波前。该模型依赖于高效的自动微分技术以及蓬勃发展的机器学习界最近开发的现代随机一阶优化技术。我们的框架为构建具有物理动机且不需要特殊校准数据的强大模型铺平了道路。本文在空间望远镜的简化装置上演示了WaveDiff模型。与现有的数据驱动方法相比，提议的框架代表了一个性能突破。在观测分辨率下，像素重建误差降低了6倍，在3倍超分辨率下，像素重建误差降低了44倍。椭圆度误差至少减少20倍，尺寸误差减少250倍以上。通过仅使用噪声宽带对焦观测，我们成功地捕获了由于衍射引起的PSF颜色变化。
<details>	<summary>英文摘要</summary>	In astronomy, upcoming space telescopes with wide-field optical instruments have a spatially varying point spread function (PSF). Certain scientific goals require a high-fidelity estimation of the PSF at target positions where no direct measurement of the PSF is provided. Even though observations of the PSF are available at some positions of the field of view (FOV), they are undersampled, noisy, and integrated in wavelength in the instrument's passband. PSF modeling requires building a model from these observations that can infer a super-resolved PSF at any wavelength and any position in the FOV. Current data-driven PSF models can tackle spatial variations and super-resolution, but are not capable of capturing chromatic variations. Our model, coined WaveDiff, proposes a paradigm shift in the data-driven modeling of the point spread function field of telescopes. By adding a differentiable optical forward model into the modeling framework, we change the data-driven modeling space from the pixels to the wavefront. The proposed model relies on efficient automatic differentiation technology as well as modern stochastic first-order optimization techniques recently developed by the thriving machine-learning community. Our framework paves the way to building powerful models that are physically motivated and do not require special calibration data. This paper demonstrates the WaveDiff model on a simplified setting of a space telescope. The proposed framework represents a performance breakthrough with respect to existing data-driven approaches. The pixel reconstruction errors decrease 6-fold at observation resolution and 44-fold for a 3x super-resolution. The ellipticity errors are reduced by a factor of at least 20 and the size error by a factor of more than 250. By only using noisy broad-band in-focus observations, we successfully capture the PSF chromatic variations due to diffraction. </details>
<details>	<summary>注释</summary>	Submitted. 44 pages, 11 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2022年03月10日</details>

# 455、基于卷积神经网络的二维流旋转平移等变超分辨
- [ ] Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks 
时间：2022年03月09日                         第一作者：Yuki Yasuda                       [链接](https://arxiv.org/abs/2202.11099).                     
<details>	<summary>邮件日期</summary>	2022年03月10日</details>

# 454、超低精度超分辨率网络的动态对偶可训练界
- [ ] Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks 
时间：2022年03月08日                         第一作者：Yunshan Zhong                       [链接](https://arxiv.org/abs/2203.03844).                     
## 摘要：轻质超分辨率（SR）模型因其在移动设备中的适用性而受到了广泛关注。许多工作采用网络量化来压缩SR模型。然而，当使用低成本分层量化器将SR模型量化到超低精度（例如，2位和3位）时，这些方法的性能会严重下降。在本文中，我们发现性能下降是由分层对称量化器和SR模型中高度不对称的激活分布之间的矛盾造成的。这种差异要么导致量化级别的浪费，要么导致重建图像中的细节损失。因此，我们提出了一种新的激活量化器，称为动态双可训练边界（DDTB），以适应激活的不对称性。具体来说，DDTB在以下方面进行了创新：1）具有可训练上下界的分层量化器，以解决高度不对称的激活问题。2） 一种动态门控制器，用于在运行时自适应调整上限和下限，以克服在不同样本上急剧变化的激活范围。为了减少额外的开销，动态门控制器被量化为2位，并根据引入的动态强度仅应用于部分SR网络。大量实验表明，我们的DDTB在超低精度方面表现出显著的性能改进。例如，当将EDSR量化为2位并将输出图像放大到x4时，我们的DDTB在Urban100基准上实现了0.70dB的PSNR增加。代码位于\url{https://github.com/zysxmu/DDTB}.
<details>	<summary>英文摘要</summary>	Light-weight super-resolution (SR) models have received considerable attention for their serviceability in mobile devices. Many efforts employ network quantization to compress SR models. However, these methods suffer from severe performance degradation when quantizing the SR models to ultra-low precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In this paper, we identify that the performance drop comes from the contradiction between the layer-wise symmetric quantizer and the highly asymmetric activation distribution in SR models. This discrepancy leads to either a waste on the quantization levels or detail loss in reconstructed images. Therefore, we propose a novel activation quantizer, referred to as Dynamic Dual Trainable Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically, DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower bounds to tackle the highly asymmetric activations. 2) A dynamic gate controller to adaptively adjust the upper and lower bounds at runtime to overcome the drastically varying activation ranges over different samples.To reduce the extra overhead, the dynamic gate controller is quantized to 2-bit and applied to only part of the SR networks according to the introduced dynamic intensity. Extensive experiments demonstrate that our DDTB exhibits significant performance improvements in ultra-low precision. For example, our DDTB achieves a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and scaling up output images to x4. Code is at \url{https://github.com/zysxmu/DDTB}. </details>
<details>	<summary>邮件日期</summary>	2022年03月09日</details>

# 453、使用条件目标的灵活风格图像超分辨率
- [ ] Flexible Style Image Super-Resolution using Conditional Objective 
时间：2022年03月08日                         第一作者：Seung Ho Park                       [链接](https://arxiv.org/abs/2201.04898).                     
<details>	<summary>注释</summary>	Will be presented in IEEE ACCESS. Code and trained models will be available at https://github.com/seungho-snu/FxSR </details>
<details>	<summary>邮件日期</summary>	2022年03月09日</details>

# 452、图像超分辨率中的回流衰减
- [ ] Reflash Dropout in Image Super-Resolution 
时间：2022年03月07日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2112.12089).                     
<details>	<summary>注释</summary>	CVPR2022 paper + supplementary file </details>
<details>	<summary>邮件日期</summary>	2022年03月08日</details>

# 451、一种新颖的视频超分辨率双密集连接网络
- [ ] A Novel Dual Dense Connection Network for Video Super-resolution 
时间：2022年03月05日                         第一作者：Guofang Li                        [链接](https://arxiv.org/abs/2203.02723).                     
## 摘要：视频超分辨率（VSR）是指从相应的低分辨率（LR）视频重建高分辨率（HR）视频。近年来，VSR受到了越来越多的关注。在本文中，我们提出了一种新的双密集连接网络，可以产生高质量的超分辨率（SR）结果。将输入帧创造性地划分为参考帧、前时间组和后时间组，代表不同时间段的信息。这种分组方法提供了不同时间段的准确信息，而不会造成时间信息混乱。同时，我们提出了一个新的损失函数，这有利于提高模型的收敛能力。实验表明，在Vid4数据集和SPMCS-11数据集上，我们的模型优于其他先进的模型。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) refers to the reconstruction of high-resolution (HR) video from the corresponding low-resolution (LR) video. Recently, VSR has received increasing attention. In this paper, we propose a novel dual dense connection network that can generate high-quality super-resolution (SR) results. The input frames are creatively divided into reference frame, pre-temporal group and post-temporal group, representing information in different time periods. This grouping method provides accurate information of different time periods without causing time information disorder. Meanwhile, we produce a new loss function, which is beneficial to enhance the convergence ability of the model. Experiments show that our model is superior to other advanced models in Vid4 datasets and SPMCS-11 datasets. </details>
<details>	<summary>邮件日期</summary>	2022年03月08日</details>

# 450、图像恢复中的自适应跨层注意
- [ ] Adaptive Cross-Layer Attention for Image Restoration 
时间：2022年03月04日                         第一作者：Yancheng Wang                       [链接](https://arxiv.org/abs/2203.03619).                     
## 摘要：非局部注意模块已被证明对图像恢复至关重要。传统的非局部注意是分别处理每一层的特征，因此存在着不同层特征之间缺乏相关性的风险。在本模块中，我们提出了跨层关注问题。每个查询像素可以关注网络前几层的关键像素，而不是在同一层中查找相关的关键像素。为了进一步提高CLA的学习能力和降低推理成本，我们进一步提出了自适应CLA（ACLA）作为一种改进的CLA。针对ACLA提出了两种自适应设计：1）在每一层自适应选择非局部注意的关键点；2） 自动搜索ACLA模块的插入位置。通过这两种自适应设计，ACLA动态地选择要聚集的密钥数量，以便在第二层引起非局部注意。此外，ACLA通过神经结构搜索方法搜索ACLA模块的最佳插入位置，以呈现具有令人信服性能的紧凑型神经网络。对图像恢复任务的大量实验，包括单幅图像超分辨率、图像去噪、图像去噪和图像压缩伪影减少，验证了ACLA的有效性和效率。
<details>	<summary>英文摘要</summary>	Non-local attention module has been proven to be crucial for image restoration. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we propose Cross-Layer Attention (CLA) module in this paper. Instead of finding correlated key pixels within the same layer, each query pixel can attend to key pixels at previous layers of the network. In order to further enhance the learning capability and reduce the inference cost of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for non-local attention at each layer; 2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects the number of keys to be aggregated for non-local attention at layer. In addition, ACLA searches for the optimal insert positions of ACLA modules by a neural architecture search method to render a compact neural network with compelling performance. Extensive experiments on image restoration tasks, including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction, validate the effectiveness and efficiency of ACLA. </details>
<details>	<summary>邮件日期</summary>	2022年03月09日</details>

# 449、基于空间角度分解核的纹理增强光场超分辨率
- [ ] Texture-enhanced Light Field Super-resolution with Spatio-Angular Decomposition Kernels 
时间：2022年03月04日                         第一作者：Zexi Hu                       [链接](https://arxiv.org/abs/2111.04069).                     
<details>	<summary>注释</summary>	Accepted by IEEE TIM </details>
<details>	<summary>邮件日期</summary>	2022年03月07日</details>

# 448、轻量级密集预测网络的快速神经结构搜索
- [ ] Fast Neural Architecture Search for Lightweight Dense Prediction Networks 
时间：2022年03月03日                         第一作者：Lam Huynh                       [链接](https://arxiv.org/abs/2203.01994).                     
## 摘要：稠密预测是一类计算机视觉问题，其目标是将输入图像的每个像素映射为某些预测值。根据问题的不同，输出值可以是连续的，也可以是离散的。例如，单目深度估计和图像超分辨率通常表示为回归，而语义分割是一个密集的分类问题，即离散问题。更具体地说，单目深度估计问题从单个图像生成密集的深度图，用于各种应用，包括机器人技术、场景理解和增强现实。单图像超分辨率（SISR）是一种低水平视觉任务，它可以从低分辨率图像生成高分辨率图像。SISR被广泛应用于医疗和监视成像，具有更精确细节的图像可以提供宝贵的信息。另一方面，语义分割可以预测给定图像中不同语义类别的密集注释地图，这对于图像理解任务至关重要。
<details>	<summary>英文摘要</summary>	Dense prediction is a class of computer vision problems aiming at mapping every pixel of the input image with some predicted values. Depending on the problem, the output values can be either continous or discrete. For instance, monocular depth estimation and image super-resolution are often formulated as regression, while semantic segmentation is a dense classification, i.e. discrete, problem. More specifically, the monocular depth estimation problem produces a dense depth map from a single image to be used in various applications including robotics, scene understanding, and augmented reality. Single image super-resolution (SISR) is a low-level vision task that generates a high-resolution image from its low-resolution counterpart. SISR is widely utilized in medical and surveillance imaging, where images with more precise details can provide invaluable information. On the other hand, semantic segmentation predicts a dense annotated map of different semantic categories from a given image that is crucial for image understanding tasks. </details>
<details>	<summary>注释</summary>	15 pages, 11 figures, 8 tables. arXiv admin note: substantial text overlap with arXiv:2108.11105 </details>
<details>	<summary>邮件日期</summary>	2022年03月07日</details>

# 447、Ad2Attack：针对实时无人机跟踪的自适应对抗攻击
- [ ] Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking 
时间：2022年03月03日                         第一作者：Changhong Fu                       [链接](https://arxiv.org/abs/2203.01516).                     
## 摘要：视觉跟踪被广泛应用于无人机相关应用中，这对无人机跟踪器的鲁棒性提出了很高的要求。然而，添加不可察觉的扰动很容易欺骗跟踪器，导致跟踪失败。这种风险往往被忽视，目前很少研究。因此，为了帮助提高对无人机跟踪潜在风险和鲁棒性的认识，本研究针对无人机目标跟踪提出了一种新的自适应对抗攻击方法，即Ad$^2$攻击。具体来说，在搜索补丁图像重新采样期间，在线生成对抗性示例，这会导致跟踪器在以下帧中丢失目标。Ad$^2$攻击由一个直接下采样模块和一个带有自适应级的超分辨率上采样模块组成。为了平衡攻击的不可察觉性和效率，提出了一种新的优化函数。在几个著名的基准测试和真实环境中进行的综合实验表明，我们的攻击方法有效，大大降低了最先进的暹罗跟踪器的性能。
<details>	<summary>英文摘要</summary>	Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad$^2$Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad$^2$Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers. </details>
<details>	<summary>注释</summary>	7 pages, 7 figures, accepted by ICRA 2022 MSC-class: 68T07(Primary) ACM-class: I.4; I.5 </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 446、基于双缩放观测的真实世界超分辨率自监督学习
- [ ] Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations 
时间：2022年03月02日                         第一作者：Zhilu Zhang                       [链接](https://arxiv.org/abs/2203.01325).                     
## 摘要：在本文中，我们考虑两个具有挑战性的问题，基于参考的超分辨率（RIFSR），（i）如何选择合适的参考图像，以及（ii）如何学习真实世界的RIFSR的自我监督的方式。特别是，我们提出了一种新的自监督学习方法，用于从双摄像机变焦的观测数据中获取真实世界的图像SR（SelfDZSR）。对于第一个问题，可以自然地利用放大（长焦）程度更高的图像作为参考，以指导缩小（短焦距）图像的SR。对于第二个问题，SelfDZSR学习深度网络，以获得与长焦图像具有相同分辨率的短焦图像的SR结果。为此，我们将长焦图像作为监控信息，而不是额外的高分辨率图像，并从中选择一个补丁作为参考，对相应的短焦距图像补丁进行超分辨率处理。为了缓解短焦距低分辨率（LR）图像和长焦地面真实（GT）图像之间各种失调的影响，我们设计了一个退化模型，并将GT映射到与GT对齐的伪LR图像。然后将伪LR和LR图像输入到所提出的自适应空间变换网络（AdaSTN）中，对LR特征进行变形。在测试过程中，SelfDZSR可以直接部署，以参考长焦图像对整个短焦距图像进行超级求解。实验表明，我们的方法在定量和定性上都取得了更好的性能。代码和预先培训的模型将公开提供。
<details>	<summary>英文摘要</summary>	In this paper, we consider two challenging issues in reference-based super-resolution (RefSR), (i) how to choose a proper reference image, and (ii) how to learn real-world RefSR in a self-supervised manner. Particularly, we present a novel self-supervised learning approach for real-world image SR from observations at dual camera zooms (SelfDZSR). For the first issue, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the SR of the lesser zoomed (short-focus) image. For the second issue, SelfDZSR learns a deep network to obtain the SR result of short-focal image and with the same resolution as the telephoto image. For this purpose, we take the telephoto image instead of an additional high-resolution image as the supervision information and select a patch from it as the reference to super-resolve the corresponding short-focus image patch. To mitigate the effect of various misalignment between the short-focus low-resolution (LR) image and telephoto ground-truth (GT) image, we design a degradation model and map the GT to a pseudo-LR image aligned with GT. Then the pseudo-LR and LR image can be fed into the proposed adaptive spatial transformer networks (AdaSTN) to deform the LR features. During testing, SelfDZSR can be directly deployed to super-solve the whole short-focus image with the reference of telephoto image. Experiments show that our method achieves better quantitative and qualitative performance against state-of-the-arts. The code and pre-trained models will be publicly available. </details>
<details>	<summary>注释</summary>	18 pages </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 445、双向任意图像重缩放：联合优化和循环幂等
- [ ] Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence 
时间：2022年03月02日                         第一作者：Zhihong Pan                       [链接](https://arxiv.org/abs/2203.00911).                     
## 摘要：基于深度学习的单幅图像超分辨率模型已经得到了广泛的研究，并在使用固定比例因子和降尺度退化核对低分辨率图像进行升尺度处理方面取得了良好的效果。为了提高这类模型在现实世界中的适用性，人们越来越有兴趣开发针对任意放大因子进行优化的模型。我们提出的方法是第一个将任意重缩放（放大和缩小）视为一个统一过程的方法。该模型通过两个方向的联合优化，能够同时学习放大和缩小，实现双向任意图像缩放。它极大地提高了当前任意放大模型的性能，同时学会了在缩小的图像中保持视觉感知质量。该模型在循环幂等性检验中具有很强的鲁棒性，当重复使用从降尺度到升尺度的循环时，重建精度不会严重下降。当这个循环可以多次应用于一张图像时，这种鲁棒性有利于在野外重新缩放图像。它在任意大尺度和非对称尺度的测试中也表现良好，即使模型没有经过此类任务的训练。我们进行了大量实验来证明我们模型的优越性能。
<details>	<summary>英文摘要</summary>	Deep learning based single image super-resolution models have been widely studied and superb results are achieved in upscaling low-resolution images with fixed scale factor and downscaling degradation kernel. To improve real world applicability of such models, there are growing interests to develop models optimized for arbitrary upscaling factors. Our proposed method is the first to treat arbitrary rescaling, both upscaling and downscaling, as one unified process. Using joint optimization of both directions, the proposed model is able to learn upscaling and downscaling simultaneously and achieve bidirectional arbitrary image rescaling. It improves the performance of current arbitrary upscaling models by a large margin while at the same time learns to maintain visual perception quality in downscaled images. The proposed model is further shown to be robust in cycle idempotence test, free of severe degradations in reconstruction accuracy when the downscaling-to-upscaling cycle is applied repetitively. This robustness is beneficial for image rescaling in the wild when this cycle could be applied to one image for multiple times. It also performs well on tests with arbitrary large scales and asymmetric scales, even when the model is not trained with such tasks. Extensive experiments are conducted to demonstrate the superior performance of our model. </details>
<details>	<summary>邮件日期</summary>	2022年03月03日</details>

# 444、用于超分辨率显微镜的时空视觉转换器
- [ ] Spatio-temporal Vision Transformer for Super-resolution Microscopy 
时间：2022年02月28日                         第一作者：Charles N. Christensen                       [链接](https://arxiv.org/abs/2203.00030).                     
## 摘要：结构照明显微镜（SIM）是一种光学超分辨率技术，可使活细胞成像超过衍射极限。SIM数据的重建容易出现伪影，在对高度动态的样本进行成像时会出现问题，因为以前的方法依赖于样本是静态的假设。我们提出了一种新的基于变换器的重建方法VSR-SIM，该方法除了使用通道注意机制外，还使用移动的三维窗口多头注意来解决SIM中的视频超分辨率（VSR）问题。研究发现，注意机制可以捕捉序列中的运动，而不需要像光流这样的普通运动估计技术。我们采用一种方法来训练网络，该方法仅依赖于模拟数据，使用具有SIM图像形成模型的自然风景视频。我们演示了一个由VSR-SIM支持的用例，称为滚动SIM成像，它将SIM中的时间分辨率提高了9倍。我们的方法可以应用于任何SIM设置，以高时间分辨率精确记录生物医学研究中的动态过程。
<details>	<summary>英文摘要</summary>	Structured illumination microscopy (SIM) is an optical super-resolution technique that enables live-cell imaging beyond the diffraction limit. Reconstruction of SIM data is prone to artefacts, which becomes problematic when imaging highly dynamic samples because previous methods rely on the assumption that samples are static. We propose a new transformer-based reconstruction method, VSR-SIM, that uses shifted 3-dimensional window multi-head attention in addition to channel attention mechanism to tackle the problem of video super-resolution (VSR) in SIM. The attention mechanisms are found to capture motion in sequences without the need for common motion estimation techniques such as optical flow. We take an approach to training the network that relies solely on simulated data using videos of natural scenery with a model for SIM image formation. We demonstrate a use case enabled by VSR-SIM referred to as rolling SIM imaging, which increases temporal resolution in SIM by a factor of 9. Our method can be applied to any SIM setup enabling precise recordings of dynamic processes in biomedical research with high temporal resolution. </details>
<details>	<summary>注释</summary>	8 pages, 9 figures. Source code: https://github.com/charlesnchr/vsr-sim </details>
<details>	<summary>邮件日期</summary>	2022年03月02日</details>

# 443、OUR-GAN：一次性超高分辨率生成对抗网络
- [ ] OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks 
时间：2022年02月28日                         第一作者：Donghwee Yoon                       [链接](https://arxiv.org/abs/2202.13799).                     
## 摘要：我们提出了第一个单次超高分辨率（UHR）图像合成框架——我们的GAN，它可以从单个训练图像生成4K或更高分辨率的非重复图像。我们的GAN在低分辨率下生成视觉上一致的图像，然后通过超分辨率逐渐提高分辨率。由于OUR-GAN从真实的UHR图像中学习，它可以合成具有精细细节的大规模形状，同时保持长距离的一致性，这对于基于从相对较小的图像中学习到的面片分布生成大型图像的传统生成模型来说是困难的。我们的GAN采用无缝分区超分辨率，在有限内存的情况下合成4k或更高的超高分辨率图像，防止边界出现不连续。此外，我们的GAN通过在特征地图中添加垂直位置嵌入来提高视觉一致性，保持多样性。在ST4K和RAISE数据集上的实验中，与现有方法相比，我们的GAN显示出更好的保真度、视觉一致性和多样性。合成图像显示在https://anonymous-62348.github.io.
<details>	<summary>英文摘要</summary>	We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image synthesis framework that generates non-repetitive images with 4K or higher resolution from a single training image. OUR-GAN generates a visually coherent image at low resolution and then gradually increases the resolution by super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize large-scale shapes with fine details while maintaining long-range coherence, which is difficult with conventional generative models that generate large images based on the patch distribution learned from relatively small images. OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or higher UHR images with limited memory, preventing discontinuity at the boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity by adding vertical positional embeddings to the feature maps. In experiments on the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual coherency, and diversity compared with existing methods. The synthesized images are presented at https://anonymous-62348.github.io. </details>
<details>	<summary>邮件日期</summary>	2022年03月01日</details>

# 442、Pix2NeRF：用于单个图像到神经辐射场转换的无监督条件$\pi$-GAN
- [ ] Pix2NeRF: Unsupervised Conditional $\pi$-GAN for Single Image to Neural Radiance Fields Translation 
时间：2022年02月26日                         第一作者：Shengqu Cai                        [链接](https://arxiv.org/abs/2202.13162).                     
## 摘要：我们提出了一个管道来生成特定类别的物体或场景的神经辐射场（NeRF），条件是单个输入图像。这是一项具有挑战性的任务，因为训练NeRF需要同一场景的多个视图，再加上相应的姿势，这些姿势很难获得。我们的方法基于$\pi$-GAN，这是一种用于无条件3D感知图像合成的生成模型，它将随机潜在代码映射到一类对象的辐射场。我们联合优化了（1）利用高保真3D感知生成的$\pi$-GAN目标和（2）精心设计的重建目标。后者包括一个与$\pi$-GAN发生器耦合的编码器，以形成一个自动编码器。与之前的几款shot NeRF方法不同，我们的管道是无监督的，能够在没有3D、多视图或姿势监督的情况下使用独立图像进行训练。我们的管道应用包括3d化身生成、以对象为中心的单输入图像新视图合成，以及3d感知超分辨率等。
<details>	<summary>英文摘要</summary>	We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object or a scene of a specific class, conditioned on a single input image. This is a challenging task, as training NeRF requires multiple views of the same scene, coupled with corresponding poses, which are hard to obtain. Our method is based on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis, which maps random latent codes to radiance fields of a class of objects. We jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity 3D-aware generation and (2) a carefully designed reconstruction objective. The latter includes an encoder coupled with $\pi$-GAN generator to form an auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is unsupervised, capable of being trained with independent images without 3D, multi-view, or pose supervision. Applications of our pipeline include 3d avatar generation, object-centric novel view synthesis with a single input image, and 3d-aware super-resolution, to name a few. </details>
<details>	<summary>注释</summary>	16 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2022年03月01日</details>

# 441、基于质量图关联时间注意网络的多图像超分辨率
- [ ] Multi-image Super-resolution via Quality Map Associated Temporal Attention Network 
时间：2022年02月26日                         第一作者：Minji Lee                       [链接](https://arxiv.org/abs/2202.13124).                     
## 摘要：随着人们对基于深度学习的遥感方法越来越感兴趣，神经网络在多图像融合和超分辨率方面取得了显著的进展。为了充分利用多图像超分辨率的优势，时间注意力至关重要，因为它允许模型关注可靠的特征，而不是噪声。尽管在QMs中，大多数时间图像的质量都没有在QMs中测试过。我们提出了一种质量图关联的时间注意网络（QA-Net），这是一种新的方法，首次将QMs结合到特征表示和融合过程中。在重复的多头注意模块中，QM特征暂时关注低分辨率特征。该方法在PROBA-V数据集中取得了最新的结果。
<details>	<summary>英文摘要</summary>	With the rising interest in deep learning-based methods in remote sensing, neural networks have made remarkable advancements in multi-image fusion and super-resolution. To fully exploit the advantages of multi-image super-resolution, temporal attention is crucial as it allows a model to focus on reliable features rather than noises. Despite the presence of quality maps (QMs) that indicate noises in images, most of the methods tested in the PROBA-V dataset have not been used QMs for temporal attention. We present a quality map associated temporal attention network (QA-Net), a novel method that incorporates QMs into both feature representation and fusion processes for the first time. Low-resolution features are temporally attended by QM features in repeated multi-head attention modules. The proposed method achieved state-of-the-art results in the PROBA-V dataset. </details>
<details>	<summary>邮件日期</summary>	2022年03月01日</details>

# 440、基于卷积神经网络的二维流旋转平移等变超分辨
- [ ] Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks 
时间：2022年02月22日                         第一作者：Yuki Yasuda                       [链接](https://arxiv.org/abs/2202.11099).                     
## 摘要：卷积神经网络（CNN）通常将向量处理为图像中没有方向的颜色。本研究考察了将矢量视为几何对象对二维流体速度超分辨率的影响。向量与标量的区别在于与基的变化相关的变换定律，可以使用等变深度学习将其作为先验知识。我们通过使每一层在旋转和平移方面等变，将现有的CNN转换为等变CNN。低分辨率和高分辨率的训练数据通过下采样或光谱微调生成。当数据继承旋转对称性时，等变CNN的精度与非等变CNN相当。由于等变CNN中的参数数量较少，这些模型可以用较小的数据量进行训练。在这种情况下，应将向量的变换定律作为先验知识，其中向量被明确地视为具有方向的量。两个例子表明，数据的对称性可能会被破坏。在第一种情况下，下采样方法使低分辨率图案和高分辨率图案之间的对应关系取决于方向。在第二种情况下，输入数据不足以识别光谱微动实验中的坐标旋转。在这两种情况下，如果强制实施等变，CNN的准确性会下降，即使矢量被处理为没有方向的量，使用常规CNN也是合理的。
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) often process vectors as quantities having no direction like colors in images. This study investigates the effect of treating vectors as geometrical objects in terms of super-resolution of velocity on two-dimensional fluids. Vector is distinguished from scalar by the transformation law associated with a change in basis, which can be incorporated as the prior knowledge using the equivariant deep learning. We convert existing CNNs into equivariant ones by making each layer equivariant with respect to rotation and translation. The training data in the low- and high-resolution are generated with the downsampling or the spectral nudging. When the data inherit the rotational symmetry, the equivariant CNNs show comparable accuracy with the non-equivariant ones. Since the number of parameters is smaller in the equivariant CNNs, these models are trainable with a smaller size of the data. In this case, the transformation law of vector should be incorporated as the prior knowledge, where vector is explicitly treated as a quantity having direction. Two examples demonstrate that the symmetry of the data can be broken. In the first case, a downsampling method makes the correspondence between low- and high-resolution patterns dependent on the orientation. In the second case, the input data are insufficient to recognize the rotation of coordinates in the experiment with the spectral nudging. In both cases, the accuracy of the CNNs deteriorates if the equivariance is forced to be imposed, and the usage of conventional CNNs may be justified even though vector is processed as a quantity having no direction. </details>
<details>	<summary>邮件日期</summary>	2022年02月24日</details>

# 439、MODIS地表温度超分辨率的卷积神经网络建模
- [ ] Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution 
时间：2022年02月22日                         第一作者：Binh Minh Nguyen                       [链接](https://arxiv.org/abs/2202.10753).                     
## 摘要：如今，热红外卫星遥感器能够在大范围内提取非常有趣的信息，尤其是地表温度（LST）。然而，此类数据在空间和/或时间分辨率上受到限制，这妨碍了精细尺度的分析。例如，MODIS卫星每天提供1Km空间分辨率的采集，这不足以处理高度异质的环境，如农业地块。因此，图像超分辨率是更好地利用MODIS LST的关键任务。本文主要探讨这一问题。本文介绍了一种基于深度学习的多残差U网算法，用于MODIS LST单幅图像的超分辨率处理。我们提出的网络是U-Net体系结构的一个改进版本，其目的是将输入的LST图像从每像素1公里超分辨率到250米。结果表明，我们的多残差U-网优于其他最先进的方法。
<details>	<summary>英文摘要</summary>	Nowadays, thermal infrared satellite remote sensors enable to extract very interesting information at large scale, in particular Land Surface Temperature (LST). However such data are limited in spatial and/or temporal resolutions which prevents from an analysis at fine scales. For example, MODIS satellite provides daily acquisitions with 1Km spatial resolutions which is not sufficient to deal with highly heterogeneous environments as agricultural parcels. Therefore, image super-resolution is a crucial task to better exploit MODIS LSTs. This issue is tackled in this paper. We introduce a deep learning-based algorithm, named Multi-residual U-Net, for super-resolution of MODIS LST single-images. Our proposed network is a modified version of U-Net architecture, which aims at super-resolving the input LST image from 1Km to 250m per pixel. The results show that our Multi-residual U-Net outperforms other state-of-the-art methods. </details>
<details>	<summary>邮件日期</summary>	2022年02月23日</details>

# 438、用一个超网络计算多幅图像重建
- [ ] Computing Multiple Image Reconstructions with a Single Hypernetwork 
时间：2022年02月22日                         第一作者：Alan Q. Wang                       [链接](https://arxiv.org/abs/2202.11009).                     
## 摘要：基于深度学习的技术在压缩感知等广泛的图像重建任务中实现了最先进的结果。这些方法几乎总是有超参数，比如平衡优化损失函数中不同项的权重系数。典型的方法是针对超参数设置训练模型，该设置是通过一些经验或理论证明确定的。因此，在推理时，模型只能计算与预先确定的超参数值对应的重构。在这项工作中，我们提出了一种基于超网络的方法，称为HyperRecon，来训练对超参数设置不可知的重建模型。在推理时，HyperRecon可以有效地生成不同的重建，每个重建对应于不同的超参数值。在这个框架中，用户有权根据自己的判断选择最有用的输出。我们使用两个大规模和公开的MRI数据集演示了我们在压缩感知、超分辨率和去噪任务中的方法。我们的代码可在https://github.com/alanqrwang/hyperrecon.
<details>	<summary>英文摘要</summary>	Deep learning based techniques achieve state-of-the-art results in a wide range of image reconstruction tasks like compressed sensing. These methods almost always have hyperparameters, such as the weight coefficients that balance the different terms in the optimized loss function. The typical approach is to train the model for a hyperparameter setting determined with some empirical or theoretical justification. Thus, at inference time, the model can only compute reconstructions corresponding to the pre-determined hyperparameter values. In this work, we present a hypernetwork based approach, called HyperRecon, to train reconstruction models that are agnostic to hyperparameter settings. At inference time, HyperRecon can efficiently produce diverse reconstructions, which would each correspond to different hyperparameter values. In this framework, the user is empowered to select the most useful output(s) based on their own judgement. We demonstrate our method in compressed sensing, super-resolution and denoising tasks, using two large-scale and publicly-available MRI datasets. Our code is available at https://github.com/alanqrwang/hyperrecon. </details>
<details>	<summary>邮件日期</summary>	2022年02月23日</details>

# 437、分离光场实现超分辨率和视差估计
- [ ] Disentangling Light Fields for Super-Resolution and Disparity Estimation 
时间：2022年02月22日                         第一作者：Yingqian Wang                       [链接](https://arxiv.org/abs/2202.10603).                     
## 摘要：光场（LF）摄像机记录光线的强度和方向，并将3D场景编码为4D LF图像。最近，许多卷积神经网络（CNN）被提出用于各种LF图像处理任务。然而，由于空间和角度信息高度交织，且存在不同的差异，CNN很难有效地处理LF图像。在本文中，我们提出了一个通用的机制来解开这些耦合信息的低频图像处理。具体来说，我们首先设计一类特定于领域的卷积来从不同维度分离LFs，然后通过设计特定于任务的模块来利用这些分离的特征。我们的解纠缠机制能够很好地融合LF结构，有效地处理4D LF数据。基于该机制，我们开发了三个网络（即DistgSSR、DistgASR和DistgDisp），用于空间超分辨率、角度超分辨率和视差估计。实验结果表明，我们的网络在所有这三项任务上都达到了最先进的性能，这证明了我们的解纠缠机制的有效性、效率和通用性。项目页面：https://yingqianwang.github.io/DistgLF/.
<details>	<summary>英文摘要</summary>	Light field (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Specifically, we first design a class of domain-specific convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-specific modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. Project page: https://yingqianwang.github.io/DistgLF/. </details>
<details>	<summary>注释</summary>	Accepted by IEEE TPAMI DOI: 10.1109/TPAMI.2022.3152488 </details>
<details>	<summary>邮件日期</summary>	2022年02月23日</details>

# 436、用于各向异性MRI语义平滑插值的低分辨率MRI自动编码
- [ ] Autoencoding Low-Resolution MRI for Semantically Smooth Interpolation of Anisotropic MRI 
时间：2022年02月18日                         第一作者：J\"org S                       [链接](https://arxiv.org/abs/2202.09258).                     
## 摘要：高分辨率医学图像有利于分析，但它们的获取可能并不总是可行的。或者，可以使用传统的上采样方法从低分辨率采集中创建高分辨率图像，但此类方法无法利用图像中包含的高级别上下文信息。最近，人们引入了性能更好的基于深度学习的超分辨率方法。然而，这些方法受其监督特性的限制，即它们需要高分辨率的示例进行训练。相反，我们提出了一种无监督的深度学习语义插值方法，该方法从编码的低分辨率示例中合成新的中间切片。为了在平面方向上实现语义平滑的插值，该方法利用了自动编码器产生的潜在空间。为了生成新的中间切片，使用两个空间相邻切片的凸组合来组合它们的潜在空间编码。随后，将组合编码解码为中间片段。为了约束模型，定义了给定数据集的语义相似性概念。为此，引入了一种新的损耗，它利用了相同体积切片之间的空间关系。在训练过程中，使用相邻切片编码的凸组合生成现有中间切片。使用公开的心脏电影、新生儿大脑和成人大脑MRI扫描对该方法进行培训和评估。在所有评估中，与三次B样条插值方法相比，新方法在结构相似性指数度量和峰值信噪比方面产生了显著更好的结果（使用单侧Wilcoxon符号秩检验，p<0.001）。鉴于该方法的无监督性质，不需要高分辨率的训练数据，因此，该方法可以很容易地应用于临床环境。
<details>	<summary>英文摘要</summary>	High-resolution medical images are beneficial for analysis but their acquisition may not always be feasible. Alternatively, high-resolution images can be created from low-resolution acquisitions using conventional upsampling methods, but such methods cannot exploit high-level contextual information contained in the images. Recently, better performing deep-learning based super-resolution methods have been introduced. However, these methods are limited by their supervised character, i.e. they require high-resolution examples for training. Instead, we propose an unsupervised deep learning semantic interpolation approach that synthesizes new intermediate slices from encoded low-resolution examples. To achieve semantically smooth interpolation in through-plane direction, the method exploits the latent space generated by autoencoders. To generate new intermediate slices, latent space encodings of two spatially adjacent slices are combined using their convex combination. Subsequently, the combined encoding is decoded to an intermediate slice. To constrain the model, a notion of semantic similarity is defined for a given dataset. For this, a new loss is introduced that exploits the spatial relationship between slices of the same volume. During training, an existing in-between slice is generated using a convex combination of its neighboring slice encodings. The method was trained and evaluated using publicly available cardiac cine, neonatal brain and adult brain MRI scans. In all evaluations, the new method produces significantly better results in terms of Structural Similarity Index Measure and Peak Signal-to-Noise Ratio (p< 0.001 using one-sided Wilcoxon signed-rank test) than a cubic B-spline interpolation approach. Given the unsupervised nature of the method, high-resolution training data is not required and hence, the method can be readily applied in clinical settings. </details>
<details>	<summary>注释</summary>	Medical Image Analysis, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月21日</details>

# 435、单图像超分辨率方法综述
- [ ] Single Image Super-Resolution Methods: A Survey 
时间：2022年02月17日                         第一作者：Bahattin Can Maral                       [链接](https://arxiv.org/abs/2202.11763).                     
## 摘要：超分辨率（SR）是从同一场景的一个或多个低分辨率观测值中获取高分辨率图像的过程，在过去几十年中，它一直是信号处理和图像处理领域的一个非常热门的研究课题。由于最近卷积神经网络的发展，随着进入门槛的显著降低，SR算法的普及率急剧上升。最近，这种流行已经蔓延到视频处理领域，发展出了实时工作的SR模型。在这篇论文中，我们比较了专门从事单一图像处理的不同SR模型，并将简要介绍它们是如何在过去的几年中演变成具有许多不同目标和形状的。
<details>	<summary>英文摘要</summary>	Super-resolution (SR), the process of obtaining high-resolution images from one or more low-resolution observations of the same scene, has been a very popular topic of research in the last few decades in both signal processing and image processing areas. Due to the recent developments in Convolutional Neural Networks, the popularity of SR algorithms has skyrocketed as the barrier of entry has been lowered significantly. Recently, this popularity has spread into video processing areas to the lengths of developing SR models that work in real-time. In this paper, we compare different SR models that specialize in single image processing and will take a glance at how they evolved to take on many different objectives and shapes over the years. </details>
<details>	<summary>注释</summary>	7 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年02月25日</details>

# 434、用于盲图像超分辨率的深度约束最小二乘法
- [ ] Deep Constrained Least Squares for Blind Image Super-Resolution 
时间：2022年02月15日                         第一作者：Ziwei Luo                       [链接](https://arxiv.org/abs/2202.07508).                     
## 摘要：在本文中，我们用一个新的退化模型和两个新的模块来解决盲图像超分辨率（SR）问题。该方法遵循盲SR的一般做法，提出了改进核估计和基于核的高分辨率图像恢复的方法。更具体地说，我们首先重新构造退化模型，使去模糊核估计可以转移到低分辨率空间。在此基础上，我们介绍了一个动态深线性滤波器模块。它不需要为所有图像学习一个固定的核，而是可以根据输入自适应地生成去模糊核权重，并产生更鲁棒的核估计。然后，应用深度约束最小二乘滤波模块，基于重构和估计的核生成干净的特征。然后将去模糊特征和低输入图像特征馈入双路径结构的SR网络，并恢复最终的高分辨率结果。为了评估我们的方法，我们进一步对几个基准进行了评估，包括Gaussian8和DIV2KRK。我们的实验表明，与现有的方法相比，该方法具有更好的准确性和视觉效果。
<details>	<summary>英文摘要</summary>	In this paper, we tackle the problem of blind image super-resolution(SR) with a reformulated degradation model and two novel modules. Following the common practices of blind SR, our method proposes to improve both the kernel estimation as well as the kernel based high resolution image restoration. To be more specific, we first reformulate the degradation model such that the deblurring kernel estimation can be transferred into the low resolution space. On top of this, we introduce a dynamic deep linear filter module. Instead of learning a fixed kernel for all images, it can adaptively generate deblurring kernel weights conditional on the input and yields more robust kernel estimation. Subsequently, a deep constrained least square filtering module is applied to generate clean features based on the reformulation and estimated kernel. The deblurred feature and the low input image feature are then fed into a dual-path structured SR network and restore the final high resolution result. To evaluate our method, we further conduct evaluations on several benchmarks, including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed method achieves better accuracy and visual improvements against state-of-the-art methods. </details>
<details>	<summary>注释</summary>	11 pages, 7 tables, 11 figures </details>
<details>	<summary>邮件日期</summary>	2022年02月16日</details>

# 433、用于引导图像超分辨率的记忆增强深度展开网络
- [ ] Memory-augmented Deep Unfolding Network for Guided Image Super-resolution 
时间：2022年02月12日                         第一作者：Man Zhou                       [链接](https://arxiv.org/abs/2203.04960).                     
## 摘要：引导图像超分辨率（GISR）旨在通过在HR图像的引导下提高低分辨率（LR）目标图像的空间分辨率来获得高分辨率（HR）目标图像。然而，以往基于模型的方法主要是将整个图像作为一个整体，并假设HR目标图像和HR引导图像之间的先验分布，而忽略了它们之间的许多非局部共同特征。为了缓解这一问题，我们首先提出了一种在HR目标图像上具有两种先验的GISR最大a后验（MAP）估计模型，即局部隐式先验和全局隐式先验。局部隐式先验的目的是从局部的角度模拟HR目标图像和HR引导图像之间的复杂关系，全局隐式先验的目的是从全局的角度考虑两幅图像之间的非局部自回归特性。其次，我们设计了一种新的交替优化算法来求解GISR模型。该算法在一个简洁的框架中，便于复制到常用的深层网络结构中。第三，为了减少迭代过程中的信息丢失，引入了持久性记忆机制，利用图像和特征空间中的长短时记忆单元（LSTM）来增强信息表示。这样就形成了一个具有一定解释力和较高表达能力的深层网络。大量实验结果验证了我们的方法在各种GISR任务上的优越性，包括泛锐化、深度图像超分辨率和MR图像超分辨率。
<details>	<summary>英文摘要</summary>	Guided image super-resolution (GISR) aims to obtain a high-resolution (HR) target image by enhancing the spatial resolution of a low-resolution (LR) target image under the guidance of a HR image. However, previous model-based methods mainly takes the entire image as a whole, and assume the prior distribution between the HR target image and the HR guidance image, simply ignoring many non-local common characteristics between them. To alleviate this issue, we firstly propose a maximal a posterior (MAP) estimation model for GISR with two types of prior on the HR target image, i.e., local implicit prior and global implicit prior. The local implicit prior aims to model the complex relationship between the HR target image and the HR guidance image from a local perspective, and the global implicit prior considers the non-local auto-regression property between the two images from a global perspective. Secondly, we design a novel alternating optimization algorithm to solve this model for GISR. The algorithm is in a concise framework that facilitates to be replicated into commonly used deep network structures. Thirdly, to reduce the information loss across iterative stages, the persistent memory mechanism is introduced to augment the information representation by exploiting the Long short-term memory unit (LSTM) in the image and feature spaces. In this way, a deep network with certain interpretation and high representation ability is built. Extensive experimental results validate the superiority of our method on a variety of GISR tasks, including Pan-sharpening, depth image super-resolution, and MR image super-resolution. </details>
<details>	<summary>注释</summary>	24 pages, 16 figures </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 432、真实世界图像超分辨率的频率感知物理退化模型
- [ ] Frequency-Aware Physics-Inspired Degradation Model for Real-World Image Super-Resolution 
时间：2022年02月11日                         第一作者：Zhenxing Dong                       [链接](https://arxiv.org/abs/2111.03301).                     
<details>	<summary>注释</summary>	22 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2022年02月14日</details>

# 431、不适定层析成像问题多数据一致解的深层生成模型流形挖掘
- [ ] Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems 
时间：2022年02月10日                         第一作者：Sayantan Bhadra                       [链接](https://arxiv.org/abs/2202.05311).                     
## 摘要：断层成像通常是一个不适定反问题。通常，从层析测量中获得所需对象的单个正则化图像估计。但是，可能有多个对象都与相同的测量数据一致。生成此类替代解决方案的能力非常重要，因为它可能会对成像系统进行新的评估。原则上，这可以通过后验抽样方法实现。近年来，深度神经网络被用于后验采样，并取得了良好的效果。然而，此类方法尚未用于大规模断层成像应用。另一方面，对于大规模成像系统，经验采样方法在计算上可能是可行的，并且能够在实际应用中实现不确定性量化。经验抽样涉及在随机优化框架内解决正则化反问题，以获得替代的数据一致性解决方案。在这项工作中，我们提出了一种新的经验抽样方法，该方法可以计算与相同测量数据一致的层析逆问题的多个解。该方法通过在基于风格的生成性对抗网络（StyleGAN）的潜在空间中反复求解优化问题来操作，并受到为超分辨率任务开发的通过潜在空间探索（PULSE）进行照片上采样（Photo Upsampling via Platential space Exploration）方法的启发。通过涉及两种典型断层成像模式的数值研究，对所提出的方法进行了演示和分析。这些研究确立了该方法进行有效经验抽样和不确定性量化的能力。
<details>	<summary>英文摘要</summary>	Tomographic imaging is in general an ill-posed inverse problem. Typically, a single regularized image estimate of the sought-after object is obtained from tomographic measurements. However, there may be multiple objects that are all consistent with the same measurement data. The ability to generate such alternate solutions is important because it may enable new assessments of imaging systems. In principle, this can be achieved by means of posterior sampling methods. In recent years, deep neural networks have been employed for posterior sampling with promising results. However, such methods are not yet for use with large-scale tomographic imaging applications. On the other hand, empirical sampling methods may be computationally feasible for large-scale imaging systems and enable uncertainty quantification for practical applications. Empirical sampling involves solving a regularized inverse problem within a stochastic optimization framework in order to obtain alternate data-consistent solutions. In this work, we propose a new empirical sampling method that computes multiple solutions of a tomographic inverse problem that are consistent with the same acquired measurement data. The method operates by repeatedly solving an optimization problem in the latent space of a style-based generative adversarial network (StyleGAN), and was inspired by the Photo Upsampling via Latent Space Exploration (PULSE) method that was developed for super-resolution tasks. The proposed method is demonstrated and analyzed via numerical studies that involve two stylized tomographic imaging modalities. These studies establish the ability of the method to perform efficient empirical sampling and uncertainty quantification. </details>
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Medical Imaging </details>
<details>	<summary>邮件日期</summary>	2022年02月14日</details>

# 430、无分布不确定性量化的图像间回归及其在成像中的应用
- [ ] Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging 
时间：2022年02月10日                         第一作者：Anastasios N Angelopoulos                       [链接](https://arxiv.org/abs/2202.05265).                     
## 摘要：图像到图像的回归是一项重要的学习任务，在生物成像中经常使用。然而，目前的算法通常不提供防止模型错误和幻觉的统计保证。为了解决这个问题，我们开发了不确定性量化技术，为图像到图像的回归问题提供了严格的统计保证。特别是，我们展示了如何推导每个像素周围的不确定性区间，这些不确定性区间保证包含用户指定置信概率的真值。我们的方法与任何基本的机器学习模型（如神经网络）结合使用，并赋予它形式化的数学保证——不管真实的未知数据分布或模型选择如何。此外，它们实现简单，计算成本低廉。我们在三个图像到图像回归任务上评估了我们的程序：定量相位显微镜、加速磁共振成像和果蝇大脑的超分辨率透射电子显微镜。
<details>	<summary>英文摘要</summary>	Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model's mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees -- regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain. </details>
<details>	<summary>注释</summary>	Code available at https://github.com/aangelopoulos/im2im-uq </details>
<details>	<summary>邮件日期</summary>	2022年02月11日</details>

# 429、二元神经网络作为设备上计算机视觉的通用计算范式
- [ ] Binary Neural Networks as a general-propose compute paradigm for on-device computer vision 
时间：2022年02月08日                         第一作者：Guhong Nie (1)                       [链接](https://arxiv.org/abs/2202.03716).                     
## 摘要：为了使二元神经网络（BNN）成为设备上计算机视觉算法的主流，它们必须实现比8位量化更好的速度与精度权衡，并在视觉任务中建立类似程度的普遍适用性。为此，我们提出了一个BNN框架，包括1）硬件友好的最小推理方案，2）高精度的参数化训练方案，以及3）适应不同视觉任务的简单程序。在分类、检测、分割、超分辨率和匹配的速度与精度权衡中，最终的框架超过了8位量化：我们的BNN不仅保留了其8位基线的精度水平，而且在移动CPU上展示了快1.3-2.4$\倍的FPS。对于典型的基于脉动阵列的人工智能加速器也可以得出类似的结论，其中我们的BNN比8位的执行周期少2.8-7$\倍，比其他BNN设计少2.1-2.7$\倍。这些结果表明，大规模采用BNN的时机已经成熟。
<details>	<summary>英文摘要</summary>	For binary neural networks (BNNs) to become the mainstream on-device computer vision algorithm, they must achieve a superior speed-vs-accuracy tradeoff than 8-bit quantization and establish a similar degree of general applicability in vision tasks. To this end, we propose a BNN framework comprising 1) a minimalistic inference scheme for hardware-friendliness, 2) an over-parameterized training scheme for high accuracy, and 3) a simple procedure to adapt to different vision tasks. The resultant framework overtakes 8-bit quantization in the speed-vs-accuracy tradeoff for classification, detection, segmentation, super-resolution and matching: our BNNs not only retain the accuracy levels of their 8-bit baselines but also showcase 1.3-2.4$\times$ faster FPS on mobile CPUs. Similar conclusions can be drawn for prototypical systolic-array-based AI accelerators, where our BNNs promise 2.8-7$\times$ fewer execution cycles than 8-bit and 2.1-2.7$\times$ fewer cycles than alternative BNN designs. These results suggest that the time for large-scale BNN adoption could be upon us. </details>
<details>	<summary>注释</summary>	13 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2022年02月09日</details>

# 428、有监督深度学习中的训练模型是一个条件风险最小化模型
- [ ] Trained Model in Supervised Deep Learning is a Conditional Risk Minimizer 
时间：2022年02月08日                         第一作者：Yutong Xie                       [链接](https://arxiv.org/abs/2202.03674).                     
## 摘要：我们证明了在有监督的深度学习中，一个经过训练的模型使每个输入的条件风险最小化（定理2.1）。这一特性提供了对训练模型行为的洞察，并在某些情况下建立了有监督和无监督学习之间的联系。此外，当标签难以处理但可以写为条件风险最小化时，我们证明了原始监督学习问题与可访问标签的等价形式（定理2.2）。我们证明了许多现有的工作，如Noise2Score、Noise2Noise和score函数估计都可以用我们的定理来解释。此外，我们利用定理2.1导出了带噪声标签的分类问题的一个性质，并利用MNIST数据集对其进行了验证。此外，我们基于定理2.2提出了一种估计图像超分辨率不确定性的方法，并使用ImageNet数据集对其进行了验证。我们的代码可以在github上找到。
<details>	<summary>英文摘要</summary>	We proved that a trained model in supervised deep learning minimizes the conditional risk for each input (Theorem 2.1). This property provided insights into the behavior of trained models and established a connection between supervised and unsupervised learning in some cases. In addition, when the labels are intractable but can be written as a conditional risk minimizer, we proved an equivalent form of the original supervised learning problem with accessible labels (Theorem 2.2). We demonstrated that many existing works, such as Noise2Score, Noise2Noise and score function estimation can be explained by our theorem. Moreover, we derived a property of classification problem with noisy labels using Theorem 2.1 and validated it using MNIST dataset. Furthermore, We proposed a method to estimate uncertainty in image super-resolution based on Theorem 2.2 and validated it using ImageNet dataset. Our code is available on github. </details>
<details>	<summary>邮件日期</summary>	2022年02月09日</details>

# 427、真实世界人脸的超分辨率
- [ ] Super-Resolution of Real-World Faces 
时间：2022年02月08日                         第一作者：Saurabh Goswami                       [链接](https://arxiv.org/abs/2011.02427).                     
<details>	<summary>注释</summary>	15 pages </details>
<details>	<summary>邮件日期</summary>	2022年02月09日</details>

# 426、HPRN：用于光谱超分辨率的整体先验嵌入关系网络
- [ ] HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution 
时间：2022年02月08日                         第一作者：Chaoxiong Wu                       [链接](https://arxiv.org/abs/2112.14608).                     
<details>	<summary>邮件日期</summary>	2022年02月09日</details>

# 425、精确的超分辨率低场脑磁共振成像
- [ ] Accurate super-resolution low-field brain MRI 
时间：2022年02月07日                         第一作者：Juan Eugenio Iglesias                       [链接](https://arxiv.org/abs/2202.03564).                     
## 摘要：最近，便携式低场MRI（LF-MRI）被引入临床环境，有可能改变神经影像学。然而，LF-MRI受较低分辨率和信噪比的限制，导致大脑区域的特征不完整。为了应对这一挑战，机器学习的最新进展促进了从一个或多个低分辨率扫描得到的高分辨率图像的合成。在这里，我们报告了机器学习超分辨率（SR）算法的扩展，以从LF-MRI T1加权和T2加权序列合成1mm各向同性MPRAGE样扫描。我们在LF和高场（HF，1.5T-3T）临床扫描配对数据集上的初步结果表明：（i）将可用的自动分割工具直接应用于LF-MRI图像会出现问题；但是（ii）当应用于与HF-MRI金标准测量值高度相关的SR图像时，分割工具成功（例如，海马体积r=0.85，丘脑r=0.84，整个大脑r=0.92）。这项工作证明了低分辨率LF-MRI序列后处理图像增强的原理。这些结果为进一步提高LF图像正常和异常的检出率，最终提高LF- MRI的诊断性能奠定了基础。我们的工具在FreeSurfer（surfer.nmr.mgh.harvard.edu/）上公开提供。
<details>	<summary>英文摘要</summary>	The recent introduction of portable, low-field MRI (LF-MRI) into the clinical setting has the potential to transform neuroimaging. However, LF-MRI is limited by lower resolution and signal-to-noise ratio, leading to incomplete characterization of brain regions. To address this challenge, recent advances in machine learning facilitate the synthesis of higher resolution images derived from one or multiple lower resolution scans. Here, we report the extension of a machine learning super-resolution (SR) algorithm to synthesize 1 mm isotropic MPRAGE-like scans from LF-MRI T1-weighted and T2-weighted sequences. Our initial results on a paired dataset of LF and high-field (HF, 1.5T-3T) clinical scans show that: (i) application of available automated segmentation tools directly to LF-MRI images falters; but (ii) segmentation tools succeed when applied to SR images with high correlation to gold standard measurements from HF-MRI (e.g., r = 0.85 for hippocampal volume, r = 0.84 for the thalamus, r = 0.92 for the whole cerebrum). This work demonstrates proof-of-principle post-processing image enhancement from lower resolution LF-MRI sequences. These results lay the foundation for future work to enhance the detection of normal and abnormal image findings at LF and ultimately improve the diagnostic performance of LF-MRI. Our tools are publicly available on FreeSurfer (surfer.nmr.mgh.harvard.edu/). </details>
<details>	<summary>邮件日期</summary>	2022年02月09日</details>

# 424、一种新的图像和视频域人脸交换方法：技术报告
- [ ] A new face swap method for image and video domains: a technical report 
时间：2022年02月07日                         第一作者：Daniil Chesakov                       [链接](https://arxiv.org/abs/2202.03046).                     
## 摘要：近几年来，深假技术成为研究的热点。研究人员研究了复杂的生成性对抗网络（GAN）、自动编码器和其他方法，以建立精确而稳健的人脸交换算法。实现的结果表明，深度伪无监督合成任务在生成数据的视觉质量方面存在问题。当专家分析这些问题时，这些问题通常会导致较高的假检测准确率。第一个问题是现有的图像到图像的方法不考虑视频域的特异性，逐帧处理导致面部抖动和其他清晰可见的失真。另一个问题是生成的数据分辨率，由于计算复杂度高，许多现有方法的分辨率较低。第三个问题出现在源面比例较大（如较大的脸颊）时，替换后，它在面边界上变得可见。我们的主要目标是开发这样一种方法，可以解决这些问题，并在许多指标上优于现有的解决方案。我们在上面提到的FaceShifter基础上引入了一个新的体系结构和解决方案。通过一种新的眼球损失功能、超分辨率块和基于高斯的面罩生成，可以提高质量，这在评估过程中得到了证实。
<details>	<summary>英文摘要</summary>	Deep fake technology became a hot field of research in the last few years. Researchers investigate sophisticated Generative Adversarial Networks (GAN), autoencoders, and other approaches to establish precise and robust algorithms for face swapping. Achieved results show that the deep fake unsupervised synthesis task has problems in terms of the visual quality of generated data. These problems usually lead to high fake detection accuracy when an expert analyzes them. The first problem is that existing image-to-image approaches do not consider video domain specificity and frame-by-frame processing leads to face jittering and other clearly visible distortions. Another problem is the generated data resolution, which is low for many existing methods due to high computational complexity. The third problem appears when the source face has larger proportions (like bigger cheeks), and after replacement it becomes visible on the face border. Our main goal was to develop such an approach that could solve these problems and outperform existing solutions on a number of clue metrics. We introduce a new face swap pipeline that is based on FaceShifter architecture and fixes the problems stated above. With a new eye loss function, super-resolution block, and Gaussian-based face mask generation leads to improvements in quality which is confirmed during evaluation. </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 423、FISR：具有多尺度时间损失的深关节帧插值和超分辨率
- [ ] FISR: Deep Joint Frame Interpolation and Super-Resolution with a Multi-scale Temporal Loss 
时间：2022年02月07日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/1912.07213).                     
<details>	<summary>注释</summary>	The first two authors contributed equally to this work. Accepted to AAAI 2020 (camera-ready version) </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 422、具有可变形注意金字塔的快速在线视频超分辨率
- [ ] Fast Online Video Super-Resolution with Deformable Attention Pyramid 
时间：2022年02月03日                         第一作者：Dario Fuoli                       [链接](https://arxiv.org/abs/2202.01731).                     
## 摘要：视频超分辨率（VSR）有许多应用程序，包括视频流和电视，这些应用程序具有严格的因果、实时和延迟限制。我们在这些设置下解决了VSR问题，由于未来帧的信息不可用，这带来了额外的重要挑战。重要的是，设计高效但有效的帧对齐和融合模块仍然是核心问题。在这项工作中，我们提出了一种基于可变形注意金字塔（DAP）的递归VSR结构。我们的DAP将来自循环状态的信息对齐并集成到当前帧预测中。为了规避传统的基于注意的方法的计算成本，我们只关注有限数量的空间位置，这些位置由DAP动态预测。综合实验和对提出的关键创新的分析表明了我们方法的有效性。与最先进的方法相比，我们显著减少了处理时间，同时保持了高性能。我们在两个标准基准上超过了最先进的EDVR-M方法，速度超过3倍。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames are unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over 3x. </details>
<details>	<summary>邮件日期</summary>	2022年02月04日</details>

# 421、结构增强图像超分辨率的梯度方差损失
- [ ] Gradient Variance Loss for Structure-Enhanced Image Super-Resolution 
时间：2022年02月02日                         第一作者：Lusine Abrahamyan                       [链接](https://arxiv.org/abs/2202.00997).                     
## 摘要：在最近的单卷积空间中，通过优化单卷积图像（CNL1-NS）或单卷积图像（CNL1-NS）的分辨率，可以获得成功。然而，当使用这些损失函数进行训练时，模型通常无法恢复高分辨率（HR）图像中存在的锐利边缘，原因是该模型倾向于给出潜在HR解的统计平均值。在我们的研究中，我们观察到，使用L1或L2损失训练的模型生成的图像梯度图的方差显著低于原始高分辨率图像的梯度图。在这项工作中，我们建议通过引入结构增强损失函数、创造梯度方差（GV）损失来缓解上述问题，并生成具有感知愉悦细节的纹理。具体地说，在模型的训练过程中，我们从目标的梯度图中提取补丁并生成输出，计算每个补丁的方差，并为这两幅图像形成方差图。此外，我们最小化计算出的方差贴图之间的距离，以强制模型生成高方差梯度贴图，从而生成边缘更清晰的高分辨率图像。实验结果表明，GV损失可以显著改善现有图像超分辨率（SR）深度学习模型的结构相似性（SSIM）和峰值信噪比（PSNR）性能。
<details>	<summary>英文摘要</summary>	Recent success in the field of single image super-resolution (SISR) is achieved by optimizing deep convolutional neural networks (CNNs) in the image space with the L1 or L2 loss. However, when trained with these loss functions, models usually fail to recover sharp edges present in the high-resolution (HR) images for the reason that the model tends to give a statistical average of potential HR solutions. During our research, we observe that gradient maps of images generated by the models trained with the L1 or L2 loss have significantly lower variance than the gradient maps of the original high-resolution images. In this work, we propose to alleviate the above issue by introducing a structure-enhancing loss function, coined Gradient Variance (GV) loss, and generate textures with perceptual-pleasant details. Specifically, during the training of the model, we extract patches from the gradient maps of the target and generated output, calculate the variance of each patch and form variance maps for these two images. Further, we minimize the distance between the computed variance maps to enforce the model to produce high variance gradient maps that will lead to the generation of high-resolution images with sharper edges. Experimental results show that the GV loss can significantly improve both Structure Similarity (SSIM) and peak signal-to-noise ratio (PSNR) performance of existing image super-resolution (SR) deep learning models. </details>
<details>	<summary>注释</summary>	ICASSP 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月03日</details>

# 420、具有最佳传输图的非成对图像超分辨率
- [ ] Unpaired Image Super-Resolution with Optimal Transport Maps 
时间：2022年02月02日                         第一作者：Milena Gazdieva                       [链接](https://arxiv.org/abs/2202.01116).                     
## 摘要：现实世界中的图像超分辨率（SR）任务通常没有成对的数据集，这限制了监督技术的应用。因此，这些任务通常是通过基于生成性对抗网络（GAN）的不成对技术来完成的，这会产生复杂的训练损失，并带有一些规则化术语，如内容和身份损失。我们从理论上研究了此类模型中出现的优化问题，并发现了两个令人惊讶的结果。首先，学习到的SR图始终是最优运输（OT）图。第二，我们的经验表明，学习的地图是有偏见的，也就是说，它实际上可能不会将低分辨率图像的分布转化为高分辨率图像。受这些发现的启发，我们提出了一种非配对SR算法，该算法学习感知传输成本的无偏OT映射。与现有的基于GAN的方案不同，我们的算法有一个简单的优化目标，减少了执行复杂超参数选择和使用额外正则化的必要性。同时，它在大规模未配对的AIM-19数据集上提供了近乎最先进的性能。
<details>	<summary>英文摘要</summary>	Real-world image super-resolution (SR) tasks often do not have paired datasets limiting the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs) which yield complex training losses with several regularization terms such as content and identity losses. We theoretically investigate the optimization problems which arise in such models and find two surprising observations. First, the learned SR map is always an optimal transport (OT) map. Second, we empirically show that the learned map is biased, i.e., it may not actually transform the distribution of low-resolution images to high-resolution images. Inspired by these findings, we propose an algorithm for unpaired SR which learns an unbiased OT map for the perceptual transport cost. Unlike existing GAN-based alternatives, our algorithm has a simple optimization objective reducing the neccesity to perform complex hyperparameter selection and use additional regularizations. At the same time, it provides nearly state-of-the-art performance on the large-scale unpaired AIM-19 dataset. </details>
<details>	<summary>邮件日期</summary>	2022年02月03日</details>

# 419、CAESR：学习空间可伸缩性的条件自动编码器和超分辨率
- [ ] CAESR: Conditional Autoencoder and Super-Resolution for Learned Spatial Scalability 
时间：2022年02月01日                         第一作者：Charles Bonnineau                       [链接](https://arxiv.org/abs/2202.00416).                     
## 摘要：在本文中，我们提出了CAESR，一种基于多功能视频编码（VVC）标准的基于混合学习的空间可伸缩性编码方法。我们的框架将VVC帧内模式编码的低分辨率信号视为基本层（BL），将HyperPrerior（AE-HP）深度条件自动编码器视为增强层（EL）模型。EL编码器将放大的BL重建和原始图像都作为输入。我们的方法依赖于条件编码，学习源图像和放大的BL图像的最佳混合，从而实现比残差编码更好的性能。在解码器端，使用超分辨率（SR）模块恢复高分辨率细节并反转条件编码过程。实验结果表明，我们的解决方案在可扩展性方面与VVC全分辨率帧内编码具有竞争力。
<details>	<summary>英文摘要</summary>	In this paper, we present CAESR, an hybrid learning-based coding approach for spatial scalability based on the versatile video coding (VVC) standard. Our framework considers a low-resolution signal encoded with VVC intra-mode as a base-layer (BL), and a deep conditional autoencoder with hyperprior (AE-HP) as an enhancement-layer (EL) model. The EL encoder takes as inputs both the upscaled BL reconstruction and the original image. Our approach relies on conditional coding that learns the optimal mixture of the source and the upscaled BL image, enabling better performance than residual coding. On the decoder side, a super-resolution (SR) module is used to recover high-resolution details and invert the conditional coding process. Experimental results have shown that our solution is competitive with the VVC full-resolution intra coding while being scalable. </details>
<details>	<summary>邮件日期</summary>	2022年02月02日</details>

# 418、用于非凸正则化即插即用收敛优化的近端去噪器
- [ ] Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization 
时间：2022年01月31日                         第一作者：Samuel Hurault                       [链接](https://arxiv.org/abs/2201.13256).                     
## 摘要：即插即用（PnP）方法通过用去噪操作替换近端算子的迭代近端算法来解决不适定逆问题。当与深度神经网络去噪器一起应用时，这些方法在图像恢复问题上显示了最先进的视觉性能。然而，他们的理论收敛性分析仍然不完整。现有的收敛结果大多考虑非扩展的去噪器，这是不现实的，或者将它们的分析局限于要解决的逆问题中的强凸数据保真度项。最近，有人提出将去噪器训练为深度神经网络参数化函数上的梯度下降步骤。使用这种去噪器可以保证半二次分裂（PnP HQS）迭代算法的PnP版本的收敛性。在本文中，我们证明了这种梯度去噪器实际上可以对应于另一个标量函数的近端算子。鉴于这一新结果，我们利用非凸环境下近端算法的收敛理论，得到了PnP-PGD（近端梯度下降）和PnP-ADMM（交替方向乘子法）的收敛结果。当建立在光滑梯度去噪器之上时，我们证明了PnP-PGD和PnP-ADMM是收敛的，并且是显式泛函的目标驻点。这些收敛结果得到了去模糊、超分辨率和修复的数值实验的证实。
<details>	<summary>英文摘要</summary>	Plug-and-Play (PnP) methods solve ill-posed inverse problems through iterative proximal algorithms by replacing a proximal operator by a denoising operation. When applied with deep neural network denoisers, these methods have shown state-of-the-art visual performance for image restoration problems. However, their theoretical convergence analysis is still incomplete. Most of the existing convergence results consider nonexpansive denoisers, which is non-realistic, or limit their analysis to strongly convex data-fidelity terms in the inverse problem to solve. Recently, it was proposed to train the denoiser as a gradient descent step on a functional parameterized by a deep neural network. Using such a denoiser guarantees the convergence of the PnP version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this paper, we show that this gradient denoiser can actually correspond to the proximal operator of another scalar function. Given this new result, we exploit the convergence theory of proximal algorithms in the nonconvex setting to obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM (Alternating Direction Method of Multipliers). When built on top of a smooth gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target stationary points of an explicit functional. These convergence results are confirmed with numerical experiments on deblurring, super-resolution and inpainting. </details>
<details>	<summary>注释</summary>	21 pages </details>
<details>	<summary>邮件日期</summary>	2022年02月01日</details>

# 417、VRT：一种视频恢复转换器
- [ ] VRT: A Video Restoration Transformer 
时间：2022年01月28日                         第一作者：Jingyun Liang                        [链接](https://arxiv.org/abs/2201.12288).                     
## 摘要：视频恢复（例如视频超分辨率）旨在从低质量帧恢复高质量帧。与单个图像恢复不同，视频恢复通常需要利用多个相邻但通常不对齐的视频帧的时间信息。现有的深度方法通常通过利用滑动窗口策略或循环架构来解决这一问题，这种策略或受到逐帧恢复的限制，或缺乏长期建模能力。在本文中，我们提出了一种具有并行帧预测和长期时间依赖建模能力的视频恢复转换器（VRT）。更具体地说，VRT由多个量表组成，每个量表由两种模块组成：时间相互自我注意（TMSA）和平行扭曲。TMSA将视频分割成多个小片段，相互关注用于联合运动估计、特征对齐和特征融合，自我关注用于特征提取。为了实现交叉剪辑交互，视频序列会每隔一层移动一次。此外，通过并行特征扭曲，并行扭曲用于进一步融合来自相邻帧的信息。在视频超分辨率、视频去模糊和视频去噪三项任务上的实验结果表明，在九个基准数据集上，VRT比最新的方法有更大的优势（$\textbf{2.16dB}$）。
<details>	<summary>英文摘要</summary>	Video restoration (e.g., video super-resolution) aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which either is restricted by frame-by-frame restoration or lacks long-range modelling ability. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction and long-range temporal dependency modelling abilities. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal mutual self attention (TMSA) and parallel warping. TMSA divides the video into small clips, on which mutual attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on three tasks, including video super-resolution, video deblurring and video denoising, demonstrate that VRT outperforms the state-of-the-art methods by large margins ($\textbf{up to 2.16dB}$) on nine benchmark datasets. </details>
<details>	<summary>注释</summary>	Sota results (+up to 2.16dB) on video SR, video deblurring and video denoising. Code: https://github.com/JingyunLiang/VRT </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 416、用于图像和视频超分辨率的深度网络
- [ ] Deep Networks for Image and Video Super-Resolution 
时间：2022年01月28日                         第一作者：Kuldeep Purohit                       [链接](https://arxiv.org/abs/2201.11996).                     
## 摘要：卷积神经网络中间层的梯度传播效率对于超分辨率任务至关重要。为此，我们提出了一种单图像超分辨率（SISR）的深层结构，该结构使用高效卷积单元构建，我们称之为混合密集连接块（MDCB）。MDCB的设计结合了剩余连接策略和密集连接策略的优点，同时克服了它们的局限性。为了实现多因子的超分辨率，我们提出了一种尺度递归框架，该框架将低尺度因子学习的滤波器递归地重新用于高尺度因子。这将提高性能，并提高更高因素的参数效率。我们训练两个版本的网络，使用不同的损耗配置来增强互补的图像质量。我们进一步将我们的网络用于视频超分辨率任务，在该任务中，我们的网络学习从多个帧聚合信息并保持时空一致性。与最先进的图像和视频超分辨率基准技术相比，拟议的网络带来了定性和定量的改进。
<details>	<summary>英文摘要</summary>	Efficiency of gradient propagation in intermediate layers of convolutional neural networks is of key importance for super-resolution task. To this end, we propose a deep architecture for single image super-resolution (SISR), which is built using efficient convolutional units we refer to as mixed-dense connection blocks (MDCB). The design of MDCB combines the strengths of both residual and dense connection strategies, while overcoming their limitations. To enable super-resolution for multiple factors, we propose a scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. This leads to improved performance and promotes parametric efficiency for higher factors. We train two versions of our network to enhance complementary image qualities using different loss configurations. We further employ our network for video super-resolution task, where our network learns to aggregate information from multiple frames and maintain spatio-temporal consistency. The proposed networks lead to qualitative and quantitative improvements over state-of-the-art techniques on image and video super-resolution benchmarks. </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 415、基于尺度递归稠密网络的图像超分辨率
- [ ] Image Superresolution using Scale-Recurrent Dense Network 
时间：2022年01月28日                         第一作者：Kuldeep Purohit                       [链接](https://arxiv.org/abs/2201.11998).                     
## 摘要：卷积神经网络（CNN）设计的最新进展显著改善了图像超分辨率（SR）的性能。性能的提升可归因于这些网络中间层中存在残余或密集连接。这种连接的有效组合可以大幅减少参数数量，同时保持恢复质量。在本文中，我们提出了一种基于残差块（残差密集块（RDB））中包含一系列密集连接的单元的大规模递归SR结构，该结构允许从图像中提取丰富的局部特征。与当前最先进的方法相比，我们的比例循环设计在提供更高比例因数的竞争性能的同时，在参数上更高效。为了进一步提高网络的性能，我们在中间层中使用了多个剩余连接（称为多个剩余密集块），这改善了现有层中的梯度传播。最近的研究发现，传统的损耗函数可以引导网络产生具有高PSNR但感知较差的结果。我们通过利用基于生成性对抗网络（GAN）的框架和深度特征（VGG）损失来训练我们的网络，从而缓解了这个问题。我们通过实验证明，VGG损耗和对抗损耗的不同加权组合使我们的网络输出能够沿着感知失真曲线移动。与现有方法相比，所提出的网络在感知和客观（基于PSNR）方面都表现良好，且参数较少。
<details>	<summary>英文摘要</summary>	Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters. </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 414、去噪扩散恢复模型
- [ ] Denoising Diffusion Restoration Models 
时间：2022年01月27日                         第一作者：Bahjat Kawar                       [链接](https://arxiv.org/abs/2201.11793).                     
## 摘要：图像恢复中许多有趣的任务都可以归结为线性逆问题。最近一系列解决这些问题的方法使用随机算法，从给定测量值的自然图像的后验分布中采样。然而，有效的解决方案通常需要特定于问题的有监督训练来建模后验，而非特定于问题的无监督方法通常依赖于低效的迭代方法。这项工作通过引入去噪扩散恢复模型（DDRM）来解决这些问题，DDRM是一种高效、无监督的后验采样方法。在变分推理的激励下，DDRM利用预先训练好的去噪扩散生成模型来求解任何线性逆问题。我们在多个图像数据集上展示了DDRM的多功能性，可以在各种测量噪声下进行超分辨率、去模糊、修复和着色。在重建质量、感知质量和运行时间方面，DDRM在不同ImageNet数据集上优于当前领先的无监督方法，比最接近的竞争对手快5倍。DDRM还可以从观测到的ImageNet训练集的分布中很好地概括自然图像。
<details>	<summary>英文摘要</summary>	Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set. </details>
<details>	<summary>注释</summary>	Our code is available at https://github.com/bahjat-kawar/ddrm </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 413、重温RCAN：改进图像超分辨率训练
- [ ] Revisiting RCAN: Improved Training for Image Super-Resolution 
时间：2022年01月27日                         第一作者：Zudi Lin                       [链接](https://arxiv.org/abs/2201.11279).                     
## 摘要：图像超分辨率（SR）是一个快速发展的领域，其新颖的结构吸引了人们的关注。然而，大多数SR模型都使用过时的训练策略进行了优化。在这项工作中，我们重新审视了流行的RCAN模型，并检查了SR中不同训练选项的效果。令人惊讶的是（或可能如预期的那样），我们表明，RCAN可以在标准基准上，通过适当的训练策略和最小的架构更改，超越或匹配RCAN之后发布的几乎所有基于CNN的SR架构。此外，尽管RCAN是一个具有400多个卷积层的非常大的SR架构，但我们得出了一个值得注意的结论，即欠拟合仍然是限制模型性能的主要问题，而不是过度拟合。我们观察到支持性证据表明，增加训练迭代次数明显提高了模型性能，而应用正则化技术通常会降低预测。我们将简单修订的RCAN表示为RCAN it，并建议从业者将其用作未来研究的基线。代码可在https://github.com/zudi-lin/rcan-it.
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) is a fast-moving field with novel architectures attracting the spotlight. However, most SR models were optimized with dated training strategies. In this work, we revisit the popular RCAN model and examine the effect of different training options in SR. Surprisingly (or perhaps as expected), we show that RCAN can outperform or match nearly all the CNN-based SR architectures published after RCAN on standard benchmarks with a proper training strategy and minimal architecture change. Besides, although RCAN is a very large SR architecture with more than four hundred convolutional layers, we draw a notable conclusion that underfitting is still the main problem restricting the model capability instead of overfitting. We observe supportive evidence that increasing training iterations clearly improves the model performance while applying regularization techniques generally degrades the predictions. We denote our simply revised RCAN as RCAN-it and recommend practitioners to use it as baselines for future research. Code is publicly available at https://github.com/zudi-lin/rcan-it. </details>
<details>	<summary>注释</summary>	13 pages with 10 tables and 4 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月28日</details>

# 412、重新审视超分辨率中的L1损失：一种概率观点及其超越
- [ ] Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond 
时间：2022年01月25日                         第一作者：Xiangyu He                       [链接](https://arxiv.org/abs/2201.10084).                     
## 摘要：超分辨率作为一个不适定问题，对于低分辨率输入有许多高分辨率候选。然而，用于最佳拟合给定的HR图像的流行的$ \ ELY1损失没有考虑图像恢复中的非唯一性这一基本性质。在这项工作中，我们通过将神经网络作为概率模型来建立超分辨率，从而修复$\ellu 1$损失中的缺失部分。它表明$\ellu_1$损失相当于一个退化的似然函数，它消除了学习过程中的随机性。通过引入一个数据自适应随机变量，我们提出了一个新的目标函数，旨在最小化所有合理解的重建误差期望。实验结果表明，在推理时，在没有额外参数或计算成本的情况下，主流体系结构得到了一致的改进。
<details>	<summary>英文摘要</summary>	Super-resolution as an ill-posed problem has many high-resolution candidates for a low-resolution input. However, the popular $\ell_1$ loss used to best fit the given HR image fails to consider this fundamental property of non-uniqueness in image restoration. In this work, we fix the missing piece in $\ell_1$ loss by formulating super-resolution with neural networks as a probabilistic model. It shows that $\ell_1$ loss is equivalent to a degraded likelihood function that removes the randomness from the learning process. By introducing a data-adaptive random variable, we present a new objective function that aims at minimizing the expectation of the reconstruction error over all plausible solutions. The experimental results show consistent improvements on mainstream architectures, with no extra parameter or computing cost at inference time. </details>
<details>	<summary>注释</summary>	Technical report </details>
<details>	<summary>邮件日期</summary>	2022年01月26日</details>

# 411、具有深先验和退化模型反演的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-resolution with Deep Priors and Degradation Model Inversion 
时间：2022年01月24日                         第一作者：Xiuheng Wang                       [链接](https://arxiv.org/abs/2201.09851).                     
## 摘要：为了克服高光谱成像系统在空间分辨率方面固有的硬件限制，基于融合的高光谱图像（HSI）超分辨率正受到越来越多的关注。该技术旨在融合低分辨率（LR）HSI和常规高分辨率（HR）RGB图像，以获得HR HSI。最近，深度学习体系结构被用于解决HSI超分辨率问题，并取得了显著的性能。然而，他们忽略了退化模型，尽管该模型有明确的物理解释，可能有助于提高性能。为了解决这个问题，我们提出了一种方法，一方面利用目标函数数据保真度项中的线性退化模型，另一方面利用卷积神经网络的输出在光谱和空间梯度域中设计深度先验正则化器。实验表明，该策略能有效地提高系统的性能。
<details>	<summary>英文摘要</summary>	To overcome inherent hardware limitations of hyperspectral imaging systems with respect to their spatial resolution, fusion-based hyperspectral image (HSI) super-resolution is attracting increasing attention. This technique aims to fuse a low-resolution (LR) HSI and a conventional high-resolution (HR) RGB image in order to obtain an HR HSI. Recently, deep learning architectures have been used to address the HSI super-resolution problem and have achieved remarkable performance. However, they ignore the degradation model even though this model has a clear physical interpretation and may contribute to improve the performance. We address this problem by proposing a method that, on the one hand, makes use of the linear degradation model in the data-fidelity term of the objective function and, on the other hand, utilizes the output of a convolutional neural network for designing a deep prior regularizer in spectral and spatial gradient domains. Experiments show the performance improvement achieved with this strategy. </details>
<details>	<summary>注释</summary>	Proc. IEEE Int. Conf. on Acoust, Speech, Signal Process. (ICASSP), to be published. Manuscript submitted October 6th, 2021; revised January 8th, 2022; accepted January 22nd, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 410、基于神经微分方程的渐进式图像超分辨率
- [ ] Progressive Image Super-Resolution via Neural Differential Equation 
时间：2022年01月24日                         第一作者：Seobin Park                        [链接](https://arxiv.org/abs/2101.08987).                     
<details>	<summary>注释</summary>	Revision on the title, abstract and main text; Remove figures to fit 4 pages; Initial accepted version of ICASSP 2022 </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 409、ERQA：视频超分辨率边缘恢复质量评估
- [ ] ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution 
时间：2022年01月24日                         第一作者：Anastasia Kirillova                       [链接](https://arxiv.org/abs/2110.09992).                     
<details>	<summary>注释</summary>	Accepted for presentation at the International Conference on Computer Vision Theory and Applications (VISAPP) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 408、用于MRI超分辨率的感知cGAN
- [ ] Perceptual cGAN for MRI Super-resolution 
时间：2022年01月23日                         第一作者：Sahar Almahfouz Nasser                       [链接](https://arxiv.org/abs/2201.09314).                     
## 摘要：获取高分辨率磁共振（MR）图像是一个耗时的过程，这使得它不适合医疗急救和儿科患者。相比之下，低分辨率磁共振成像比高分辨率成像速度更快，但它在更精确诊断所需的细节上有所妥协。当超分辨率（SR）应用于低分辨率MR图像时，可以通过在几乎没有额外时间的情况下合成生成高分辨率图像来帮助提高其利用率。在本文中，我们提出了一种基于生成性对抗网络（GANs）的MR图像SR技术，该技术已被证明在生成SR中的锐利细节方面非常有用。我们引入了一种具有感知损失的条件GAN，它以输入的低分辨率图像为条件，从而提高了各向同性和各向异性MRI超分辨率的性能。
<details>	<summary>英文摘要</summary>	Capturing high-resolution magnetic resonance (MR) images is a time consuming process, which makes it unsuitable for medical emergencies and pediatric patients. Low-resolution MR imaging, by contrast, is faster than its high-resolution counterpart, but it compromises on fine details necessary for a more precise diagnosis. Super-resolution (SR), when applied to low-resolution MR images, can help increase their utility by synthetically generating high-resolution images with little additional time. In this paper, we present a SR technique for MR images that is based on generative adversarial networks (GANs), which have proven to be quite useful in generating sharp-looking details in SR. We introduce a conditional GAN with perceptual loss, which is conditioned upon the input low-resolution image, which improves the performance for isotropic and anisotropic MRI super-resolution. </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 407、带变压器的光场图像超分辨率
- [ ] Light Field Image Super-Resolution with Transformers 
时间：2022年01月23日                         第一作者：Zhengyu Liang                       [链接](https://arxiv.org/abs/2108.07597).                     
<details>	<summary>注释</summary>	This paper has been accepted by IEEE Signal Processing Letters. The current version on arXiv is identical to the final accepted version in content, but integrates the supplemental material (i.e., related work and visual comparisons) to the main body of the paper. Moreover, figures and tables of the arxiv version were zoomed for better visualization </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 406、基于深度学习的图像超分辨率技术综述
- [ ] A Review of Deep Learning Based Image Super-resolution Techniques 
时间：2022年01月22日                         第一作者：Fangyuan Zhu                       [链接](https://arxiv.org/abs/2201.10521).                     
## 摘要：图像超分辨率技术是从一幅或多幅低分辨率图像中获取高分辨率图像的过程。随着深度学习的发展，基于深度学习方法的图像超分辨率技术应运而生。本文综述了深度学习方法在图像超分辨率领域应用的研究进展，从几个方面介绍了这种超分辨率工作，并展望了深度学习方法在图像超分辨率领域的进一步应用。通过收集和统计深度学习在图像超分辨率领域应用的相关文献，初步总结了深度学习方法在图像超分辨率领域的应用成果，报道了基于深度学习方法的图像超分辨率技术的最新进展。
<details>	<summary>英文摘要</summary>	Image super-resolution technology is the process of obtaining high-resolution images from one or more low-resolution images. With the development of deep learning, image super-resolution technology based on deep learning method is emerging. This paper reviews the research progress of the application of depth learning method in the field of image super-resolution, introduces this kind of super-resolution work from several aspects, and looks forward to the further application of depth learning method in the field of image super-resolution. By collecting and counting the relevant literature on the application of depth learning in the field of image super-resolution, we preliminarily summarizes the application results of depth learning method in the field of image super-resolution, and reports the latest progress of image super-resolution technology based on depth learning method. </details>
<details>	<summary>邮件日期</summary>	2022年01月26日</details>

# 405、人脸的鲁棒性非配对单幅图像超分辨率
- [ ] Robust Unpaired Single Image Super-Resolution of Faces 
时间：2022年01月22日                         第一作者：Saurabh Goswami                       [链接](https://arxiv.org/abs/2201.09109).                     
## 摘要：我们提出了一种针对特定人脸类别的单图像超分辨率（SISR）方法的对抗性攻击。现有的攻击，如快速梯度符号方法（FGSM）或投影梯度下降（PGD）方法，在这些网络上要么快速但无效，要么有效但缓慢得令人望而却步。通过仔细检查用于训练此类网络的MSE损耗在不同退化情况下跟踪的表面，我们能够确定其可参数化特性。我们利用这一特性提出了一种Adverastrial攻击，该攻击能够在不需要多个梯度上升步骤（快速）的情况下定位最佳降级（有效）。我们的实验表明，对于未配对的人脸和特定于类的SISR任务，该方法能够实现比现有对抗性攻击（如FGSM和PGD）更好的速度与效率权衡。
<details>	<summary>英文摘要</summary>	We propose an adversarial attack for facial class-specific Single Image Super-Resolution (SISR) methods. Existing attacks, such as the Fast Gradient Sign Method (FGSM) or the Projected Gradient Descent (PGD) method, are either fast but ineffective, or effective but prohibitively slow on these networks. By closely inspecting the surface that the MSE loss, used to train such networks, traces under varying degradations, we were able to identify its parameterizable property. We leverage this property to propose an adverasrial attack that is able to locate the optimum degradation (effective) without needing multiple gradient-ascent steps (fast). Our experiments show that the proposed method is able to achieve a better speed vs effectiveness trade-off than the state-of-theart adversarial attacks, such as FGSM and PGD, for the task of unpaired facial as well as class-specific SISR. </details>
<details>	<summary>注释</summary>	8 pages </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 404、SparseAlign：低温电子层析成像中自动标记定位和变形估计的超分辨率算法
- [ ] SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography 
时间：2022年01月21日                         第一作者：Poulami Somanya Ganguly                       [链接](https://arxiv.org/abs/2201.08706).                     
## 摘要：在低温电子层析成像中，倾斜序列对准是获得高分辨率重建的关键。仅从低对比度样品很难估计样品的束流诱导局部变形，通常需要基准金珠标记。最先进的变形估计方法使用投影数据中（半）手动标记的标记位置来拟合多项式变形模型的参数。当数据有噪声或投影数据中的标记重叠时，很难获得手动标记的标记位置。通过扩展单分子定位显微镜首次提出的无网格超分辨率算法，我们提出了一种同时进行标记定位和变形估计的数学方法。我们的方法不需要标记位置；相反，我们使用一种基于图像的丢失方法，将标记的正向投影与观测数据进行比较。我们为该标记定位方案配备了一个额外的变形估计组件，并解决了减少变形参数数量的问题。通过对仅标记样本的大量数值研究，我们表明，我们的方法可以在没有标记数据的情况下自动找到标记并可靠地估计样本变形。我们进一步证明了我们的方法适用于广泛的模型失配场景，包括冰上金标记的实验电子层析数据。
<details>	<summary>英文摘要</summary>	Tilt-series alignment is crucial to obtaining high-resolution reconstructions in cryo-electron tomography. Beam-induced local deformation of the sample is hard to estimate from the low-contrast sample alone, and often requires fiducial gold bead markers. The state-of-the-art approach for deformation estimation uses (semi-)manually labelled marker locations in projection data to fit the parameters of a polynomial deformation model. Manually-labelled marker locations are difficult to obtain when data are noisy or markers overlap in projection data. We propose an alternative mathematical approach for simultaneous marker localization and deformation estimation by extending a grid-free super-resolution algorithm first proposed in the context of single-molecule localization microscopy. Our approach does not require labelled marker locations; instead, we use an image-based loss where we compare the forward projection of markers with the observed data. We equip this marker localization scheme with an additional deformation estimation component and solve for a reduced number of deformation parameters. Using extensive numerical studies on marker-only samples, we show that our approach automatically finds markers and reliably estimates sample deformation without labelled marker data. We further demonstrate the applicability of our approach for a broad range of model mismatch scenarios, including experimental electron tomography data of gold markers on ice. </details>
<details>	<summary>邮件日期</summary>	2022年01月24日</details>

# 403、自监督深盲视频超分辨率
- [ ] Self-Supervised Deep Blind Video Super-Resolution 
时间：2022年01月19日                         第一作者：Haoran Bai                        [链接](https://arxiv.org/abs/2201.07422).                     
## 摘要：现有的基于深度学习的视频超分辨率（SR）方法通常依赖于监督学习方法，其中训练数据通常由具有已知或预定义核（例如双三次核）的模糊操作生成，然后进行抽取操作。然而，这并不适用于实际应用，因为退化过程很复杂，不能用这些理想情况很好地近似。此外，在现实场景中获取高分辨率（HR）视频和相应的低分辨率（LR）视频是困难的。为了克服这些问题，我们提出了一种自监督学习方法来解决盲视频SR问题，该方法同时从LR视频中估计模糊核和HR视频。由于直接使用LR视频作为监控通常会导致琐碎的解决方案，我们开发了一种简单有效的方法，根据视频SR的图像形成，从原始LR视频中生成辅助配对数据，以便在模糊核估计和潜在HR视频恢复中更好地约束网络。此外，我们还引入了光流估计模块，利用相邻帧的信息进行HR视频恢复。实验表明，我们的方法在基准测试和真实视频中的性能优于最先进的方法。
<details>	<summary>英文摘要</summary>	Existing deep learning-based video super-resolution (SR) methods usually depend on the supervised learning approach, where the training data is usually generated by the blurring operation with known or predefined kernels (e.g., Bicubic kernel) followed by a decimation operation. However, this does not hold for real applications as the degradation process is complex and cannot be approximated by these idea cases well. Moreover, obtaining high-resolution (HR) videos and the corresponding low-resolution (LR) ones in real-world scenarios is difficult. To overcome these problems, we propose a self-supervised learning method to solve the blind video SR problem, which simultaneously estimates blur kernels and HR videos from the LR videos. As directly using LR videos as supervision usually leads to trivial solutions, we develop a simple and effective method to generate auxiliary paired data from original LR videos according to the image formation of video SR, so that the networks can be better constrained by the generated paired data for both blur kernel estimation and latent HR video restoration. In addition, we introduce an optical flow estimation module to exploit the information from adjacent frames for HR video restoration. Experiments show that our method performs favorably against state-of-the-art ones on benchmarks and real-world videos. </details>
<details>	<summary>注释</summary>	Project website: https://github.com/csbhr/Self-Blind-VSR </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 402、使用条件目标的灵活风格图像超分辨率
- [ ] Flexible Style Image Super-Resolution using Conditional Objective 
时间：2022年01月18日                         第一作者：Seung Ho Park                       [链接](https://arxiv.org/abs/2201.04898).                     
<details>	<summary>注释</summary>	Will be presented in IEEE ACCESS. Code and trained models will be available at https://github.com/seungho-snu/FxSR </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 401、使用自动X射线扫描质量增强算法提高临床诊断性能
- [ ] Improving Clinical Diagnosis Performance with Automated X-ray Scan Quality Enhancement Algorithms 
时间：2022年01月17日                         第一作者：Karthik K                        [链接](https://arxiv.org/abs/2201.06250).                     
## 摘要：在临床诊断中，从扫描设备获得的诊断图像可作为提供优质医疗服务过程中进一步调查的初步证据。然而，医学图像通常可能包含由噪声、模糊和故障设备引起的故障伪影。其原因可能是低质量或较旧的扫描设备、测试环境或技术人员缺乏培训等；然而，最终的结果是，快速可靠的诊断过程受到阻碍。在医院临床工作流程中，自动解决这些问题可以产生显著的积极影响，通常情况下，除了使用有故障/较旧的设备或资质不高的放射技师，别无选择。本文针对医学图像超分辨率的任务，对自动图像质量改进方法进行了调整和基准测试。在对标准开放数据集进行实验评估期间，观察结果表明，某些算法的性能更好，并且在医学扫描的诊断质量方面表现出显著改善，从而能够更好地为人类诊断目的实现可视化。
<details>	<summary>英文摘要</summary>	In clinical diagnosis, diagnostic images that are obtained from the scanning devices serve as preliminary evidence for further investigation in the process of delivering quality healthcare. However, often the medical image may contain fault artifacts, introduced due to noise, blur and faulty equipment. The reason for this may be the low-quality or older scanning devices, the test environment or technicians lack of training etc; however, the net result is that the process of fast and reliable diagnosis is hampered. Resolving these issues automatically can have a significant positive impact in a hospital clinical workflow, where often, there is no other way but to work with faulty/older equipment or inadequately qualified radiology technicians. In this paper, automated image quality improvement approaches for adapted and benchmarked for the task of medical image super-resolution. During experimental evaluation on standard open datasets, the observations showed that certain algorithms perform better and show significant improvement in the diagnostic quality of medical scans, thereby enabling better visualization for human diagnostic purposes. </details>
<details>	<summary>注释</summary>	Presented and Accepted in International Conference on Advances in Systems, Control and Computing (AISCC-2020) at Malaviya National Institute of Technology, Jaipur, India, February 27-28, 2020 Journal-ref: International Conference on Advances in Systems, Control and Computing (AISCC-2020) at Malaviya National Institute of Technology, Jaipur, India, February 27-28, 2020 </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 400、基于ESRGAN的单图像超分辨率双感知损耗
- [ ] Dual Perceptual Loss for Single Image Super-Resolution Using ESRGAN 
时间：2022年01月17日                         第一作者：Jie Song                        [链接](https://arxiv.org/abs/2201.06383).                     
## 摘要：感知损失的提出解决了单像素差分损失函数导致重建图像过于平滑的问题，在单图像超分辨率重建领域取得了重大进展。此外，将生成对抗网络应用于超分辨率领域，有效地提高了重建图像的视觉质量。然而，在高放大因子条件下，网络的过度异常推理会产生一些扭曲的结构，使得重建图像与地面真值图像之间存在一定的偏差。为了从根本上提高重建图像的质量，本文提出了一种有效的双感知损耗（DP-Loss）方法，用来代替原始的感知损耗来解决单幅图像的超分辨率重建问题。由于VGG特征和ResNet特征之间的互补性，所提出的DP损失考虑了同时学习两个特征的优点，从而显著提高了图像的重建效果。对基准数据集的定性和定量分析表明，我们提出的方法优于最先进的超分辨率方法。
<details>	<summary>英文摘要</summary>	The proposal of perceptual loss solves the problem that per-pixel difference loss function causes the reconstructed image to be overly-smooth, which acquires a significant progress in the field of single image super-resolution reconstruction. Furthermore, the generative adversarial networks (GAN) is applied to the super-resolution field, which effectively improves the visual quality of the reconstructed image. However, under the condtion of high upscaling factors, the excessive abnormal reasoning of the network produces some distorted structures, so that there is a certain deviation between the reconstructed image and the ground-truth image. In order to fundamentally improve the quality of reconstructed images, this paper proposes a effective method called Dual Perceptual Loss (DP Loss), which is used to replace the original perceptual loss to solve the problem of single image super-resolution reconstruction. Due to the complementary property between the VGG features and the ResNet features, the proposed DP Loss considers the advantages of learning two features simultaneously, which significantly improves the reconstruction effect of images. The qualitative and quantitative analysis on benchmark datasets demonstrates the superiority of our proposed method over state-of-the-art super-resolution methods. </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 399、多重退化盲超分辨的条件元网络
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2022年01月17日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>注释</summary>	Under review. Our evaluation code has been released! </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 398、压缩图像超分辨率网络
- [ ] CISRNet: Compressed Image Super-Resolution Network 
时间：2022年01月16日                         第一作者：Agus Gunawan                       [链接](https://arxiv.org/abs/2201.06045).                     
## 摘要：近年来，人们对单幅图像的超分辨率（SISR）进行了大量的研究。然而，据我们所知，这些研究很少主要集中在压缩图像上。复杂的压缩伪影等问题阻碍了这项研究的进展，尽管它具有很高的实用价值。为了解决这个问题，我们提出了CISRNet；一种采用两阶段粗到精学习框架的网络，主要针对压缩图像超分辨率问题进行优化。具体而言，CISNET由两个主要子网组成；粗化和细化网络，其中递归和剩余学习分别在这两个网络中使用。大量实验表明，经过仔细的设计选择，CISRNet在压缩图像超分辨率任务中的表现优于竞争对手的单图像超分辨率方法。
<details>	<summary>英文摘要</summary>	In recent years, tons of research has been conducted on Single Image Super-Resolution (SISR). However, to the best of our knowledge, few of these studies are mainly focused on compressed images. A problem such as complicated compression artifacts hinders the advance of this study in spite of its high practical values. To tackle this problem, we proposed CISRNet; a network that employs a two-stage coarse-to-fine learning framework that is mainly optimized for Compressed Image Super-Resolution Problem. Specifically, CISRNet consists of two main subnetworks; the coarse and refinement network, where recursive and residual learning is employed within these two networks respectively. Extensive experiments show that with a careful design choice, CISRNet performs favorably against competing Single-Image Super-Resolution methods in the Compressed Image Super-Resolution tasks. </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 397、SDT-DCSCN用于文本图像的同时超分辨率和去模糊
- [ ] SDT-DCSCN for Simultaneous Super-Resolution and Deblurring of Text Images 
时间：2022年01月15日                         第一作者：Hala Neji                       [链接](https://arxiv.org/abs/2201.05865).                     
## 摘要：深度卷积神经网络（Deep-CNN）在单图像超分辨率方面取得了令人鼓舞的性能。特别是，深度CNN跳过连接和网络中网络（DCSCN）架构已成功应用于自然图像超分辨率。在这项工作中，我们提出了一种称为SDT-DCSCN的方法，该方法基于DCSCN联合执行低分辨率模糊文本图像的超分辨率和去模糊。我们的方法使用输入中的二次采样模糊图像和原始锐利图像作为地面真相。所使用的架构由输入CNN层中的更多过滤器组成，以便更好地分析文本细节。对不同数据集的定量和定性评估证明了我们的模型在重建高分辨率和清晰文本图像方面的高性能。此外，在计算时间方面，我们提出的方法与最先进的方法相比具有竞争力。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks (Deep CNN) have achieved hopeful performance for single image super-resolution. In particular, the Deep CNN skip Connection and Network in Network (DCSCN) architecture has been successfully applied to natural images super-resolution. In this work we propose an approach called SDT-DCSCN that jointly performs super-resolution and deblurring of low-resolution blurry text images based on DCSCN. Our approach uses subsampled blurry images in the input and original sharp images as ground truth. The used architecture is consists of a higher number of filters in the input CNN layer to a better analysis of the text details. The quantitative and qualitative evaluation on different datasets prove the high performance of our model to reconstruct high-resolution and sharp text images. In addition, in terms of computational time, our proposed method gives competitive performance compared to state of the art methods. </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 396、使用条件目标的灵活风格图像超分辨率
- [ ] Flexible Style Image Super-Resolution using Conditional Objective 
时间：2022年01月13日                         第一作者：Seung Ho Park                       [链接](https://arxiv.org/abs/2201.04898).                     
## 摘要：最近的研究利用卷积神经网络（CNN）显著提高了单图像超分辨率（SR）的性能。虽然对于给定的输入可以有许多高分辨率（HR）解决方案，但大多数现有的基于CNN的方法在推理过程中不会探索替代解决方案。获得替代SR结果的典型方法是训练具有不同损失权重的多个SR模型，并利用这些模型的组合。与使用多个模型相比，我们提出了一种更有效的方法，通过利用多任务学习，针对各种损失组合训练单个可调SR模型。具体来说，我们在训练过程中优化了一个具有条件目标的SR模型，其中目标是不同特征水平下多个感知损失的加权和。权重根据给定的条件而变化，权重集被定义为样式控制器。此外，我们还提出了一种适用于该训练方案的结构，即带有空间特征转换层的剩余密集块中的剩余部分。在推理阶段，我们训练的模型可以根据风格控制图生成局部不同的输出。大量实验表明，所提出的SR模型可以产生各种理想的重建，没有伪影，并且产生与最先进的SR方法相当的定量性能。
<details>	<summary>英文摘要</summary>	Recent studies have significantly enhanced the performance of single-image super-resolution (SR) using convolutional neural networks (CNNs). While there can be many high-resolution (HR) solutions for a given input, most existing CNN-based methods do not explore alternative solutions during the inference. A typical approach to obtaining alternative SR results is to train multiple SR models with different loss weightings and exploit the combination of these models. Instead of using multiple models, we present a more efficient method to train a single adjustable SR model on various combinations of losses by taking advantage of multi-task learning. Specifically, we optimize an SR model with a conditional objective during training, where the objective is a weighted sum of multiple perceptual losses at different feature levels. The weights vary according to given conditions, and the set of weights is defined as a style controller. Also, we present an architecture appropriate for this training scheme, which is the Residual-in-Residual Dense Block equipped with spatial feature transformation layers. At the inference phase, our trained model can generate locally different outputs conditioned on the style control map. Extensive experiments show that the proposed SR model produces various desirable reconstructions without artifacts and yields comparable quantitative performance to state-of-the-art SR methods. </details>
<details>	<summary>邮件日期</summary>	2022年01月14日</details>

# 395、基于参考的超分辨率粗到精嵌入补丁匹配和多尺度动态聚合
- [ ] Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution 
时间：2022年01月12日                         第一作者：Bin Xia                       [链接](https://arxiv.org/abs/2201.04358).                     
## 摘要：基于参考的超分辨率（RefSR）在使用外部参考（Ref）图像生成真实纹理方面取得了重大进展。然而，现有的RefSR方法获得了高质量的对应匹配，消耗了关于输入大小的二次计算资源，限制了其应用。此外，这些方法通常会出现低分辨率（LR）图像和参考图像之间的比例失调。在本文中，我们提出了一种基于参考的超分辨率加速多尺度聚合网络（AMSA），包括从粗到精的嵌入式补丁匹配（CFE PatchMatch）和多尺度动态聚合（MSDA）模块。为了提高匹配效率，我们设计了一种具有随机样本传播的新型嵌入式PatchMacth方案，该方案包括端到端的训练，计算量与输入大小呈渐近线性关系。为了进一步降低计算量和加快收敛速度，我们在构成CFE PatchMatch的嵌入式PatchMacth上应用了从粗到精的策略。为了在多个尺度上充分利用参考信息并增强对尺度失调的鲁棒性，我们开发了由动态聚合和多尺度聚合组成的MSDA模块。动态聚合通过动态聚合特征来纠正小规模的偏差，而多尺度聚合通过融合多尺度信息来实现对大规模偏差的鲁棒性。实验结果表明，所提出的AMSA在定量和定性评估方面都优于最新的方法。
<details>	<summary>英文摘要</summary>	Reference-based super-resolution (RefSR) has made significant progress in producing realistic textures using an external reference (Ref) image. However, existing RefSR methods obtain high-quality correspondence matchings consuming quadratic computation resources with respect to the input size, limiting its application. Moreover, these approaches usually suffer from scale misalignments between the low-resolution (LR) image and Ref image. In this paper, we propose an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch) and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching efficiency, we design a novel Embedded PatchMacth scheme with random samples propagation, which involves end-to-end training with asymptotic linear computational cost to the input size. To further reduce computational cost and speed up convergence, we apply the coarse-to-fine strategy on Embedded PatchMacth constituting CFE-PatchMatch. To fully leverage reference information across multiple scales and enhance robustness to scale misalignment, we develop the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation. The Dynamic Aggregation corrects minor scale misalignment by dynamically aggregating features, and the Multi-Scale Aggregation brings robustness to large scale misalignment by fusing multi-scale information. Experimental results show that the proposed AMSA achieves superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations. </details>
<details>	<summary>邮件日期</summary>	2022年01月13日</details>

# 394、SCSNet：同时学习图像着色和超分辨率的有效范例
- [ ] SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-Resolution 
时间：2022年01月12日                         第一作者：Jiangning Zhang                       [链接](https://arxiv.org/abs/2201.04364).                     
## 摘要：在恢复低分辨率灰度图像的实际应用中，我们通常需要对目标设备运行图像着色、超分辨率和dows采样操作三个单独的过程。然而，对于独立的进程来说，这个管道是冗余的，效率低下，而且一些内部功能本来可以共享。因此，我们提出了一种同时进行{S}彩色化和{S}超分辨率（SCS）的有效范例，并提出了一种端到端SCSNet来实现这一目标。该方法由两部分组成：用于学习颜色信息的彩色化分支，该分支使用所提出的即插即用模块来聚合源图像和参考图像之间的特征映射；以及超分辨率分支，用于集成颜色和纹理信息来预测目标图像，该分支使用设计的\emph{Continuous Pixel Mapping}（CPM）模块来预测连续放大的高分辨率图像。此外，我们的SCSNet支持自动模式和参考模式，这在实际应用中更加灵活。大量实验证明了我们生成真实图像的方法优于最先进的方法，例如，与自动和参考模式的当前最佳分数相比，FID平均降低1.8$\downarrow$和5.1$\downarrow$，同时拥有更少的参数（超过$\times$2$\downarrow$）和更快的运行速度（超过$\times$3$\uparrow$）。
<details>	<summary>英文摘要</summary>	In the practical application of restoring low-resolution gray-scale images, we generally need to run three separate processes of image colorization, super-resolution, and dows-sampling operation for the target device. However, this pipeline is redundant and inefficient for the independent processes, and some inner features could have been shared. Therefore, we present an efficient paradigm to perform {S}imultaneously Image {C}olorization and {S}uper-resolution (SCS) and propose an end-to-end SCSNet to achieve this goal. The proposed method consists of two parts: colorization branch for learning color information that employs the proposed plug-and-play \emph{Pyramid Valve Cross Attention} (PVCAttn) module to aggregate feature maps between source and reference images; and super-resolution branch for integrating color and texture information to predict target images, which uses the designed \emph{Continuous Pixel Mapping} (CPM) module to predict high-resolution images at continuous magnification. Furthermore, our SCSNet supports both automatic and referential modes that is more flexible for practical application. Abundant experiments demonstrate the superiority of our method for generating authentic images over state-of-the-art methods, e.g., averagely decreasing FID by 1.8$\downarrow$ and 5.1 $\downarrow$ compared with current best scores for automatic and referential modes, respectively, while owning fewer parameters (more than $\times$2$\downarrow$) and faster running speed (more than $\times$3$\uparrow$). </details>
<details>	<summary>邮件日期</summary>	2022年01月13日</details>

# 393、MoViDNN：一个使用深度神经网络评估视频质量增强的移动平台
- [ ] MoViDNN: A Mobile Platform for Evaluating Video Quality Enhancement with Deep Neural Networks 
时间：2022年01月12日                         第一作者：Ekrem \c{C}etinkaya                        [链接](https://arxiv.org/abs/2201.04402).                     
## 摘要：近年来，基于深度神经网络（DNN）的视频质量改善方法得到了广泛的研究。由于计算成本高，这些方法主要针对桌面设备。然而，随着近年来移动设备性能的提高，在移动设备中执行基于DNN的方法成为可能。尽管具有所需的计算能力，但利用DNN改善移动设备的视频质量仍然是一个活跃的研究领域。在本文中，我们提出了一个开源移动平台，即MoViDNN，来评估基于DNN的视频质量增强方法，如超分辨率、去噪和去块。我们提出的平台可用于客观和主观地评估基于DNN的方法。为了进行客观评估，我们报告了执行时间、PSNR和SSIM等常用指标。对于主观评价，报告平均得分意见（MOS）。建议的平台可在以下网站公开获取：https://github.com/cd-athena/MoViDNN
<details>	<summary>英文摘要</summary>	Deep neural network (DNN) based approaches have been intensively studied to improve video quality thanks to their fast advancement in recent years. These approaches are designed mainly for desktop devices due to their high computational cost. However, with the increasing performance of mobile devices in recent years, it became possible to execute DNN based approaches in mobile devices. Despite having the required computational power, utilizing DNNs to improve the video quality for mobile devices is still an active research area. In this paper, we propose an open-source mobile platform, namely MoViDNN, to evaluate DNN based video quality enhancement methods, such as super-resolution, denoising, and deblocking. Our proposed platform can be used to evaluate the DNN based approaches both objectively and subjectively. For objective evaluation, we report common metrics such as execution time, PSNR, and SSIM. For subjective evaluation, Mean Score Opinion (MOS) is reported. The proposed platform is available publicly at https://github.com/cd-athena/MoViDNN </details>
<details>	<summary>注释</summary>	8 pages, 3 figures ACM-class: H.5.1; I.4.9 </details>
<details>	<summary>邮件日期</summary>	2022年01月13日</details>

# 392、图像超分辨率的高效非局部对比注意
- [ ] Efficient Non-Local Contrastive Attention for Image Super-Resolution 
时间：2022年01月11日                         第一作者：Bin Xia                       [链接](https://arxiv.org/abs/2201.03794).                     
## 摘要：非局部注意（NLA）通过利用自然图像中固有的特征相关性，显著提高了单图像超分辨率（SISR）。然而，NLA给噪声信息赋予了很大的权重，并且相对于输入大小消耗了二次计算资源，限制了其性能和应用。在本文中，我们提出了一种新的有效的非局部对比注意（ENLCA）来进行远程视觉建模，并利用更多相关的非局部特征。具体来说，ENLCA由两部分组成：有效非局部注意（ENLA）和稀疏聚集。ENLA采用核方法逼近指数函数，得到线性计算复杂度。对于稀疏聚合，我们将输入乘以放大因子，以关注信息特征，但近似值的方差呈指数增长。因此，对比学习被用来进一步分离相关和不相关的特征。为了证明ENLCA的有效性，我们通过在一个简单的主干中添加几个模块，构建了一个称为高效非本地对比网络（ENLCN）的体系结构。大量的实验结果表明，ENLCN在定量和定性评估方面都优于最先进的方法。
<details>	<summary>英文摘要</summary>	Non-Local Attention (NLA) brings significant improvement for Single Image Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural images. However, NLA gives noisy information large weights and consumes quadratic computation resources with respect to the input size, limiting its performance and application. In this paper, we propose a novel Efficient Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling and leverage more relevant non-local features. Specifically, ENLCA consists of two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA adopts the kernel method to approximate exponential function and obtains linear computation complexity. For Sparse Aggregation, we multiply inputs by an amplification factor to focus on informative features, yet the variance of approximation increases exponentially. Therefore, contrastive learning is applied to further separate relevant and irrelevant features. To demonstrate the effectiveness of ENLCA, we build an architecture called Efficient Non-Local Contrastive Network (ENLCN) by adding a few of our modules in a simple backbone. Extensive experimental results show that ENLCN reaches superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations. </details>
<details>	<summary>邮件日期</summary>	2022年01月12日</details>

# 391、基于傅里叶环相关的图像质量测量和去噪
- [ ] Image quality measurements and denoising using Fourier Ring Correlations 
时间：2022年01月11日                         第一作者：J. Kaczmar-Michalska                       [链接](https://arxiv.org/abs/2201.03992).                     
## 摘要：图像质量是一个模糊的概念，对不同的人有不同的含义。为了量化图像质量，通常会计算损坏图像和地面真实图像之间的相对差异。但我们应该用什么标准来衡量这种差异呢？理想情况下，该指标在自然和科学图像中都应该表现良好。结构相似性指数（SSIM）是衡量人类如何感知图像相似性的一个很好的指标，但对显微镜下具有科学意义的差异不敏感。在电子显微镜和超分辨率显微镜中，经常使用傅里叶环相关（FRC），但在这些领域之外，人们知之甚少。这里我们展示了FRC同样适用于自然图像，例如Google Open images数据集。然后，我们定义了一个基于FRC的损失函数，证明了它是解析可微的，并用它来训练用于图像去噪的U网络。这种基于FRC的损耗函数允许网络比使用基于L1或L2的损耗时更快地训练，并获得类似或更好的结果。我们还研究了基于FRC分析的神经网络去噪的特性和局限性。
<details>	<summary>英文摘要</summary>	Image quality is a nebulous concept with different meanings to different people. To quantify image quality a relative difference is typically calculated between a corrupted image and a ground truth image. But what metric should we use for measuring this difference? Ideally, the metric should perform well for both natural and scientific images. The structural similarity index (SSIM) is a good measure for how humans perceive image similarities, but is not sensitive to differences that are scientifically meaningful in microscopy. In electron and super-resolution microscopy, the Fourier Ring Correlation (FRC) is often used, but is little known outside of these fields. Here we show that the FRC can equally well be applied to natural images, e.g. the Google Open Images dataset. We then define a loss function based on the FRC, show that it is analytically differentiable, and use it to train a U-net for denoising of images. This FRC-based loss function allows the network to train faster and achieve similar or better results than when using L1- or L2- based losses. We also investigate the properties and limitations of neural network denoising with the FRC analysis. </details>
<details>	<summary>邮件日期</summary>	2022年01月12日</details>

# 390、用于高光谱图像超分辨率的反馈改进局部-全局网络
- [ ] Feedback Refined Local-Global Network for Super-Resolution of Hyperspectral Imagery 
时间：2022年01月10日                         第一作者：Zhenjie Tang                       [链接](https://arxiv.org/abs/2103.04354).                     
<details>	<summary>邮件日期</summary>	2022年01月11日</details>

# 389、交叉SRN：具有交叉卷积的保结构超分辨率网络
- [ ] Cross-SRN: Structure-Preserving Super-Resolution Network with Cross Convolution 
时间：2022年01月07日                         第一作者：Yuqing Liu                       [链接](https://arxiv.org/abs/2201.01458).                     
<details>	<summary>邮件日期</summary>	2022年01月10日</details>

# 388、MoCoPnet：探索红外小目标超分辨率的局部运动和对比度先验
- [ ] MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution 
时间：2022年01月06日                         第一作者：Xinyi Ying                       [链接](https://arxiv.org/abs/2201.01014).                     
<details>	<summary>邮件日期</summary>	2022年01月07日</details>

# 387、交叉SRN：具有交叉卷积的保结构超分辨率网络
- [ ] Cross-SRN: Structure-Preserving Super-Resolution Network with Cross Convolution 
时间：2022年01月05日                         第一作者：Yuqing Liu                       [链接](https://arxiv.org/abs/2201.01458).                     
## 摘要：将低分辨率（LR）图像恢复为具有正确清晰细节的超分辨率（SR）图像是一项挑战。现有的深度学习工作几乎忽略了图像固有的结构信息，这些信息对SR结果的视觉感知起着重要作用。在本文中，我们设计了一个分层特征挖掘网络，以多尺度特征融合的方式探测和保存结构信息。首先，我们在传统边缘检测器的基础上提出了交叉卷积来定位和表示边缘特征。然后，设计了具有特征归一化和信道注意的交叉卷积块（CCBS），以考虑特征的内在相关性。最后，我们利用多尺度特征融合组（MFFG）嵌入交叉卷积块，并调用一个名为cross-SRN的轻量级结构保持网络，在不同尺度上分层发展结构特征之间的关系。实验结果表明，交叉SRN在结构细节准确清晰的情况下，与最先进的方法相比，具有竞争性或优越的恢复性能。此外，我们还设置了一个标准来选择具有丰富结构纹理的图像。所提出的交叉SRN在选定的基准上优于最先进的方法，这表明我们的网络在保留边缘方面具有显著优势。
<details>	<summary>英文摘要</summary>	It is challenging to restore low-resolution (LR) images to super-resolution (SR) images with correct and clear details. Existing deep learning works almost neglect the inherent structural information of images, which acts as an important role for visual perception of SR results. In this paper, we design a hierarchical feature exploitation network to probe and preserve structural information in a multi-scale feature fusion manner. First, we propose a cross convolution upon traditional edge detectors to localize and represent edge features. Then, cross convolution blocks (CCBs) are designed with feature normalization and channel attention to consider the inherent correlations of features. Finally, we leverage multi-scale feature fusion group (MFFG) to embed the cross convolution blocks and develop the relations of structural features in different scales hierarchically, invoking a lightweight structure-preserving network named as Cross-SRN. Experimental results demonstrate the Cross-SRN achieves competitive or superior restoration performances against the state-of-the-art methods with accurate and clear structural details. Moreover, we set a criterion to select images with rich structural textures. The proposed Cross-SRN outperforms the state-of-the-art methods on the selected benchmark, which demonstrates that our network has a significant advantage in preserving edges. </details>
<details>	<summary>邮件日期</summary>	2022年01月06日</details>

# 386、用于图像超分辨率的迭代网络
- [ ] Iterative Network for Image Super-Resolution 
时间：2022年01月05日                         第一作者：Yuqing Liu                       [链接](https://arxiv.org/abs/2005.09964).                     
<details>	<summary>注释</summary>	This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2022年01月06日</details>

# 385、MoCoPnet：探索红外小目标超分辨率的局部运动和对比度先验
- [ ] MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution 
时间：2022年01月05日                         第一作者：Xinyi Ying                       [链接](https://arxiv.org/abs/2201.01014).                     
<details>	<summary>邮件日期</summary>	2022年01月06日</details>

# 384、3DVSR：基于3D EPI体积的角度和空间光场图像超分辨率方法
- [ ] 3DVSR: 3D EPI Volume-based Approach for Angular and Spatial Light field Image Super-resolution 
时间：2022年01月04日                         第一作者：Trung-Hieu Tran                       [链接](https://arxiv.org/abs/2201.01294).                     
## 摘要：光场（LF）成像可以捕获场景的空间和角度信息，无疑对许多应用都是有益的。虽然已经提出了各种LF捕获技术，但实现角度和空间高分辨率LF仍然是一项技术挑战。本文提出了一种基于学习的三维极线图像重建方法。通过两阶段超分辨率框架，所提出的方法有效地解决了各种LF超分辨率（SR）问题，即空间SR、角度SR和角度空间SR。第一阶段提供灵活的选项，将样本EPI体积提高到所需的分辨率，第二阶段包括一个新的基于EPI体积的细化网络（EVRN），大大提高了高分辨率EPI卷的质量。对来自7个已发布数据集的90个具有挑战性的合成和真实光场场景进行了广泛评估，结果表明，对于空间和角度超分辨率问题，所提出的方法在很大程度上都优于最先进的方法，即，在空间SR$\times 2$、空间SR$\times 4$中，平均峰值信噪比提高了2.0 dB、1.4 dB和3.14 dB以上，和角SR。与之前的作品相比，重建的4D光场在所有透视图像中表现出平衡的性能分布，并呈现出优越的视觉质量。
<details>	<summary>英文摘要</summary>	Light field (LF) imaging, which captures both spatial and angular information of a scene, is undoubtedly beneficial to numerous applications. Although various techniques have been proposed for LF acquisition, achieving both angularly and spatially high-resolution LF remains a technology challenge. In this paper, a learning-based approach applied to 3D epipolar image (EPI) is proposed to reconstruct high-resolution LF. Through a 2-stage super-resolution framework, the proposed approach effectively addresses various LF super-resolution (SR) problems, i.e., spatial SR, angular SR, and angular-spatial SR. While the first stage provides flexible options to up-sample EPI volume to the desired resolution, the second stage, which consists of a novel EPI volume-based refinement network (EVRN), substantially enhances the quality of the high-resolution EPI volume. An extensive evaluation on 90 challenging synthetic and real-world light field scenes from 7 published datasets shows that the proposed approach outperforms state-of-the-art methods to a large extend for both spatial and angular super-resolution problem, i.e., an average peak signal to noise ratio improvement of more than 2.0 dB, 1.4 dB, and 3.14 dB in spatial SR $\times 2$, spatial SR $\times 4$, and angular SR respectively. The reconstructed 4D light field demonstrates a balanced performance distribution across all perspective images and presents superior visual quality compared to the previous works. </details>
<details>	<summary>邮件日期</summary>	2022年01月05日</details>

# 383、MoCoPnet：探索红外小目标超分辨率的局部运动和对比度先验
- [ ] MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution 
时间：2022年01月04日                         第一作者：Xinyi Ying                       [链接](https://arxiv.org/abs/2201.01014).                     
## 摘要：红外小目标超分辨率（SR）的目标是从低分辨率图像中恢复具有高对比度目标的可靠、详细的高分辨率图像。由于红外小目标缺乏颜色和精细结构信息，利用序列图像中的补充信息来增强目标具有重要意义。本文提出了第一种红外小目标局部运动和对比度先验驱动深度网络（MoCoPnet）方法，将红外小目标的领域知识集成到深度网络中，可以缓解红外小目标固有的特征稀缺性。具体来说，基于时空维度的局部运动先验，我们提出了一个局部时空注意模块来执行隐式帧对齐，并结合局部时空信息来增强局部特征（尤其是对于小目标）。基于空间维度上的局部对比度优先，我们提出了一个中心差分残差组，将中心差分卷积纳入特征提取主干，实现了面向中心的梯度感知特征提取，进一步提高了目标对比度。大量实验表明，该方法可以恢复精确的空间相关性，提高目标对比度。对比结果表明，MoCoPnet在SR性能和目标增强方面都优于最先进的视频SR和单图像SR方法。基于SR结果，我们进一步研究了SR对红外小目标检测的影响，实验结果表明，MoCoPnet提高了检测性能。该代码可在https://github.com/XinyiYing/MoCoPnet.
<details>	<summary>英文摘要</summary>	Infrared small target super-resolution (SR) aims to recover reliable and detailed high-resolution image with highcontrast targets from its low-resolution counterparts. Since the infrared small target lacks color and fine structure information, it is significant to exploit the supplementary information among sequence images to enhance the target. In this paper, we propose the first infrared small target SR method named local motion and contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge of infrared small target into deep network, which can mitigate the intrinsic feature scarcity of infrared small targets. Specifically, motivated by the local motion prior in the spatio-temporal dimension, we propose a local spatiotemporal attention module to perform implicit frame alignment and incorporate the local spatio-temporal information to enhance the local features (especially for small targets). Motivated by the local contrast prior in the spatial dimension, we propose a central difference residual group to incorporate the central difference convolution into the feature extraction backbone, which can achieve center-oriented gradient-aware feature extraction to further improve the target contrast. Extensive experiments have demonstrated that our method can recover accurate spatial dependency and improve the target contrast. Comparative results show that MoCoPnet can outperform the state-of-the-art video SR and single image SR methods in terms of both SR performance and target enhancement. Based on the SR results, we further investigate the influence of SR on infrared small target detection and the experimental results demonstrate that MoCoPnet promotes the detection performance. The code is available at https://github.com/XinyiYing/MoCoPnet. </details>
<details>	<summary>邮件日期</summary>	2022年01月05日</details>

# 382、图像超分辨率双参考深度学习的数据采集与准备
- [ ] Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution 
时间：2022年01月03日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
<details>	<summary>邮件日期</summary>	2022年01月04日</details>

# 381、用于光场图像超分辨率的细节保持变压器
- [ ] Detail-Preserving Transformer for Light Field Image Super-Resolution 
时间：2022年01月02日                         第一作者：Shunzhou Wang                       [链接](https://arxiv.org/abs/2201.00346).                     
## 摘要：近年来，人们开发了许多算法来解决光场超分辨率（LFSR）问题，即超分辨率低分辨率光场以获得高分辨率视图。尽管取得了令人鼓舞的结果，但这些方法都是基于卷积的，并且在子孔径图像的全局关系建模中自然很弱，这必须用来描述光场的固有结构。在本文中，我们提出了一种基于变压器的新公式，将LFSR视为序列到序列的重建任务。特别是，我们的模型将每个垂直或水平角度视图的子孔径图像视为一个序列，并通过空间角度局部增强的自我注意层在每个序列内建立长期几何依赖关系，该层也保持每个子孔径图像的局部性。此外，为了更好地恢复图像细节，我们提出了一种细节保持变换器（称为DPT），利用光场的梯度映射来指导序列学习。DPT由两个分支组成，每个分支都与用于从原始图像序列或梯度图像序列学习的变压器相关联。最后将这两个分支融合，得到用于重建的综合特征表示。评估是在许多光场数据集上进行的，包括真实场景和合成数据。与其他最先进的方案相比，该方法取得了优越的性能。我们的代码可在以下网站公开获取：https://github.com/BITszwang/DPT.
<details>	<summary>英文摘要</summary>	Recently, numerous algorithms have been developed to tackle the problem of light field super-resolution (LFSR), i.e., super-resolving low-resolution light fields to gain high-resolution views. Despite delivering encouraging results, these approaches are all convolution-based, and are naturally weak in global relation modeling of sub-aperture images necessarily to characterize the inherent structure of light fields. In this paper, we put forth a novel formulation built upon Transformers, by treating LFSR as a sequence-to-sequence reconstruction task. In particular, our model regards sub-aperture images of each vertical or horizontal angular view as a sequence, and establishes long-range geometric dependencies within each sequence via a spatial-angular locally-enhanced self-attention layer, which maintains the locality of each sub-aperture image as well. Additionally, to better recover image details, we propose a detail-preserving Transformer (termed as DPT), by leveraging gradient maps of light field to guide the sequence learning. DPT consists of two branches, with each associated with a Transformer for learning from an original or gradient image sequence. The two branches are finally fused to obtain comprehensive feature representations for reconstruction. Evaluations are conducted on a number of light field datasets, including real-world scenes and synthetic data. The proposed method achieves superior performance comparing with other state-of-the-art schemes. Our code is publicly available at: https://github.com/BITszwang/DPT. </details>
<details>	<summary>注释</summary>	AAAI2022, Code: https://github.com/BITszwang/DPT </details>
<details>	<summary>邮件日期</summary>	2022年01月04日</details>

# 380、DiffuseVAE：从低维潜伏物中高效、可控和高保真地生成
- [ ] DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents 
时间：2022年01月02日                         第一作者：Kushagra P                       [链接](https://arxiv.org/abs/2201.00308).                     
## 摘要：扩散概率模型已被证明能在多个竞争性图像合成基准上生成最先进的结果，但缺乏低维、可解释的潜在空间，生成速度较慢。另一方面，变分自动编码器（VAE）通常可以访问低维潜在空间，但样本质量较差。尽管最近取得了一些进展，VAE通常需要潜在代码的高维层次结构来生成高质量的样本。我们提出了一种新的生成框架DiffuseVAE，它将VAE集成到扩散模型框架中，并利用它为扩散模型设计了一种新的条件参数化。结果表明，该模型在采样效率方面优于无条件扩散模型，同时也为扩散模型配备了低维VAE推断的潜在代码。此外，我们还表明，该模型可以生成高分辨率的样本，并且在标准基准上显示出与最先进的模型相当的合成质量。最后，我们证明了所提出的方法可以用于可控图像合成，并且对于图像超分辨率和去噪等下游任务也具有开箱即用的能力。为了重现性，我们的源代码可在\url公开获取{https://github.com/kpandey008/DiffuseVAE}.
<details>	<summary>英文摘要</summary>	Diffusion Probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. Despite recent advances, VAEs usually require high-dimensional hierarchies of the latent codes to generate high-quality samples. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design a novel conditional parameterization for diffusion models. We show that the resulting model can improve upon the unconditional diffusion model in terms of sampling efficiency while also equipping diffusion models with the low-dimensional VAE inferred latent code. Furthermore, we show that the proposed model can generate high-resolution samples and exhibits synthesis quality comparable to state-of-the-art models on standard benchmarks. Lastly, we show that the proposed method can be used for controllable image synthesis and also exhibits out-of-the-box capabilities for downstream tasks like image super-resolution and denoising. For reproducibility, our source code is publicly available at \url{https://github.com/kpandey008/DiffuseVAE}. </details>
<details>	<summary>邮件日期</summary>	2022年01月04日</details>

# 379、使用双路径连接和多尺度学习的高效单图像超分辨率
- [ ] Efficient Single Image Super-Resolution Using Dual Path Connections with Multiple Scale Learning 
时间：2021年12月31日                         第一作者：Bin-Cheng Yang                        [链接](https://arxiv.org/abs/2112.15386).                     
## 摘要：近年来，深卷积神经网络已被证明对SISR是有效的。一方面，剩余连接和密集连接被广泛用于缓解前向信息和后向梯度流，以提高性能。然而，目前的方法在大多数网络层中以次优的方式分别使用剩余连接和密集连接。另一方面，虽然已经设计了各种网络和方法来提高计算效率、保存参数，或者相互利用多个尺度因子的训练数据来提高性能，但它要么在HR空间中进行超分辨率处理，从而产生较高的计算成本，要么不能在不同尺度因子的模型之间共享参数，从而节省参数和推理时间。为了应对这些挑战，我们提出了一种高效的单图像超分辨率网络，该网络使用双路径连接和多尺度学习，名为EMSRDPN。通过在EMSRDPN中引入受双路径网络启发的双路径连接，它在大多数网络层中以集成方式使用剩余连接和密集连接。双路径连接既可以重用剩余连接的公共特征，也可以探索密集连接的新特征，从而学习SISR的良好表示。为了利用多个尺度因子的特征相关性，EMSRDPN在不同尺度因子之间共享LR空间中的所有网络单元来学习共享特征，并且每个尺度因子只使用一个单独的重构单元，它可以利用多个尺度因子的训练数据来帮助彼此提高性能，同时可以节省参数，支持多尺度因子的共享推理，提高效率。实验表明，与SOTA方法相比，EMSRDPN具有更好的性能和可比甚至更好的参数和推理效率。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks have been demonstrated to be effective for SISR in recent years. On the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. However, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. On the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, it either do super-resolution in HR space to have a high computation cost or can not share parameters between models of different scale factors to save parameters and inference time. To tackle these challenges, we propose an efficient single image super-resolution network using dual path connections with multiple scale learning named as EMSRDPN. By introducing dual path connections inspired by Dual Path Networks into EMSRDPN, it uses residual connections and dense connections in an integrated way in most network layers. Dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for SISR. To utilize the feature correlation of multiple scale factors, EMSRDPN shares all network units in LR space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile which can save parameters and support shared inference for multiple scale factors to improve efficiency. Experiments show EMSRDPN achieves better performance and comparable or even better parameter and inference efficiency over SOTA methods. </details>
<details>	<summary>注释</summary>	20 pages, 9 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2022年01月03日</details>

# 378、过渡学习：探索盲超分辨率退化的过渡状态
- [ ] Transitional Learning: Exploring the Transition States of Degradation for Blind Super-resolution 
时间：2021年12月31日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2103.15290).                     
<details>	<summary>注释</summary>	16 pages, submitted to IEEE Transactions, code is available at github.com/YuanfeiHuang/TLSR </details>
<details>	<summary>邮件日期</summary>	2022年01月03日</details>

# 377、一种用于医学图像可变形配准的分辨率增强方法
- [ ] A Resolution Enhancement Plug-in for Deformable Registration of Medical Images 
时间：2021年12月30日                         第一作者：Kaicong Sun                       [链接](https://arxiv.org/abs/2112.15180).                     
## 摘要：图像配准是医学成像的一项基本任务。在配准过程中需要对强度值进行重采样，使用更精细和更清晰的结构获得更好的空间分辨率可以提高重采样性能，从而提高配准精度。超分辨率（SR）是一种以空间分辨率增强为目标的算法技术，它可以实现超出硬件限制的图像分辨率。在这项工作中，我们认为SR作为预处理技术，并提出了一种基于美国有线电视新闻网的分辨率增强模块（REM），可以很容易地插入到注册网络级联。研究了REM的不同残差方案和网络结构，以获得有效的REM结构设计。事实上，REM不仅限于图像配准，还可以直接集成到其他视觉任务中以提高分辨率。在不同的放大因子下，对所提出的REM在医学图像上的可变形配准进行了定量和定性的评估。在LPBA40脑MRI数据集上的实验表明，REM不仅提高了配准精度，尤其是当输入图像的空间分辨率降低时，而且生成了分辨率增强的图像，可用于后续诊断。
<details>	<summary>英文摘要</summary>	Image registration is a fundamental task for medical imaging. Resampling of the intensity values is required during registration and better spatial resolution with finer and sharper structures can improve the resampling performance and hence the registration accuracy. Super-resolution (SR) is an algorithmic technique targeting at spatial resolution enhancement which can achieve an image resolution beyond the hardware limitation. In this work, we consider SR as a preprocessing technique and present a CNN-based resolution enhancement module (REM) which can be easily plugged into the registration network in a cascaded manner. Different residual schemes and network configurations of REM are investigated to obtain an effective architecture design of REM. In fact, REM is not confined to image registration, it can also be straightforwardly integrated into other vision tasks for enhanced resolution. The proposed REM is thoroughly evaluated for deformable registration on medical images quantitatively and qualitatively at different upscaling factors. Experiments on LPBA40 brain MRI dataset demonstrate that REM not only improves the registration accuracy, especially when the input images suffer from degraded spatial resolution, but also generates resolution enhanced images which can be exploited for successive diagnosis. </details>
<details>	<summary>邮件日期</summary>	2022年01月03日</details>

# 376、HPRN：用于光谱超分辨率的整体先验嵌入关系网络
- [ ] HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution 
时间：2021年12月29日                         第一作者：Chaoxiong Wu                       [链接](https://arxiv.org/abs/2112.14608).                     
## 摘要：光谱超分辨率（SSR）是指从RGB对应物中恢复高光谱图像（HSI）。由于SSR问题的一对多性质，单个RGB图像可以重新投影到多个HSI。解决这一问题的关键是插入多源先验信息，如自然RGB空间背景先验、深度特征先验或固有HSI统计先验等，以提高重建光谱的可信度和保真度。然而，大多数现有的方法只考虑一般的和有限的先验在设计定制卷积神经网络（CNS），这导致无法有效地减轻不适定性的程度。为了解决这些问题，我们提出了一种新的用于SSR的整体优先嵌入关系网络（HPRN）。基本上，核心框架由多个多残差关系块（MRB）巧妙组合而成，充分促进了RGB信号低频内容的传输和利用。创新性地，引入RGB输入的语义先验来识别类别属性，提出了语义驱动的空间关系模块（SSRM），利用语义嵌入的关系矩阵在聚类的相似特征之间进行特征聚合。此外，我们还开发了一个基于转换器的通道关系模块（TCRM），该模块打破了在之前的深度特征中使用标量作为通道关系描述符的习惯，并将其替换为特定向量，以及转换器样式的特征交互，支持更具区分性的表示。为了保持高光谱波段之间的数学相关性和光谱一致性，在损失函数中引入二阶先验约束（SOPC）来指导HSI重建过程。
<details>	<summary>英文摘要</summary>	Spectral super-resolution (SSR) refers to the hyperspectral image (HSI) recovery from an RGB counterpart. Due to the one-to-many nature of the SSR problem, a single RGB image can be reprojected to many HSIs. The key to tackle this illposed problem is to plug into multi-source prior information such as the natural RGB spatial context-prior, deep feature-prior or inherent HSI statistical-prior, etc., so as to improve the confidence and fidelity of reconstructed spectra. However, most current approaches only consider the general and limited priors in their designing the customized convolutional neural networks (CNNs), which leads to the inability to effectively alleviate the degree of ill-posedness. To address the problematic issues, we propose a novel holistic prior-embedded relation network (HPRN) for SSR. Basically, the core framework is delicately assembled by several multi-residual relation blocks (MRBs) that fully facilitate the transmission and utilization of the low-frequency content prior of RGB signals. Innovatively, the semantic prior of RGB input is introduced to identify category attributes and a semantic-driven spatial relation module (SSRM) is put forward to perform the feature aggregation among the clustered similar characteristics using a semantic-embedded relation matrix. Additionally, we develop a transformer-based channel relation module (TCRM), which breaks the habit of employing scalars as the descriptors of channel-wise relations in the previous deep feature-prior and replaces them with certain vectors, together with Transformerstyle feature interactions, supporting the representations to be more discriminative. In order to maintain the mathematical correlation and spectral consistency between hyperspectral bands, the second-order prior constraints (SOPC) are incorporated into the loss function to guide the HSI reconstruction process. </details>
<details>	<summary>邮件日期</summary>	2021年12月30日</details>

# 375、Best Buddy GANs提供高细节图像超分辨率
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年12月28日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
<details>	<summary>邮件日期</summary>	2021年12月30日</details>

# 374、DSRGAN：基于生成对抗网络的细节优先辅助感知单幅图像超分辨率
- [ ] DSRGAN: Detail Prior-Assisted Perceptual Single Image Super-Resolution via Generative Adversarial Networks 
时间：2021年12月25日                         第一作者：Ziyang Liu                       [链接](https://arxiv.org/abs/2112.13191).                     
## 摘要：将生成对抗网络（GAN）成功地应用于感知单幅图像超分辨率（SISR）的研究。然而，GAN通常倾向于生成高频细节与真实细节不一致的图像。受传统细节增强算法的启发，我们提出了一种新的先验知识，即细节先验知识，以帮助GAN缓解这个问题并恢复更真实的细节。该方法名为DSRGAN，包括一个精心设计的细节提取算法，用于从图像中捕获最重要的高频信息。然后，使用两个鉴别器分别对图像域和细节域恢复进行监督。DSRGAN通过细节增强方式将恢复的细节合并到最终输出中。DSRGAN的特殊设计充分利用了基于模型的传统算法和数据驱动的深度学习网络的优点。实验结果表明，DSRGAN在感知度量方面优于最先进的SISR方法，同时在保真度度量方面也取得了类似的结果。继DSRGAN之后，将其他传统的图像处理算法合并到深度学习网络中以形成基于模型的深度SISR是可行的。
<details>	<summary>英文摘要</summary>	The generative adversarial network (GAN) is successfully applied to study the perceptual single image superresolution (SISR). However, the GAN often tends to generate images with high frequency details being inconsistent with the real ones. Inspired by conventional detail enhancement algorithms, we propose a novel prior knowledge, the detail prior, to assist the GAN in alleviating this problem and restoring more realistic details. The proposed method, named DSRGAN, includes a well designed detail extraction algorithm to capture the most important high frequency information from images. Then, two discriminators are utilized for supervision on image-domain and detail-domain restorations, respectively. The DSRGAN merges the restored detail into the final output via a detail enhancement manner. The special design of DSRGAN takes advantages from both the model-based conventional algorithm and the data-driven deep learning network. Experimental results demonstrate that the DSRGAN outperforms the state-of-the-art SISR methods on perceptual metrics and achieves comparable results in terms of fidelity metrics simultaneously. Following the DSRGAN, it is feasible to incorporate other conventional image processing algorithms into a deep learning network to form a model-based deep SISR. </details>
<details>	<summary>邮件日期</summary>	2021年12月28日</details>

# 373、在超分辨率网络中发现“语义”
- [ ] Discovering "Semantics" in Super-Resolution Networks 
时间：2021年12月24日                         第一作者：Yihao Liu                       [链接](https://arxiv.org/abs/2108.00406).                     
<details>	<summary>注释</summary>	discovering and interpreting deep degradation representations (DDR) in super-resolution networks </details>
<details>	<summary>邮件日期</summary>	2021年12月28日</details>

# 372、图像超分辨率中的回流衰减
- [ ] Reflash Dropout in Image Super-Resolution 
时间：2021年12月22日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2112.12089).                     
## 摘要：辍学旨在缓解高级视觉任务中的过度拟合问题，但很少应用于低级视觉任务，如图像超分辨率（SR）。作为一个经典的回归问题，SR表现出与高级任务不同的行为，并且对退出操作非常敏感。然而，在本文中，我们证明了适当使用辍学有利于SR网络，并提高了泛化能力。具体来说，辍学更好地嵌入在网络末端，对多重降级设置有很大帮助。这一发现打破了我们的常识，激励我们探索其工作机制。我们进一步使用了两种分析工具——一种来自最近的网络解释工作，另一种是专门为这项任务设计的。分析结果为我们的实验结果提供了侧面证据，并为我们理解SR网络提供了一个新的视角。
<details>	<summary>英文摘要</summary>	Dropout is designed to relieve the overfitting problem in high-level vision tasks but is rarely applied in low-level vision tasks, like image super-resolution (SR). As a classic regression problem, SR exhibits a different behaviour as high-level tasks and is sensitive to the dropout operation. However, in this paper, we show that appropriate usage of dropout benefits SR networks and improves the generalization ability. Specifically, dropout is better embedded at the end of the network and is significantly helpful for the multi-degradation settings. This discovery breaks our common sense and inspires us to explore its working mechanism. We further use two analysis tools -- one is from recent network interpretation works, and the other is specially designed for this task. The analysis results provide side proofs to our experimental findings and show us a new perspective to understand SR networks. </details>
<details>	<summary>邮件日期</summary>	2021年12月23日</details>

# 371、我们能用神经正则化来解决深度超分辨率问题吗？
- [ ] Can We Use Neural Regularization to Solve Depth Super-Resolution? 
时间：2021年12月21日                         第一作者：Milena Gazdieva                       [链接](https://arxiv.org/abs/2112.11085).                     
## 摘要：使用商品传感器捕获的深度图通常需要超分辨率才能用于应用。在这项工作中，我们研究了一种基于Tikhonov正则化的变分问题陈述的超分辨率方法，其中正则化子用深度神经网络参数化。这种方法曾成功地应用于光声层析成像。我们的实验表明，将其应用于深度图超分辨率是困难的，并对其原因提出了建议。
<details>	<summary>英文摘要</summary>	Depth maps captured with commodity sensors often require super-resolution to be used in applications. In this work we study a super-resolution approach based on a variational problem statement with Tikhonov regularization where the regularizer is parametrized with a deep neural network. This approach was previously applied successfully in photoacoustic tomography. We experimentally show that its application to depth map super-resolution is difficult, and provide suggestions about the reasons for that. </details>
<details>	<summary>注释</summary>	9 pages </details>
<details>	<summary>邮件日期</summary>	2021年12月22日</details>

# 370、SelFSR：通过流场退化网络在野外实现自我调节的人脸超分辨率
- [ ] SelFSR: Self-Conditioned Face Super-Resolution in the Wild via Flow Field Degradation Network 
时间：2021年12月20日                         第一作者：Xianfang Zeng                       [链接](https://arxiv.org/abs/2112.10683).                     
## 摘要：尽管在基准数据集上取得了成功，但大多数先进的人脸超分辨率模型在真实场景中表现不佳，因为真实图像和合成训练对之间存在显著的领域差距。为了解决这个问题，我们提出了一种新的域自适应退化网络用于野外人脸超分辨率检测。该退化网络预测流场以及中低分辨率图像。然后，通过扭曲中间图像生成退化的对应物。由于偏好捕捉运动模糊，这种模型在保持原始图像和退化图像之间的身份一致性方面表现得更好。我们进一步提出了超分辨率网络的自适应块。该块将输入图像作为条件项，以有效利用面部结构信息，消除对显式先验的依赖，例如面部地标或边界。我们的模型在CelebA和真实人脸数据集上都达到了最先进的性能。前者展示了我们提出的体系结构强大的生成能力，而后者在现实世界的图像中显示了巨大的身份一致性和感知质量。
<details>	<summary>英文摘要</summary>	In spite of the success on benchmark datasets, most advanced face super-resolution models perform poorly in real scenarios since the remarkable domain gap between the real images and the synthesized training pairs. To tackle this problem, we propose a novel domain-adaptive degradation network for face super-resolution in the wild. This degradation network predicts a flow field along with an intermediate low resolution image. Then, the degraded counterpart is generated by warping the intermediate image. With the preference of capturing motion blur, such a model performs better at preserving identity consistency between the original images and the degraded. We further present the self-conditioned block for super-resolution network. This block takes the input image as a condition term to effectively utilize facial structure information, eliminating the reliance on explicit priors, e.g. facial landmarks or boundary. Our model achieves state-of-the-art performance on both CelebA and real-world face dataset. The former demonstrates the powerful generative ability of our proposed architecture while the latter shows great identity consistency and perceptual quality in real-world images. </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 369、基于潜在扩散模型的高分辨率图像合成
- [ ] High-Resolution Image Synthesis with Latent Diffusion Models 
时间：2021年12月20日                         第一作者：Robin Rombach                        [链接](https://arxiv.org/abs/2112.10752).                     
## 摘要：通过将图像形成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据和其他方面实现了最先进的合成结果。此外，它们的公式允许引导机制来控制图像生成过程，而无需重新训练。然而，由于这些模型通常直接在像素空间中运行，因此对功能强大的DMs进行优化通常需要数百天的GPU时间，并且由于顺序评估，推理成本很高。为了在有限的计算资源上进行DM训练，同时保持其质量和灵活性，我们将其应用于强大的预训练自动编码器的潜在空间。与之前的工作不同，基于这种表示的训练扩散模型首次允许在复杂性降低和细节保留之间达到接近最佳的点，从而大大提高了视觉保真度。通过在模型架构中引入交叉注意层，我们将扩散模型转化为功能强大且灵活的生成器，用于文本或边界框等一般条件输入，并以卷积方式实现高分辨率合成。我们的潜在扩散模型（LDM）在图像修复方面达到了新的水平，并在各种任务上获得了极具竞争力的性能，包括无条件图像生成、语义场景合成和超分辨率，同时与基于像素的DMs相比，显著降低了计算要求。代码可在https://github.com/CompVis/latent-diffusion .
<details>	<summary>英文摘要</summary>	By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion . </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 368、高光谱图像超分辨率的隐式神经表示学习
- [ ] Implicit Neural Representation Learning for Hyperspectral Image Super-Resolution 
时间：2021年12月20日                         第一作者：Kaiwei Zhang                       [链接](https://arxiv.org/abs/2112.10541).                     
## 摘要：高光谱图像（HSI）由于其高维光谱模式，在没有额外辅助图像的情况下实现超分辨率仍然是一个持续的挑战，学习有效的空间和光谱表示是一个基本问题。近年来，隐式神经表征（INRs）作为一种新的、有效的表征方法，尤其是在重建任务中取得了长足的进展。因此，在这项工作中，我们提出了一种新的基于INR的HSI重建模型，该模型通过将空间坐标映射到其相应光谱辐射值的连续函数来表示HSI。特别是，作为INR的具体实现，参数模型的参数由一个超网络预测，该超网络使用卷积网络进行特征提取。它使连续函数以内容感知的方式将空间坐标映射到像素值。此外，周期性空间编码与重建过程紧密结合，使得我们的模型能够恢复更多的高频细节。为了验证我们模型的有效性，我们在三个HSI数据集（CAVE、NUS和NTIR2018）上进行了实验。实验结果表明，与现有的重建方法相比，该模型可以获得具有竞争力的重建性能。此外，我们还提供了一项关于模型各个组成部分影响的消融研究。希望本文能为今后的研究提供有力的参考。
<details>	<summary>英文摘要</summary>	Hyperspectral image (HSI) super-resolution without additional auxiliary image remains a constant challenge due to its high-dimensional spectral patterns, where learning an effective spatial and spectral representation is a fundamental issue. Recently, Implicit Neural Representations (INRs) are making strides as a novel and effective representation, especially in the reconstruction task. Therefore, in this work, we propose a novel HSI reconstruction model based on INR which represents HSI by a continuous function mapping a spatial coordinate to its corresponding spectral radiance values. In particular, as a specific implementation of INR, the parameters of parametric model are predicted by a hypernetwork that operates on feature extraction using convolution network. It makes the continuous functions map the spatial coordinates to pixel values in a content-aware manner. Moreover, periodic spatial encoding are deeply integrated with the reconstruction procedure, which makes our model capable of recovering more high frequency details. To verify the efficacy of our model, we conduct experiments on three HSI datasets (CAVE, NUS, and NTIRE2018). Experimental results show that the proposed model can achieve competitive reconstruction performance in comparison with the state-of-the-art methods. In addition, we provide an ablation study on the effect of individual components of our model. We hope this paper could server as a potent reference for future research. </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 367、低水平视觉的有效变换和图像预训练
- [ ] On Efficient Transformer and Image Pre-training for Low-level Vision 
时间：2021年12月19日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2112.10175).                     
## 摘要：预培训标志着高级计算机视觉领域的众多技术水平，但很少有人尝试研究预培训在图像处理系统中的作用。本文对图像预训练进行了深入研究。为了在坚实的基础上进行这项具有实用价值的研究，我们首先提出了一种通用的、经济高效的基于变压器的图像处理框架。它在一系列低级别任务中产生了极具竞争力的性能，尽管参数和计算复杂度受到限制。然后，基于该框架，我们设计了一整套有原则的评估工具，对不同任务中的图像预训练进行认真、全面的诊断，并揭示其对内部网络表征的影响。我们发现，在低水平任务中，预培训扮演着截然不同的角色。例如，预训练将更多的局部信息引入超分辨率（SR）的更高层，从而产生显著的性能提升，而预训练几乎不会影响去噪中的内部特征表示，从而产生少量提升。此外，我们探索了不同的预训练方法，发现多任务预训练更有效，数据效率更高。所有代码和型号将于https://github.com/fenglinglwb/EDT.
<details>	<summary>英文摘要</summary>	Pre-training has marked numerous state of the arts in high-level computer vision, but few attempts have ever been made to investigate how pre-training acts in image processing systems. In this paper, we present an in-depth study of image pre-training. To conduct this study on solid ground with practical value in mind, we first propose a generic, cost-effective Transformer-based framework for image processing. It yields highly competitive performance across a range of low-level tasks, though under constrained parameters and computational complexity. Then, based on this framework, we design a whole set of principled evaluation tools to seriously and comprehensively diagnose image pre-training in different tasks, and uncover its effects on internal network representations. We find pre-training plays strikingly different roles in low-level tasks. For example, pre-training introduces more local information to higher layers in super-resolution (SR), yielding significant performance gains, while pre-training hardly affects internal feature representations in denoising, resulting in a little gain. Further, we explore different methods of pre-training, revealing that multi-task pre-training is more effective and data-efficient. All codes and models will be released at https://github.com/fenglinglwb/EDT. </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 366、A-ESRGAN：用注意力U-Net鉴别器训练真实世界的盲超分辨率
- [ ] A-ESRGAN: Training Real-World Blind Super-Resolution with Attention U-Net Discriminators 
时间：2021年12月19日                         第一作者：Zihao Wei                       [链接](https://arxiv.org/abs/2112.10046).                     
## 摘要：盲图像超分辨率（SR）是CV中的一项长期任务，旨在恢复遭受未知和复杂失真的低分辨率图像。最近的工作主要集中在采用更复杂的退化模型来模拟真实世界的退化。由此产生的模型在感知损失方面取得了突破，并产生了令人信服的感知结果。然而，当前生成性对抗性网络结构带来的限制仍然很明显：平等对待像素会导致忽略图像的结构特征，并导致性能缺陷，如扭曲线和背景过度锐化或模糊。在本文中，我们提出了A-ESRGAN，这是一种用于盲SR任务的GAN模型，具有基于注意U网络的多尺度鉴别器，可以与其他生成器无缝集成。据我们所知，这是首次引入注意U网络结构作为GAN的鉴别器来解决盲SR问题。本文还对多尺度注意U-Net背后的机制进行了解释，该机制为该模型带来了性能突破。通过与以往工作的对比实验，我们的模型在非参考自然图像质量评估指标上表现出了最先进的水平。我们的烧蚀研究表明，使用我们的鉴别器，基于RRDB的生成器可以在多个尺度上利用图像的结构特征，因此与之前的工作相比，生成更逼真的高分辨率图像。
<details>	<summary>英文摘要</summary>	Blind image super-resolution(SR) is a long-standing task in CV that aims to restore low-resolution images suffering from unknown and complex distortions. Recent work has largely focused on adopting more complicated degradation models to emulate real-world degradations. The resulting models have made breakthroughs in perceptual loss and yield perceptually convincing results. However, the limitation brought by current generative adversarial network structures is still significant: treating pixels equally leads to the ignorance of the image's structural features, and results in performance drawbacks such as twisted lines and background over-sharpening or blurring. In this paper, we present A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net based, multi-scale discriminator that can be seamlessly integrated with other generators. To our knowledge, this is the first work to introduce attention U-Net structure as the discriminator of GAN to solve blind SR problems. And the paper also gives an interpretation for the mechanism behind multi-scale attention U-Net that brings performance breakthrough to the model. Through comparison experiments with prior works, our model presents state-of-the-art level performance on the non-reference natural image quality evaluator metric. And our ablation studies have shown that with our discriminator, the RRDB based generator can leverage the structural features of an image in multiple scales, and consequently yields more perceptually realistic high-resolution images compared to prior works. </details>
<details>	<summary>注释</summary>	6 pages, 9 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 365、通过超分辨率增强楼层平面中的目标检测
- [ ] Enhanced Object Detection in Floor-plan through Super Resolution 
时间：2021年12月18日                         第一作者：Dev Khare                       [链接](https://arxiv.org/abs/2112.09844).                     
## 摘要：建筑信息建模（BIM）软件使用可伸缩向量格式，以实现行业内平面布置图的灵活设计。建筑领域中的平面图可以来自许多来源，这些来源可能是可伸缩矢量格式，也可能不是。将平面图图像转换为完全带注释的矢量图像是一个现在可以通过计算机视觉实现的过程。该领域的新数据集已被用于训练卷积神经网络（CNN）结构，用于目标检测。超分辨率图像增强（SR）也是计算机视觉中基于CNN的网络，用于将低分辨率图像转换为高分辨率图像。这项工作的重点是创建一个多组件模块，将SR模型堆叠在楼层平面对象检测模型上。与相应的普通目标检测模型相比，所提出的堆叠模型表现出更高的性能。在最佳情况下，加入SR后，在香草网络上的目标检测性能提高了39.47%。数据和代码在https://github.com/rbg-research/Floor-Plan-Detection.
<details>	<summary>英文摘要</summary>	Building Information Modelling (BIM) software use scalable vector formats to enable flexible designing of floor plans in the industry. Floor plans in the architectural domain can come from many sources that may or may not be in scalable vector format. The conversion of floor plan images to fully annotated vector images is a process that can now be realized by computer vision. Novel datasets in this field have been used to train Convolutional Neural Network (CNN) architectures for object detection. Image enhancement through Super-Resolution (SR) is also an established CNN based network in computer vision that is used for converting low resolution images to high resolution ones. This work focuses on creating a multi-component module that stacks a SR model on a floor plan object detection model. The proposed stacked model shows greater performance than the corresponding vanilla object detection model. For the best case, the the inclusion of SR showed an improvement of 39.47% in object detection over the vanilla network. Data and code are made publicly available at https://github.com/rbg-research/Floor-Plan-Detection. </details>
<details>	<summary>注释</summary>	3rd International Conference on Machine Learning, Image Processing, Network Security and Data Sciences MSC-class: 68T45 ACM-class: I.2; I.4 </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 364、基于A-net深度学习网络的细胞骨架图像超分辨率重建
- [ ] Super-resolution reconstruction of cytoskeleton image based on A-net deep learning network 
时间：2021年12月17日                         第一作者：Qian Chen                       [链接](https://arxiv.org/abs/2112.09574).                     
## 摘要：到目前为止，纳米级的活细胞成像仍然具有挑战性。尽管超分辨率显微镜方法能够在光学分辨率极限以下显示亚细胞结构，但空间分辨率仍远远不足以在体内重建生物分子的结构（即约24 nm厚的微管纤维）。在本研究中，我们提出了一个A-net网络，并表明将A-net深度学习网络与基于退化模型的DWDC算法相结合，可以显著提高共焦显微镜捕获的细胞骨架图像的分辨率。利用DWDC算法构建新的数据集，并利用A-net神经网络的特性（即相当少的层），我们成功地去除了噪声和絮状结构，它们最初会干扰原始图像中的细胞结构，并使用相对较小的数据集将空间分辨率提高了10倍。因此，我们得出结论，将A-net神经网络与DWDC方法相结合的算法是从低分辨率图像中提取生物分子、细胞和器官结构细节的合适且通用的方法。
<details>	<summary>英文摘要</summary>	To date, live-cell imaging at the nanometer scale remains challenging. Even though super-resolution microscopy methods have enabled visualization of subcellular structures below the optical resolution limit, the spatial resolution is still far from enough for the structural reconstruction of biomolecules in vivo (i.e. ~24 nm thickness of microtubule fiber). In this study, we proposed an A-net network and showed that the resolution of cytoskeleton images captured by a confocal microscope can be significantly improved by combining the A-net deep learning network with the DWDC algorithm based on degradation model. Utilizing the DWDC algorithm to construct new datasets and taking advantage of A-net neural network's features (i.e., considerably fewer layers), we successfully removed the noise and flocculent structures, which originally interfere with the cellular structure in the raw image, and improved the spatial resolution by 10 times using relatively small dataset. We, therefore, conclude that the proposed algorithm that combines A-net neural network with the DWDC method is a suitable and universal approach for exacting structural details of biomolecules, cells and organs from low-resolution images. </details>
<details>	<summary>注释</summary>	The manuscript has 17 pages, 10 figures and 58 references </details>
<details>	<summary>邮件日期</summary>	2021年12月20日</details>

# 363、轻量级超分辨率图像的特征提取交互加权网络
- [ ] Feature Distillation Interaction Weighting Network for Lightweight Image Super-Resolution 
时间：2021年12月16日                         第一作者：Guangwei Gao                       [链接](https://arxiv.org/abs/2112.08655).                     
## 摘要：基于卷积神经网络的单幅图像超分辨率（SISR）技术近年来取得了很大的进展。然而，由于计算和内存成本的原因，很难将这些方法应用到实际场景中。同时，如何在有限的参数和计算约束下充分利用中间特性也是一个巨大的挑战。为了缓解这些问题，我们提出了一种轻量级但高效的特征提取交互加权网络（FDIWN）。具体来说，FDIWN利用一系列专门设计的特征洗牌加权群（FSWG）作为主干，几个新的相互广泛的残余蒸馏相互作用块（WDIB）形成一个FSWG。此外，为了更好地进行特征提取，WDIB中引入了宽相同剩余加权（WIRW）单元和宽卷积剩余加权（WCRW）单元。此外，还提出了一种宽残余蒸馏连接（WRDC）框架和一个自校准融合（SCF）单元，以更灵活、高效地与不同尺度的特征进行交互。大量实验表明，我们的FDIWN在模型性能和效率之间取得了良好的平衡，优于其他模型。该代码可在https://github.com/IVIPLab/FDIWN.
<details>	<summary>英文摘要</summary>	Convolutional neural networks based single-image super-resolution (SISR) has made great progress in recent years. However, it is difficult to apply these methods to real-world scenarios due to the computational and memory cost. Meanwhile, how to take full advantage of the intermediate features under the constraints of limited parameters and calculations is also a huge challenge. To alleviate these issues, we propose a lightweight yet efficient Feature Distillation Interaction Weighted Network (FDIWN). Specifically, FDIWN utilizes a series of specially designed Feature Shuffle Weighted Groups (FSWG) as the backbone, and several novel mutual Wide-residual Distillation Interaction Blocks (WDIB) form an FSWG. In addition, Wide Identical Residual Weighting (WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are introduced into WDIB for better feature distillation. Moreover, a Wide-Residual Distillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF) unit are proposed to interact features with different scales more flexibly and efficiently.Extensive experiments show that our FDIWN is superior to other models to strike a good balance between model performance and efficiency. The code is available at https://github.com/IVIPLab/FDIWN. </details>
<details>	<summary>注释</summary>	9 pages, 9 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年12月17日</details>

# 362、稳定的长期重复视频超分辨率
- [ ] Stable Long-Term Recurrent Video Super-Resolution 
时间：2021年12月16日                         第一作者：Benjamin Naoto Chiche                       [链接](https://arxiv.org/abs/2112.08950).                     
## 摘要：与基于滑动窗口的模型相比，基于深度学习（DL）的视频超分辨率（VSR）中的递归模型由于其更高的计算效率、时间感受野和时间一致性而受到欢迎。然而，当对呈现低运动的长视频序列（即场景的某些部分几乎不移动）进行推断时，递归模型通过递归处理发散，产生高频伪影。据我们所知，没有任何关于VSR的研究指出这种不稳定性问题，这对于一些实际应用来说可能是至关重要的。视频监控是一个典型的例子，在这种情况下会出现伪影，因为相机和场景都会长时间保持静止。在这项工作中，我们揭示了在低运动的长序列上现有循环VSR网络的不稳定性。我们在我们创建的一个新的长序列数据集准静态视频集上演示了它。最后，基于Lipschitz稳定性理论，我们提出了一种新的循环VSR网络框架，它既稳定又有竞争性。基于这个框架，我们提出了一种新的递归VSR网络，即中间递归视频超分辨率（MRVSR）。我们通过经验证明了它在低运动的长序列上的竞争性能。
<details>	<summary>英文摘要</summary>	Recurrent models have gained popularity in deep learning (DL) based video super-resolution (VSR), due to their increased computational efficiency, temporal receptive field and temporal consistency compared to sliding-window based models. However, when inferring on long video sequences presenting low motion (i.e. in which some parts of the scene barely move), recurrent models diverge through recurrent processing, generating high frequency artifacts. To the best of our knowledge, no study about VSR pointed out this instability problem, which can be critical for some real-world applications. Video surveillance is a typical example where such artifacts would occur, as both the camera and the scene stay static for a long time. In this work, we expose instabilities of existing recurrent VSR networks on long sequences with low motion. We demonstrate it on a new long sequence dataset Quasi-Static Video Set, that we have created. Finally, we introduce a new framework of recurrent VSR networks that is both stable and competitive, based on Lipschitz stability theory. We propose a new recurrent VSR network, coined Middle Recurrent Video Super-Resolution (MRVSR), based on this framework. We empirically show its competitive performance on long sequences with low motion. </details>
<details>	<summary>注释</summary>	9 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月17日</details>

# 361、物理增强数字岩石图像分辨率的成对与非成对深度学习方法的比较研究
- [ ] A comparative study of paired versus unpaired deep learning methods for physically enhancing digital rock image resolution 
时间：2021年12月16日                         第一作者：Yufu Niu                       [链接](https://arxiv.org/abs/2112.08644).                     
## 摘要：X射线显微计算机断层扫描（micro CT）已被广泛用于表征地下多孔岩石的孔隙尺度几何结构。使用深度学习的超分辨率（SR）方法的最新发展允许在大空间尺度上对低分辨率（LR）图像进行数字增强，从而创建与高分辨率（HR）地面真相相当的SR图像。这绕过了传统的分辨率和视野权衡。一个突出的问题是成对（注册）LR和HR数据的使用，这在此类方法的培训步骤中经常需要，但很难获得。在这项工作中，我们严格比较了两种不同的最先进的SR深度学习技术，使用配对和非配对数据，以及相似的地面真相数据。第一种方法需要成对图像来训练卷积神经网络（CNN），而第二种方法使用未成对图像来训练生成性对抗网络（GAN）。利用具有复杂微孔结构的显微CT碳酸盐岩样品，对这两种方法进行了比较。我们实施了各种基于图像和数值的验证以及实验验证，以定量评估这两种方法的物理精度和灵敏度。我们的定量结果表明，非配对GAN方法可以像成对CNN方法一样精确地重建超分辨率图像，并且训练时间和数据集要求相当。这为使用非配对深度学习方法的微CT图像增强打开了新的应用程序；在数据处理阶段不再需要图像配准。从数据存储平台分离的图像可以更有效地用于训练SR数字岩石应用的网络。这为非均质多孔介质中多尺度流动模拟的各种应用开辟了一条新途径。
<details>	<summary>英文摘要</summary>	X-ray micro-computed tomography (micro-CT) has been widely leveraged to characterise pore-scale geometry in subsurface porous rock. Recent developments in super resolution (SR) methods using deep learning allow the digital enhancement of low resolution (LR) images over large spatial scales, creating SR images comparable to the high resolution (HR) ground truth. This circumvents traditional resolution and field-of-view trade-offs. An outstanding issue is the use of paired (registered) LR and HR data, which is often required in the training step of such methods but is difficult to obtain. In this work, we rigorously compare two different state-of-the-art SR deep learning techniques, using both paired and unpaired data, with like-for-like ground truth data. The first approach requires paired images to train a convolutional neural network (CNN) while the second approach uses unpaired images to train a generative adversarial network (GAN). The two approaches are compared using a micro-CT carbonate rock sample with complicated micro-porous textures. We implemented various image based and numerical verifications and experimental validation to quantitatively evaluate the physical accuracy and sensitivities of the two methods. Our quantitative results show that unpaired GAN approach can reconstruct super-resolution images as precise as paired CNN method, with comparable training times and dataset requirement. This unlocks new applications for micro-CT image enhancement using unpaired deep learning methods; image registration is no longer needed during the data processing stage. Decoupled images from data storage platforms can be exploited more efficiently to train networks for SR digital rock applications. This opens up a new pathway for various applications of multi-scale flow simulation in heterogeneous porous media. </details>
<details>	<summary>注释</summary>	26 pages, 11 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年12月17日</details>

# 360、用于联合MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年12月15日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
<details>	<summary>邮件日期</summary>	2021年12月16日</details>

# 359、核盲分辨超突发
- [ ] Kernel-aware Raw Burst Blind Super-Resolution 
时间：2021年12月14日                         第一作者：Wenyi Lian                        [链接](https://arxiv.org/abs/2112.07315).                     
## 摘要：突发超分辨率（SR）提供了从低质量图像恢复丰富细节的可能性。然而，由于低分辨率（LR）图像在实际应用中存在多种复杂且未知的退化，现有的非盲（如双三次）设计网络通常会导致恢复高分辨率（HR）图像的性能严重下降。此外，处理多个未对齐的噪声原始输入也是一项挑战。在本文中，我们解决了从现代手持设备获取的原始突发序列重建HR图像的问题。核心思想是一种内核引导策略，它可以通过两个步骤解决突发SR：内核建模和HR恢复。前者根据原始输入估计突发核，而后者根据估计的核预测超分辨率图像。此外，我们还引入了一个核感知的可变形对齐模块，该模块可以在考虑模糊先验的情况下有效地对齐原始图像。在合成数据集和真实数据集上的大量实验表明，该方法在突发随机共振问题中具有良好的性能。
<details>	<summary>英文摘要</summary>	Burst super-resolution (SR) provides a possibility of restoring rich details from low-quality images. However, since low-resolution (LR) images in practical applications have multiple complicated and unknown degradations, existing non-blind (e.g., bicubic) designed networks usually lead to a severe performance drop in recovering high-resolution (HR) images. Moreover, handling multiple misaligned noisy raw inputs is also challenging. In this paper, we address the problem of reconstructing HR images from raw burst sequences acquired from modern handheld devices. The central idea is a kernel-guided strategy which can solve the burst SR with two steps: kernel modeling and HR restoring. The former estimates burst kernels from raw inputs, while the latter predicts the super-resolved image based on the estimated kernels. Furthermore, we introduce a kernel-aware deformable alignment module which can effectively align the raw images with consideration of the blurry priors. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method can perform favorable state-of-the-art performance in the burst SR problem. </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 358、为单图像超分辨率降低通道噪声
- [ ] Mitigating Channel-wise Noise for Single Image Super Resolution 
时间：2021年12月14日                         第一作者：Srimanta M                       [链接](https://arxiv.org/abs/2112.07589).                     
## 摘要：实际上，对于不同的颜色通道，图像可能包含不同数量的噪声，这是现有超分辨率方法所不承认的。在本文中，我们提出通过联合考虑颜色通道来超分辨率含噪彩色图像。噪声统计是从输入的低分辨率图像中盲估计的，用于在数据开销中为不同的颜色通道分配不同的权重。视觉数据的隐式低秩结构是通过核范数最小化和自适应权重来实现的，自适应权重作为一个正则化项加入到代价中。此外，图像的多尺度细节通过另一个正则化项添加到模型中，该正则化项涉及到PCA基础上的投影，该PCA基础是使用在输入图像的不同尺度上提取的相似面片构建的。结果表明，该方法在实际场景中具有超强的分辨能力。
<details>	<summary>英文摘要</summary>	In practice, images can contain different amounts of noise for different color channels, which is not acknowledged by existing super-resolution approaches. In this paper, we propose to super-resolve noisy color images by considering the color channels jointly. Noise statistics are blindly estimated from the input low-resolution image and are used to assign different weights to different color channels in the data cost. Implicit low-rank structure of visual data is enforced via nuclear norm minimization in association with adaptive weights, which is added as a regularization term to the cost. Additionally, multi-scale details of the image are added to the model through another regularization term that involves projection onto PCA basis, which is constructed using similar patches extracted across different scales of the input image. The results demonstrate the super-resolving capability of the approach in real scenarios. </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 357、SphereSR:360{\deg}图像超分辨率，通过连续球面图像表示进行任意投影
- [ ] SphereSR: 360{\deg} Image Super-Resolution with Arbitrary Projection via Continuous Spherical Image Representation 
时间：2021年12月14日                         第一作者：Youngho Yoon                       [链接](https://arxiv.org/abs/2112.06536).                     
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 356、文本格式塔：笔划感知场景文本图像超分辨率
- [ ] Text Gestalt: Stroke-Aware Scene Text Image Super-Resolution 
时间：2021年12月13日                         第一作者：Jingye Chen                       [链接](https://arxiv.org/abs/2112.08171).                     
## 摘要：近十年来，随着深度学习的蓬勃发展，场景文本识别技术得到了迅速发展。然而，低分辨率场景文本图像的识别仍然是一个挑战。尽管已经提出了一些超分辨率方法来解决这个问题，但它们通常将文本图像视为一般图像，而忽略了笔画（文本的原子单位）的视觉质量对文本识别起着至关重要的作用这一事实。格式塔心理学认为，在先验知识的指导下，人类能够将部分细节组合成最相似的对象。同样，当人类观察低分辨率文本图像时，他们会固有地使用部分笔划级别的细节来恢复整体角色的外观。受格式塔心理学的启发，我们提出了一种笔划感知的场景文本图像超分辨率方法，该方法包含一个笔划聚焦模块（SFM），用于关注文本图像中字符的笔划级内部结构。具体来说，我们试图设计规则，在笔划级别分解英文字符和数字，然后预先训练文本识别器，以提供笔划级别的注意图作为位置线索，目的是控制生成的超分辨率图像和高分辨率地面真相之间的一致性。大量的实验结果验证了该方法确实可以在TextZoom和人工构建的汉字数据集Degraded-IC13上生成更清晰的图像。此外，由于建议的SFM仅用于在训练时提供中风水平指导，因此在测试阶段不会带来任何时间开销。代码可在https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt.
<details>	<summary>英文摘要</summary>	In the last decade, the blossom of deep learning has witnessed the rapid development of scene text recognition. However, the recognition of low-resolution scene text images remains a challenge. Even though some super-resolution methods have been proposed to tackle this problem, they usually treat text images as general images while ignoring the fact that the visual quality of strokes (the atomic unit of text) plays an essential role for text recognition. According to Gestalt Psychology, humans are capable of composing parts of details into the most similar objects guided by prior knowledge. Likewise, when humans observe a low-resolution text image, they will inherently use partial stroke-level details to recover the appearance of holistic characters. Inspired by Gestalt Psychology, we put forward a Stroke-Aware Scene Text Image Super-Resolution method containing a Stroke-Focused Module (SFM) to concentrate on stroke-level internal structures of characters in text images. Specifically, we attempt to design rules for decomposing English characters and digits at stroke-level, then pre-train a text recognizer to provide stroke-level attention maps as positional clues with the purpose of controlling the consistency between the generated super-resolution image and high-resolution ground truth. The extensive experimental results validate that the proposed method can indeed generate more distinguishable images on TextZoom and manually constructed Chinese character dataset Degraded-IC13. Furthermore, since the proposed SFM is only used to provide stroke-level guidance when training, it will not bring any time overhead during the test phase. Code is available at https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt. </details>
<details>	<summary>注释</summary>	Accepted to AAAI2022. Code is available at https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt </details>
<details>	<summary>邮件日期</summary>	2021年12月16日</details>

# 355、挖掘自相似性：用表位表示标记超分辨率
- [ ] Mining self-similarity: Label super-resolution with epitomic representations 
时间：2021年12月13日                         第一作者：Nikolay Malkin                       [链接](https://arxiv.org/abs/2004.11498).                     
<details>	<summary>注释</summary>	ECCV 2020 final version </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 354、深度注意引导图像滤波
- [ ] Deep Attentional Guided Image Filtering 
时间：2021年12月13日                         第一作者：Zhiwei Zhong                       [链接](https://arxiv.org/abs/2112.06401).                     
## 摘要：引导滤波器是计算机视觉和计算机图形学中的一种基本工具，其目的是将结构信息从引导图像传输到目标图像。现有的大多数方法都是从制导本身构造滤波核，而没有考虑制导和目标之间的相互依赖性。然而，由于两幅图像中通常存在显著不同的边缘，因此简单地将制导的所有结构信息传输到目标将导致各种伪影。为了解决这个问题，我们提出了一种有效的深度注意引导图像过滤框架，该框架的过滤过程可以充分整合两幅图像中包含的互补信息。具体来说，我们提出了一个注意核学习模块，分别从引导和目标生成两组过滤核，然后通过建模两幅图像之间的像素相关性自适应地组合它们。同时，我们提出了一个多尺度引导图像滤波模块，以从粗到精的方式逐步生成具有构造核的滤波结果。相应地，引入了多尺度融合策略，以便在从粗到精的过程中重用中间结果。大量实验表明，该框架在引导超分辨率、跨模态恢复、纹理去除和语义分割等多种引导图像滤波应用中，均优于现有的方法。
<details>	<summary>英文摘要</summary>	Guided filter is a fundamental tool in computer vision and computer graphics which aims to transfer structure information from guidance image to target image. Most existing methods construct filter kernels from the guidance itself without considering the mutual dependency between the guidance and the target. However, since there typically exist significantly different edges in the two images, simply transferring all structural information of the guidance to the target would result in various artifacts. To cope with this problem, we propose an effective framework named deep attentional guided image filtering, the filtering process of which can fully integrate the complementary information contained in both images. Specifically, we propose an attentional kernel learning module to generate dual sets of filter kernels from the guidance and the target, respectively, and then adaptively combine them by modeling the pixel-wise dependency between the two images. Meanwhile, we propose a multi-scale guided image filtering module to progressively generate the filtering result with the constructed kernels in a coarse-to-fine manner. Correspondingly, a multi-scale fusion strategy is introduced to reuse the intermediate results in the coarse-to-fine process. Extensive experiments show that the proposed framework compares favorably with the state-of-the-art methods in a wide range of guided image filtering applications, such as guided super-resolution, cross-modality restoration, texture removal, and semantic segmentation. </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 353、SphereSR
- [ ] SphereSR 
时间：2021年12月13日                         第一作者：Youngho Yoon                       [链接](https://arxiv.org/abs/2112.06536).                     
## 摘要：360成像最近受到了极大关注；然而，它的角度分辨率相对低于窄视场（FOV）透视图像，因为它是使用具有相同传感器尺寸的鱼眼镜头拍摄的。因此，超级解析360度图像是有益的。虽然已经做出了一些尝试，但大多数人认为等矩形投影（ERP）是360度图像表示的方法之一，尽管存在与纬度相关的失真。在这种情况下，由于输出的高分辨率（HR）图像始终与低分辨率（LR）输入的ERP格式相同，因此在将HR图像转换为其他投影类型时，可能会发生另一种信息丢失。在本文中，我们提出了SphereSR，这是一种从LR 360图像生成连续球面图像表示的新框架，旨在预测给定球面坐标下的RGB值，以实现任意360图像投影的超分辨率。具体来说，我们首先提出了一个特征提取模块，该模块基于二十面体表示球面数据，并有效地提取球面上的特征。然后，我们提出了一个球形局部隐式图像函数（SLIF）来预测球坐标下的RGB值。因此，SphereSR可以在任意投影类型下灵活地重新构建HR图像。在各种基准数据集上的实验表明，我们的方法明显优于现有的方法。
<details>	<summary>英文摘要</summary>	The 360 imaging has recently gained great attention; however, its angular resolution is relatively lower than that of a narrow field-of-view (FOV) perspective image as it is captured by using fisheye lenses with the same sensor size. Therefore, it is beneficial to super-resolve a 360 image. Some attempts have been made but mostly considered the equirectangular projection (ERP) as one of the way for 360 image representation despite of latitude-dependent distortions.In that case, as the output high-resolution(HR) image is always in the same ERP format as the low-resolution (LR) input, another information loss may occur when transforming the HR image to other projection types.In this paper, we propose SphereSR, a novel framework to generate a continuous spherical image representation from an LR 360 image, aiming at predicting the RGB values at given spherical coordinates for super-resolution with an arbitrary 360 image projection. Specifically, we first pro-pose a feature extraction module that represents the spherical data based on icosahedron and efficiently extracts features on the spherical surface. We then propose a spherical local implicit image function (SLIIF) to predict RGB values at the spherical coordinates. As such, SphereSR flexibly re-constructs an HR image under an arbitrary projection type.Experiments on various benchmark datasets show that our method significantly surpasses existing methods. </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 352、屏幕内容图像连续超分辨率隐式变换网络
- [ ] Implicit Transformer Network for Screen Content Image Continuous Super-Resolution 
时间：2021年12月12日                         第一作者：Jingyu Yang                       [链接](https://arxiv.org/abs/2112.06174).                     
## 摘要：如今，由于屏幕共享、远程合作和在线教育的广泛应用，屏幕内容呈爆炸式增长。为了匹配有限的终端带宽，可以对高分辨率（HR）屏幕内容进行降采样和压缩。在接收器端，HR显示器或用户对低分辨率（LR）屏幕内容图像（SCI）的超分辨率（SR）有很高的要求，以便放大进行详细观察。然而，由于不同的图像特征以及SCI在任意尺度下浏览的需求，大多数针对自然图像设计的图像SR方法不能很好地适用于SCI。为此，我们提出了一种新的用于SCISR的隐式变压器超分辨率网络（ITSN）。对于任意比例的高质量连续SR，使用隐式变换器从关键坐标处的图像特征推断查询坐标处的像素值，并提出隐式位置编码方案，以聚集与查询坐标相似的相邻像素值。我们用LR和HR SCI对构建基准SCI1K和SCI1K压缩数据集。大量的实验表明，对于压缩和未压缩的SCI，所提出的ITSRN显著优于几种有竞争力的连续和离散SR方法。
<details>	<summary>英文摘要</summary>	Nowadays, there is an explosive growth of screen contents due to the wide application of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth, high-resolution (HR) screen contents may be downsampled and compressed. At the receiver side, the super-resolution (SR) of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation. However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets with LR and HR SCI pairs. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs. </details>
<details>	<summary>注释</summary>	24 pages with 3 figures, NeurIPS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 351、用于视频超分辨率的信息预建递归重建网络
- [ ] Information Prebuilt Recurrent Reconstruction Network for Video Super-Resolution 
时间：2021年12月10日                         第一作者：Ming Yu                       [链接](https://arxiv.org/abs/2112.05755).                     
## 摘要：基于递归卷积网络的视频超分辨率（VSR）方法对视频序列具有很强的时间建模能力。然而，单向递归卷积网络中不同递归单元接收到的输入信息是不平衡的。早期重建帧接收的时间信息较少，导致结果模糊或伪影。虽然双向递归卷积网络可以缓解这个问题，但它大大增加了重建时间和计算复杂度。它也不适用于许多应用场景，例如在线超分辨率。为了解决上述问题，我们提出了一种端到端的信息预建递归重建网络（IPRRN），由信息预建网络（IPNet）和递归重建网络（RRNet）组成。通过整合来自视频前端的足够信息来构建初始重现单元所需的隐藏状态，以帮助恢复早期帧，信息预构建网络在没有反向传播的情况下平衡前后的输入信息差异。此外，我们还展示了一个紧凑的循环重建网络，该网络在恢复质量和时间效率方面有显著改进。大量实验验证了我们提出的网络的有效性，与现有的先进方法相比，我们的方法可以有效地实现更高的定量和定性评估性能。
<details>	<summary>英文摘要</summary>	The video super-resolution (VSR) method based on the recurrent convolutional network has strong temporal modeling capability for video sequences. However, the input information received by different recurrent units in the unidirectional recurrent convolutional network is unbalanced. Early reconstruction frames receive less temporal information, resulting in fuzzy or artifact results. Although the bidirectional recurrent convolution network can alleviate this problem, it greatly increases reconstruction time and computational complexity. It is also not suitable for many application scenarios, such as online super-resolution. To solve the above problems, we propose an end-to-end information prebuilt recurrent reconstruction network (IPRRN), consisting of an information prebuilt network (IPNet) and a recurrent reconstruction network (RRNet). By integrating sufficient information from the front of the video to build the hidden state needed for the initially recurrent unit to help restore the earlier frames, the information prebuilt network balances the input information difference before and after without backward propagation. In addition, we demonstrate a compact recurrent reconstruction network, which has significant improvements in recovery quality and time efficiency. Many experiments have verified the effectiveness of our proposed network, and compared with the existing state-of-the-art methods, our method can effectively achieve higher quantitative and qualitative evaluation performance. </details>
<details>	<summary>注释</summary>	11 pages,9 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 350、基于位置编码的图像超分辨率多尺度内隐学习
- [ ] Enhancing Multi-Scale Implicit Learning in Image Super-Resolution with Integrated Positional Encoding 
时间：2021年12月10日                         第一作者：Ying-Tian Liu                       [链接](https://arxiv.org/abs/2112.05756).                     
## 摘要：中心位置是否完全能够代表一个像素？在离散图像表示中用像素表示像素没有错误，但是在图像超分辨率（SR）上下文中考虑每个像素作为来自局部区域的信号的聚集更为合理。尽管基于坐标的隐式表示在任意尺度的图像SR中有很强的能力，但这一区域的像素性质并没有得到充分考虑。为此，我们提出了集成位置编码（IPE），通过在像素区域聚集频率信息来扩展传统的位置编码。我们将IPE应用于最新的任意尺度图像超分辨率方法：局部隐式图像函数（LIIF），提出了IPE-LIIF。我们通过定量和定性评估证明了IPE-LIIF的有效性，并进一步证明了IPE对更大图像尺度和多隐式方法的泛化能力。代码将被发布。
<details>	<summary>英文摘要</summary>	Is the center position fully capable of representing a pixel? There is nothing wrong to represent pixels with their centers in a discrete image representation, but it makes more sense to consider each pixel as the aggregation of signals from a local area in an image super-resolution (SR) context. Despite the great capability of coordinate-based implicit representation in the field of arbitrary-scale image SR, this area's nature of pixels is not fully considered. To this end, we propose integrated positional encoding (IPE), extending traditional positional encoding by aggregating frequency information over the pixel area. We apply IPE to the state-of-the-art arbitrary-scale image super-resolution method: local implicit image function (LIIF), presenting IPE-LIIF. We show the effectiveness of IPE-LIIF by quantitative and qualitative evaluations, and further demonstrate the generalization ability of IPE to larger image scales and multiple implicit-based methods. Code will be released. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 349、更快地靠近扩散：通过随机收缩加速反问题的条件扩散模型
- [ ] Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction 
时间：2021年12月09日                         第一作者：Hyungjin Chung                       [链接](https://arxiv.org/abs/2112.05146).                     
## 摘要：由于扩散模型作为生成模型的强大性能，最近在社区内引起了极大的兴趣。此外，它在反问题中的应用展示了最先进的性能。不幸的是，扩散模型有一个严重的缺点——它们固有的采样速度很慢，需要几千步的迭代才能从纯高斯噪声中生成图像。在这项工作中，我们表明，从高斯噪声开始是不必要的。相反，从具有更好初始化的单个正向扩散开始，可以显著减少反向条件扩散中的采样步骤数。这种现象是由随机差分方程的收缩理论正式解释的，比如我们的条件扩散策略——反向扩散的交替应用，然后是非扩展的数据一致性步骤。新的采样策略被称为“更接近扩散更快”（CCDF），它还揭示了一种新的见解，即如何将反问题的现有前馈神经网络方法与扩散模型协同结合。超分辨率、图像修复和压缩感知MRI的实验结果表明，我们的方法可以在显著减少采样步长的情况下实现最先进的重建性能。
<details>	<summary>英文摘要</summary>	Diffusion models have recently attained significant interest within the community owing to their strong performance as generative models. Furthermore, its application to inverse problems have demonstrated state-of-the-art performance. Unfortunately, diffusion models have a critical downside - they are inherently slow to sample from, needing few thousand steps of iteration to generate images from pure Gaussian noise. In this work, we show that starting from Gaussian noise is unnecessary. Instead, starting from a single forward diffusion with better initialization significantly reduces the number of sampling steps in the reverse conditional diffusion. This phenomenon is formally explained by the contraction theory of the stochastic difference equations like our conditional diffusion strategy - the alternating applications of reverse diffusion followed by a non-expansive data consistency step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also reveals a new insight on how the existing feed-forward neural network approaches for inverse problems can be synergistically combined with the diffusion models. Experimental results with super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method can achieve state-of-the-art reconstruction performance at significantly reduced sampling steps. </details>
<details>	<summary>邮件日期</summary>	2021年12月13日</details>

# 348、Best Buddy GANs提供高细节图像超分辨率
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年12月08日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
<details>	<summary>邮件日期</summary>	2021年12月09日</details>

# 347、PP-MSVSR：多级视频超分辨率
- [ ] PP-MSVSR: Multi-Stage Video Super-Resolution 
时间：2021年12月06日                         第一作者：Lielin Jiang                        [链接](https://arxiv.org/abs/2112.02828).                     
## 摘要：与单图像超分辨率（SISR）任务不同，视频超分辨率（VSR）任务的关键是充分利用帧间的互补信息重建高分辨率序列。由于来自不同帧的图像具有不同的运动和场景，准确对齐多帧并有效融合不同帧一直是VSR任务的关键研究工作。为了利用相邻帧丰富的互补信息，本文提出了一种多级VSR深度结构，称为PP-MSVSR，该结构带有局部融合模块、辅助损耗和重新对齐模块，以逐步细化增强结果。具体来说，为了加强特征传播中的跨帧特征融合，在第一阶段设计了局部融合模块，在特征传播之前进行局部特征融合。此外，我们在第二阶段引入了一个辅助损失，使传播模块获得的特征保留更多与HR空间相连的相关信息，并在第三阶段引入了一个重新对齐模块，以充分利用前一阶段的特征信息。大量实验证明，PP-MSVSR在Vid4数据集上取得了很好的性能，仅用1.45M的参数就可获得28.13dB的峰值信噪比。PP-MSVSR-L在具有大量参数的REDS4数据集上超过了所有最先进的方法。代码和模型将在Packegan\footnote中发布{https://github.com/PaddlePaddle/PaddleGAN.}.
<details>	<summary>英文摘要</summary>	Different from the Single Image Super-Resolution(SISR) task, the key for Video Super-Resolution(VSR) task is to make full use of complementary information across frames to reconstruct the high-resolution sequence. Since images from different frames with diverse motion and scene, accurately aligning multiple frames and effectively fusing different frames has always been the key research work of VSR tasks. To utilize rich complementary information of neighboring frames, in this paper, we propose a multi-stage VSR deep architecture, dubbed as PP-MSVSR, with local fusion module, auxiliary loss and re-align module to refine the enhanced result progressively. Specifically, in order to strengthen the fusion of features across frames in feature propagation, a local fusion module is designed in stage-1 to perform local feature fusion before feature propagation. Moreover, we introduce an auxiliary loss in stage-2 to make the features obtained by the propagation module reserve more correlated information connected to the HR space, and introduce a re-align module in stage-3 to make full use of the feature information of the previous stage. Extensive experiments substantiate that PP-MSVSR achieves a promising performance of Vid4 datasets, which achieves a PSNR of 28.13dB with only 1.45M parameters. And the PP-MSVSR-L exceeds all state of the art method on REDS4 datasets with considerable parameters. Code and models will be released in PaddleGAN\footnote{https://github.com/PaddlePaddle/PaddleGAN.}. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, 3 tables </details>
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 346、一种无数据集的自适应红外和可见光图像超分辨率融合自监督解纠缠学习方法
- [ ] A Dataset-free Self-supervised Disentangled Learning Method for Adaptive Infrared and Visible Images Super-resolution Fusion 
时间：2021年12月06日                         第一作者：Yuanjie Gu                       [链接](https://arxiv.org/abs/2112.02869).                     
## 摘要：本研究提出了一种基于物理模型的通用无数据集自监督学习框架——自监督解纠缠学习（SDL），并提出了一种将SDL框架、生成网络和Retinex理论应用于红外和可见光图像超分辨率融合的深度Retinex融合（DRF）方法。同时，设计了生成型双路径融合网络ZipperNet和自适应融合损耗函数Retinex loss，实现了高质量的融合。DRF（基于SDL）的核心思想包括两部分：一是使用生成网络生成与物理模型分离的组件；另一种是基于物理关系设计的损耗函数，在训练阶段生成的分量由损耗函数组合而成。此外，为了验证我们提出的DRF的有效性，我们在三个不同的红外和可见光数据集上与六种最先进的方法进行了定性和定量比较。我们的代码将很快在https://github.com/GuYuanjie/Deep-Retinex-fusion.
<details>	<summary>英文摘要</summary>	This study proposes a novel general dataset-free self-supervised learning framework based-on physical model named self-supervised disentangled learning (SDL), and proposes a novel method named Deep Retinex fusion (DRF) which applies SDL framework with generative networks and Retinex theory in infrared and visible images super-resolution fusion. Meanwhile, a generative dual-path fusion network ZipperNet and adaptive fusion loss function Retinex loss are designed for effectively high-quality fusion. The core idea of DRF (based-on SDL) consists of two parts: one is generating components which are disentangled from physical model using generative networks; the other is loss functions which are designed based-on physical relation, and generated components are combined by loss functions in training phase. Furthermore, in order to verify the effectiveness of our proposed DRF, qualitative and quantitative comparisons compared with six state-of-the-art methods are performed on three different infrared and visible datasets. Our code will be open source available soon at https://github.com/GuYuanjie/Deep-Retinex-fusion. </details>
<details>	<summary>注释</summary>	10 pages, 9 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 345、基于扩散模型的标签有效语义分割
- [ ] Label-Efficient Semantic Segmentation with Diffusion Models 
时间：2021年12月06日                         第一作者：Dmitry Baranchuk                       [链接](https://arxiv.org/abs/2112.03126).                     
## 摘要：去噪扩散概率模型最近受到了广泛的研究关注，因为它们的性能优于GANs等替代方法，并且目前提供了最先进的生成性能。扩散模型优越的性能使其在修复、超分辨率和语义编辑等多种应用中成为一种极具吸引力的工具。在本文中，我们证明了扩散模型也可以作为语义分割的工具，尤其是在标记数据稀缺的情况下。特别是，对于几个预训练扩散模型，我们研究了执行反向扩散过程马尔可夫步的网络的中间激活。我们表明，这些激活有效地捕获了输入图像的语义信息，并且似乎是分割问题的优秀像素级表示。基于这些观察，我们描述了一种简单的分割方法，即使只提供了少量的训练图像，该方法也可以工作。我们的方法在多个数据集上显著优于现有的替代方法，以获得相同数量的人类监督。
<details>	<summary>英文摘要</summary>	Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision. </details>
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 344、DAQ：深图像超分辨率网络的信道分布感知量化
- [ ] DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks 
时间：2021年12月06日                         第一作者：Cheeun Hong                       [链接](https://arxiv.org/abs/2012.11230).                     
<details>	<summary>注释</summary>	WACV 2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 343、基于特征的超分辨率图像识别框架
- [ ] Feature-based Recognition Framework for Super-resolution Images 
时间：2021年12月04日                         第一作者：Jing Hu                       [链接](https://arxiv.org/abs/2112.02270).                     
## 摘要：在实际应用中，识别网络应用于超分辨率图像时，其性能往往会下降。在本文中，我们提出了一种结合GAN（FGAN）的基于特征的识别网络。我们的网络通过从SR图像中提取更多有利于识别的特征来提高识别精度。在实验中，我们使用三种不同的超分辨率算法构建了三个数据集，与ReaNet50和DenseNet121相比，我们的网络将识别准确率提高了6%以上。
<details>	<summary>英文摘要</summary>	In practical application, the performance of recognition network usually decreases when being applied on super-resolution images. In this paper, we propose a feature-based recognition network combined with GAN (FGAN). Our network improves the recognition accuracy by extracting more features that benefit recognition from SR images. In the experiment, we build three datasets using three different super-resolution algorithm, and our network increases the recognition accuracy by more than 6% comparing with ReaNet50 and DenseNet121. </details>
<details>	<summary>注释</summary>	7 pages, 2 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 342、小结构可视化的超分辨率CEST-MRI研究进展
- [ ] Towards Super-Resolution CEST MRI for Visualization of Small Structures 
时间：2021年12月03日                         第一作者：Lukas Folle                       [链接](https://arxiv.org/abs/2112.01905).                     
## 摘要：风湿性疾病（如类风湿性关节炎）的发病通常是亚临床的，这导致该疾病的早期检测具有挑战性。然而，解剖结构的特征性变化可以通过MRI或CT等成像技术检测到。现代成像技术，如化学交换饱和转移（CEST）MRI，通过对体内代谢物的成像，有望进一步提高早期检测。为了成像患者关节中的小结构，通常是由于疾病发生变化的第一个区域之一，CEST MR成像的高分辨率是必要的。然而，由于收购的潜在物理限制，目前CEST MR固有的分辨率较低。在这项工作中，我们比较了已建立的上采样技术和基于神经网络的超分辨率方法。我们可以证明，神经网络能够比现有方法更好地学习从低分辨率到高分辨率非饱和CEST图像的映射。在测试集上，使用ResNet神经网络可以实现32.29dB（+10%）、0.14（+28%）的NRMSE和0.85（+15%）的SSIM，从而显著改善基线。这项工作为超分辨率CEST MRI神经网络的前瞻性研究铺平了道路，并可能导致风湿性疾病发病的早期检测。
<details>	<summary>英文摘要</summary>	The onset of rheumatic diseases such as rheumatoid arthritis is typically subclinical, which results in challenging early detection of the disease. However, characteristic changes in the anatomy can be detected using imaging techniques such as MRI or CT. Modern imaging techniques such as chemical exchange saturation transfer (CEST) MRI drive the hope to improve early detection even further through the imaging of metabolites in the body. To image small structures in the joints of patients, typically one of the first regions where changes due to the disease occur, a high resolution for the CEST MR imaging is necessary. Currently, however, CEST MR suffers from an inherently low resolution due to the underlying physical constraints of the acquisition. In this work we compared established up-sampling techniques to neural network-based super-resolution approaches. We could show, that neural networks are able to learn the mapping from low-resolution to high-resolution unsaturated CEST images considerably better than present methods. On the test set a PSNR of 32.29dB (+10%), a NRMSE of 0.14 (+28%), and a SSIM of 0.85 (+15%) could be achieved using a ResNet neural network, improving the baseline considerably. This work paves the way for the prospective investigation of neural networks for super-resolution CEST MRI and, followingly, might lead to a earlier detection of the onset of rheumatic diseases. </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 341、直接体绘制的快速神经表示法
- [ ] Fast Neural Representations for Direct Volume Rendering 
时间：2021年12月02日                         第一作者：Sebastian Weiss                       [链接](https://arxiv.org/abs/2112.01579).                     
## 摘要：尽管神经场景表示可以有效地压缩高重建质量的三维标量场，但使用场景表示网络的训练和数据重建步骤的计算复杂性限制了它们在实际应用中的应用。在本文中，我们分析了场景表示网络是否可以被修改以减少这些限制，以及这些架构是否也可以用于时间重建任务。我们提出了一种新的场景表示网络设计，使用GPU张量核将重建无缝集成到芯片光线跟踪核中。此外，我们还研究了图像引导网络训练作为经典数据驱动方法的替代方法的使用，并探讨了这种替代方法在质量和速度方面的潜在优势和劣势。作为时变场空间超分辨率方法的替代方案，我们提出了一种基于潜在空间插值的解决方案，以实现任意粒度的随机存取重建。我们总结了我们的研究结果，评估了科学可视化任务中场景表示网络的优势和局限性，并概述了该领域未来的研究方向。
<details>	<summary>英文摘要</summary>	Despite the potential of neural scene representations to effectively compress 3D scalar fields at high reconstruction quality, the computational complexity of the training and data reconstruction step using scene representation networks limits their use in practical applications. In this paper, we analyze whether scene representation networks can be modified to reduce these limitations and whether these architectures can also be used for temporal reconstruction tasks. We propose a novel design of scene representation networks using GPU tensor cores to integrate the reconstruction seamlessly into on-chip raytracing kernels. Furthermore, we investigate the use of image-guided network training as an alternative to classical data-driven approaches, and we explore the potential strengths and weaknesses of this alternative regarding quality and speed. As an alternative to spatial super-resolution approaches for time-varying fields, we propose a solution that builds upon latent-space interpolation to enable random access reconstruction at arbitrary granularity. We summarize our findings in the form of an assessment of the strengths and limitations of scene representation networks for scientific visualization tasks and outline promising future research directions in this field. </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 340、多重退化盲超分辨的条件元网络
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年12月01日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>注释</summary>	Under review. Our code will be released after reviewing! </details>
<details>	<summary>邮件日期</summary>	2021年12月02日</details>

# 339、SwiftSRGAN——重新思考高效实时推理的超分辨率
- [ ] SwiftSRGAN -- Rethinking Super-Resolution for Efficient and Real-time Inference 
时间：2021年11月29日                         第一作者：Koushik Sivarama Krishnan                       [链接](https://arxiv.org/abs/2111.14320).                     
## 摘要：近年来，利用最先进的基于深度学习的体系结构，在图像超分辨率任务方面取得了一些进展。以前发布的许多基于超分辨率的技术都需要高端的顶级图形处理单元（GPU）来执行图像超分辨率。随着深度学习方法的不断进步，神经网络变得越来越需要计算。我们后退了一步，专注于创建实时高效的解决方案。我们提出了一种在内存占用方面更快、更小的体系结构。该体系结构使用深度可分离卷积来提取特征，在保持实时推理和低内存占用的同时，其性能与其他超分辨率GAN（生成性对抗网络）相当。实时超分辨率支持流式传输高分辨率媒体内容，即使在带宽条件较差的情况下也是如此。在保持准确度和延迟之间的有效权衡的同时，我们能够生成一个可比的性能模型，其大小是超分辨率GaN的八分之一（1/8），计算速度是超分辨率GaN的74倍。
<details>	<summary>英文摘要</summary>	In recent years, there have been several advancements in the task of image super-resolution using the state of the art Deep Learning-based architectures. Many super-resolution-based techniques previously published, require high-end and top-of-the-line Graphics Processing Unit (GPUs) to perform image super-resolution. With the increasing advancements in Deep Learning approaches, neural networks have become more and more compute hungry. We took a step back and, focused on creating a real-time efficient solution. We present an architecture that is faster and smaller in terms of its memory footprint. The proposed architecture uses Depth-wise Separable Convolutions to extract features and, it performs on-par with other super-resolution GANs (Generative Adversarial Networks) while maintaining real-time inference and a low memory footprint. A real-time super-resolution enables streaming high resolution media content even under poor bandwidth conditions. While maintaining an efficient trade-off between the accuracy and latency, we are able to produce a comparable performance model which is one-eighth (1/8) the size of super-resolution GANs and computes 74 times faster than super-resolution GANs. </details>
<details>	<summary>注释</summary>	6 pages, 3 figures, "to be published in" International Conference on Intelligent Cybernetics Technology & Applications 2021 (ICICyTA) </details>
<details>	<summary>邮件日期</summary>	2021年11月30日</details>

# 338、一种实用的单幅图像超分辨率对比学习框架
- [ ] A Practical Contrastive Learning Framework for Single Image Super-Resolution 
时间：2021年11月27日                         第一作者：Gang Wu                        [链接](https://arxiv.org/abs/2111.13924).                     
## 摘要：对比学习在各种高水平任务上取得了显著的成功，但针对低水平任务提出的方法较少。将针对高级视觉任务提出的普通对比学习技术直接应用于低级视觉任务是一个挑战，因为获得的全局视觉表征不足以用于需要丰富纹理和上下文信息的低级任务。本文提出了一种新的单图像超分辨率对比学习框架。我们从样本构建和特征嵌入两个角度研究了基于对比学习的SISR。现有的方法提出了一些简单的样本构建方法（例如，将低质量的输入视为负样本，将基本事实视为正样本），并采用先验模型（例如，预训练的VGG模型）来获得特征嵌入，而不是探索任务友好的模型。为此，我们提出了一个实用的SISR对比学习框架，该框架涉及在频率空间中生成许多信息丰富的正样本和硬样本。我们没有使用额外的预训练网络，而是从鉴别器网络继承设计了一个简单但有效的嵌入网络，并且可以与主SR网络进行迭代优化，使其任务可概括。最后，与基准方法相比，我们对我们的方法进行了广泛的实验评估，结果表明，与目前最先进的SISR方法相比，我们的方法获得了高达0.21dB的显著增益。
<details>	<summary>英文摘要</summary>	Contrastive learning has achieved remarkable success on various high-level tasks, but there are fewer methods proposed for low-level tasks. It is challenging to adopt vanilla contrastive learning technologies proposed for high-level visual tasks straight to low-level visual tasks since the acquired global visual representations are insufficient for low-level tasks requiring rich texture and context information. In this paper, we propose a novel contrastive learning framework for single image super-resolution (SISR). We investigate the contrastive learning-based SISR from two perspectives: sample construction and feature embedding. The existing methods propose some naive sample construction approaches (e.g., considering the low-quality input as a negative sample and the ground truth as a positive sample) and they adopt a prior model (e.g., pre-trained VGG model) to obtain the feature embedding instead of exploring a task-friendly one. To this end, we propose a practical contrastive learning framework for SISR that involves the generation of many informative positive and hard negative samples in frequency space. Instead of utilizing an additional pre-trained network, we design a simple but effective embedding network inherited from the discriminator network and can be iteratively optimized with the primary SR network making it task-generalizable. Finally, we conduct an extensive experimental evaluation of our method compared with benchmark methods and show remarkable gains of up to 0.21 dB over the current state-of-the-art approaches for SISR. </details>
<details>	<summary>注释</summary>	11 pages, 2 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月30日</details>

# 337、AdaDM：实现图像超分辨率的标准化
- [ ] AdaDM: Enabling Normalization for Image Super-Resolution 
时间：2021年11月27日                         第一作者：Jie Liu                       [链接](https://arxiv.org/abs/2111.13905).                     
## 摘要：与批量标准化（BN）类似的标准化技术是一项里程碑式的技术，可以在深度学习中标准化中间层的分布，从而实现更快的训练和更好的泛化精度。然而，在保真度图像超分辨率（SR）中，人们认为标准化层通过对特征进行标准化来消除范围灵活性，并将其从现代SR网络中移除。本文对这一现象进行了定量和定性研究。我们发现，在标准化层之后，残差特征的标准差会大幅缩小，这导致SR网络的性能下降。标准偏差反映了像素值的变化量。当变化变小时，边缘对网络的分辨力就会降低。为了解决这个问题，我们提出了一种自适应偏差调制器（AdaDM），其中自适应预测调制因子以放大像素偏差。为了获得更好的泛化性能，我们将BN应用于具有AdaDM的先进SR网络中。同时，AdaDM中的偏差放大策略使特征中的边缘信息更具可分辨性。因此，使用BN和我们的AdaDM的SR网络可以在基准数据集上获得实质性的性能改进。大量的实验已经证明了我们方法的有效性。
<details>	<summary>英文摘要</summary>	Normalization like Batch Normalization (BN) is a milestone technique to normalize the distributions of intermediate layers in deep learning, enabling faster training and better generalization accuracy. However, in fidelity image Super-Resolution (SR), it is believed that normalization layers get rid of range flexibility by normalizing the features and they are simply removed from modern SR networks. In this paper, we study this phenomenon quantitatively and qualitatively. We found that the standard deviation of the residual feature shrinks a lot after normalization layers, which causes the performance degradation in SR networks. Standard deviation reflects the amount of variation of pixel values. When the variation becomes smaller, the edges will become less discriminative for the network to resolve. To address this problem, we propose an Adaptive Deviation Modulator (AdaDM), in which a modulation factor is adaptively predicted to amplify the pixel deviation. For better generalization performance, we apply BN in state-of-the-art SR networks with the proposed AdaDM. Meanwhile, the deviation amplification strategy in AdaDM makes the edge information in the feature more distinguishable. As a consequence, SR networks with BN and our AdaDM can get substantial performance improvements on benchmark datasets. Extensive experiments have been conducted to show the effectiveness of our method. </details>
<details>	<summary>邮件日期</summary>	2021年11月30日</details>

# 336、学习3D-CNN和变压器先验知识以获得超光谱图像超分辨率
- [ ] Learning A 3D-CNN and Transformer Prior for Hyperspectral Image Super-Resolution 
时间：2021年11月27日                         第一作者：Qing Ma                        [链接](https://arxiv.org/abs/2111.13923).                     
## 摘要：为了解决高光谱图像超分辨率（HSISR）的不适定问题，通常的方法是利用高光谱图像（HSIs）的先验信息作为正则化项来约束目标函数。基于模型的方法使用手工制作的先验不能完全描述HSI的特性。基于学习的方法通常使用卷积神经网络（CNN）来学习HSI的内隐先验。然而，CNN的学习能力有限，它只考虑HSI的空间特性，而忽略了光谱特性，卷积对于长程依赖建模是无效的。仍有很大的改进空间。在本文中，我们提出了一种新的HSISR方法，使用Transformer代替CNN来学习HSIs的先验知识。具体来说，我们首先使用近似梯度算法来求解HSISR模型，然后使用一个展开网络来模拟迭代求解过程。变形金刚的自我注意层使其具有空间全局交互的能力。此外，我们在变压器层后面添加3D-CNN，以更好地探索HSI的空间-光谱相关性。在两个广泛使用的HSI数据集和真实数据集上的定量和可视化结果表明，与所有主流算法（包括最具竞争力的传统方法和最近提出的基于深度学习的方法）相比，该方法获得了相当大的增益。
<details>	<summary>英文摘要</summary>	To solve the ill-posed problem of hyperspectral image super-resolution (HSISR), an usually method is to use the prior information of the hyperspectral images (HSIs) as a regularization term to constrain the objective function. Model-based methods using hand-crafted priors cannot fully characterize the properties of HSIs. Learning-based methods usually use a convolutional neural network (CNN) to learn the implicit priors of HSIs. However, the learning ability of CNN is limited, it only considers the spatial characteristics of the HSIs and ignores the spectral characteristics, and convolution is not effective for long-range dependency modeling. There is still a lot of room for improvement. In this paper, we propose a novel HSISR method that uses Transformer instead of CNN to learn the prior of HSIs. Specifically, we first use the proximal gradient algorithm to solve the HSISR model, and then use an unfolding network to simulate the iterative solution processes. The self-attention layer of Transformer makes it have the ability of spatial global interaction. In addition, we add 3D-CNN behind the Transformer layers to better explore the spatio-spectral correlation of HSIs. Both quantitative and visual results on two widely used HSI datasets and the real-world dataset demonstrate that the proposed method achieves a considerable gain compared to all the mainstream algorithms including the most competitive conventional methods and the recently proposed deep learning-based methods. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月30日</details>

# 335、基于条件像元合成的卫星图像时空超分辨率
- [ ] Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis 
时间：2021年11月25日                         第一作者：Yutong He                       [链接](https://arxiv.org/abs/2106.11485).                     
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 334、研究真实世界视频超分辨率中的权衡
- [ ] Investigating Tradeoffs in Real-World Video Super-Resolution 
时间：2021年11月24日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2111.12704).                     
## 摘要：真实世界视频超分辨率（VSR）中退化的多样性和复杂性对推理和训练提出了不可忽视的挑战。首先，在轻度退化的情况下，长期繁殖可提高性能，而在野生环境中，严重的退化可通过繁殖被夸大，从而损害输出质量。为了平衡细节合成和伪影抑制之间的平衡，我们发现在传播之前，图像预清洁阶段对于减少噪声和伪影是必不可少的。配备精心设计的清洁模块，我们的RealBasicVSR在质量和效率方面都优于现有方法。第二，现实世界的VSR模型通常使用不同的降级进行训练，以提高通用性，需要增加批量以产生稳定的梯度。不可避免地，增加的计算负担会导致各种问题，包括1）速度性能权衡和2）批次长度权衡。为了缓解第一个折衷，我们提出了一种随机退化方案，在不牺牲性能的情况下，可减少多达40%的训练时间。然后，我们分析了不同的训练设置，并建议在训练过程中使用更长的序列而不是更大的批次，这样可以更有效地利用时间信息，从而在推理过程中获得更稳定的性能。为了便于公平比较，我们提出了新的VideoLQ数据集，该数据集包含大量真实世界的低质量视频序列，其中包含丰富的纹理和模式。我们的数据集可以作为基准测试的共同基础。代码、模型和数据集将公开。
<details>	<summary>英文摘要</summary>	The diversity and complexity of degradations in real-world video super-resolution (VSR) pose non-trivial challenges in inference and training. First, while long-term propagation leads to improved performance in cases of mild degradations, severe in-the-wild degradations could be exaggerated through propagation, impairing output quality. To balance the tradeoff between detail synthesis and artifact suppression, we found an image pre-cleaning stage indispensable to reduce noises and artifacts prior to propagation. Equipped with a carefully designed cleaning module, our RealBasicVSR outperforms existing methods in both quality and efficiency. Second, real-world VSR models are often trained with diverse degradations to improve generalizability, requiring increased batch size to produce a stable gradient. Inevitably, the increased computational burden results in various problems, including 1) speed-performance tradeoff and 2) batch-length tradeoff. To alleviate the first tradeoff, we propose a stochastic degradation scheme that reduces up to 40\% of training time without sacrificing performance. We then analyze different training settings and suggest that employing longer sequences rather than larger batches during training allows more effective uses of temporal information, leading to more stable performance during inference. To facilitate fair comparisons, we propose the new VideoLQ dataset, which contains a large variety of real-world low-quality video sequences containing rich textures and patterns. Our dataset can serve as a common ground for benchmarking. Code, models, and the dataset will be made publicly available. </details>
<details>	<summary>注释</summary>	Tech report, 14 pages, 14 figures. Code can be found at https://github.com/ckkelvinchan/RealBasicVSR </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 333、用微分光学模型重新思考望远镜仪器响应的建模
- [ ] Rethinking the modeling of the instrumental response of telescopes with a differentiable optical model 
时间：2021年11月24日                         第一作者：Tobias Liaudat                        [链接](https://arxiv.org/abs/2111.12541).                     
## 摘要：我们提出了望远镜仪器响应场数据驱动建模的范式转变。通过在建模框架中加入可微光学正演模型，我们将数据驱动的建模空间从像素变为波前。这允许将大量复杂性从仪器响应转移到正向模型中，同时能够适应观测，保持数据驱动。我们的框架为构建具有物理动机、可解释性且不需要特殊校准数据的强大模型提供了一条前进的道路。我们表明，对于简化的空间望远镜设置，与现有的数据驱动方法相比，该框架代表了真正的性能突破，在观测分辨率下重建误差减少了5倍，在3倍超分辨率下重建误差减少了10倍以上。我们仅使用噪声宽带聚焦观测成功地模拟了仪器响应的色度变化。
<details>	<summary>英文摘要</summary>	We propose a paradigm shift in the data-driven modeling of the instrumental response field of telescopes. By adding a differentiable optical forward model into the modeling framework, we change the data-driven modeling space from the pixels to the wavefront. This allows to transfer a great deal of complexity from the instrumental response into the forward model while being able to adapt to the observations, remaining data-driven. Our framework allows a way forward to building powerful models that are physically motivated, interpretable, and that do not require special calibration data. We show that for a simplified setting of a space telescope, this framework represents a real performance breakthrough compared to existing data-driven approaches with reconstruction errors decreasing 5 fold at observation resolution and more than 10 fold for a 3x super-resolution. We successfully model chromatic variations of the instrument's response only using noisy broad-band in-focus observations. </details>
<details>	<summary>注释</summary>	10 pages. Accepted for the Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 332、单幅图像超分辨率的局部选择性特征提取
- [ ] Local-Selective Feature Distillation for Single Image Super-Resolution 
时间：2021年11月22日                         第一作者：SeongUk Park                       [链接](https://arxiv.org/abs/2111.10988).                     
## 摘要：基于卷积神经网络（CNN）的单图像超分辨率（SISR）方法的最新改进主要依赖于构建网络结构，而不是寻找合适的训练算法，而不仅仅是最小化回归损失。采用知识提取（KD）可以为SISR的进一步改进开辟一条道路，同时也有利于提高模型的效率。KD是一种模型压缩方法，可在不使用额外参数进行测试的情况下提高深层神经网络（DNN）的性能。它最近因其在提供更好的容量性能权衡方面的能力而备受关注。本文提出了一种适用于SISR的特征提取（FD）方法。我们指出了现有的基于FitNet的FD算法在SISR任务中的局限性，并建议对现有的FD算法进行修改，以关注局部特征信息。此外，我们还提出了一种基于师生差异的软特征注意方法，该方法选择性地关注特定的像素位置以提取特征信息。我们称我们的方法为局部选择特征蒸馏（LSFD），并验证了我们的方法在SISR问题上优于传统的FD方法。
<details>	<summary>英文摘要</summary>	Recent improvements in convolutional neural network (CNN)-based single image super-resolution (SISR) methods rely heavily on fabricating network architectures, rather than finding a suitable training algorithm other than simply minimizing the regression loss. Adapting knowledge distillation (KD) can open a way for bringing further improvement for SISR, and it is also beneficial in terms of model efficiency. KD is a model compression method that improves the performance of Deep Neural Networks (DNNs) without using additional parameters for testing. It is getting the limelight recently for its competence at providing a better capacity-performance tradeoff. In this paper, we propose a novel feature distillation (FD) method which is suitable for SISR. We show the limitations of the existing FitNet-based FD method that it suffers in the SISR task, and propose to modify the existing FD algorithm to focus on local feature information. In addition, we propose a teacher-student-difference-based soft feature attention method that selectively focuses on specific pixel locations to extract feature information. We call our method local-selective feature distillation (LSFD) and verify that our method outperforms conventional FD methods in SISR problems. </details>
<details>	<summary>注释</summary>	in review </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 331、frequenet：一种基于双余弦变换的频域图像超分辨率网络
- [ ] FreqNet: A Frequency-domain Image Super-Resolution Network with Dicrete Cosine Transform 
时间：2021年11月21日                         第一作者：Runyuan Cai                       [链接](https://arxiv.org/abs/2111.10800).                     
## 摘要：单图像超分辨率（SISR）是一个病态问题，其目的是从低分辨率（LR）输入中获得高分辨率（HR）输出，在此过程中需要添加额外的高频信息以提高感知质量。现有的SISR工作主要是通过最小化均方重建误差在空间域进行的。尽管峰值信噪比（PSNR）结果很高，但很难确定模型是否正确添加了所需的高频细节。提出了一些基于残差的结构来引导模型隐式地关注高频特征。然而，由于空间域度量的解释是有限的，因此如何验证这些人工细节的保真度仍然是一个问题。在本文中，我们提出了FreqNet，一个从频域角度直观的管道，来解决这个问题。受现有频域工作的启发，我们将图像转换为离散余弦变换（DCT）块，然后对其进行重构，得到DCT特征映射，作为模型的输入和目标。设计了一个专门的管道，我们进一步提出了一个频率损失函数，以适应我们频域任务的性质。我们提出的频域SISR方法能够清晰地学习高频信息，为SR图像提供逼真度和良好的感知质量。我们进一步观察到，我们的模型可以与其他空间超分辨率模型合并，以提高其原始SR输出的质量。
<details>	<summary>英文摘要</summary>	Single image super-resolution(SISR) is an ill-posed problem that aims to obtain high-resolution (HR) output from low-resolution (LR) input, during which extra high-frequency information is supposed to be added to improve the perceptual quality. Existing SISR works mainly operate in the spatial domain by minimizing the mean squared reconstruction error. Despite the high peak signal-to-noise ratios(PSNR) results, it is difficult to determine whether the model correctly adds desired high-frequency details. Some residual-based structures are proposed to guide the model to focus on high-frequency features implicitly. However, how to verify the fidelity of those artificial details remains a problem since the interpretation from spatial-domain metrics is limited. In this paper, we propose FreqNet, an intuitive pipeline from the frequency domain perspective, to solve this problem. Inspired by existing frequency-domain works, we convert images into discrete cosine transform (DCT) blocks, then reform them to obtain the DCT feature maps, which serve as the input and target of our model. A specialized pipeline is designed, and we further propose a frequency loss function to fit the nature of our frequency-domain task. Our SISR method in the frequency domain can learn the high-frequency information explicitly, provide fidelity and good perceptual quality for the SR images. We further observe that our model can be merged with other spatial super-resolution models to enhance the quality of their original SR output. </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 330、AGA-GAN：用于面部幻觉的属性引导注意生成对抗网络
- [ ] AGA-GAN: Attribute Guided Attention Generative Adversarial Network with U-Net for Face Hallucination 
时间：2021年11月20日                         第一作者：Abhishek Srivastava                       [链接](https://arxiv.org/abs/2111.10591).                     
## 摘要：人脸超分辨率方法的性能取决于其有效恢复人脸结构和显著特征的能力。尽管基于卷积神经网络和生成性对抗网络的方法在人脸幻觉任务中表现出令人印象深刻的性能，但使用与低分辨率图像相关的属性来提高性能的能力并不令人满意。在本文中，我们提出了一种属性引导注意生成对抗网络，该网络使用新的属性引导注意（AGA）模块来识别和聚焦图像中各种面部特征的生成过程。堆叠多个AGA模块可以恢复高层次和低层次的面部结构。我们设计了鉴别器，利用高分辨率图像与其对应的人脸属性标注之间的关系来学习鉴别特征。然后，我们探索使用基于U-Net的架构来改进现有的预测并合成进一步的面部细节。跨多个指标的大量实验表明，我们的AGA-GAN和AGA-GAN+U-Net框架优于其他几种尖端的面部幻觉最新方法。我们还证明了当每个属性描述符都未知时，我们的方法的可行性，从而建立了它在真实场景中的应用。
<details>	<summary>英文摘要</summary>	The performance of facial super-resolution methods relies on their ability to recover facial structures and salient features effectively. Even though the convolutional neural network and generative adversarial network-based methods deliver impressive performances on face hallucination tasks, the ability to use attributes associated with the low-resolution images to improve performance is unsatisfactory. In this paper, we propose an Attribute Guided Attention Generative Adversarial Network which employs novel attribute guided attention (AGA) modules to identify and focus the generation process on various facial features in the image. Stacking multiple AGA modules enables the recovery of both high and low-level facial structures. We design the discriminator to learn discriminative features exploiting the relationship between the high-resolution image and their corresponding facial attribute annotations. We then explore the use of U-Net based architecture to refine existing predictions and synthesize further facial details. Extensive experiments across several metrics show that our AGA-GAN and AGA-GAN+U-Net framework outperforms several other cutting-edge face hallucination state-of-the-art methods. We also demonstrate the viability of our method when every attribute descriptor is not known and thus, establishing its application in real-world scenarios. </details>
<details>	<summary>注释</summary>	27 pages, 9 Figures </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 329、基于人脸子空间先验的身份保持姿态鲁棒人脸幻觉
- [ ] Identity-Preserving Pose-Robust Face Hallucination Through Face Subspace Prior 
时间：2021年11月20日                         第一作者：Ali Abbasi                        [链接](https://arxiv.org/abs/2111.10634).                     
## 摘要：在过去的几十年中，人们做了许多尝试来解决从相应的低分辨率（LR）人脸图像中恢复高分辨率（HR）人脸图像的问题，这项任务通常被称为面部幻觉。尽管基于位置修补和深度学习的方法取得了令人印象深刻的性能，但大多数这些技术仍然无法恢复人脸的特定身份特征。前一组算法通常会产生模糊和过度平滑的输出，尤其是在存在更高级别的退化时，而后一组算法生成的人脸有时与输入图像中的个体完全不同。本文将介绍一种新的人脸超分辨率方法，该方法将幻觉人脸强制放置在由可用训练人脸构成的子空间中。因此，与大多数现有的面部幻觉技术相比，由于该面部子空间先验，执行重建有利于恢复特定于人的面部特征，而不仅仅是增加图像定量分数。此外，受三维人脸重建领域最新进展的启发，还提出了一种有效的三维字典对齐方案，通过该方案，该算法能够处理在非受控条件下拍摄的低分辨率人脸。在几个著名的人脸数据集上进行的大量实验中，所提出的算法通过生成详细和接近地面的真实结果显示了显著的性能，在定量和定性评估方面均优于最先进的人脸幻觉算法。
<details>	<summary>英文摘要</summary>	Over the past few decades, numerous attempts have been made to address the problem of recovering a high-resolution (HR) facial image from its corresponding low-resolution (LR) counterpart, a task commonly referred to as face hallucination. Despite the impressive performance achieved by position-patch and deep learning-based methods, most of these techniques are still unable to recover identity-specific features of faces. The former group of algorithms often produces blurry and oversmoothed outputs particularly in the presence of higher levels of degradation, whereas the latter generates faces which sometimes by no means resemble the individuals in the input images. In this paper, a novel face super-resolution approach will be introduced, in which the hallucinated face is forced to lie in a subspace spanned by the available training faces. Therefore, in contrast to the majority of existing face hallucination techniques and thanks to this face subspace prior, the reconstruction is performed in favor of recovering person-specific facial features, rather than merely increasing image quantitative scores. Furthermore, inspired by recent advances in the area of 3D face reconstruction, an efficient 3D dictionary alignment scheme is also presented, through which the algorithm becomes capable of dealing with low-resolution faces taken in uncontrolled conditions. In extensive experiments carried out on several well-known face datasets, the proposed algorithm shows remarkable performance by generating detailed and close to ground truth results which outperform the state-of-the-art face hallucination algorithms by significant margins both in quantitative and qualitative evaluations. </details>
<details>	<summary>注释</summary>	A shorter version of this paper has been submitted to IEEE Transactions on Image Processing </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 328、用于血管流动时间超分辨率模拟的电阻-时间联合调制点网
- [ ] Resistance-Time Co-Modulated PointNet for Temporal Super-Resolution Simulation of Blood Vessel Flows 
时间：2021年11月19日                         第一作者：Zhizheng Jiang                       [链接](https://arxiv.org/abs/2111.10372).                     
## 摘要：本文提出了一种新的用于血管流时间超分辨率模拟的深度学习框架，该框架利用低时间分辨率的模拟结果生成高时间分辨率的时变血管流模拟。在我们的框架中，用点云表示复杂的血管模型，用电阻时间辅助点网模型提取时变流场的时空特征，最后通过解码模块重构出高精度、高分辨率的流场。特别地，根据速度的矢量特性，提出了速度的振幅损失和方向损失。这两个指标的组合构成了网络训练的最终损失函数。文中给出了几个例子来说明所提出的血管流时域超分辨率模拟框架的有效性和有效性。
<details>	<summary>英文摘要</summary>	In this paper, a novel deep learning framework is proposed for temporal super-resolution simulation of blood vessel flows, in which a high-temporal-resolution time-varying blood vessel flow simulation is generated from a low-temporal-resolution flow simulation result. In our framework, point-cloud is used to represent the complex blood vessel model, resistance-time aided PointNet model is proposed for extracting the time-space features of the time-varying flow field, and finally we can reconstruct the high-accuracy and high-resolution flow field through the Decoder module. In particular, the amplitude loss and the orientation loss of the velocity are proposed from the vector characteristics of the velocity. And the combination of these two metrics constitutes the final loss function for network training. Several examples are given to illustrate the effective and efficiency of the proposed framework for temporal super-resolution simulation of blood vessel flows. </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 327、利用大规模视频转录推进高分辨率视频语言表达
- [ ] Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions 
时间：2021年11月19日                         第一作者：Hongwei Xue                       [链接](https://arxiv.org/abs/2111.10337).                     
## 摘要：我们研究联合视频和语言（VL）预训练，以实现跨模态学习，并从丰富的VL任务中获益。现有的研究要么提取低质量的视频特征，要么学习有限的文本嵌入，而忽略了高分辨率视频和多样化的语义可以显著改善跨模态学习。在本文中，我们提出了一种新的高分辨率和多样化的视频语言预训练模型（HD-VILA），用于许多视觉任务。特别是，我们收集了一个具有两个不同属性的大型数据集：1）第一个高分辨率数据集，包括371.5k小时的720p视频；2）最多样化的数据集，涵盖15个流行的YouTube类别。为了实现VL预训练，我们通过一个学习丰富时空特征的混合变换器和一个多模式变换器联合优化HD-VILA模型，该变换器将学习到的视频特征与各种文本进行交互。我们的预训练模型在10个VL理解任务和2个更新颖的文本到视觉生成任务中取得了最新成果。例如，我们的表现优于SOTA模型，相对增长38.5%R@1在零镜头MSR-VTT文本到视频检索任务中，53.6%在高分辨率数据集LSMDC中。学习到的VL嵌入也能有效地在文本到视觉操作和超分辨率任务中生成视觉愉悦和语义相关的结果。
<details>	<summary>英文摘要</summary>	We study joint video and language (VL) pre-training to enable cross-modality learning and benefit plentiful downstream VL tasks. Existing works either extract low-quality video features or learn limited text embedding, while neglecting that high-resolution videos and diversified semantics can significantly improve cross-modality learning. In this paper, we propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HD-VILA) for many visual tasks. In particular, we collect a large dataset with two distinct properties: 1) the first high-resolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular YouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA model by a hybrid Transformer that learns rich spatiotemporal features, and a multimodal Transformer that enforces interactions of the learned video features with diversified texts. Our pre-training model achieves new state-of-the-art results in 10 VL understanding tasks and 2 more novel text-to-visual generation tasks. For example, we outperform SOTA models with relative increases of 38.5% R@1 in zero-shot MSR-VTT text-to-video retrieval task, and 53.6% in high-resolution dataset LSMDC. The learned VL embedding is also effective in generating visually pleasing and semantically relevant results in text-to-visual manipulation and super-resolution tasks. </details>
<details>	<summary>邮件日期</summary>	2021年11月22日</details>

# 326、隐式表示函数的局部纹理估计
- [ ] Local Texture Estimator for Implicit Representation Function 
时间：2021年11月17日                         第一作者：Jaewon Lee                        [链接](https://arxiv.org/abs/2111.08918).                     
## 摘要：最近使用隐式神经函数的工作揭示了以任意分辨率表示图像的方法。然而，一个独立的多层感知器（MLP）在学习高频分量方面表现出有限的性能。在本文中，我们提出了一种局部纹理估计器（LTE），一种用于自然图像的主频估计器，使隐式函数能够在连续重建图像的同时捕获细节。当与深度超分辨率（SR）架构联合训练时，LTE能够在2D傅里叶空间中表征图像纹理。我们证明了基于LTE的神经函数在任意尺度下对所有数据集和所有尺度因子都优于现有的深度SR方法。此外，我们还证明了与以前的工作相比，我们的实现需要最短的运行时间。源代码将是开放的。
<details>	<summary>英文摘要</summary>	Recent works with an implicit neural function shed light on representing images in arbitrary resolution. However, a standalone multi-layer perceptron (MLP) shows limited performance in learning high-frequency components. In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, enabling an implicit function to capture fine details while reconstructing images in a continuous manner. When jointly trained with a deep super-resolution (SR) architecture, LTE is capable of characterizing image textures in 2D Fourier space. We show that an LTE-based neural function outperforms existing deep SR methods within an arbitrary-scale for all datasets and all scale factors. Furthermore, we demonstrate that our implementation takes the shortest running time compared to previous works. Source code will be open. </details>
<details>	<summary>邮件日期</summary>	2021年11月18日</details>

# 325、使用T-Tetromino像素的图像超分辨率
- [ ] Image Super-Resolution Using T-Tetromino Pixels 
时间：2021年11月17日                         第一作者：Simon Grosche                       [链接](https://arxiv.org/abs/2111.09013).                     
## 摘要：对于现代高分辨率成像传感器，在低照明条件下以及需要高帧速率的情况下执行像素装箱。为了恢复原始空间分辨率，可以应用单图像超分辨率技术进行放大。为了在放大后获得更高的图像质量，我们提出了一种新的使用tetromino形状像素的分块概念。为此，我们在文献中首次研究了使用tetromino像素的重建质量。与文献中针对传感器布局提出的使用不同类型的四溴甲烷不同，我们表明使用仅由四个T-四溴甲烷组成的小型重复单元就足够了。对于重建，我们使用了一个局部完全连接重建（LFCR）网络以及压缩感知领域的两种经典重建方法。与使用甚深超分辨率（VDSR）网络的传统单图像超分辨率相比，使用LFCR网络结合拟议的tetromino布局，我们在PSNR、SSIM和视觉上实现了卓越的图像质量。对于峰值信噪比，可获得高达+1.92 dB的增益。
<details>	<summary>英文摘要</summary>	For modern high-resolution imaging sensors, pixel binning is performed in low-lighting conditions and in case high frame rates are required. To recover the original spatial resolution, single-image super-resolution techniques can be applied for upscaling. To achieve a higher image quality after upscaling, we propose a novel binning concept using tetromino-shaped pixels. In doing so, we investigate the reconstruction quality using tetromino pixels for the first time in literature. Instead of using different types of tetrominoes as proposed in the literature for a sensor layout, we show that using a small repeating cell consisting of only four T-tetrominoes is sufficient. For reconstruction, we use a locally fully connected reconstruction (LFCR) network as well as two classical reconstruction methods from the field of compressed sensing. Using the LFCR network in combination with the proposed tetromino layout, we achieve superior image quality in terms of PSNR, SSIM, and visually compared to conventional single-image super-resolution using the very deep super-resolution (VDSR) network. For the PSNR, a gain of up to +1.92 dB is achieved. </details>
<details>	<summary>注释</summary>	8 pages, 9 figures, 2 tables. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2021年11月18日</details>

# 324、单帧结构照明显微镜超分辨率快速轻量化网络
- [ ] Fast and Light-Weight Network for Single Frame Structured Illumination Microscopy Super-Resolution 
时间：2021年11月17日                         第一作者：Xi Cheng                       [链接](https://arxiv.org/abs/2111.09103).                     
## 摘要：结构照明显微镜（SIM）是一种重要的超分辨率显微镜技术，它突破了衍射极限，增强了光学显微镜系统。随着生物学和医学工程的发展，在极弱光和短曝光环境下，对实时、稳健的SIM成像提出了更高的要求。现有SIM技术通常需要多个结构化照明帧来生成高分辨率图像。本文提出了一种基于深度学习的单帧结构照明显微镜（SF-SIM）。我们的SF-SIM只需要结构化照明帧的一次拍摄，与通常需要15次拍摄的传统SIM系统相比，可以产生类似的结果。在我们的SF-SIM中，我们提出了一种噪声估计器，该估计器可以有效地抑制图像中的噪声，并使我们的方法能够在弱光和短曝光环境下工作，而不需要堆叠多个帧来进行非局部去噪。我们还设计了一个带通注意模块，使我们的深层网络对频率变化更加敏感，并提高了成像质量。在获得类似结果时，我们提出的SF-SIM比传统SIM方法快近14倍。因此，我们的方法对微生物学和医学的发展具有重要价值。
<details>	<summary>英文摘要</summary>	Structured illumination microscopy (SIM) is an important super-resolution based microscopy technique that breaks the diffraction limit and enhances optical microscopy systems. With the development of biology and medical engineering, there is a high demand for real-time and robust SIM imaging under extreme low light and short exposure environments. Existing SIM techniques typically require multiple structured illumination frames to produce a high-resolution image. In this paper, we propose a single-frame structured illumination microscopy (SF-SIM) based on deep learning. Our SF-SIM only needs one shot of a structured illumination frame and generates similar results compared with the traditional SIM systems that typically require 15 shots. In our SF-SIM, we propose a noise estimator which can effectively suppress the noise in the image and enable our method to work under the low light and short exposure environment, without the need for stacking multiple frames for non-local denoising. We also design a bandpass attention module that makes our deep network more sensitive to the change of frequency and enhances the imaging quality. Our proposed SF-SIM is almost 14 times faster than traditional SIM methods when achieving similar results. Therefore, our method is significantly valuable for the development of microbiology and medicine. </details>
<details>	<summary>注释</summary>	9 pages </details>
<details>	<summary>邮件日期</summary>	2021年11月18日</details>

# 323、用于单图像超分辨率的图像特定卷积核调制
- [ ] Image-specific Convolutional Kernel Modulation for Single Image Super-resolution 
时间：2021年11月16日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2111.08362).                     
## 摘要：近年来，基于深度学习的超分辨方法取得了优异的性能，但主要集中于通过输入大量样本来训练单个广义深度网络。然而，从直觉上看，每幅图像都有其表示形式，并期望获得一个自适应模型。针对这个问题，我们提出了一种新的图像特定卷积核调制（IKM），通过利用图像或特征的全局上下文信息来生成注意权重，以自适应地调制卷积核，它的性能优于普通卷积和几种现有的注意机制，同时嵌入到最先进的体系结构中，无需任何附加参数。特别是，为了在小批量训练中优化IKM，我们引入了一种图像特定优化（IsO）算法，该算法比传统的小批量SGD优化更有效。此外，我们还研究了IKM对最先进体系结构的影响，并开发了一种新的具有U型剩余学习和沙漏密集块学习的主干结构，即术语U-沙漏密集网络（U-HDN），它是一种合适的体系结构，可以在理论和实验上最大限度地提高IKM的有效性。在单幅图像超分辨率上的大量实验表明，所提出的方法比现有的方法具有更好的性能。代码可从github.com/YuanfeiHuang/IKM获得。
<details>	<summary>英文摘要</summary>	Recently, deep-learning-based super-resolution methods have achieved excellent performances, but mainly focus on training a single generalized deep network by feeding numerous samples. Yet intuitively, each image has its representation, and is expected to acquire an adaptive model. For this issue, we propose a novel image-specific convolutional kernel modulation (IKM) by exploiting the global contextual information of image or feature to generate an attention weight for adaptively modulating the convolutional kernels, which outperforms the vanilla convolution and several existing attention mechanisms while embedding into the state-of-the-art architectures without any additional parameters. Particularly, to optimize our IKM in mini-batch training, we introduce an image-specific optimization (IsO) algorithm, which is more effective than the conventional mini-batch SGD optimization. Furthermore, we investigate the effect of IKM on the state-of-the-art architectures and exploit a new backbone with U-style residual learning and hourglass dense block learning, terms U-Hourglass Dense Network (U-HDN), which is an appropriate architecture to utmost improve the effectiveness of IKM theoretically and experimentally. Extensive experiments on single image super-resolution show that the proposed methods achieve superior performances over state-of-the-art methods. Code is available at github.com/YuanfeiHuang/IKM. </details>
<details>	<summary>注释</summary>	13 pages, submitted to IEEE Transactions, codes are available at https://github.com/YuanfeiHuang/IKM </details>
<details>	<summary>邮件日期</summary>	2021年11月17日</details>

# 322、一种用于高光谱图像超分辨率的潜在编码器耦合生成对抗网络（LE-GAN）
- [ ] A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for Efficient Hyperspectral Image Super-resolution 
时间：2021年11月16日                         第一作者：Yue Shi                       [链接](https://arxiv.org/abs/2111.08685).                     
## 摘要：真实高光谱图像（HSI）超分辨率（SR）技术旨在从低分辨率（LR）图像生成具有更高光谱和空间保真度的高分辨率（HR）HSI。生成性对抗网络（GAN）已被证明是一种有效的图像超分辨率深度学习框架。然而，现有GAN基模型的优化过程经常遇到模式崩溃问题，导致光谱空间不变重建能力有限。这可能会导致生成HSI上的光谱空间失真，尤其是在放大因子较大的情况下。为了缓解模式崩溃的问题，本工作提出了一种新的GAN模型，该模型与潜在编码器（LE-GAN）耦合，可以将生成的光谱空间特征从图像空间映射到潜在空间，并产生耦合分量来正则化生成的样本。本质上，我们将HSI视为嵌入在潜在空间中的高维流形。因此，GAN模型的优化转化为学习高分辨率HSI样本在潜在空间中的分布的问题，使生成的超分辨率HSI的分布更接近其原始高分辨率对应物的分布。我们对超分辨率的模型性能及其缓解模式崩溃的能力进行了实验评估。基于两个具有不同传感器的真实HSI数据集（即AVIRIS和UHD-185），针对各种放大因子和附加噪声级，对所提出的方法进行了测试和验证，并与最先进的超分辨率模型（即HyCoNet、LTTR、BAGAN、SR-GAN、WGAN）进行了比较。
<details>	<summary>英文摘要</summary>	Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to generate a high-resolution (HR) HSI with higher spectral and spatial fidelity from its low-resolution (LR) counterpart. The generative adversarial network (GAN) has proven to be an effective deep learning framework for image super-resolution. However, the optimisation process of existing GAN-based models frequently suffers from the problem of mode collapse, leading to the limited capacity of spectral-spatial invariant reconstruction. This may cause the spectral-spatial distortion on the generated HSI, especially with a large upscaling factor. To alleviate the problem of mode collapse, this work has proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can map the generated spectral-spatial features from the image space to the latent space and produce a coupling component to regularise the generated samples. Essentially, we treat an HSI as a high-dimensional manifold embedded in a latent space. Thus, the optimisation of GAN models is converted to the problem of learning the distributions of high-resolution HSI samples in the latent space, making the distributions of the generated super-resolution HSIs closer to those of their original high-resolution counterparts. We have conducted experimental evaluations on the model performance of super-resolution and its capability in alleviating mode collapse. The proposed approach has been tested and validated based on two real HSI datasets with different sensors (i.e. AVIRIS and UHD-185) for various upscaling factors and added noise levels, and compared with the state-of-the-art super-resolution models (i.e. HyCoNet, LTTR, BAGAN, SR- GAN, WGAN). </details>
<details>	<summary>注释</summary>	18 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月17日</details>

# 321、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年11月15日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
<details>	<summary>注释</summary>	Accepted by WACV 2022 Workshop WACI(Workshop on Applications of Computational Imaging) </details>
<details>	<summary>邮件日期</summary>	2021年11月16日</details>

# 320、小的还是远的？利用深超分辨率和高度数据进行空中动物监测
- [ ] Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance 
时间：2021年11月12日                         第一作者：Mowen Xue                       [链接](https://arxiv.org/abs/2111.06830).                     
## 摘要：高空飞行的无人机捕捉到的图像越来越多地用于评估全球生物多样性和动物种群动态。然而，尽管有超高分辨率摄像机，但具有挑战性的采集场景和空中图像中的微小动物描绘，迄今为止一直是成功应用计算机视觉探测器的限制因素。在本文中，我们首次将深部目标探测器与超分辨率技术和高度数据相结合来解决这个问题。特别是，我们表明，将基于整体注意网络的超分辨率方法和定制的高度数据利用网络集成到标准识别管道中可以显著提高现实环境中的检测效率。我们在两个公共大型空中捕获动物数据集SAVMAP和AED上评估了该系统。我们发现，对于这两个数据集，所提出的方法可以持续改善烧蚀基线和最先进的性能。此外，我们还对动物分辨率和检测性能之间的关系进行了系统分析。我们得出结论，超分辨率和高度知识利用技术可以显著提高不同环境下的基准，因此，在航空图像中检测微小分辨率的动物时，应常规使用。
<details>	<summary>英文摘要</summary>	Visuals captured by high-flying aerial drones are increasingly used to assess biodiversity and animal population dynamics around the globe. Yet, challenging acquisition scenarios and tiny animal depictions in airborne imagery, despite ultra-high resolution cameras, have so far been limiting factors for applying computer vision detectors successfully with high confidence. In this paper, we address the problem for the first time by combining deep object detectors with super-resolution techniques and altitude data. In particular, we show that the integration of a holistic attention network based super-resolution approach and a custom-built altitude data exploitation network into standard recognition pipelines can considerably increase the detection efficacy in real-world settings. We evaluate the system on two public, large aerial-capture animal datasets, SAVMAP and AED. We find that the proposed approach can consistently improve over ablated baselines and the state-of-the-art performance for both datasets. In addition, we provide a systematic analysis of the relationship between animal resolution and detection performance. We conclude that super-resolution and altitude knowledge exploitation techniques can significantly increase benchmarks across settings and, thus, should be used routinely when detecting minutely resolved animals in aerial imagery. </details>
<details>	<summary>注释</summary>	11 pages, 7 figures, 2 tables MSC-class: 65D19 </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 319、基于噪声和核函数的精细退化盲图像超分辨率建模
- [ ] Blind Image Super-resolution with Elaborate Degradation Modeling on Noise and Kernel 
时间：2021年11月11日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
<details>	<summary>邮件日期</summary>	2021年11月12日</details>

# 318、2019冠状病毒疾病数据集的解释分析与纠正
- [ ] Explanatory Analysis and Rectification of the Pitfalls in COVID-19 Datasets 
时间：2021年11月10日                         第一作者：Samyak Prajapati                       [链接](https://arxiv.org/abs/2111.05679).                     
## 摘要：自从2019冠状病毒疾病爆发以来，数百万人死于这种致命病毒。人们曾多次尝试设计一种能够检测病毒的自动化测试方法。全球不同的研究者已经提出了基于深度学习的方法来使用胸部X射线来检测COVID-19。然而，对于大多数研究人员使用的公开的胸部X射线数据集中是否存在偏差提出了疑问。在本文中，我们提出了一个两阶段的方法来解决这个问题。作为该方法第1阶段的一部分，已经进行了两个实验，以证明数据集中存在偏差。随后，在该方法的第2阶段提出了一种图像分割、超分辨率和基于CNN的管道以及不同的图像增强技术，以减少偏差的影响。InceptionResNetV2在胸部X射线图像上进行训练，该图像在通过第2阶段中提出的管道时经过直方图均衡化和伽马校正后得到增强，对于3级（正常、肺炎和新冠病毒-19）分类任务，最高准确率为90.47%。
<details>	<summary>英文摘要</summary>	Since the onset of the COVID-19 pandemic in 2020, millions of people have succumbed to this deadly virus. Many attempts have been made to devise an automated method of testing that could detect the virus. Various researchers around the globe have proposed deep learning based methodologies to detect the COVID-19 using Chest X-Rays. However, questions have been raised on the presence of bias in the publicly available Chest X-Ray datasets which have been used by the majority of the researchers. In this paper, we propose a 2 staged methodology to address this topical issue. Two experiments have been conducted as a part of stage 1 of the methodology to exhibit the presence of bias in the datasets. Subsequently, an image segmentation, super-resolution and CNN based pipeline along with different image augmentation techniques have been proposed in stage 2 of the methodology to reduce the effect of bias. InceptionResNetV2 trained on Chest X-Ray images that were augmented with Histogram Equalization followed by Gamma Correction when passed through the pipeline proposed in stage 2, yielded a top accuracy of 90.47% for 3-class (Normal, Pneumonia, and COVID-19) classification task. </details>
<details>	<summary>邮件日期</summary>	2021年11月11日</details>

# 317、RBSRICNN：通过迭代卷积神经网络的原始突发超分辨率
- [ ] RBSRICNN: Raw Burst Super-Resolution through Iterative Convolutional Neural Network 
时间：2021年11月10日                         第一作者：Rao Muhammad Umer                       [链接](https://arxiv.org/abs/2110.13217).                     
<details>	<summary>注释</summary>	Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月11日</details>

# 316、GDCA:GAN基单图像超分辨率双鉴别器和通道注意
- [ ] GDCA: GAN-based single image super resolution with Dual discriminators and Channel Attention 
时间：2021年11月09日                         第一作者：Thanh Nguyen                       [链接](https://arxiv.org/abs/2111.05014).                     
## 摘要：单图像超分辨率（SISR）是一个非常活跃的研究领域。本文通过使用基于GAN的双鉴别器方法并将其与注意机制相结合来解决SISR。实验结果表明，与其他传统方法相比，GDCA可以生成更清晰、更令人满意的图像。
<details>	<summary>英文摘要</summary>	Single Image Super-Resolution (SISR) is a very active research field. This paper addresses SISR by using a GAN-based approach with dual discriminators and incorporating it with an attention mechanism. The experimental results show that GDCA can generate sharper and high pleasing images compare to other conventional methods. </details>
<details>	<summary>邮件日期</summary>	2021年11月10日</details>

# 315、用于区域适应的合成磁共振图像：在胎儿脑组织分割中的应用
- [ ] Synthetic magnetic resonance images for domain adaptation: Application to fetal brain tissue segmentation 
时间：2021年11月08日                         第一作者：Priscille de Dumast                       [链接](https://arxiv.org/abs/2111.04737).                     
## 摘要：对子宫内发育中的人脑进行定量评估对于全面了解神经发育至关重要。因此，自动多组织胎儿大脑分割算法正在开发中，这反过来又需要对注释数据进行训练。然而，现有的带注释的胎儿大脑数据集在数量和异质性方面都是有限的，阻碍了稳健分割的领域适应策略。在此背景下，我们使用FaBiAN（一种胎儿大脑磁共振采集数字模型）模拟胎儿大脑的各种真实磁共振图像及其类别标签。我们证明，这些无成本生成并使用目标超分辨率技术进一步重建的多个合成注释数据可以成功地用于深度学习方法的领域自适应，该方法可分割七个脑组织。总的来说，分割的准确性显著提高，特别是在皮质灰质、白质、小脑、深灰质和脑干。
<details>	<summary>英文摘要</summary>	The quantitative assessment of the developing human brain in utero is crucial to fully understand neurodevelopment. Thus, automated multi-tissue fetal brain segmentation algorithms are being developed, which in turn require annotated data to be trained. However, the available annotated fetal brain datasets are limited in number and heterogeneity, hampering domain adaptation strategies for robust segmentation. In this context, we use FaBiAN, a Fetal Brain magnetic resonance Acquisition Numerical phantom, to simulate various realistic magnetic resonance images of the fetal brain along with its class labels. We demonstrate that these multiple synthetic annotated data, generated at no cost and further reconstructed using the target super-resolution technique, can be successfully used for domain adaptation of a deep learning method that segments seven brain tissues. Overall, the accuracy of the segmentation is significantly enhanced, especially in the cortical gray matter, the white matter, the cerebellum, the deep gray matter and the brain stem. </details>
<details>	<summary>注释</summary>	4 pages, 4 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2021年11月10日</details>

# 314、S3RP：对流扩散过程的自监督超分辨率和预测
- [ ] S3RP: Self-Supervised Super-Resolution and Prediction for Advection-Diffusion Process 
时间：2021年11月08日                         第一作者：Chulin Wang                       [链接](https://arxiv.org/abs/2111.04639).                     
## 摘要：我们提出了一个具有有限信息的对流扩散过程的超分辨率模型。虽然大多数超分辨率模型在训练中假设高分辨率（HR）地面真实数据，但在许多情况下，此类HR数据集不容易访问。在这里，我们证明了用基于物理的正则化训练的递归卷积网络能够在没有HR地面真值数据的情况下重建HR信息。此外，考虑到超分辨率问题的不适定性，我们采用了递归Wasserstein自动编码器来建模不确定性。
<details>	<summary>英文摘要</summary>	We present a super-resolution model for an advection-diffusion process with limited information. While most of the super-resolution models assume high-resolution (HR) ground-truth data in the training, in many cases such HR dataset is not readily accessible. Here, we show that a Recurrent Convolutional Network trained with physics-based regularizations is able to reconstruct the HR information without having the HR ground-truth data. Moreover, considering the ill-posed nature of a super-resolution problem, we employ the Recurrent Wasserstein Autoencoder to model the uncertainty. </details>
<details>	<summary>注释</summary>	9 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 313、基于空间角度分解核的纹理增强光场超分辨率
- [ ] Texture-enhanced Light Field Super-resolution with Spatio-Angular Decomposition Kernels 
时间：2021年11月07日                         第一作者：Zexi Hu                       [链接](https://arxiv.org/abs/2111.04069).                     
## 摘要：尽管最近卷积神经网络在光场超分辨率（LFSR）方面取得了一些进展，但由于4D LF数据的复杂性，光场（LF）图像的相关信息还没有得到充分的研究和利用。为了处理这样的高维LF数据，现有的LFSR方法大多是将其分解为低维，然后对分解后的子空间进行优化。然而，这些方法本身就有局限性，因为它们忽略了分解操作的特征，只利用有限的LF子空间集，最终无法全面提取空间角度特征，并导致性能瓶颈。为了克服这些局限性，本文深入挖掘了LF分解的潜力，提出了分解核的新概念。特别是，我们系统地将不同子空间的分解操作统一为一系列这样的分解核，这些核被纳入我们提出的分解核网络（DKNet）中，用于全面的空间角度特征提取。经实验验证，与最先进的方法相比，在2x、3x和4x LFSR标度下，所提出的DKNet分别实现了1.35 dB、0.83 dB和1.80 dB的峰值信噪比大幅改善。为了进一步提高DKNet的LFSR效果，在VGG网络的基础上，我们提出了一种LFVGG损失，以指导纹理增强DKNet（TE DKNet）生成丰富的真实纹理，显著提高LF图像的视觉质量。我们还提出了一个利用LF材料识别的间接评估指标，以客观评估LFVGG丢失带来的感知增强。
<details>	<summary>英文摘要</summary>	Despite the recent progress in light field super-resolution (LFSR) achieved by convolutional neural networks, the correlation information of light field (LF) images has not been sufficiently studied and exploited due to the complexity of 4D LF data. To cope with such high-dimensional LF data, most of the existing LFSR methods resorted to decomposing it into lower dimensions and subsequently performing optimization on the decomposed sub-spaces. However, these methods are inherently limited as they neglected the characteristics of the decomposition operations and only utilized a limited set of LF sub-spaces ending up failing to comprehensively extract spatio-angular features and leading to a performance bottleneck. To overcome these limitations, in this paper, we thoroughly discover the potentials of LF decomposition and propose a novel concept of decomposition kernels. In particular, we systematically unify the decomposition operations of various sub-spaces into a series of such decomposition kernels, which are incorporated into our proposed Decomposition Kernel Network (DKNet) for comprehensive spatio-angular feature extraction. The proposed DKNet is experimentally verified to achieve substantial improvements by 1.35 dB, 0.83 dB, and 1.80 dB PSNR in 2x, 3x and 4x LFSR scales, respectively, when compared with the state-of-the-art methods. To further improve DKNet in producing more visually pleasing LFSR results, based on the VGG network, we propose a LFVGG loss to guide the Texture-Enhanced DKNet (TE-DKNet) to generate rich authentic textures and enhance LF images' visual quality significantly. We also propose an indirect evaluation metric by taking advantage of LF material recognition to objectively assess the perceptual enhancement brought by the LFVGG loss. </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 312、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年11月07日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
<details>	<summary>注释</summary>	11 pages, 10 figures. Codes are available at https://github.com/haoyuc/A2N </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 311、遥感图像超分辨率和目标检测：基准和最新进展
- [ ] Remote Sensing Image Super-resolution and Object Detection: Benchmark and State of the Art 
时间：2021年11月05日                         第一作者：Yi Wang                       [链接](https://arxiv.org/abs/2111.03260).                     
## 摘要：在过去的二十年中，人们一直在努力开发遥感图像中的目标检测方法。在大多数情况下，遥感图像中用于小目标检测的数据集不足。许多研究人员使用场景分类数据集进行目标检测，这有其局限性；例如，在对象类别中，大型对象的数量超过小型对象。因此，它们缺乏多样性；这进一步影响了遥感图像中小目标检测器的检测性能。本文回顾了当前遥感图像的数据集和目标检测方法（基于深度学习）。我们还提出了一个大规模、公开的基准遥感超分辨率目标检测（RSSOD）数据集。RSSOD数据集由1759幅手工注释图像和22091幅空间分辨率为~0.05 m的甚高分辨率（VHR）图像组成。有五个类别，每个类别的标签频率不同。从卫星图像中提取图像块，包括真实图像畸变，如切向尺度畸变和倾斜畸变。我们还提出了一种新的多类循环超分辨率生成对抗网络（MCGR）和辅助YOLOv5检测器，对基于图像超分辨率的目标检测进行基准测试，并与现有的基于图像超分辨率（SR）的方法进行了比较。与目前最先进的NLSN方法相比，所提出的MCGR实现了图像SR的最先进性能，PSNR提高了1.2dB。对于五类、四类、两类和单类，MCGR分别获得了0.758、0.881、0.841和0.983的最佳目标检测图，分别超过了最先进的目标检测器YOLOv5、EfficientSet、更快的RCNN、SSD和RetinaNet的性能。
<details>	<summary>英文摘要</summary>	For the past two decades, there have been significant efforts to develop methods for object detection in Remote Sensing (RS) images. In most cases, the datasets for small object detection in remote sensing images are inadequate. Many researchers used scene classification datasets for object detection, which has its limitations; for example, the large-sized objects outnumber the small objects in object categories. Thus, they lack diversity; this further affects the detection performance of small object detectors in RS images. This paper reviews current datasets and object detection methods (deep learning-based) for remote sensing images. We also propose a large-scale, publicly available benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of very high resolution (VHR) images with a spatial resolution of ~0.05 m. There are five classes with varying frequencies of labels per class. The image patches are extracted from satellite images, including real image distortions such as tangential scale distortion and skew distortion. We also propose a novel Multi-class Cyclic super-resolution Generative adversarial network with Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark image super-resolution-based object detection and compare with the existing state-of-the-art methods based on image super-resolution (SR). The proposed MCGR achieved state-of-the-art performance for image SR with an improvement of 1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved best object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for five-class, four-class, two-class, and single classes, respectively surpassing the performance of the state-of-the-art object detectors YOLOv5, EfficientDet, Faster RCNN, SSD, and RetinaNet. </details>
<details>	<summary>注释</summary>	39 pages, 15 figures, 5 tables. Submitted to Elsevier journal for review </details>
<details>	<summary>邮件日期</summary>	2021年11月08日</details>

# 310、标准化流作为一个灵活的保真度目标，用于照片真实感超分辨率
- [ ] Normalizing Flow as a Flexible Fidelity Objective for Photo-Realistic Super-resolution 
时间：2021年11月05日                         第一作者：Andreas Lugmayr                       [链接](https://arxiv.org/abs/2111.03649).                     
## 摘要：超分辨率是一个不适定问题，其中地面真实高分辨率图像仅代表可行解空间中的一种可能性。然而，主要的范例是采用像素级的损失，如L_1，这将使预测趋于模糊的平均值。这将导致根本上相互冲突的目标，再加上对抗性损失，从而降低最终质量。我们通过重新研究L_1损失来解决这个问题，并表明它对应于一层条件流。受这一关系的启发，我们将通用流程作为基于保真度的L_1目标替代方案进行探索。我们证明，当与对抗性损失相结合时，深度流的灵活性会导致更好的视觉质量和一致性。我们对三个数据集和比例因子进行了广泛的用户研究，结果表明，我们的方法在照片真实感超分辨率方面优于最先进的方法。代码和经过培训的模型将在以下位置提供：git.io/AdFlow
<details>	<summary>英文摘要</summary>	Super-resolution is an ill-posed problem, where a ground-truth high-resolution image represents only one possibility in the space of plausible solutions. Yet, the dominant paradigm is to employ pixel-wise losses, such as L_1, which drive the prediction towards a blurry average. This leads to fundamentally conflicting objectives when combined with adversarial losses, which degrades the final quality. We address this issue by revisiting the L_1 loss and show that it corresponds to a one-layer conditional flow. Inspired by this relation, we explore general flows as a fidelity-based alternative to the L_1 objective. We demonstrate that the flexibility of deeper flows leads to better visual quality and consistency when combined with adversarial losses. We conduct extensive user studies for three datasets and scale factors, where our approach is shown to outperform state-of-the-art methods for photo-realistic super-resolution. Code and trained models will be available at: git.io/AdFlow </details>
<details>	<summary>邮件日期</summary>	2021年11月08日</details>

# 309、具有辐射一致性损失的Sentinel-2多光谱多图像超分辨率及其对建筑物轮廓的影响
- [ ] Multi-Spectral Multi-Image Super-Resolution of Sentinel-2 with Radiometric Consistency Losses and Its Effect on Building Delineation 
时间：2021年11月05日                         第一作者：Muhammed Razzak                       [链接](https://arxiv.org/abs/2111.03231).                     
## 摘要：高分辨率遥感图像用于广泛的任务，包括目标探测和分类。然而，高分辨率图像价格昂贵，而低分辨率图像通常免费提供，公众可以将其用于各种社会公益应用。为此，我们策划了一个多光谱多图像超分辨率数据集，使用SpaceNet 7 challenge的PlanetScope图像作为高分辨率参考，并对同一图像和低分辨率图像进行多次Sentinel-2重访。我们介绍了首次将多图像超分辨率（MISR）应用于多光谱遥感图像的结果。此外，我们在MISR模型中引入了辐射一致性模块，以保持Sentinel-2传感器的高辐射分辨率。我们表明，在一系列图像保真度指标上，MISR优于单图像超分辨率和其他基线。此外，我们还首次评估了多图像超分辨率在建筑物描绘中的效用，结果表明，利用多个图像可以在这些下游任务中获得更好的性能。
<details>	<summary>英文摘要</summary>	High resolution remote sensing imagery is used in broad range of tasks, including detection and classification of objects. High-resolution imagery is however expensive, while lower resolution imagery is often freely available and can be used by the public for range of social good applications. To that end, we curate a multi-spectral multi-image super-resolution dataset, using PlanetScope imagery from the SpaceNet 7 challenge as the high resolution reference and multiple Sentinel-2 revisits of the same imagery as the low-resolution imagery. We present the first results of applying multi-image super-resolution (MISR) to multi-spectral remote sensing imagery. We, additionally, introduce a radiometric consistency module into MISR model the to preserve the high radiometric resolution of the Sentinel-2 sensor. We show that MISR is superior to single-image super-resolution and other baselines on a range of image fidelity metrics. Furthermore, we conduct the first assessment of the utility of multi-image super-resolution on building delineation, showing that utilising multiple images results in better performance in these downstream tasks. </details>
<details>	<summary>邮件日期</summary>	2021年11月08日</details>

# 308、真实世界图像超分辨率的频率感知物理退化模型
- [ ] Frequency-Aware Physics-Inspired Degradation Model for Real-World Image Super-Resolution 
时间：2021年11月05日                         第一作者：Zhenxing Dong                       [链接](https://arxiv.org/abs/2111.03301).                     
## 摘要：当前基于学习的单图像超分辨率（SISR）算法在实际数据上表现不佳，这是因为假设的退化过程与实际场景中的退化过程存在偏差。传统的退化过程考虑在高分辨率（HR）图像上应用模糊、噪声和下采样（典型的双三次下采样）来合成低分辨率（LR）对应物。然而，退化建模方面的工作很少考虑光学成像系统的物理方面。在本文中，我们对成像系统进行了光学分析，并在空间频域中研究了真实世界LR-HR对的特性。我们通过考虑hoppics和sensordegration，建立了一个真实世界的物理启发的degradation模型；成像系统的物理退化被建模为低通滤波器，其截止频率由物体距离、透镜焦距和图像传感器的像素大小决定。特别是，我们建议使用卷积神经网络（CNN）来学习真实世界退化过程的截止频率。然后将学习到的网络用于从未配对的HR图像合成LR图像。合成HR-LR图像对随后用于训练SISR网络。我们评估了该退化模型在不同成像系统拍摄的真实图像上的有效性和泛化能力。实验结果表明，使用我们的合成数据训练的SISR网络相对于使用传统退化模型的网络表现良好。此外，我们的结果与使用真实世界LR-HR对训练的同一网络获得的结果相当，这在真实场景中很难获得。
<details>	<summary>英文摘要</summary>	Current learning-based single image super-resolution (SISR) algorithms underperform on real data due to the deviation in the assumed degrada-tion process from that in the real-world scenario. Conventional degradation processes consider applying blur, noise, and downsampling (typicallybicubic downsampling) on high-resolution (HR) images to synthesize low-resolution (LR) counterparts. However, few works on degradation modelling have taken the physical aspects of the optical imaging system intoconsideration. In this paper, we analyze the imaging system optically andexploit the characteristics of the real-world LR-HR pairs in the spatial frequency domain. We formulate a real-world physics-inspired degradationmodel by considering bothopticsandsensordegradation; The physical degradation of an imaging system is modelled as a low-pass filter, whose cut-off frequency is dictated by the object distance, the focal length of thelens, and the pixel size of the image sensor. In particular, we propose to use a convolutional neural network (CNN) to learn the cutoff frequency of real-world degradation process. The learned network is then applied to synthesize LR images from unpaired HR images. The synthetic HR-LR image pairs are later used to train an SISR network. We evaluatethe effectiveness and generalization capability of the proposed degradation model on real-world images captured by different imaging systems. Experimental results showcase that the SISR network trained by using our synthetic data performs favorably against the network using the traditional degradation model. Moreover, our results are comparable to that obtained by the same network trained by using real-world LR-HR pairs, which are challenging to obtain in real scenes. </details>
<details>	<summary>注释</summary>	18 pages,8 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月08日</details>

# 307、基于深度学习的六边形采样图像重采样与超分辨率
- [ ] Resampling and super-resolution of hexagonally sampled images using deep learning 
时间：2021年11月03日                         第一作者：Dylan Flaute                       [链接](https://arxiv.org/abs/2111.02520).                     
## 摘要：超分辨率（SR）旨在提高图像的分辨率。应用包括安全、医学成像和物体识别。我们提出了一种基于深度学习的SR系统，该系统以六边形采样的低分辨率图像作为输入，生成矩形采样的SR图像作为输出。对于培训和测试，我们使用了一个真实的观察模型，其中包括衍射导致的光学退化和探测器集成导致的传感器退化。我们的SR方法首先使用非均匀插值对观测到的六边形图像进行部分上采样，并将其转换为矩形网格。然后，我们利用为SR设计的最先进的卷积神经网络（CNN）架构，称为剩余通道注意网络（RCAN）。特别是，我们使用RCAN进一步增加采样并恢复图像，以生成最终的SR图像估计。我们证明了该系统优于直接将RCAN应用于具有等效采样密度的矩形采样LR图像。六边形取样的理论优势是众所周知的。然而，据我们所知，根据现代加工技术（如RCAN SR），六角取样的实际好处迄今尚未得到验证。我们的SR系统在六边形SR中使用改进的RCAN时，展示了六边形采样图像的显著优势。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) aims to increase the resolution of imagery. Applications include security, medical imaging, and object recognition. We propose a deep learning-based SR system that takes a hexagonally sampled low-resolution image as an input and generates a rectangularly sampled SR image as an output. For training and testing, we use a realistic observation model that includes optical degradation from diffraction and sensor degradation from detector integration. Our SR approach first uses non-uniform interpolation to partially upsample the observed hexagonal imagery and convert it to a rectangular grid. We then leverage a state-of-the-art convolutional neural network (CNN) architecture designed for SR known as Residual Channel Attention Network (RCAN). In particular, we use RCAN to further upsample and restore the imagery to produce the final SR image estimate. We demonstrate that this system is superior to applying RCAN directly to rectangularly sampled LR imagery with equivalent sample density. The theoretical advantages of hexagonal sampling are well known. However, to the best of our knowledge, the practical benefit of hexagonal sampling in light of modern processing techniques such as RCAN SR is heretofore untested. Our SR system demonstrates a notable advantage of hexagonally sampled imagery when employing a modified RCAN for hexagonal SR. </details>
<details>	<summary>注释</summary>	31 pages, 16 figures, 5 tables. \c{opyright} 2021 Society of Photo-Optical Instrumentation Engineers (SPIE) MSC-class: 68T45 ACM-class: I.4.4; I.4.10 Journal-ref: Optical Engineering 60(10), 103105 (29 October 2021) DOI: 10.1117/1.OE.60.10.103105 </details>
<details>	<summary>邮件日期</summary>	2021年11月05日</details>

# 306、AdaPool：用于信息保留下采样的指数自适应池
- [ ] AdaPool: Exponential Adaptive Pooling for Information-Retaining Downsampling 
时间：2021年11月01日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2111.00772).                     
## 摘要：池层是卷积神经网络（CNN）的基本构造块，可减少计算开销并增加进行卷积运算的感受域。他们的目标是产生与输入体积非常相似的降采样体积，同时在理想情况下，在计算和内存效率方面也是如此。共同满足这两个要求是一个挑战。为此，我们提出了一种自适应的指数加权池方法$\textit{adaPool}$。我们提出的方法使用两组池核的参数化融合，分别基于Dice-Sorensen系数的指数和指数最大值。adaPool的一个关键特性是它的双向性。与普通池方法不同，权重可用于对下采样激活图进行上采样。我们将此方法命名为$\textit{adaunpol}$。我们将演示adaPool如何通过一系列任务（包括图像和视频分类以及对象检测）改进细节的保存。然后，我们评估Adampool在图像和视频帧超分辨率和帧插值任务上的性能。为了进行基准测试，我们引入了$\textit{Inter4K}$，一种新型的高质量、高帧速率视频数据集。我们的组合实验表明，adaPool在任务和主干架构之间系统性地实现了更好的结果，同时引入了较小的额外计算和内存开销。
<details>	<summary>英文摘要</summary>	Pooling layers are essential building blocks of Convolutional Neural Networks (CNNs) that reduce computational overhead and increase the receptive fields of proceeding convolutional operations. They aim to produce downsampled volumes that closely resemble the input volume while, ideally, also being computationally and memory efficient. It is a challenge to meet both requirements jointly. To this end, we propose an adaptive and exponentially weighted pooling method named $\textit{adaPool}$. Our proposed method uses a parameterized fusion of two sets of pooling kernels that are based on the exponent of the Dice-Sorensen coefficient and the exponential maximum, respectively. A key property of adaPool is its bidirectional nature. In contrast to common pooling methods, weights can be used to upsample a downsampled activation map. We term this method $\textit{adaUnPool}$. We demonstrate how adaPool improves the preservation of detail through a range of tasks including image and video classification and object detection. We then evaluate adaUnPool on image and video frame super-resolution and frame interpolation tasks. For benchmarking, we introduce $\textit{Inter4K}$, a novel high-quality, high frame-rate video dataset. Our combined experiments demonstrate that adaPool systematically achieves better results across tasks and backbone architectures, while introducing a minor additional computational and memory overhead. </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 305、FREGAN：生成对抗网络在提高视频帧速率中的应用
- [ ] FREGAN : an application of generative adversarial networks in enhancing the frame rate of videos 
时间：2021年11月01日                         第一作者：Rishik Mishra                       [链接](https://arxiv.org/abs/2111.01105).                     
## 摘要：数字视频是单个帧的集合，而流式传输视频时，场景利用每个帧的时间片。高刷新率和高帧速率是所有高科技应用的需求。由于高刷新率，视频中的动作跟踪变得更容易，游戏应用中的运动变得更平滑。它提供了更快的响应，因为屏幕上显示的每个帧之间的时间更短。FREGAN（帧速率增强生成对抗网络）模型被提出，该模型基于过去帧序列预测视频序列的未来帧。在本文中，我们研究了GAN模型，并提出了用于视频中帧速率增强的FREGAN模型。我们在建议的FREGAN中使用Huber损耗作为损耗函数。它在超分辨率方面提供了极好的结果，我们也尝试在帧速率增强应用中实现这种性能。我们在标准数据集（UCF101和RFree500）上验证了所提出模型的有效性。实验结果表明，该模型的峰值信噪比（PSNR）为34.94，结构相似性指数（SSIM）为0.95。
<details>	<summary>英文摘要</summary>	A digital video is a collection of individual frames, while streaming the video the scene utilized the time slice for each frame. High refresh rate and high frame rate is the demand of all high technology applications. The action tracking in videos becomes easier and motion becomes smoother in gaming applications due to the high refresh rate. It provides a faster response because of less time in between each frame that is displayed on the screen. FREGAN (Frame Rate Enhancement Generative Adversarial Network) model has been proposed, which predicts future frames of a video sequence based on a sequence of past frames. In this paper, we investigated the GAN model and proposed FREGAN for the enhancement of frame rate in videos. We have utilized Huber loss as a loss function in the proposed FREGAN. It provided excellent results in super-resolution and we have tried to reciprocate that performance in the application of frame rate enhancement. We have validated the effectiveness of the proposed model on the standard datasets (UCF101 and RFree500). The experimental outcomes illustrate that the proposed model has a Peak signal-to-noise ratio (PSNR) of 34.94 and a Structural Similarity Index (SSIM) of 0.95. </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 304、基于互相关嵌入全卷积网络的稳健单像素粒子图像测速
- [ ] A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded 
时间：2021年10月31日                         第一作者：Qi Gao                       [链接](https://arxiv.org/abs/2111.00395).                     
## 摘要：粒子图像测速技术（PIV）在实验流体动力学中是必不可少的。在当前的工作中，我们提出了一种新的速度场估计范式，它实现了深度学习方法和传统互相关方法的协同结合。具体来说，深度学习方法用于优化和纠正粗略的速度猜测，以实现超分辨率计算。互相关方法提供了基于粗相关和大询问窗口的初始速度场。作为参考，粗略的速度猜测有助于提高算法的鲁棒性。这种嵌入互相关的完全卷积网络被称为CC-FCN。CC-FCN有两种类型的输入层，一种用于粒子图像，另一种用于使用粗分辨率互相关计算的初始速度场。首先，两个金字塔模块分别提取粒子图像和初始速度场的特征。然后，融合模块适当地融合这些特征。最后，CC-FCN通过一系列反褶积层实现超分辨率计算，得到单像素速度场。当考虑监督学习策略时，生成包括地面真实流体运动的合成数据集来训练网络参数。合成和真实实验PIV数据集用于测试训练后的神经网络的精度、精度、空间分辨率和鲁棒性。测试结果表明，CC-FCN的这些属性与其他PIV算法相比有了进一步的改进。因此，所提出的模型可以为PIV实验提供具有竞争力和鲁棒性的估计。
<details>	<summary>英文摘要</summary>	Particle image velocimetry (PIV) is essential in experimental fluid dynamics. In the current work, we propose a new velocity field estimation paradigm, which achieves a synergetic combination of the deep learning method and the traditional cross-correlation method. Specifically, the deep learning method is used to optimize and correct a coarse velocity guess to achieve a super-resolution calculation. And the cross-correlation method provides the initial velocity field based on a coarse correlation with a large interrogation window. As a reference, the coarse velocity guess helps with improving the robustness of the proposed algorithm. This fully convolutional network with embedded cross-correlation is named as CC-FCN. CC-FCN has two types of input layers, one is for the particle images, and the other is for the initial velocity field calculated using cross-correlation with a coarse resolution. Firstly, two pyramidal modules extract features of particle images and initial velocity field respectively. Then the fusion module appropriately fuses these features. Finally, CC-FCN achieves the super-resolution calculation through a series of deconvolution layers to obtain the single-pixel velocity field. As the supervised learning strategy is considered, synthetic data sets including ground-truth fluid motions are generated to train the network parameters. Synthetic and real experimental PIV data sets are used to test the trained neural network in terms of accuracy, precision, spatial resolution and robustness. The test results show that these attributes of CC-FCN are further improved compared with those of other tested PIV algorithms. The proposed model could therefore provide competitive and robust estimations for PIV experiments. </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 303、用于参数图像恢复问题的函数神经网络
- [ ] Functional Neural Networks for Parametric Image Restoration Problems 
时间：2021年10月30日                         第一作者：Fangzhou Luo                       [链接](https://arxiv.org/abs/2111.00361).                     
## 摘要：几乎每个图像恢复问题都有一个密切相关的参数，如超分辨率中的比例因子、图像去噪中的噪声级以及JPEG去块中的质量因子。尽管由于深度神经网络的发展，最近对图像恢复问题的研究取得了巨大的成功，但它们以一种简单的方式处理所涉及的参数。大多数以前的研究者要么将具有不同参数水平的问题作为独立的任务来处理，要么为每个参数水平训练一个特定的模型；或者干脆忽略参数，为所有参数级别训练单个模型。这两种流行的方法各有缺点。前者在计算上效率低下，后者在性能上效率低下。在这项工作中，我们提出了一种称为函数神经网络（FuncNet）的新系统来解决单一模型的参数图像恢复问题。与普通神经网络不同，FuncNet的最小概念元素不再是浮点变量，而是问题参数的函数。此功能使其对于参数化问题既高效又有效。我们将FuncNet应用于超分辨率、图像去噪和JPEG去块。实验结果表明，与现有技术相比，我们的FuncNet在所有三种参数化图像恢复任务上都具有优越性。
<details>	<summary>英文摘要</summary>	Almost every single image restoration problem has a closely related parameter, such as the scale factor in super-resolution, the noise level in image denoising, and the quality factor in JPEG deblocking. Although recent studies on image restoration problems have achieved great success due to the development of deep neural networks, they handle the parameter involved in an unsophisticated way. Most previous researchers either treat problems with different parameter levels as independent tasks, and train a specific model for each parameter level; or simply ignore the parameter, and train a single model for all parameter levels. The two popular approaches have their own shortcomings. The former is inefficient in computing and the latter is ineffective in performance. In this work, we propose a novel system called functional neural network (FuncNet) to solve a parametric image restoration problem with a single model. Unlike a plain neural network, the smallest conceptual element of our FuncNet is no longer a floating-point variable, but a function of the parameter of the problem. This feature makes it both efficient and effective for a parametric problem. We apply FuncNet to super-resolution, image denoising, and JPEG deblocking. The experimental results show the superiority of our FuncNet on all three parametric image restoration tasks over the state of the arts. </details>
<details>	<summary>注释</summary>	NeurIPS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 302、连续尺度超分辨率的尺度感知动态网络
- [ ] Scale-Aware Dynamic Network for Continuous-Scale Super-Resolution 
时间：2021年10月29日                         第一作者：Hanlin Wu                       [链接](https://arxiv.org/abs/2110.15655).                     
## 摘要：随着深度学习技术的发展，具有固定和离散尺度因子的单图像超分辨率（SR）技术取得了巨大的进步。然而，连续尺度SR，其目的是使用单一模型来处理任意（整数或非整数）尺度因子，仍然是一项具有挑战性的任务。现有的SR模型一般采用静态卷积来提取特征，无法有效感知尺度因子的变化，从而限制了多尺度SR任务的泛化性能。此外，现有的连续尺度上采样模块没有充分利用多尺度特征，并面临SR结果中棋盘效应和高计算复杂度等问题。为了解决上述问题，我们提出了一种用于连续尺度SR的尺度感知动态网络（SADN）。首先，我们提出了一种用于多个不同尺度SR任务特征学习的尺度感知动态卷积（SAD Conv）层。SAD Conv层可以根据比例因子自适应调整多个卷积核的注意权重，从而在不增加计算量的情况下提高模型的表达能力。其次，我们设计了一个具有多双线性局部隐式函数（MBLIF）的连续尺度上采样模块（CSUM），用于任意尺度上采样。CSUM构造多个尺度逐渐增大的特征空间来逼近图像的连续特征表示，然后MBLIF充分利用多尺度特征将任意坐标映射到高分辨率空间中的RGB值。我们使用各种基准评估我们的SADN。实验结果表明，CSUM可以取代以前的固定尺度上采样层，在保持性能的同时获得连续尺度的SR网络。我们的SADN使用更少的参数，性能优于最先进的SR方法。
<details>	<summary>英文摘要</summary>	Single-image super-resolution (SR) with fixed and discrete scale factors has achieved great progress due to the development of deep learning technology. However, the continuous-scale SR, which aims to use a single model to process arbitrary (integer or non-integer) scale factors, is still a challenging task. The existing SR models generally adopt static convolution to extract features, and thus unable to effectively perceive the change of scale factor, resulting in limited generalization performance on multi-scale SR tasks. Moreover, the existing continuous-scale upsampling modules do not make full use of multi-scale features and face problems such as checkerboard artifacts in the SR results and high computational complexity. To address the above problems, we propose a scale-aware dynamic network (SADN) for continuous-scale SR. First, we propose a scale-aware dynamic convolutional (SAD-Conv) layer for the feature learning of multiple SR tasks with various scales. The SAD-Conv layer can adaptively adjust the attention weights of multiple convolution kernels based on the scale factor, which enhances the expressive power of the model with a negligible extra computational cost. Second, we devise a continuous-scale upsampling module (CSUM) with the multi-bilinear local implicit function (MBLIF) for any-scale upsampling. The CSUM constructs multiple feature spaces with gradually increasing scales to approximate the continuous feature representation of an image, and then the MBLIF makes full use of multi-scale features to map arbitrary coordinates to RGB values in high-resolution space. We evaluate our SADN using various benchmarks. The experimental results show that the CSUM can replace the previous fixed-scale upsampling layers and obtain a continuous-scale SR network while maintaining performance. Our SADN uses much fewer parameters and outperforms the state-of-the-art SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年11月01日</details>

# 301、基于隐式神经表示的三维磁共振图像任意尺度超分辨率方法
- [ ] An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic Resonance Image using Implicit Neural Representation 
时间：2021年10月29日                         第一作者：Qing Wu                       [链接](https://arxiv.org/abs/2110.14476).                     
<details>	<summary>注释</summary>	18 pages, 13 figures, 4 tables; submitted to Medical Image Analysis </details>
<details>	<summary>邮件日期</summary>	2021年11月01日</details>

# 300、MEGAN：用于时空视频超分辨率的内存增强型图形注意力网络
- [ ] MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution 
时间：2021年10月28日                         第一作者：Chenyu You                       [链接](https://arxiv.org/abs/2110.15327).                     
## 摘要：空时视频超分辨率（STVSR）的目标是从相应的低帧率、低分辨率视频序列中构造出一个高空时分辨率的视频序列。灵感来自最近的成功考虑时空信息的时空超分辨率，我们的主要目标是在这项工作中充分考虑空间和时间的相关性内的视频序列的快速动态事件。为此，我们提出了一种新的用于时空视频超分辨率的单级记忆增强图形注意网络（MEGA）。具体地说，我们构建了一个新的长程记忆图聚合（LMGA）模块来动态捕获特征映射的通道维度上的相关性，并自适应地聚合通道特征以增强特征表示。我们引入了一个非局部残差块，它使每个通道特征都能参与全局空间层次特征。此外，我们采用渐进式融合模块，通过广泛利用多帧的时空相关性，进一步增强表示能力。实验结果表明，与现有方法相比，该方法在定量和可视化方面取得了更好的效果。
<details>	<summary>英文摘要</summary>	Space-time video super-resolution (STVSR) aims to construct a high space-time resolution video sequence from the corresponding low-frame-rate, low-resolution video sequence. Inspired by the recent success to consider spatial-temporal information for space-time super-resolution, our main goal in this work is to take full considerations of spatial and temporal correlations within the video sequences of fast dynamic events. To this end, we propose a novel one-stage memory enhanced graph attention network (MEGAN) for space-time video super-resolution. Specifically, we build a novel long-range memory graph aggregation (LMGA) module to dynamically capture correlations along the channel dimensions of the feature maps and adaptively aggregate channel features to enhance the feature representations. We introduce a non-local residual block, which enables each channel-wise feature to attend global spatial hierarchical features. In addition, we adopt a progressive fusion module to further enhance the representation ability by extensively exploiting spatial-temporal correlations from multiple frames. Experiment results demonstrate that our method achieves better results compared with the state-of-the-art methods quantitatively and visually. </details>
<details>	<summary>邮件日期</summary>	2021年10月29日</details>

# 299、用于单图像超分辨率的高效变换器
- [ ] Efficient Transformer for Single Image Super-Resolution 
时间：2021年10月28日                         第一作者：Zhisheng Lu                       [链接](https://arxiv.org/abs/2108.11084).                     
<details>	<summary>邮件日期</summary>	2021年10月29日</details>

# 298、EFENet：具有增强流估计的基于参考的视频超分辨率
- [ ] EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation 
时间：2021年10月28日                         第一作者：Yaping Zhao                       [链接](https://arxiv.org/abs/2110.07797).                     
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月29日</details>

# 297、使用元注意层提高超分辨率性能
- [ ] Improving Super-Resolution Performance using Meta-Attention Layers 
时间：2021年10月27日                         第一作者：Matthew Aquilina                       [链接](https://arxiv.org/abs/2110.14638).                     
## 摘要：卷积神经网络（CNN）在许多超分辨率（SR）和图像恢复任务中取得了令人印象深刻的结果。虽然许多这样的网络可以仅使用原始像素级信息来提高低分辨率（LR）图像的分辨率，但SR的病态性质使得对经历了多次不同退化的图像进行精确的超分辨率处理变得困难。描述退化过程的附加信息（元数据）（如应用的模糊内核、压缩级别等）可以指导网络以更高的保真度超分辨率LR图像到原始源。以前试图向SR网络提供降级参数的尝试确实能够在许多情况下提高性能。然而，由于许多SR网络的完全卷积性质，大多数元数据融合方法要么需要完全的架构更改，要么需要增加额外的复杂性。因此，如果不进行大量的设计修改，这些方法很难引入到任意SR网络中。在本文中，我们介绍了元注意，这是一种简单的机制，允许任何SR CNN利用相关退化参数中可用的信息。该机制通过将元数据转换为通道注意向量来发挥作用，而通道注意向量又选择性地调节网络的特征映射。将元注意纳入SR网络是很简单的，因为它不需要特定类型的架构才能正常工作。广泛的测试表明，当提供相关的退化元数据时，元关注可以持续提高最先进（SOTA）网络的像素级精度。对于峰值信噪比，模糊/下采样（X4）图像的增益分别为0.2969 dB（平均值）和0.3320 dB（SOTA通用和面部SR模型）。
<details>	<summary>英文摘要</summary>	Convolutional Neural Networks (CNNs) have achieved impressive results across many super-resolution (SR) and image restoration tasks. While many such networks can upscale low-resolution (LR) images using just the raw pixel-level information, the ill-posed nature of SR can make it difficult to accurately super-resolve an image which has undergone multiple different degradations. Additional information (metadata) describing the degradation process (such as the blur kernel applied, compression level, etc.) can guide networks to super-resolve LR images with higher fidelity to the original source. Previous attempts at informing SR networks with degradation parameters have indeed been able to improve performance in a number of scenarios. However, due to the fully-convolutional nature of many SR networks, most of these metadata fusion methods either require a complete architectural change, or necessitate the addition of significant extra complexity. Thus, these approaches are difficult to introduce into arbitrary SR networks without considerable design alterations. In this paper, we introduce meta-attention, a simple mechanism which allows any SR CNN to exploit the information available in relevant degradation parameters. The mechanism functions by translating the metadata into a channel attention vector, which in turn selectively modulates the network's feature maps. Incorporating meta-attention into SR networks is straightforward, as it requires no specific type of architecture to function correctly. Extensive testing has shown that meta-attention can consistently improve the pixel-level accuracy of state-of-the-art (SOTA) networks when provided with relevant degradation metadata. For PSNR, the gain on blurred/downsampled (X4) images is of 0.2969 dB (on average) and 0.3320 dB for SOTA general and face SR models, respectively. </details>
<details>	<summary>注释</summary>	Accepted for publication in the IEEE Signal Processing Letters. This is the accepted version of the paper, for the final formatted version and supplementary information, please visit the IEEE's publication at the linked DOI DOI: 10.1109/LSP.2021.3116518 </details>
<details>	<summary>邮件日期</summary>	2021年10月29日</details>

# 296、基于隐式神经表示的三维磁共振图像任意尺度超分辨率方法
- [ ] An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic Resonance Image using Implicit Neural Representation 
时间：2021年10月27日                         第一作者：Qing Wu                       [链接](https://arxiv.org/abs/2110.14476).                     
## 摘要：高分辨率（HR）医学图像提供了丰富的解剖结构细节，有助于早期准确诊断。在MRI中，受硬件容量、扫描时间和患者协作能力的限制，各向同性3D HR图像采集通常要求较长的扫描时间，导致较小的空间覆盖和较低的信噪比。最近的研究表明，利用深度卷积神经网络，可以通过单图像超分辨率（SISR）算法从低分辨率（LR）输入中恢复各向同性HR MR图像。然而，大多数现有的SISR方法倾向于接近LR和HR图像之间的特定比例投影，因此这些方法只能处理固定的采样率。为了实现不同的上采样率，必须分别建立多个SR网络，这是非常耗时和资源密集的。在本文中，我们提出了ArSSR，一种用于恢复3D HR MR图像的任意尺度超分辨率方法。在ArSSR模型中，具有不同放大率的HR图像的重建被定义为从观察到的LR图像学习连续的隐式体素函数。然后，从一组成对的HR-LR训练示例中，通过深度神经网络将SR任务转换为隐式体素函数。ArSSR模型由编码器网络和解码器网络组成。具体而言，卷积编码网络用于从LR输入图像中提取特征映射，全连接解码器网络用于近似隐式体素函数。由于学习函数的连续性，单个ArSSR模型可以在训练后从任意输入LR图像重建任意上采样率的HR图像。在三个数据集上的实验结果表明，ArSSR模型可以实现3D HR MR图像重建的最新SR性能，而使用单个训练模型可以实现任意上采样比例。
<details>	<summary>英文摘要</summary>	High Resolution (HR) medical images provide rich anatomical structure details to facilitate early and accurate diagnosis. In MRI, restricted by hardware capacity, scan time, and patient cooperation ability, isotropic 3D HR image acquisition typically requests long scan time and, results in small spatial coverage and low SNR. Recent studies showed that, with deep convolutional neural networks, isotropic HR MR images could be recovered from low-resolution (LR) input via single image super-resolution (SISR) algorithms. However, most existing SISR methods tend to approach a scale-specific projection between LR and HR images, thus these methods can only deal with a fixed up-sampling rate. For achieving different up-sampling rates, multiple SR networks have to be built up respectively, which is very time-consuming and resource-intensive. In this paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for recovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images with different up-scaling rates is defined as learning a continuous implicit voxel function from the observed LR images. Then the SR task is converted to represent the implicit voxel function via deep neural networks from a set of paired HR-LR training examples. The ArSSR model consists of an encoder network and a decoder network. Specifically, the convolutional encoder network is to extract feature maps from the LR input images and the fully-connected decoder network is to approximate the implicit voxel function. Due to the continuity of the learned function, a single ArSSR model can achieve arbitrary up-sampling rate reconstruction of HR images from any input LR image after training. Experimental results on three datasets show that the ArSSR model can achieve state-of-the-art SR performance for 3D HR MR image reconstruction while using a single trained model to achieve arbitrary up-sampling scales. </details>
<details>	<summary>注释</summary>	18 pages, 13 figures, 4 tables; submitted to Medical Image Analysis </details>
<details>	<summary>邮件日期</summary>	2021年10月28日</details>

# 295、RBSRICNN：通过迭代卷积神经网络的原始突发超分辨率
- [ ] RBSRICNN: Raw Burst Super-Resolution through Iterative Convolutional Neural Network 
时间：2021年10月25日                         第一作者：Rao Muhammad Umer                       [链接](https://arxiv.org/abs/2110.13217).                     
## 摘要：现代数码相机和智能手机大多依靠图像信号处理（ISP）管道来生成逼真的彩色RGB图像。然而，与单反相机相比，由于其物理限制，在许多带有小型相机传感器的便携式移动设备中通常会获得低质量的图像。低质量图像具有多个退化，即，由于摄像机运动而导致的亚像素偏移、由于摄像机颜色滤波器阵列而导致的马赛克图案、由于摄像机传感器较小而导致的低分辨率，以及其余信息被噪声破坏。这种退化限制了当前单图像超分辨率（SISR）方法从单个低分辨率（LR）图像恢复高分辨率（HR）图像细节的性能。在这项工作中，我们提出了一种原始突发超分辨率迭代卷积神经网络（RBSRICNN），它通过正向（物理）模型作为一个整体跟踪突发摄影管道。与现有的黑盒数据驱动方法相比，所提出的突发SR方案利用经典的图像正则化、凸优化和深度学习技术解决了该问题。所提出的网络通过迭代细化中间SR估计来产生最终输出。我们在定量和定性实验中证明了我们所提出的方法的有效性，该实验利用可用于训练的onl合成突发数据，稳健地推广到真实LR突发输入。
<details>	<summary>英文摘要</summary>	Modern digital cameras and smartphones mostly rely on image signal processing (ISP) pipelines to produce realistic colored RGB images. However, compared to DSLR cameras, low-quality images are usually obtained in many portable mobile devices with compact camera sensors due to their physical limitations. The low-quality images have multiple degradations i.e., sub-pixel shift due to camera motion, mosaick patterns due to camera color filter array, low-resolution due to smaller camera sensors, and the rest information are corrupted by the noise. Such degradations limit the performance of current Single Image Super-resolution (SISR) methods in recovering high-resolution (HR) image details from a single low-resolution (LR) image. In this work, we propose a Raw Burst Super-Resolution Iterative Convolutional Neural Network (RBSRICNN) that follows the burst photography pipeline as a whole by a forward (physical) model. The proposed Burst SR scheme solves the problem with classical image regularization, convex optimization, and deep learning techniques, compared to existing black-box data-driven methods. The proposed network produces the final output by an iterative refinement of the intermediate SR estimates. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments that generalize robustly to real LR burst inputs with onl synthetic burst data available for training. </details>
<details>	<summary>注释</summary>	Machine Learning and the Physical Sciences workshop at the 35th Conference on Neural Information Processing Systems (NeurIPS), 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月27日</details>

# 294、精确盲图像超分辨率的谱核转换
- [ ] Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution 
时间：2021年10月23日                         第一作者：Guangpin Tao                       [链接](https://arxiv.org/abs/2110.12151).                     
## 摘要：基于深度学习的超分辨率（SR）方法在模糊核已知的非盲环境下表现出良好的性能。然而，在不同的实际应用中，低分辨率（LR）图像的模糊核通常是未知的。当训练图像的退化过程偏离真实图像的退化过程时，可能会导致性能显著下降。在本文中，我们提出了一种新的盲SR框架来超分辨被任意模糊核退化的LR图像，并在频域进行精确的核估计。据我们所知，这是第一种在频域进行模糊核估计的深度学习方法。具体来说，我们首先证明了频域特征表示比空域特征表示更有利于模糊核重建。接下来，我们提出一个频谱到核（S$2$K）网络来估计各种形式的一般模糊核。我们使用条件GAN（CGAN）结合面向SR的优化目标来学习从退化图像光谱到未知核的端到端转换。在合成图像和真实图像上的大量实验表明，我们提出的方法充分降低了模糊核估计误差，从而使现有的非盲SR方法能够在盲设置下有效工作，并且与最先进的盲SR方法相比，获得了更优的性能，平均为1.39dB，在普通盲SR设置（带高斯核）上分别为$2乘以$和$4乘以$时为0.48dB。
<details>	<summary>英文摘要</summary>	Deep-learning based Super-Resolution (SR) methods have exhibited promising performance under non-blind setting where blur kernel is known. However, blur kernels of Low-Resolution (LR) images in different practical applications are usually unknown. It may lead to significant performance drop when degradation process of training images deviates from that of real images. In this paper, we propose a novel blind SR framework to super-resolve LR images degraded by arbitrary blur kernel with accurate kernel estimation in frequency domain. To our best knowledge, this is the first deep learning method which conducts blur kernel estimation in frequency domain. Specifically, we first demonstrate that feature representation in frequency domain is more conducive for blur kernel reconstruction than in spatial domain. Next, we present a Spectrum-to-Kernel (S$2$K) network to estimate general blur kernels in diverse forms. We use a Conditional GAN (CGAN) combined with SR-oriented optimization target to learn the end-to-end translation from degraded images' spectra to unknown kernels. Extensive experiments on both synthetic and real-world images demonstrate that our proposed method sufficiently reduces blur kernel estimation error, thus enables the off-the-shelf non-blind SR methods to work under blind setting effectively, and achieves superior performance over state-of-the-art blind SR methods, averagely by 1.39dB, 0.48dB on commom blind SR setting (with Gaussian kernels) for scales $2\times$ and $4\times$, respectively. </details>
<details>	<summary>注释</summary>	Accepted to NeurIPS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 293、用于光场图像超分辨率的稠密双注意网络
- [ ] Dense Dual-Attention Network for Light Field Image Super-Resolution 
时间：2021年10月23日                         第一作者：Yu Mo                       [链接](https://arxiv.org/abs/2110.12114).                     
## 摘要：光场（LF）图像可以用来提高图像超分辨率（SR）的性能，因为它同时具有角度和空间信息。对于LF图像SR，将来自不同视图的独特信息结合起来是一个挑战。此外，随着网络深度的增加，来自前几层的长期信息可能会减弱。在本文中，我们提出了一种用于LF图像SR的稠密双注意网络。具体来说，我们设计了一个视图注意模块来自适应地捕获不同视图中的区别性特征，以及一个通道注意模块来选择性地关注所有通道中的信息。这两个模块被馈送到两个分支，并以链式结构分别堆叠，以实现分层特征的自适应融合和有效信息的提取。同时，采用密集连接，充分利用多级信息。大量实验表明，我们的稠密双注意机制可以跨视图和通道捕获信息，从而提高SR性能。比较结果表明，我们的方法在公共数据集上优于最先进的方法。
<details>	<summary>英文摘要</summary>	Light field (LF) images can be used to improve the performance of image super-resolution (SR) because both angular and spatial information is available. It is challenging to incorporate distinctive information from different views for LF image SR. Moreover, the long-term information from the previous layers can be weakened as the depth of network increases. In this paper, we propose a dense dual-attention network for LF image SR. Specifically, we design a view attention module to adaptively capture discriminative features across different views and a channel attention module to selectively focus on informative information across all channels. These two modules are fed to two branches and stacked separately in a chain structure for adaptive fusion of hierarchical features and distillation of valid information. Meanwhile, a dense connection is used to fully exploit multi-level information. Extensive experiments demonstrate that our dense dual-attention mechanism can capture informative information across views and channels to improve SR performance. Comparative results show the advantage of our method over state-of-the-art methods on public datasets. </details>
<details>	<summary>注释</summary>	Accept by IEEE Transactions on Circuits and Systems for Video Technology DOI: 10.1109/TCSVT.2021.3121679 </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 292、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年10月23日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 291、基于模型的无监督高光谱图像超分辨率自动编码器
- [ ] Model Inspired Autoencoder for Unsupervised Hyperspectral Image Super-Resolution 
时间：2021年10月22日                         第一作者：Jianjun Liu                       [链接](https://arxiv.org/abs/2110.11591).                     
## 摘要：本文主要研究高光谱图像（HSI）超分辨率，目的是融合低空间分辨率HSI和高空间分辨率多光谱图像，形成高空间分辨率HSI（HR-HSI）。现有的基于深度学习的方法大多依赖于大量的标记训练样本进行监督，这是不现实的。常用的基于模型的方法是无监督和灵活的，但依赖于手工操作的先验知识。受模型特性的启发，我们首次尝试以无监督的方式为HSI超分辨率设计一个模型启发的深度网络。该方法包括一个基于目标HR-HSI的隐式自动编码器网络，该网络将每个像素视为一个单独的样本。目标HR-HSI的非负矩阵分解（NMF）被集成到自动编码器网络中，其中NMF的两个部分，谱矩阵和空间矩阵，分别被视为解码器参数和隐藏输出。在编码阶段，我们提出了一个像素级融合模型来直接估计隐藏输出，然后重新构造和展开该模型的算法以形成编码器网络。在特定的体系结构下，所提出的网络类似于基于多个先验的模型，可以逐块而不是整个图像进行训练。此外，我们还提出了一个额外的无监督网络来估计点扩散函数和谱响应函数。在合成数据集和真实数据集上的实验结果证明了该方法的有效性。
<details>	<summary>英文摘要</summary>	This paper focuses on hyperspectral image (HSI) super-resolution that aims to fuse a low-spatial-resolution HSI and a high-spatial-resolution multispectral image to form a high-spatial-resolution HSI (HR-HSI). Existing deep learning-based approaches are mostly supervised that rely on a large number of labeled training samples, which is unrealistic. The commonly used model-based approaches are unsupervised and flexible but rely on hand-craft priors. Inspired by the specific properties of model, we make the first attempt to design a model inspired deep network for HSI super-resolution in an unsupervised manner. This approach consists of an implicit autoencoder network built on the target HR-HSI that treats each pixel as an individual sample. The nonnegative matrix factorization (NMF) of the target HR-HSI is integrated into the autoencoder network, where the two NMF parts, spectral and spatial matrices, are treated as decoder parameters and hidden outputs respectively. In the encoding stage, we present a pixel-wise fusion model to estimate hidden outputs directly, and then reformulate and unfold the model's algorithm to form the encoder network. With the specific architecture, the proposed network is similar to a manifold prior-based model, and can be trained patch by patch rather than the entire image. Moreover, we propose an additional unsupervised network to estimate the point spread function and spectral response function. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. </details>
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 290、多模态Boost：基于小波变换的多注意网络的多模态医学图像超分辨率
- [ ] Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform 
时间：2021年10月22日                         第一作者：Farah Deeba                       [链接](https://arxiv.org/abs/2110.11684).                     
## 摘要：多模态医学图像被临床医生和内科医生广泛用于以非侵入性方式从高分辨率图像中分析和检索补充信息。相应图像分辨率的损失会降低医学图像诊断的整体性能。基于深度学习的单图像超分辨率（SISR）算法通过不断改进卷积神经网络（CNN）在低分辨率图像上的结构组件和训练策略，彻底改变了整体诊断框架。然而，现有工作在两个方面缺乏：i）生成的SR输出显示较差的纹理细节，并且经常产生模糊的边缘；ii）大多数模型都是针对单一模态开发的，因此需要修改以适应新的模态。这项工作涉及（i）通过提出具有深度多注意模块的生成性对抗网络（GAN），从低频数据中学习高频信息。基于GAN的现有方法已经产生了良好的SR结果；然而，实验证实，其SR输出的纹理细节对于医学图像尤其不足。在我们提出的SR模型中，小波变换（WT）和GANs的集成解决了上述关于文本的限制。WT将LR图像划分为多个频带，而传输的GAN利用多个注意和上采样块来预测高频分量。此外，我们提出了一种学习技术，用于将特定领域的分类器训练为感知损失函数。将多注意力损失与感知损失函数相结合，可获得可靠且高效的性能。将同一模型应用于来自不同模式的医学图像是一项挑战，我们的工作通过迁移学习对（ii）几种模式进行培训和执行。
<details>	<summary>英文摘要</summary>	Multimodal medical images are widely used by clinicians and physicians to analyze and retrieve complementary information from high-resolution images in a non-invasive manner. The loss of corresponding image resolution degrades the overall performance of medical image diagnosis. Deep learning based single image super resolution (SISR) algorithms has revolutionized the overall diagnosis framework by continually improving the architectural components and training strategies associated with convolutional neural networks (CNN) on low-resolution images. However, existing work lacks in two ways: i) the SR output produced exhibits poor texture details, and often produce blurred edges, ii) most of the models have been developed for a single modality, hence, require modification to adapt to a new one. This work addresses (i) by proposing generative adversarial network (GAN) with deep multi-attention modules to learn high-frequency information from low-frequency data. Existing approaches based on the GAN have yielded good SR results; however, the texture details of their SR output have been experimentally confirmed to be deficient for medical images particularly. The integration of wavelet transform (WT) and GANs in our proposed SR model addresses the aforementioned limitation concerning textons. The WT divides the LR image into multiple frequency bands, while the transferred GAN utilizes multiple attention and upsample blocks to predict high-frequency components. Moreover, we present a learning technique for training a domain-specific classifier as a perceptual loss function. Combining multi-attention GAN loss with a perceptual loss function results in a reliable and efficient performance. Applying the same model for medical images from diverse modalities is challenging, our work addresses (ii) by training and performing on several modalities via transfer learning. </details>
<details>	<summary>注释</summary>	14 pages, 12 Figures, and 3 Tables. Submitted to IEEE/ACM TCBB </details>
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 289、利用生成对抗网络结合互补二维和三维图像数据实现多相材料的超分辨率
- [ ] Super-resolution of multiphase materials by combining complementary 2D and 3D image data using generative adversarial networks 
时间：2021年10月22日                         第一作者：Amir Dahari                       [链接](https://arxiv.org/abs/2110.11281).                     
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 288、利用生成对抗网络结合互补二维和三维图像数据实现多相材料的超分辨率
- [ ] Super-resolution of multiphase materials by combining complementary 2D and 3D image data using generative adversarial networks 
时间：2021年10月21日                         第一作者：Amir Dahari                       [链接](https://arxiv.org/abs/2110.11281).                     
## 摘要：模拟材料细观结构对设备级性能的影响通常需要访问包含所有相关信息的3D图像数据，以定义模拟域的几何结构。该图像数据必须包括相位之间的足够对比度，以区分每种材料，具有足够高的分辨率以捕捉关键细节，并且具有足够大的视野，以代表一般材料。从单一成像技术中获得具有所有这些特性的数据几乎是不可能的。在本文中，我们提出了一种方法，用于组合来自不同但互补的成像技术对的信息，以便准确地重建所需的多相位、高分辨率、代表性的三维图像。具体来说，我们使用深度卷积生成对抗网络来实现超分辨率、风格转换和维度扩展。为了证明该工具的广泛适用性，使用两对数据集来验证通过融合成对成像技术产生的信息生成的体积的质量。在每种情况下计算了三个关键的细观结构指标，以表明该方法的准确性。在对我们的方法的准确性充满信心的情况下，我们通过将其应用于锂离子电池电极的真实数据对来证明其威力，因为在文献中，所需的3D高分辨率图像数据在任何地方都不可用。我们相信这种方法在保真度和易用性方面都优于先前报道的统计材料重建方法。此外，培训该算法所需的许多数据已经存在于文献中，等待合并。因此，我们的开放存取代码可以通过生成模拟中尺度行为所需的难以获得的高质量图像体积来加速阶跃变化。
<details>	<summary>英文摘要</summary>	Modelling the impact of a material's mesostructure on device level performance typically requires access to 3D image data containing all the relevant information to define the geometry of the simulation domain. This image data must include sufficient contrast between phases to distinguish each material, be of high enough resolution to capture the key details, but also have a large enough field-of-view to be representative of the material in general. It is rarely possible to obtain data with all of these properties from a single imaging technique. In this paper, we present a method for combining information from pairs of distinct but complementary imaging techniques in order to accurately reconstruct the desired multi-phase, high resolution, representative, 3D images. Specifically, we use deep convolutional generative adversarial networks to implement super-resolution, style transfer and dimensionality expansion. To demonstrate the widespread applicability of this tool, two pairs of datasets are used to validate the quality of the volumes generated by fusing the information from paired imaging techniques. Three key mesostructural metrics are calculated in each case to show the accuracy of this method. Having confidence in the accuracy of our method, we then demonstrate its power by applying to a real data pair from a lithium ion battery electrode, where the required 3D high resolution image data is not available anywhere in the literature. We believe this approach is superior to previously reported statistical material reconstruction methods both in terms of its fidelity and ease of use. Furthermore, much of the data required to train this algorithm already exists in the literature, waiting to be combined. As such, our open-access code could precipitate a step change by generating the hard to obtain high quality image volumes necessary to simulate behaviour at the mesoscale. </details>
<details>	<summary>邮件日期</summary>	2021年10月22日</details>

# 287、通过基于硬件的自适应退化模型实现真实世界的图像超分辨率
- [ ] Toward Real-world Image Super-resolution via Hardware-based Adaptive Degradation Models 
时间：2021年10月20日                         第一作者：Rui Ma                       [链接](https://arxiv.org/abs/2110.10755).                     
## 摘要：大多数单图像超分辨率（SR）方法是在合成低分辨率（LR）和高分辨率（HR）图像对上开发的，这些图像对通过预定的降级操作（例如双三次下采样）进行模拟。然而，这些方法只学习预定操作的逆过程，无法对真实的LR图像进行超分辨；真正的公式偏离预定的操作。为了解决这个问题，我们提出了一种新的有监督的方法来模拟未知的退化过程，包括成像系统的先验硬件知识。我们在监督学习框架下设计了自适应模糊层（ABL）来估计目标LR图像。ABL的超参数可以根据不同的成像硬件进行调整。在真实数据集上的实验验证了我们的退化模型可以比预定的退化操作更准确地估计LR图像，并且有助于现有SR方法比传统方法更准确地对真实LR图像进行重建。
<details>	<summary>英文摘要</summary>	Most single image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs, which are simulated by a predetermined degradation operation, e.g., bicubic downsampling. However, these methods only learn the inverse process of the predetermined operation, so they fail to super resolve the real-world LR images; the true formulation deviates from the predetermined operation. To address this problem, we propose a novel supervised method to simulate an unknown degradation process with the inclusion of the prior hardware knowledge of the imaging system. We design an adaptive blurring layer (ABL) in the supervised learning framework to estimate the target LR images. The hyperparameters of the ABL can be adjusted for different imaging hardware. The experiments on the real-world datasets validate that our degradation model can estimate LR images more accurately than the predetermined degradation operation, as well as facilitate existing SR methods to perform reconstructions on real-world LR images more accurately than the conventional approaches. </details>
<details>	<summary>邮件日期</summary>	2021年10月22日</details>

# 286、一种用于照片真实感图像超分辨率的高效深度神经网络
- [ ] Efficient Deep Neural Network for Photo-realistic Image Super-Resolution 
时间：2021年10月20日                         第一作者：Namhyuk Ahn                       [链接](https://arxiv.org/abs/1903.02240).                     
<details>	<summary>邮件日期</summary>	2021年10月22日</details>

# 285、ERQA：视频超分辨率边缘恢复质量评估
- [ ] ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution 
时间：2021年10月19日                         第一作者：Anastasia Kirillova                       [链接](https://arxiv.org/abs/2110.09992).                     
## 摘要：尽管视频超分辨率（VSR）越来越流行，但仍然没有很好的方法来评估放大帧中恢复细节的质量。某些SR方法可能产生错误的数字或完全不同的面。方法的结果是否可信取决于它恢复真实细节的程度。图像超分辨率可以使用自然分布生成与真实图像稍有相似的高分辨率图像。VSR允许探索相邻帧中的附加信息，以从原始场景恢复细节。我们在本文中提出的ERQA度量旨在估计模型使用VSR恢复真实细节的能力。在边缘对细节和字符识别具有重要意义的假设下，我们选择边缘保真度作为该度量的基础。我们工作的实验验证基于MSU视频超分辨率基准，该基准包括最困难的细节恢复模式，并验证原始帧细节的保真度。建议指标的代码可在https://github.com/msu-video-group/ERQA.
<details>	<summary>英文摘要</summary>	Despite the growing popularity of video super-resolution (VSR), there is still no good way to assess the quality of the restored details in upscaled frames. Some SR methods may produce the wrong digit or an entirely different face. Whether a method's results are trustworthy depends on how well it restores truthful details. Image super-resolution can use natural distributions to produce a high-resolution image that is only somewhat similar to the real one. VSR enables exploration of additional information in neighboring frames to restore details from the original scene. The ERQA metric, which we propose in this paper, aims to estimate a model's ability to restore real details using VSR. On the assumption that edges are significant for detail and character recognition, we chose edge fidelity as the foundation for this metric. Experimental validation of our work is based on the MSU Video Super-Resolution Benchmark, which includes the most difficult patterns for detail restoration and verifies the fidelity of details from the original frame. Code for the proposed metric is publicly available at https://github.com/msu-video-group/ERQA. </details>
<details>	<summary>注释</summary>	preprint </details>
<details>	<summary>邮件日期</summary>	2021年10月20日</details>

# 284、盲运动去模糊超分辨率：当动态时空学习满足静态图像理解时
- [ ] Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding 
时间：2021年10月19日                         第一作者：Wenjia Niu                       [链接](https://arxiv.org/abs/2105.13077).                     
<details>	<summary>注释</summary>	To appear in IEEE Transactions on Image Processing (TIP) DOI: 10.1109/TIP.2021.3101402 </details>
<details>	<summary>邮件日期</summary>	2021年10月20日</details>

# 283、用于图像质量评价的局部自适应结构和纹理相似性
- [ ] Locally Adaptive Structure and Texture Similarity for Image Quality Assessment 
时间：2021年10月16日                         第一作者：Keyan Ding                       [链接](https://arxiv.org/abs/2110.08521).                     
## 摘要：全参考图像质量评估（IQA）的最新进展涉及基于深度表示的统一结构和纹理相似性。然而，由此产生的深层图像结构和纹理相似性（DISTS）度量进行了相当全面的质量测量，忽略了自然摄影图像在空间和尺度上是局部结构和纹理的这一事实。在本文中，我们描述了一个局部自适应结构和纹理相似性指数，用于全参考IQA，我们称之为a-DISTS。具体地说，我们依靠一个单一的统计特征，即色散指数，在不同的尺度上定位纹理区域。估计的概率（一个面片是纹理）反过来用于自适应地汇集局部结构和纹理测量。由此产生的A-DISTS适合于局部图像内容，并且不需要昂贵的人类感知分数来进行监督训练。我们展示了A-DISTS在与十个IQA数据库上的人类数据相关和优化单图像超分辨率方法方面的优势。
<details>	<summary>英文摘要</summary>	The latest advances in full-reference image quality assessment (IQA) involve unifying structure and texture similarity based on deep representations. The resulting Deep Image Structure and Texture Similarity (DISTS) metric, however, makes rather global quality measurements, ignoring the fact that natural photographic images are locally structured and textured across space and scale. In this paper, we describe a locally adaptive structure and texture similarity index for full-reference IQA, which we term A-DISTS. Specifically, we rely on a single statistical feature, namely the dispersion index, to localize texture regions at different scales. The estimated probability (of one patch being texture) is in turn used to adaptively pool local structure and texture measurements. The resulting A-DISTS is adapted to local image content, and is free of expensive human perceptual scores for supervised training. We demonstrate the advantages of A-DISTS in terms of correlation with human data on ten IQA databases and optimization of single image super-resolution methods. </details>
<details>	<summary>邮件日期</summary>	2021年10月19日</details>

# 282、EFENet：具有增强流估计的基于参考的视频超分辨率
- [ ] EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation 
时间：2021年10月15日                         第一作者：Yaping Zhao                       [链接](https://arxiv.org/abs/2110.07797).                     
## 摘要：在本文中，我们考虑基于参考的视频超分辨率（RIFVSR）的问题，即，如何利用高分辨率（HR）参考帧来超分辨低分辨率（LR）视频序列。现有的RefVSR方法主要是在存在分辨率差和长时间范围的情况下，将参考序列和输入序列对齐。然而，它们要么忽略输入序列中的时间结构，要么遭受累积对齐错误。为了解决这些问题，我们建议EFENet同时利用HR参考中包含的视觉线索和LR序列中包含的时间信息。EFENet首先全局估计参考帧和每个LR帧之间的交叉尺度流。然后，我们新颖的EFENet流细化模块使用所有估计流来细化关于最远帧的流，这利用了序列中的全局时间信息，因此有效地减少了对齐错误。我们提供了全面的评估，以验证我们方法的优势，并证明所提出的框架优于最先进的方法。代码可在https://github.com/IndigoPurple/EFENet.
<details>	<summary>英文摘要</summary>	In this paper, we consider the problem of reference-based video super-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference frame to super-resolve a low-resolution (LR) video sequence. The existing approaches to RefVSR essentially attempt to align the reference and the input sequence, in the presence of resolution gap and long temporal range. However, they either ignore temporal structure within the input sequence, or suffer accumulative alignment errors. To address these issues, we propose EFENet to exploit simultaneously the visual cues contained in the HR reference and the temporal information contained in the LR sequence. EFENet first globally estimates cross-scale flow between the reference and each LR frame. Then our novel flow refinement module of EFENet refines the flow regarding the furthest frame using all the estimated flows, which leverages the global temporal information within the sequence and therefore effectively reduces the alignment errors. We provide comprehensive evaluations to validate the strengths of our approach, and to demonstrate that the proposed framework outperforms the state-of-the-art methods. Code is available at https://github.com/IndigoPurple/EFENet. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 281、边缘SR：质量的超分辨率
- [ ] edge-SR: Super-Resolution For The Masses 
时间：2021年10月15日                         第一作者：Pablo Navarrete Michelini                       [链接](https://arxiv.org/abs/2108.10335).                     
<details>	<summary>注释</summary>	In WACV 2022. Code available in https://github.com/pnavarre/eSR </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 280、基于光流复用的时空视频超分辨率双向递归网络
- [ ] Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution 
时间：2021年10月13日                         第一作者：Yuantong Zhang                       [链接](https://arxiv.org/abs/2110.06786).                     
## 摘要：在本文中，我们考虑的任务的时空视频超分辨率（ST-VSR），同时增加了空间分辨率和帧速率给定的视频。然而，现有方法在如何有效利用大范围相邻帧的信息或使用可变形ConvLSTM对齐策略避免推理速度下降方面通常存在困难最近一些基于LSTM的ST-VSR方法取得了令人满意的结果。为了解决现有方法的上述问题，我们提出了一种由粗到精的双向递归神经网络，而不是使用convlsm来利用相邻帧之间的知识。具体来说，我们首先使用双向光流来更新隐藏状态，然后使用特征细化模块（FRM）来细化结果。由于我们可以充分利用大范围的相邻帧，因此我们的方法可以更有效地利用局部和全局信息。此外，我们提出了一种光流重用策略，可以重用相邻帧的中间流，与现有的基于LSTM的设计相比，这大大减少了帧对齐的计算负担。大量实验表明，基于光流复用的双向递归网络（OFR-BRN）在精度和效率上都优于现有的方法。
<details>	<summary>英文摘要</summary>	In this paper, we consider the task of space-time video super-resolution (ST-VSR), which simultaneously increases the spatial resolution and frame rate for a given video. However, existing methods typically suffer from difficulties in how to efficiently leverage information from a large range of neighboring frames or avoiding the speed degradation in the inference using deformable ConvLSTM strategies for alignment. % Some recent LSTM-based ST-VSR methods have achieved promising results. To solve the above problem of the existing methods, we propose a coarse-to-fine bidirectional recurrent neural network instead of using ConvLSTM to leverage knowledge between adjacent frames. Specifically, we first use bi-directional optical flow to update the hidden state and then employ a Feature Refinement Module (FRM) to refine the result. Since we could fully utilize a large range of neighboring frames, our method leverages local and global information more effectively. In addition, we propose an optical flow-reuse strategy that can reuse the intermediate flow of adjacent frames, which considerably reduces the computation burden of frame alignment compared with existing LSTM-based designs. Extensive experiments demonstrate that our optical-flow-reuse-based bidirectional recurrent network(OFR-BRN) is superior to the state-of-the-art methods both in terms of accuracy and efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年10月14日</details>

# 279、COMISR：基于压缩的视频超分辨率
- [ ] COMISR: Compression-Informed Video Super-Resolution 
时间：2021年10月12日                         第一作者：Yinxiao Li                       [链接](https://arxiv.org/abs/2105.01237).                     
<details>	<summary>注释</summary>	13 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月13日</details>

# 278、StyleGAN诱导的反问题数据驱动正则化
- [ ] StyleGAN-induced data-driven regularization for inverse problems 
时间：2021年10月07日                         第一作者：Arthur Conmy                       [链接](https://arxiv.org/abs/2110.03814).                     
## 摘要：生成性对抗网络（GAN）的最新进展为生成以前无法生成的高分辨率照片真实感图像开辟了可能性。GANs从高维分布中采样的能力自然促使研究人员利用其能力在反问题中对图像进行建模。我们通过开发一个贝叶斯图像重建框架来扩展这一研究领域，该框架充分利用了预先训练的StyleGAN2生成器（目前占主导地位的GAN架构）的潜力，用于构建基础图像上的先验分布。我们提出的方法，我们称之为生成模型学习贝叶斯重建（L-BRGM），需要对样式代码和输入潜在代码进行联合优化，并通过允许样式代码对于不同的生成器层是不同的来增强预训练样式生成器的表达能力。考虑到图像修复和超分辨率的逆问题，我们证明了所提出的方法与最新的基于GAN的图像重建方法相竞争，有时甚至优于这些方法。
<details>	<summary>英文摘要</summary>	Recent advances in generative adversarial networks (GANs) have opened up the possibility of generating high-resolution photo-realistic images that were impossible to produce previously. The ability of GANs to sample from high-dimensional distributions has naturally motivated researchers to leverage their power for modeling the image prior in inverse problems. We extend this line of research by developing a Bayesian image reconstruction framework that utilizes the full potential of a pre-trained StyleGAN2 generator, which is the currently dominant GAN architecture, for constructing the prior distribution on the underlying image. Our proposed approach, which we refer to as learned Bayesian reconstruction with generative models (L-BRGM), entails joint optimization over the style-code and the input latent code, and enhances the expressive power of a pre-trained StyleGAN2 generator by allowing the style-codes to be different for different generator layers. Considering the inverse problems of image inpainting and super-resolution, we demonstrate that the proposed approach is competitive with, and sometimes superior to, state-of-the-art GAN-based image reconstruction methods. </details>
<details>	<summary>注释</summary>	Submitted to IEEE ICASSP 2022. Under review </details>
<details>	<summary>邮件日期</summary>	2021年10月11日</details>

# 277、用于收敛即插即用的梯度步进去噪器
- [ ] Gradient Step Denoiser for convergent Plug-and-Play 
时间：2021年10月07日                         第一作者：Samuel Hurault                       [链接](https://arxiv.org/abs/2110.03220).                     
## 摘要：即插即用方法构成了一类用于成像问题的迭代算法，其中正则化由现成的去噪器执行。尽管即插即用方法可以为各种图像问题带来巨大的视觉性能，但现有的少数收敛保证是基于去噪器上不现实（或次优）的假设，或仅限于强凸数据项。在这项工作中，我们提出了一种新型的即插即用方法，基于半二次分裂，该方法将去噪器作为一个梯度下降步骤，在一个由深度神经网络参数化的函数上实现。利用非凸环境下近似梯度下降算法的收敛结果，我们证明了所提出的即插即用算法是一种收敛的迭代格式，其目标是显式全局泛函的平稳点。此外，实验表明，与其他用于即插即用方案的最先进的深度去噪器相比，在不影响性能的情况下学习这种深度去噪器是可能的。我们将我们的近似梯度算法应用于各种不适定反问题，例如去模糊、超分辨率和修复。对于所有这些应用，数值结果从经验上证实了收敛结果。实验还表明，该算法在定量和定性上都达到了最新水平。
<details>	<summary>英文摘要</summary>	Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data terms. In this work, we propose a new type of Plug-and-Play methods, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the non-convex setting, we show that the proposed Plug-and-Play algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, super-resolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively. </details>
<details>	<summary>邮件日期</summary>	2021年10月08日</details>

# 276、突发图像恢复与增强
- [ ] Burst Image Restoration and Enhancement 
时间：2021年10月07日                         第一作者：Akshay Dudhane                       [链接](https://arxiv.org/abs/2110.03680).                     
## 摘要：现代手持设备可以快速连续地获取突发图像序列。然而，由于相机抖动和对象运动，单个采集的帧遭受多次降级，并且未对齐。突发图像恢复的目标是在多个突发帧之间有效地组合互补线索，以生成高质量的输出。为了实现这一目标，我们开发了一种新的方法，只关注突发帧之间的有效信息交换，从而在保留和增强实际场景细节的同时过滤掉退化。我们的中心思想是创建一组\emph{pseudo burst}特性，将来自所有输入突发帧的互补信息结合起来，以无缝地交换信息。伪突发表示对原始突发图像的信道特征进行编码，从而使模型更容易学习由多个突发帧提供的独特信息。但是，除非各个突发帧正确对齐以减少帧间移动，否则无法成功创建伪突发。因此，我们的方法首先从每个突发帧中提取预处理的特征，并使用边缘增强突发对齐模块进行匹配。然后使用多尺度上下文信息创建和丰富伪突发特征。我们的最后一步是自适应地聚合来自伪突发特征的信息，以在合并伪突发特征的同时在多个阶段逐步提高分辨率。与通常采用单级上采样的后期融合方案的现有作品相比，我们的方法表现良好，在突发超分辨率和微光图像增强任务中提供了最先进的性能。我们的代码和模型将公开发布。
<details>	<summary>英文摘要</summary>	Modern handheld devices can acquire burst image sequence in a quick succession. However, the individual acquired frames suffer from multiple degradations and are misaligned due to camera shake and object motions. The goal of Burst Image Restoration is to effectively combine complimentary cues across multiple burst frames to generate high-quality outputs. Towards this goal, we develop a novel approach by solely focusing on the effective information exchange between burst frames, such that the degradations get filtered out while the actual scene details are preserved and enhanced. Our central idea is to create a set of \emph{pseudo-burst} features that combine complimentary information from all the input burst frames to seamlessly exchange information. The pseudo-burst representations encode channel-wise features from the original burst images, thus making it easier for the model to learn distinctive information offered by multiple burst frames. However, the pseudo-burst cannot be successfully created unless the individual burst frames are properly aligned to discount inter-frame movements. Therefore, our approach initially extracts preprocessed features from each burst frame and matches them using an edge-boosting burst alignment module. The pseudo-burst features are then created and enriched using multi-scale contextual information. Our final step is to adaptively aggregate information from the pseudo-burst features to progressively increase resolution in multiple stages while merging the pseudo-burst features. In comparison to existing works that usually follow a late fusion scheme with single-stage upsampling, our approach performs favorably, delivering state of the art performance on burst super-resolution and low-light image enhancement tasks. Our codes and models will be released publicly. </details>
<details>	<summary>邮件日期</summary>	2021年10月08日</details>

# 275、用于模态间和模态内多分辨率脑图对齐和合成的阶梯图
- [ ] StairwayGraphNet for Inter- and Intra-modality Multi-resolution Brain Graph Alignment and Synthesis 
时间：2021年10月06日                         第一作者：Islem Mhiri                       [链接](https://arxiv.org/abs/2110.04279).                     
## 摘要：综合多模态医学数据提供补充知识，帮助医生做出准确的临床决策。尽管前景看好，但现有的多模态脑图合成框架存在一些局限性。首先，它们主要只处理一个问题（模态内或模态间），限制了它们的通用性，即同时合成模态间和模态内。第二，虽然很少有技术能够在单一模式（即内部模式）内处理超分辨率低分辨率脑图，但模式间图的超分辨率仍有待探索，尽管这可以避免昂贵的数据收集和处理。更重要的是，目标域和源域可能具有不同的分布，这会导致它们之间的域断开。为了填补这些空白，我们提出了一个多分辨率GraphNet（SG-Net）框架，以基于给定的模态和域间和域内的超分辨率脑图联合推断目标图模态。我们的SG网络基于三个主要贡献：（i）基于新的图形生成对抗网络，在内部（如形态功能）和内部（如功能）域从源图形预测目标图形，（ii）生成高分辨率脑图而无需诉诸耗时且昂贵的MRI处理步骤，以及（iii）使用模态间对准器来放松损失函数以优化，强制源分布以匹配地面真值图。此外，我们还设计了一个新的保留基本真值的损失函数来指导两个生成器更准确地学习基本真值脑图的拓扑结构。我们使用多分辨率阶梯从源图预测目标脑图的综合实验表明，与其变体和最先进的方法相比，我们的方法具有更好的性能。
<details>	<summary>英文摘要</summary>	Synthesizing multimodality medical data provides complementary knowledge and helps doctors make precise clinical decisions. Although promising, existing multimodal brain graph synthesis frameworks have several limitations. First, they mainly tackle only one problem (intra- or inter-modality), limiting their generalizability to synthesizing inter- and intra-modality simultaneously. Second, while few techniques work on super-resolving low-resolution brain graphs within a single modality (i.e., intra), inter-modality graph super-resolution remains unexplored though this would avoid the need for costly data collection and processing. More importantly, both target and source domains might have different distributions, which causes a domain fracture between them. To fill these gaps, we propose a multi-resolution StairwayGraphNet (SG-Net) framework to jointly infer a target graph modality based on a given modality and super-resolve brain graphs in both inter and intra domains. Our SG-Net is grounded in three main contributions: (i) predicting a target graph from a source one based on a novel graph generative adversarial network in both inter (e.g., morphological-functional) and intra (e.g., functional-functional) domains, (ii) generating high-resolution brain graphs without resorting to the time consuming and expensive MRI processing steps, and (iii) enforcing the source distribution to match that of the ground truth graphs using an inter-modality aligner to relax the loss function to optimize. Moreover, we design a new Ground Truth-Preserving loss function to guide both generators in learning the topological structure of ground truth brain graphs more accurately. Our comprehensive experiments on predicting target brain graphs from source graphs using a multi-resolution stairway showed the outperformance of our method in comparison with its variants and state-of-the-art method. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2107.06281 </details>
<details>	<summary>邮件日期</summary>	2021年10月11日</details>

# 274、光场超分辨率的深度选择组合嵌入和一致性正则化
- [ ] Deep Selective Combinatorial Embedding and Consistency Regularization for Light Field Super-resolution 
时间：2021年10月06日                         第一作者：Jing Jin                        [链接](https://arxiv.org/abs/2009.12537).                     
<details>	<summary>注释</summary>	14 pages, 12 figures. arXiv admin note: substantial text overlap with arXiv:2004.02215 </details>
<details>	<summary>邮件日期</summary>	2021年10月11日</details>

# 273、利用师生学习预测高分辨率脑网络的域间比对
- [ ] Inter-Domain Alignment for Predicting High-Resolution Brain Networks Using Teacher-Student Learning 
时间：2021年10月06日                         第一作者：Basar Demir                       [链接](https://arxiv.org/abs/2110.03452).                     
## 摘要：准确和自动化的超分辨率图像合成是非常需要的，因为它具有巨大的潜力，可以避免获取高成本的医学扫描和耗时的神经成像数据预处理管道。然而，现有的深度学习框架仅设计用于从低分辨率（LR）图像预测高分辨率（HR）图像，这限制了它们对大脑图形（即连接体）的泛化能力。一小部分工作集中在超分辨率脑图上，目标是从单个LR图预测HR图。尽管前景看好，但现有工作主要集中在属于同一领域（如功能）的超分辨图上，忽略了多模态脑数据分布（如形态学和结构）之间存在的领域断裂。为此，我们提出了一种新的跨域自适应框架，即学习用知识提取网络（L2S-KDnet）超分辨脑图，该框架采用师生范式来超分辨脑图。我们的教师网络是一个图形编码器-解码器，它首先学习LR大脑图形嵌入，然后学习如何使用对抗性正则化将生成的潜在表示与HR地面真实数据分布对齐。最后，它从对齐的嵌入中解码HR图。接下来，我们的学生网络学习对齐的大脑图的知识，以及从教师转移的预测HR图的拓扑结构。我们进一步利用教师的解码器来优化学生网络。L2S KDnet提出了第一个为基于域间对齐的脑图超分辨率合成定制的TS体系结构。我们的实验结果表明，与基准测试方法相比，性能有了显著提高。
<details>	<summary>英文摘要</summary>	Accurate and automated super-resolution image synthesis is highly desired since it has the great potential to circumvent the need for acquiring high-cost medical scans and a time-consuming preprocessing pipeline of neuroimaging data. However, existing deep learning frameworks are solely designed to predict high-resolution (HR) image from a low-resolution (LR) one, which limits their generalization ability to brain graphs (i.e., connectomes). A small body of works has focused on superresolving brain graphs where the goal is to predict a HR graph from a single LR graph. Although promising, existing works mainly focus on superresolving graphs belonging to the same domain (e.g., functional), overlooking the domain fracture existing between multimodal brain data distributions (e.g., morphological and structural). To this aim, we propose a novel inter-domain adaptation framework namely, Learn to SuperResolve Brain Graphs with Knowledge Distillation Network (L2S-KDnet), which adopts a teacher-student paradigm to superresolve brain graphs. Our teacher network is a graph encoder-decoder that firstly learns the LR brain graph embeddings, and secondly learns how to align the resulting latent representations to the HR ground truth data distribution using an adversarial regularization. Ultimately, it decodes the HR graphs from the aligned embeddings. Next, our student network learns the knowledge of the aligned brain graphs as well as the topological structure of the predicted HR graphs transferred from the teacher. We further leverage the decoder of the teacher to optimize the student network. L2S-KDnet presents the first TS architecture tailored for brain graph super-resolution synthesis that is based on inter-domain alignment. Our experimental results demonstrate substantial performance gains over benchmark methods. </details>
<details>	<summary>邮件日期</summary>	2021年10月08日</details>

# 272、用改进的超分辨率CNN增强动画图像放大
- [ ] Enhancement of Anime Imaging Enlargement using Modified Super-Resolution CNN 
时间：2021年10月05日                         第一作者：Tanakit Intaniyom                       [链接](https://arxiv.org/abs/2110.02321).                     
## 摘要：动漫是一种类似于电影和书籍的讲故事媒介。动画图像是一种艺术作品，几乎完全是手工绘制的。因此，用更大的尺寸和更高质量的图像复制现有的动画是昂贵的。因此，我们提出了一种基于卷积神经网络的模型来提取图像的突出特征，放大这些图像，提高动画图像的质量。我们使用160张图像的训练集和20张图像的验证集对模型进行训练。我们用一组20幅图像对训练后的模型进行了测试。实验结果表明，与现有的常用图像放大方法和原始SRCNN方法相比，该模型在图像尺寸较大的情况下成功地提高了图像质量。
<details>	<summary>英文摘要</summary>	Anime is a storytelling medium similar to movies and books. Anime images are a kind of artworks, which are almost entirely drawn by hand. Hence, reproducing existing Anime with larger sizes and higher quality images is expensive. Therefore, we proposed a model based on convolutional neural networks to extract outstanding features of images, enlarge those images, and enhance the quality of Anime images. We trained the model with a training set of 160 images and a validation set of 20 images. We tested the trained model with a testing set of 20 images. The experimental results indicated that our model successfully enhanced the image quality with a larger image-size when compared with the common existing image enlargement and the original SRCNN method. </details>
<details>	<summary>注释</summary>	6 pages, 11 figures, to be published in The 11th Joint Symposium on Computational Intelligence (JSCI11) </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 271、一种实用的深盲图像超分辨率退化模型的设计
- [ ] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution 
时间：2021年09月30日                         第一作者：Kai Zhang                       [链接](https://arxiv.org/abs/2103.14006).                     
<details>	<summary>注释</summary>	ICCV 2021. Code: https://github.com/cszn/BSRGAN </details>
<details>	<summary>邮件日期</summary>	2021年10月01日</details>

# 270、具有对抗性渐进属性诱导网络的LR-to-HR人脸幻觉
- [ ] LR-to-HR Face Hallucination with an Adversarial Progressive Attribute-Induced Network 
时间：2021年09月29日                         第一作者：Nitin Balach                       [链接](https://arxiv.org/abs/2109.14690).                     
## 摘要：由于在幻觉过程中，低分辨率（LR）人脸图像可能对应于多个高分辨率（HR）人脸图像，并导致最终超分辨率结果的显著身份变化，因此人脸超分辨率是一个具有挑战性且高度不适定的问题。因此，为了解决这个问题，我们提出了一个端到端的渐进式学习框架，该框架结合了人脸属性，并加强了来自多尺度鉴别器的额外监督。通过将面部属性纳入学习过程并逐步解析面部图像，LR和HR图像之间的映射受到更多约束，这显著有助于减少一对多映射中的模糊性和不确定性。此外，我们在之前工作的基础上对CelebA数据集进行了全面的评估（即，从微小的16x16人脸图像超分辨率8倍），结果表明，所提出的方法可以产生令人满意的人脸幻觉图像，优于其他最先进的方法。
<details>	<summary>英文摘要</summary>	Face super-resolution is a challenging and highly ill-posed problem since a low-resolution (LR) face image may correspond to multiple high-resolution (HR) ones during the hallucination process and cause a dramatic identity change for the final super-resolved results. Thus, to address this problem, we propose an end-to-end progressive learning framework incorporating facial attributes and enforcing additional supervision from multi-scale discriminators. By incorporating facial attributes into the learning process and progressively resolving the facial image, the mapping between LR and HR images is constrained more, and this significantly helps to reduce the ambiguity and uncertainty in one-to-many mapping. In addition, we conduct thorough evaluations on the CelebA dataset following the settings of previous works (i.e. super-resolving by a factor of 8x from tiny 16x16 face images.), and the results demonstrate that the proposed approach can yield satisfactory face hallucination images outperforming other state-of-the-art approaches. </details>
<details>	<summary>邮件日期</summary>	2021年10月01日</details>

# 269、神经刀：修补神经隐式表示网络
- [ ] Neural Knitworks: Patched Neural Implicit Representation Networks 
时间：2021年09月29日                         第一作者：Mikolaj Czerkawski                       [链接](https://arxiv.org/abs/2109.14406).                     
## 摘要：基于坐标的多层感知器（MLP）网络尽管能够学习神经隐式表示，但在内部图像合成应用中并不适用。卷积神经网络（CNN）通常用于各种内部生成任务，代价是更大的模型。我们提出了Neural Knitwork，这是一种用于自然图像神经隐式表示学习的体系结构，它通过以敌对方式优化图像面片的分布和加强面片预测之间的一致性来实现图像合成。据我们所知，这是第一次实现基于坐标的MLP，专门用于图像修复、超分辨率和去噪等合成任务。我们通过对这三项任务的培训来证明所提出的技术的实用性。结果表明，使用面片而不是像素对自然图像进行建模，可以产生更高的保真度。结果模型所需的参数比基于CNN的备选解决方案少80%，同时实现了可比的性能和培训时间。
<details>	<summary>英文摘要</summary>	Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable of learning neural implicit representations, are not performant for internal image synthesis applications. Convolutional Neural Networks (CNNs) are typically used instead for a variety of internal generative tasks, at the cost of a larger model. We propose Neural Knitwork, an architecture for neural implicit representation learning of natural images that achieves image synthesis by optimizing the distribution of image patches in an adversarial manner and by enforcing consistency between the patch predictions. To the best of our knowledge, this is the first implementation of a coordinate-based MLP tailored for synthesis tasks such as image inpainting, super-resolution, and denoising. We demonstrate the utility of the proposed technique by training on these three tasks. The results show that modeling natural images using patches, rather than pixels, produces results of higher fidelity. The resulting model requires 80% fewer parameters than alternative CNN-based solutions while achieving comparable performance and training time. </details>
<details>	<summary>注释</summary>	11 pages, 9 figures, Rejected from NeurIPS </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 268、从初学者到大师：基于深度学习的单幅图像超分辨率研究综述
- [ ] From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution 
时间：2021年09月29日                         第一作者：Juncheng Li                       [链接](https://arxiv.org/abs/2109.14335).                     
## 摘要：单图像超分辨率（SISR）是图像处理中的一项重要任务，旨在提高成像系统的分辨率。最近，在深度学习（DL）的帮助下，SISR取得了巨大的飞跃，并取得了可喜的成果。在本综述中，我们概述了基于DL的SISR方法，并根据其目标（如重建效率、重建精度和感知精度）对其进行分组。具体来说，我们首先介绍了SISR的问题定义、研究背景和意义。其次，我们介绍了一些相关的工作，包括基准数据集、上采样方法、优化目标和图像质量评估方法。第三，我们对SISR进行了详细的研究，并给出了一些特定领域的应用。第四，我们给出了一些经典SISR方法的重建结果，以直观地了解它们的性能。最后，我们讨论了SISR中仍然存在的一些问题，并总结了一些新的趋势和未来的方向。这是对SISR的详尽调查，有助于研究人员更好地理解SISR，并激发该领域更令人兴奋的研究。SISR的调查项目见https://github.com/CV-JunchengLi/SISR-Survey.
<details>	<summary>英文摘要</summary>	Single-image super-resolution (SISR) is an important task in image processing, which aims to enhance the resolution of imaging systems. Recently, SISR has made a huge leap and has achieved promising results with the help of deep learning (DL). In this survey, we give an overview of DL-based SISR methods and group them according to their targets, such as reconstruction efficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we first introduce the problem definition, research background, and the significance of SISR. Secondly, we introduce some related works, including benchmark datasets, upsampling methods, optimization objectives, and image quality assessment methods. Thirdly, we provide a detailed investigation of SISR and give some domain-specific applications of it. Fourthly, we present the reconstruction results of some classic SISR methods to intuitively know their performance. Finally, we discuss some issues that still exist in SISR and summarize some new trends and future directions. This is an exhaustive survey of SISR, which can help researchers better understand SISR and inspire more exciting research in this field. An investigation project for SISR is provided in https://github.com/CV-JunchengLi/SISR-Survey. </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 267、一种高效的人脸视频超分辨率网络设计
- [ ] An Efficient Network Design for Face Video Super-resolution 
时间：2021年09月28日                         第一作者：Feng Yu                       [链接](https://arxiv.org/abs/2109.13626).                     
## 摘要：人脸视频超分辨率算法的目标是通过连续输入的视频序列重建真实的人脸细节。然而，现有的视频处理算法通常包含冗余参数，以保证不同的超分辨率场景。在这项工作中，我们着重于原始视频场景中人脸区域的超分辨率，而其余区域是插值的。这种特殊的超分辨率任务使得在一般视频超分辨率网络中切割冗余参数成为可能。我们构建了一个完全由人脸视频序列组成的用于网络训练和评估的数据集，并在实验中进行了超参数优化。我们使用三种组合策略来优化网络参数，同时使用列车评估方法来加速优化过程。结果表明，同时训练评估方法提高了训练速度，有利于生成有效的网络。生成的网络可以减少至少52.4%的参数和20.7%的触发器，与最先进的视频超分辨率算法相比，在PSNR、SSIM上实现更好的性能。在处理36x36x1x3输入视频帧序列时，高效网络提供47.62 FPS的实时处理性能。我们将我们的提案命名为人脸视频超分辨率超参数优化（HO-FVSR），该提案在https://github.com/yphone/efficient-network-for-face-VSR.
<details>	<summary>英文摘要</summary>	Face video super-resolution algorithm aims to reconstruct realistic face details through continuous input video sequences. However, existing video processing algorithms usually contain redundant parameters to guarantee different super-resolution scenes. In this work, we focus on super-resolution of face areas in original video scenes, while rest areas are interpolated. This specific super-resolved task makes it possible to cut redundant parameters in general video super-resolution networks. We construct a dataset consisting entirely of face video sequences for network training and evaluation, and conduct hyper-parameter optimization in our experiments. We use three combined strategies to optimize the network parameters with a simultaneous train-evaluation method to accelerate optimization process. Results show that simultaneous train-evaluation method improves the training speed and facilitates the generation of efficient networks. The generated network can reduce at least 52.4% parameters and 20.7% FLOPs, achieve better performance on PSNR, SSIM compared with state-of-art video super-resolution algorithms. When processing 36x36x1x3 input video frame sequences, the efficient network provides 47.62 FPS real-time processing performance. We name our proposal as hyper-parameter optimization for face Video Super-Resolution (HO-FVSR), which is open-sourced at https://github.com/yphone/efficient-network-for-face-VSR. </details>
<details>	<summary>注释</summary>	8 pages, 7 figures, conference workshop </details>
<details>	<summary>邮件日期</summary>	2021年09月29日</details>

# 266、保结构图像超分辨率
- [ ] Structure-Preserving Image Super-Resolution 
时间：2021年09月26日                         第一作者：Cheng Ma                       [链接](https://arxiv.org/abs/2109.12530).                     
## 摘要：单图像超分辨率（SISR）中的结构问题。得益于生成性对抗网络（GANs），最近的研究通过恢复照片真实感图像促进了SISR的发展。然而，在恢复的图像中仍然存在不希望的结构失真。在本文中，我们提出了一种结构保持超分辨率（SPSR）方法来缓解上述问题，同时保持基于GAN的方法的优点，以生成令人愉悦的感知细节。首先，我们通过利用图像的梯度映射从两个方面指导恢复，提出了带梯度引导的SPSR（SPSR-G）。一方面，我们通过梯度分支恢复高分辨率梯度贴图，为SR过程提供额外的结构优先级。另一方面，我们提出了一种梯度损失对超分辨率图像施加二阶限制，这有助于生成网络更专注于几何结构。其次，由于梯度图是手工制作的，可能只能捕获结构信息的有限方面，我们通过引入可学习的神经结构提取器（NSE）进一步扩展了SPSR-G，以挖掘更丰富的局部结构，并为SR提供更强的监督。我们提出了两种自监督结构学习方法，对比预测和解决拼图游戏，培训NSE。我们的方法是模型不可知的，可以潜在地用于现成的SR网络。在五个基准数据集上的实验结果表明，在LPIPS、PSNR和SSIM度量下，所提出的方法优于最新的感知驱动SR方法。视觉结果证明了我们的方法在恢复结构的同时生成自然SR图像的优越性。代码可在https://github.com/Maclory/SPSR.
<details>	<summary>英文摘要</summary>	Structures matter in single image super-resolution (SISR). Benefiting from generative adversarial networks (GANs), recent studies have promoted the development of SISR by recovering photo-realistic images. However, there are still undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super-resolution (SPSR) method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Firstly, we propose SPSR with gradient guidance (SPSR-G) by exploiting gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss to impose a second-order restriction on the super-resolved images, which helps generative networks concentrate more on geometric structures. Secondly, since the gradient maps are handcrafted and may only be able to capture limited aspects of structural information, we further extend SPSR-G by introducing a learnable neural structure extractor (NSE) to unearth richer local structures and provide stronger supervision for SR. We propose two self-supervised structure learning methods, contrastive prediction and solving jigsaw puzzles, to train the NSEs. Our methods are model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results on five benchmark datasets show that the proposed methods outperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and SSIM metrics. Visual results demonstrate the superiority of our methods in restoring structures while generating natural SR images. Code is available at https://github.com/Maclory/SPSR. </details>
<details>	<summary>注释</summary>	Accepted by T-PAMI. Journal version of arXiv:2003.13081 (CVPR 2020) DOI: 10.1109/TPAMI.2021.3114428 </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 265、基于锚点的移动图像超分辨率平面网
- [ ] Anchor-based Plain Net for Mobile Image Super-Resolution 
时间：2021年09月25日                         第一作者：Zongcai Du                       [链接](https://arxiv.org/abs/2105.09750).                     
<details>	<summary>注释</summary>	accepted by CVPR2021 MAI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 264、WiSoSuper：对风能和太阳能数据的超分辨率方法进行基准测试
- [ ] WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data 
时间：2021年09月23日                         第一作者：Rupa Kurinchi-Vendhan                       [链接](https://arxiv.org/abs/2109.08770).                     
<details>	<summary>邮件日期</summary>	2021年09月24日</details>

# 263、视频到视频转换网络在计算流体力学中的应用
- [ ] Application of Video-to-Video Translation Networks to Computational Fluid Dynamics 
时间：2021年09月12日                         第一作者：Hiromitsu Kigure                       [链接](https://arxiv.org/abs/2109.10679).                     
## 摘要：近年来，人工智能，特别是深度学习的发展非常引人注目，其在各个领域的应用也迅速增长。在本文中，我报告了生成对抗网络（GAN），特别是视频到视频转换网络在计算流体力学（CFD）模拟中的应用结果。本研究的目的是降低使用GANs进行CFD模拟的计算成本。在本研究中，GANs的结构是图像到图像翻译网络（即所谓的“pix2pix”）和长-短期记忆（LSTM）的组合。结果表明，高成本和高精度模拟（使用高分辨率计算网格）的结果可以从低成本和低精度模拟（使用低分辨率网格）的结果估计出来。特别是，高分辨率网格情况下密度分布的时间演化由低分辨率网格情况下的时间演化通过GANs再现，并且由GANs生成的图像估计的密度不均匀性以良好的精度恢复地面真实。文中还对该方法与几种超分辨率算法的结果进行了定性和定量比较。
<details>	<summary>英文摘要</summary>	In recent years, the evolution of artificial intelligence, especially deep learning, has been remarkable, and its application to various fields has been growing rapidly. In this paper, I report the results of the application of generative adversarial networks (GANs), specifically video-to-video translation networks, to computational fluid dynamics (CFD) simulations. The purpose of this research is to reduce the computational cost of CFD simulations with GANs. The architecture of GANs in this research is a combination of the image-to-image translation networks (the so-called "pix2pix") and Long Short-Term Memory (LSTM). It is shown that the results of high-cost and high-accuracy simulations (with high-resolution computational grids) can be estimated from those of low-cost and low-accuracy simulations (with low-resolution grids). In particular, the time evolution of density distributions in the cases of a high-resolution grid is reproduced from that in the cases of a low-resolution grid through GANs, and the density inhomogeneity estimated from the image generated by GANs recovers the ground truth with good accuracy. Qualitative and quantitative comparisons of the results of the proposed method with those of several super-resolution algorithms are also presented. </details>
<details>	<summary>注释</summary>	Published in Frontiers in Artificial Intelligence DOI: 10.3389/frai.2021.670208 </details>
<details>	<summary>邮件日期</summary>	2021年09月23日</details>

# 262、具有EfficientNetV2的DEM超分辨率
- [ ] DEM Super-Resolution with EfficientNetV2 
时间：2021年09月20日                         第一作者：Bekir Z Demiray                       [链接](https://arxiv.org/abs/2109.09661).                     
## 摘要：高效的气候变化监测和建模依赖于高质量的地理空间和环境数据集。由于技术能力或资源的限制，获取许多环境学科的高质量数据成本高昂。数字高程模型（DEM）数据集就是这样的例子，而它们的低分辨率版本是广泛可用的，而高分辨率数据集则很少。为了解决这个问题，我们提出并评估了一个基于EfficientNetV2的模型。该模型在不增加额外信息的情况下，将DEM的空间分辨率提高了16倍。
<details>	<summary>英文摘要</summary>	Efficient climate change monitoring and modeling rely on high-quality geospatial and environmental datasets. Due to limitations in technical capabilities or resources, the acquisition of high-quality data for many environmental disciplines is costly. Digital Elevation Model (DEM) datasets are such examples whereas their low-resolution versions are widely available, high-resolution ones are scarce. In an effort to rectify this problem, we propose and assess an EfficientNetV2 based model. The proposed model increases the spatial resolution of DEMs up to 16times without additional information. </details>
<details>	<summary>注释</summary>	6 pages, 2 figures, 3 tables </details>
<details>	<summary>邮件日期</summary>	2021年09月21日</details>

# 261、使用图像统计的简单高效的未配对真实世界超分辨率
- [ ] Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics 
时间：2021年09月19日                         第一作者：Kwangjin Yoon                       [链接](https://arxiv.org/abs/2109.09071).                     
## 摘要：没有成对的低分辨率（LR）和高分辨率（HR）图像，学习超分辨率（SR）网络非常困难，因为无法通过相应的HR对应图像进行直接监控。最近，许多真实世界的SR研究利用了非成对图像到图像的转换技术。也就是说，他们使用了两个或多个生成性对抗网络（GAN），每个GAN将图像从一个域转换到另一个域，例如，将图像从HR域转换到LR域。然而，使用未配对的数据使用GANs来稳定地学习这样的翻译并不容易。在这项研究中，我们提出了一种简单有效的训练真实世界SR网络的方法。为了稳定地训练网络，我们使用图像块的统计信息，例如均值和方差。我们的真实SR框架由两个GAN组成，一个用于将HR图像转换为LR图像（降级任务），另一个用于将LR转换为HR（SR任务）。我们认为，利用我们提出的数据采样策略，即方差匹配，可以有效地学习使用GANs的未成对图像翻译。我们在NTIR2020真实世界SR数据集上测试了我们的方法。我们的方法在SSIM度量方面优于当前最先进的方法，并且在LPIPS度量上产生了可比较的结果。
<details>	<summary>英文摘要</summary>	Learning super-resolution (SR) network without the paired low resolution (LR) and high resolution (HR) image is difficult because direct supervision through the corresponding HR counterpart is unavailable. Recently, many real-world SR researches take advantage of the unpaired image-to-image translation technique. That is, they used two or more generative adversarial networks (GANs), each of which translates images from one domain to another domain, \eg, translates images from the HR domain to the LR domain. However, it is not easy to stably learn such a translation with GANs using unpaired data. In this study, we present a simple and efficient method of training of real-world SR network. To stably train the network, we use statistics of an image patch, such as means and variances. Our real-world SR framework consists of two GANs, one for translating HR images to LR images (degradation task) and the other for translating LR to HR (SR task). We argue that the unpaired image translation using GANs can be learned efficiently with our proposed data sampling strategy, namely, variance matching. We test our method on the NTIRE 2020 real-world SR dataset. Our method outperforms the current state-of-the-art method in terms of the SSIM metric as well as produces comparable results on the LPIPS metric. </details>
<details>	<summary>邮件日期</summary>	2021年09月21日</details>

# 260、WiSoSuper：对风能和太阳能数据的超分辨率方法进行基准测试
- [ ] WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data 
时间：2021年09月17日                         第一作者：Rupa Kurinchi-Vendhan                       [链接](https://arxiv.org/abs/2109.08770).                     
## 摘要：向绿色能源电网的过渡取决于详细的风能和太阳能预测，以优化可再生能源发电的选址和调度。然而，数值天气预报模型的业务预报的空间分辨率仅为10至20公里，这导致可再生能源农场的使用和发展处于次优状态。气象科学家一直在开发超分辨率方法来提高分辨率，但通常依赖于简单的插值技术或计算昂贵的基于微分方程的模型。最近，基于机器学习的模型，特别是基于物理的分辨率增强生成性对抗网络（PhIREGAN），已经超越了传统的降尺度方法。我们提供了领先的基于深度学习的超分辨率技术的全面和可扩展的基准，包括基于风能和太阳能数据的增强型超分辨率生成对抗网络（ESRGAN）和增强型深度超分辨率（EDSR）网络。我们为基准测试提供了一个新的公共的、经过处理的、支持机器学习的数据集，用于对风能和太阳能数据的超分辨率方法进行基准测试。
<details>	<summary>英文摘要</summary>	The transition to green energy grids depends on detailed wind and solar forecasts to optimize the siting and scheduling of renewable energy generation. Operational forecasts from numerical weather prediction models, however, only have a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and development of renewable energy farms. Weather scientists have been developing super-resolution methods to increase the resolution, but often rely on simple interpolation techniques or computationally expensive differential equation-based models. Recently, machine learning-based models, specifically the physics-informed resolution-enhancing generative adversarial network (PhIREGAN), have outperformed traditional downscaling methods. We provide a thorough and extensible benchmark of leading deep learning-based super-resolution techniques, including the enhanced super-resolution generative adversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR) network, on wind and solar data. We accompany the benchmark with a novel public, processed, and machine learning-ready dataset for benchmarking super-resolution methods on wind and solar data. </details>
<details>	<summary>邮件日期</summary>	2021年09月21日</details>

# 259、TANet：通过CNN聚合网络实现全局人脸超分辨率的新范式
- [ ] TANet: A new Paradigm for Global Face Super-resolution via Transformer-CNN Aggregation Network 
时间：2021年09月16日                         第一作者：Yuanzhi Wang                       [链接](https://arxiv.org/abs/2109.08174).                     
## 摘要：最近，人脸超分辨率（FSR）方法要么将整个人脸图像反馈到卷积神经网络（CNN）中，要么利用额外的人脸先验信息（例如，人脸解析图、人脸标记）来关注人脸结构，从而在恢复人脸细节的同时保持人脸结构的一致性。然而，CNN有限的感受野和不准确的面部先验信息会降低重建人脸的自然度和逼真度。在本文中，我们提出了一种基于自我注意机制（即变压器的核心）的新范式，以充分挖掘面部结构特征的表征能力。具体来说，我们设计了一个Transformer CNN聚合网络（TANet），该网络由两条路径组成，其中一条路径使用CNN来恢复细粒度的面部细节，而另一条路径使用资源友好的Transformer通过利用远程视觉关系建模来捕获全局信息。通过对上述两条路径的特征进行聚合，同时增强了全局人脸结构的一致性和局部人脸细节恢复的逼真度。人脸重建和识别的实验结果表明，该方法的性能明显优于现有的人脸识别方法。
<details>	<summary>英文摘要</summary>	Recently, face super-resolution (FSR) methods either feed whole face image into convolutional neural networks (CNNs) or utilize extra facial priors (e.g., facial parsing maps, facial landmarks) to focus on facial structure, thereby maintaining the consistency of the facial structure while restoring facial details. However, the limited receptive fields of CNNs and inaccurate facial priors will reduce the naturalness and fidelity of the reconstructed face. In this paper, we propose a novel paradigm based on the self-attention mechanism (i.e., the core of Transformer) to fully explore the representation capacity of the facial structure feature. Specifically, we design a Transformer-CNN aggregation network (TANet) consisting of two paths, in which one path uses CNNs responsible for restoring fine-grained facial details while the other utilizes a resource-friendly Transformer to capture global information by exploiting the long-distance visual relation modeling. By aggregating the features from the above two paths, the consistency of global facial structure and fidelity of local facial detail restoration are strengthened simultaneously. Experimental results of face reconstruction and recognition verify that the proposed method can significantly outperform the state-of-the-art methods. </details>
<details>	<summary>注释</summary>	9 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月20日</details>

# 258、D2C-SR：一种用于实际图像超分辨率的发散收敛方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution 
时间：2021年09月15日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月16日</details>

# 257、D2C-SR：一种用于实际图像超分辨率的发散收敛方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution 
时间：2021年09月14日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月15日</details>

# 256、通过自适应下采样模型实现真实世界的超分辨率
- [ ] Toward Real-World Super-Resolution via Adaptive Downsampling Models 
时间：2021年09月08日                         第一作者：Sanghyun Son                        [链接](https://arxiv.org/abs/2109.03444).                     
## 摘要：大多数图像超分辨率（SR）方法是在合成低分辨率（LR）和高分辨率（HR）图像对上开发的，这些图像对是通过预定操作（例如双三次下采样）构建的。由于现有方法通常学习特定函数的逆映射，因此当应用于精确公式不同且未知的真实图像时，会产生模糊结果。因此，有几种方法试图合成更多样化的LR样本或学习真实的下采样模型。然而，由于对下采样过程的限制性假设，它们仍然存在偏见，不太具有普遍性。本研究提出了一种新的方法来模拟未知的下采样过程，而不施加限制性的先验知识。我们在对抗性训练框架中提出了一种广义低频损耗（LFL）方法，以模拟目标LR图像的分布，而无需使用任何成对的例子。此外，我们还为下采样器设计了一种自适应数据丢失（ADL），它可以在训练循环中自适应地从数据中学习和更新。大量的实验验证了我们的下采样模型可以促进现有的SR方法对各种合成和真实示例执行比传统方法更精确的重建。
<details>	<summary>英文摘要</summary>	Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. Therefore, several methods attempt to synthesize much more diverse LR samples or learn a realistic downsampling model. However, due to restrictive assumptions on the downsampling process, they are still biased and less generalizable. This study proposes a novel method to simulate an unknown downsampling process without imposing restrictive prior knowledge. We propose a generalizable low-frequency loss (LFL) in the adversarial training framework to imitate the distribution of target LR images without using any paired examples. Furthermore, we design an adaptive data loss (ADL) for the downsampler, which can be adaptively learned and updated from the data during the training loops. Extensive experiments validate that our downsampling model can facilitate existing SR methods to perform more accurate reconstructions on various synthetic and real-world examples than the conventional approaches. </details>
<details>	<summary>注释</summary>	Accepted at TPAMI DOI: 10.1109/TPAMI.2021.3106790 </details>
<details>	<summary>邮件日期</summary>	2021年09月09日</details>

# 255、用物理引导神经网络重建高分辨率湍流
- [ ] Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks 
时间：2021年09月06日                         第一作者：Shengyu Chen                       [链接](https://arxiv.org/abs/2109.03327).                     
## 摘要：湍流的直接数值模拟（DNS）在计算上非常昂贵，无法应用于雷诺数较大的流动。大涡模拟（LES）是一种计算要求较低的替代方法，但无法准确捕捉湍流输送的所有尺度。我们在这项工作中的目标是建立一种新的基于超分辨率技术的数据驱动方法，以从LES预测中重建DNS数据。我们利用潜在的物理关系来规范不同物理变量之间的关系。我们还引入了分层生成过程和反向降级过程，以充分探索DNS和LES数据之间的对应关系。我们通过一个单快照实验和一个跨时间实验证明了该方法的有效性。结果表明，在像素级重建误差和结构相似性方面，我们的方法能够更好地在空间和时间上重建高分辨率DNS数据。视觉比较表明，我们的方法在捕捉精细水平流动力学方面表现得更好。
<details>	<summary>英文摘要</summary>	Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Large eddy simulation (LES) is an alternative that is computationally less demanding, but is unable to capture all of the scales of turbulent transport accurately. Our goal in this work is to build a new data-driven methodology based on super-resolution techniques to reconstruct DNS data from LES predictions. We leverage the underlying physical relationships to regularize the relationships amongst different physical variables. We also introduce a hierarchical generative process and a reverse degradation process to fully explore the correspondence between DNS and LES data. We demonstrate the effectiveness of our method through a single-snapshot experiment and a cross-time experiment. The results confirm that our method can better reconstruct high-resolution DNS data over space and over time in terms of pixel-wise reconstruction error and structural similarity. Visual comparisons show that our method performs much better in capturing fine-level flow dynamics. </details>
<details>	<summary>邮件日期</summary>	2021年09月09日</details>

# 254、双耳声网络：用双耳声音预测语义、深度和运动
- [ ] Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds 
时间：2021年09月06日                         第一作者：Dengxin Dai                       [链接](https://arxiv.org/abs/2109.02763).                     
## 摘要：人类可以通过视觉和/或听觉线索来稳健地识别和定位物体。虽然机器已经能够对视觉数据进行同样的处理，但对声音的处理却很少。这项工作开发了一种完全基于双耳声音的场景理解方法。所考虑的任务包括预测发声对象的语义掩码、发声对象的运动以及场景的深度图。为此，我们提出了一种新的传感器设置，并使用八个专业的双耳麦克风和一个360度摄像机记录了一个新的街景视听数据集。视觉和听觉线索的共存被用于监督转移。特别是，我们采用了一个跨模式蒸馏框架，该框架由多个视觉教师方法和一个良好的学生方法组成——学生方法经过训练，可以产生与教师方法相同的结果。这样，听觉系统就可以在不使用人类注释的情况下进行训练。为了进一步提高性能，我们提出了另一个新的辅助任务，即创造空间声音超分辨率，以提高声音的方向分辨率。然后，我们将这四个任务组成一个端到端可训练的多任务网络，以提高整体性能。实验结果表明：1）我们的方法在所有四项任务中都取得了良好的效果；2）这四项任务是互利的——共同训练它们可以获得最佳性能；3）麦克风的数量和方向都很重要，4）从标准频谱图学习的特征和通过经典信号处理管道获得的特征对于听觉感知任务是互补的。数据和代码已发布。
<details>	<summary>英文摘要</summary>	Humans can robustly recognize and localize objects by using visual and/or auditory cues. While machines are able to do the same with visual data already, less work has been done with sounds. This work develops an approach for scene understanding purely based on binaural sounds. The considered tasks include predicting the semantic masks of sound-making objects, the motion of sound-making objects, and the depth map of the scene. To this aim, we propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360-degree camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of multiple vision teacher methods and a sound student method -- the student method is trained to generate the same results as the teacher methods do. This way, the auditory system can be trained without using human annotations. To further boost the performance, we propose another novel auxiliary task, coined Spatial Sound Super-Resolution, to increase the directional resolution of sounds. We then formulate the four tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results show that 1) our method achieves good results for all four tasks, 2) the four tasks are mutually beneficial -- training them together achieves the best performance, 3) the number and orientation of microphones are both important, and 4) features learned from the standard spectrogram and features obtained by the classic signal processing pipeline are complementary for auditory perception tasks. The data and code are released. </details>
<details>	<summary>注释</summary>	Journal extension of our ECCV'20 Paper -- 15 pages. arXiv admin note: substantial text overlap with arXiv:2003.04210 </details>
<details>	<summary>邮件日期</summary>	2021年09月08日</details>

# 253、双摄像头超分辨率，带对齐注意模块
- [ ] Dual-Camera Super-Resolution with Aligned Attention Modules 
时间：2021年09月06日                         第一作者：Tengfei Wang                       [链接](https://arxiv.org/abs/2109.01349).                     
<details>	<summary>注释</summary>	Accepted to ICCV 2021 (oral) </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 252、Fusformer：一种基于变换器的高光谱图像超分辨率融合方法
- [ ] Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution 
时间：2021年09月05日                         第一作者：Jin-Fan Hu                        [链接](https://arxiv.org/abs/2109.02079).                     
## 摘要：高光谱图像由于其丰富的光谱信息而变得越来越重要。然而，受现有成像机制的限制，其空间分辨率较差。目前，许多卷积神经网络被提出用于高光谱图像的超分辨率问题。然而卷积神经网络（美国有线电视新闻网）的方法只考虑局部信息而不是全局信息，而卷积运算中接收域的核大小有限。本文设计了一种基于变压器的网络，用于融合低分辨率高光谱图像和高分辨率多光谱图像，获得高分辨率高光谱图像。由于transformer的表示能力，我们的方法能够在全局范围内探索特性的内在关系。此外，考虑到LR HSI是主要的光谱结构，网络将重点放在空间细节估计上，以减轻重建整个数据的负担。它减少了网络的映射空间，提高了网络的最终性能。各种实验和质量指标表明，与其他先进方法相比，我们的方法具有优越性。
<details>	<summary>英文摘要</summary>	Hyperspectral image has become increasingly crucial due to its abundant spectral information. However, It has poor spatial resolution with the limitation of the current imaging mechanism. Nowadays, many convolutional neural networks have been proposed for the hyperspectral image super-resolution problem. However, convolutional neural network (CNN) based methods only consider the local information instead of the global one with the limited kernel size of receptive field in the convolution operation. In this paper, we design a network based on the transformer for fusing the low-resolution hyperspectral images and high-resolution multispectral images to obtain the high-resolution hyperspectral images. Thanks to the representing ability of the transformer, our approach is able to explore the intrinsic relationships of features globally. Furthermore, considering the LR-HSIs hold the main spectral structure, the network focuses on the spatial detail estimation releasing from the burden of reconstructing the whole data. It reduces the mapping space of the proposed network, which enhances the final performance. Various experiments and quality indexes show our approach's superiority compared with other state-of-the-art methods. </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 251、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年09月04日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 250、双摄像头超分辨率，带对齐注意模块
- [ ] Dual-Camera Super-Resolution with Aligned Attention Modules 
时间：2021年09月03日                         第一作者：Tengfei Wang                       [链接](https://arxiv.org/abs/2109.01349).                     
## 摘要：我们提出了一种新的基于参考的超分辨率（RefSR）方法，重点是双摄像机超分辨率（DCSR），它利用参考图像获得高质量和高保真的结果。我们提出的方法推广了标准的基于面片的特征匹配和空间对齐操作。我们进一步探索了RefSR的一个有前途的应用——双摄像机超分辨率，并构建了一个数据集，该数据集由智能手机中主摄像机和长焦摄像机的146个图像对组成。为了弥补真实世界图像和训练图像之间的领域差距，我们提出了一种针对真实世界图像的自监督领域自适应策略。在我们的数据集和一个公共基准上进行的大量实验表明，我们的方法在定量评估和视觉比较方面都比最先进的方法有明显的改进。
<details>	<summary>英文摘要</summary>	We present a novel approach to reference-based super-resolution (RefSR) with the focus on dual-camera super-resolution (DCSR), which utilizes reference images for high-quality and high-fidelity results. Our proposed method generalizes the standard patch-based feature matching with spatial alignment operations. We further explore the dual-camera super-resolution that is one promising application of RefSR, and build a dataset that consists of 146 image pairs from the main and telephoto cameras in a smartphone. To bridge the domain gaps between real-world images and the training images, we propose a self-supervised domain adaptation strategy for real-world images. Extensive experiments on our dataset and a public benchmark demonstrate clear improvement achieved by our method over state of the art in both quantitative evaluation and visual comparisons. </details>
<details>	<summary>注释</summary>	ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年09月06日</details>

# 249、高光谱图像去噪、光谱校正和高分辨率RGB重建的深度学习方法
- [ ] Deep Learning Approach for Hyperspectral Image Demosaicking, Spectral Correction and High-resolution RGB Reconstruction 
时间：2021年09月03日                         第一作者：Peichao Li                       [链接](https://arxiv.org/abs/2109.01403).                     
## 摘要：高光谱成像是术中组织特征化最有前途的技术之一。快照马赛克相机可以在一次曝光中捕获高光谱数据，有可能使手术决策的实时高光谱成像系统成为可能。然而，对捕获数据的优化利用需要解决不适定的解模糊问题，并应用额外的光谱校正来恢复图像的空间和光谱信息。在这项工作中，我们提出了一种基于深度学习的基于监督学习方法的快照高光谱图像去噪算法。由于缺乏使用快照马赛克相机获取的公共可用医学图像，因此提出了一种合成图像生成方法，以模拟由高分辨率但速度较慢的高光谱成像设备捕获的现有医学图像数据集中的快照图像。利用卷积神经网络实现高光谱图像的超分辨率重建，然后利用传感器特定的校准矩阵进行串扰和泄漏校正。所得到的去马赛克图像进行了定量和定性评估，与使用线性插值的基线去马赛克方法相比，显示出明显的图像质量改进。此外，我们的算法为最先进的快照马赛克相机获得每帧超分辨率RGB或氧饱和度图的快速处理时间约为45 \，ms，表明其无缝集成到实时外科高光谱成像应用中的潜力。
<details>	<summary>英文摘要</summary>	Hyperspectral imaging is one of the most promising techniques for intraoperative tissue characterisation. Snapshot mosaic cameras, which can capture hyperspectral data in a single exposure, have the potential to make a real-time hyperspectral imaging system for surgical decision-making possible. However, optimal exploitation of the captured data requires solving an ill-posed demosaicking problem and applying additional spectral corrections to recover spatial and spectral information of the image. In this work, we propose a deep learning-based image demosaicking algorithm for snapshot hyperspectral images using supervised learning methods. Due to the lack of publicly available medical images acquired with snapshot mosaic cameras, a synthetic image generation approach is proposed to simulate snapshot images from existing medical image datasets captured by high-resolution, but slow, hyperspectral imaging devices. Image reconstruction is achieved using convolutional neural networks for hyperspectral image super-resolution, followed by cross-talk and leakage correction using a sensor-specific calibration matrix. The resulting demosaicked images are evaluated both quantitatively and qualitatively, showing clear improvements in image quality compared to a baseline demosaicking method using linear interpolation. Moreover, the fast processing time of~45\,ms of our algorithm to obtain super-resolved RGB or oxygenation saturation maps per image frame for a state-of-the-art snapshot mosaic camera demonstrates the potential for its seamless integration into real-time surgical hyperspectral imaging applications. </details>
<details>	<summary>邮件日期</summary>	2021年09月06日</details>

# 248、基于非均匀卷积WGAN的红外图像超分辨率分析
- [ ] Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN 
时间：2021年09月02日                         第一作者：Yongsong Huang                       [链接](https://arxiv.org/abs/2109.00960).                     
## 摘要：图像超分辨率在监控、遥感等领域具有重要意义。然而，由于光学设备相对昂贵，红外（IR）图像通常具有低分辨率。近年来，深度学习方法在图像超分辨率方面占据主导地位，并在可见光图像上取得了显著的效果；然而，红外图像受到的关注较少。红外图像具有较少的模式，因此，深度神经网络（DNN）很难从红外图像中学习不同的特征。在本文中，我们提出了一个采用异构卷积和对抗性训练的红外图像超分辨率框架，即基于异构核的超分辨率Wasserstein-GAN（HetSRWGAN）。HetSRWGAN算法是一种轻量级的GAN体系结构，它应用了一种即插即用的异构内核剩余块。此外，采用了一种新的基于图像梯度的损耗函数，该函数可以应用于任意模型。拟议的HetSRWGAN在定性和定量评估方面都取得了一致的更好的绩效。实验结果表明，整个训练过程较为稳定。
<details>	<summary>英文摘要</summary>	Image super-resolution is important in many fields, such as surveillance and remote sensing. However, infrared (IR) images normally have low resolution since the optical equipment is relatively expensive. Recently, deep learning methods have dominated image super-resolution and achieved remarkable performance on visible images; however, IR images have received less attention. IR images have fewer patterns, and hence, it is difficult for deep neural networks (DNNs) to learn diverse features from IR images. In this paper, we present a framework that employs heterogeneous convolution and adversarial training, namely, heterogeneous kernel-based super-resolution Wasserstein GAN (HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a lightweight GAN architecture that applies a plug-and-play heterogeneous kernel-based residual block. Moreover, a novel loss function that employs image gradients is adopted, which can be applied to an arbitrary model. The proposed HetSRWGAN achieves consistently better performance in both qualitative and quantitative evaluations. According to the experimental results, the whole training process is more stable. </details>
<details>	<summary>注释</summary>	To be published in the 18th Pacific Rim International Conference on Artificial Intelligence (PRICAI-2021) </details>
<details>	<summary>邮件日期</summary>	2021年09月03日</details>

# 247、基于CNN的图像超分辨率双参考训练数据采集方法
- [ ] An Efficient Dual-reference Training Data Acquisition Method for CNN-Based Image Super-Resolution 
时间：2021年09月02日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
<details>	<summary>邮件日期</summary>	2021年09月03日</details>

# 246、基于深度学习的人脸超分辨率研究综述
- [ ] Deep Learning-based Face Super-Resolution: A Survey 
时间：2021年09月01日                         第一作者：Junjun Jiang                       [链接](https://arxiv.org/abs/2101.03749).                     
<details>	<summary>注释</summary>	Accepted to ACM Computing Surveys </details>
<details>	<summary>邮件日期</summary>	2021年09月02日</details>

# 245、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年08月31日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
<details>	<summary>邮件日期</summary>	2021年09月02日</details>

# 244、用于高光谱人脸超分辨率的光谱分裂和聚合网络
- [ ] Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution 
时间：2021年08月31日                         第一作者：Junjun Jiang                        [链接](https://arxiv.org/abs/2108.13584).                     
## 摘要：高分辨率（HR）高光谱人脸图像在非受控条件下（如弱光环境和欺骗攻击）与人脸相关的计算机视觉任务中起着重要作用。然而，高光谱人脸图像的密集光谱带是以有限数量的光子平均到达狭窄的光谱窗口为代价的，这大大降低了高光谱人脸图像的空间分辨率。在本文中，我们研究了如何将深度学习技术应用于高光谱人脸图像超分辨率（HFSR），特别是在训练样本非常有限的情况下。利用谱带的数量，每个谱带都可以看作一幅图像，我们提出了一个训练样本有限的HFSR谱分裂和聚合网络（SSANet）。在浅层中，我们将高光谱图像分成不同的光谱组，并将每个光谱组作为一个单独的训练样本（从某种意义上说，每个光谱组将被送入同一个网络）。然后，我们逐渐聚集更深层次的相邻波段，以利用光谱相关性。通过这种光谱分割和聚集策略（SSAS），我们可以将原始高光谱图像分割成多个样本，以支持网络的有效训练，并有效地利用光谱之间的光谱相关性。为了应对小训练样本量（S3）问题的挑战，我们建议通过自表示模型和对称诱导增强来扩展训练样本。实验表明，引入的SSANet能够很好地模拟空间和光谱信息的联合相关性。通过扩展训练样本，我们提出的方法可以有效地缓解S3问题。比较结果表明，我们提出的方法优于现有的方法。
<details>	<summary>英文摘要</summary>	High-resolution (HR) hyperspectral face image plays an important role in face related computer vision tasks under uncontrolled conditions, such as low-light environment and spoofing attacks. However, the dense spectral bands of hyperspectral face images come at the cost of limited amount of photons reached a narrow spectral window on average, which greatly reduces the spatial resolution of hyperspectral face images. In this paper, we investigate how to adapt the deep learning techniques to hyperspectral face image super-resolution (HFSR), especially when the training samples are very limited. Benefiting from the amount of spectral bands, in which each band can be seen as an image, we present a spectral splitting and aggregation network (SSANet) for HFSR with limited training samples. In the shallow layers, we split the hyperspectral image into different spectral groups and take each of them as an individual training sample (in the sense that each group will be fed into the same network). Then, we gradually aggregate the neighbor bands at the deeper layers to exploit the spectral correlations. By this spectral splitting and aggregation strategy (SSAS), we can divide the original hyperspectral image into multiple samples to support the efficient training of the network and effectively exploit the spectral correlations among spectrum. To cope with the challenge of small training sample size (S3) problem, we propose to expand the training samples by a self-representation model and symmetry-induced augmentation. Experiments show that the introduced SSANet can well model the joint correlations of spatial and spectral information. By expanding the training samples, our proposed method can effectively alleviate the S3 problem. The comparison results demonstrate that our proposed method can outperform the state-of-the-arts. </details>
<details>	<summary>注释</summary>	12 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 243、基于注意的图像超分辨率多参考学习
- [ ] Attention-based Multi-Reference Learning for Image Super-Resolution 
时间：2021年08月31日                         第一作者：Marco Pesavento                       [链接](https://arxiv.org/abs/2108.13697).                     
## 摘要：本文提出了一种新的基于注意的多参考超分辨率网络（AMRSR），该网络在给定低分辨率图像的情况下，学习将多参考图像中最相似的纹理自适应地传输到超分辨率输出，同时保持空间一致性。在多个基准数据集上，使用多个参考图像和基于注意的采样可以显著提高最先进的参考超分辨率方法的性能。参考超分辨率方法最近被提出，通过提供来自高分辨率参考图像的附加信息来克服图像超分辨率的不适定问题。多参考超分辨率通过提供更多样化的图像特征池来扩展此方法，以克服固有的信息不足，同时保持内存效率。提出了一种新的基于分层注意的抽样方法，用于基于感知损失的低分辨率图像特征与多幅参考图像之间的相似性学习。消融证明了多参考和基于分层注意的抽样对整体表现的贡献。即使参考图像明显偏离目标图像，感知和定量的地面真实度评估也显示出显著的性能改进。项目网站可在以下网址找到：https://marcopesavento.github.io/AMRSR/
<details>	<summary>英文摘要</summary>	This paper proposes a novel Attention-based Multi-Reference Super-resolution network (AMRSR) that, given a low-resolution image, learns to adaptively transfer the most similar texture from multiple reference images to the super-resolution output whilst maintaining spatial coherence. The use of multiple reference images together with attention-based sampling is demonstrated to achieve significantly improved performance over state-of-the-art reference super-resolution approaches on multiple benchmark datasets. Reference super-resolution approaches have recently been proposed to overcome the ill-posed problem of image super-resolution by providing additional information from a high-resolution reference image. Multi-reference super-resolution extends this approach by providing a more diverse pool of image features to overcome the inherent information deficit whilst maintaining memory efficiency. A novel hierarchical attention-based sampling approach is introduced to learn the similarity between low-resolution image features and multiple reference images based on a perceptual loss. Ablation demonstrates the contribution of both multi-reference and hierarchical attention-based sampling to overall performance. Perceptual and quantitative ground-truth evaluation demonstrates significant improvement in performance even when the reference images deviate significantly from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/ </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 242、4D人体表演的超分辨率外观转移
- [ ] Super-Resolution Appearance Transfer for 4D Human Performances 
时间：2021年08月31日                         第一作者：Marco Pesavento                       [链接](https://arxiv.org/abs/2108.13739).                     
## 摘要：从多视点视频中对人进行4D重建的一个常见问题是捕获的动态纹理外观的质量，这取决于相机分辨率和捕获体积。通常，要求对摄像机进行帧处理以捕获动态性能的体积（$>50m^3$）导致人员仅占据视野的一小部分$<$10%。即使使用超高清晰度4k视频采集，这也会导致以低于标准清晰度0.5k视频分辨率对人进行采样，从而导致低质量渲染。在本文中，我们提出了一种解决方案，通过使用数字静物照相机（$>8k$）从静态高分辨率外观捕捉装置进行超分辨率外观转移，以小体积（$<8m^3$）捕捉人物。提出了一种从高分辨率静态捕获到动态视频性能捕获的超分辨率外观转换管道，以生成超分辨率动态纹理。这解决了两个关键问题：不同摄像机系统之间的颜色映射；并利用学习到的模型进行动态纹理贴图超分辨率处理。对比评估表明，在呈现具有超分辨率动态纹理外观的4D性能捕获方面，在定性和定量方面都有显著改进。所提出的方法再现了静态捕获的高分辨率细节，同时保持捕获视频的外观动态。
<details>	<summary>英文摘要</summary>	A common problem in the 4D reconstruction of people from multi-view video is the quality of the captured dynamic texture appearance which depends on both the camera resolution and capture volume. Typically the requirement to frame cameras to capture the volume of a dynamic performance ($>50m^3$) results in the person occupying only a small proportion $<$ 10% of the field of view. Even with ultra high-definition 4k video acquisition this results in sampling the person at less-than standard definition 0.5k video resolution resulting in low-quality rendering. In this paper we propose a solution to this problem through super-resolution appearance transfer from a static high-resolution appearance capture rig using digital stills cameras ($> 8k$) to capture the person in a small volume ($<8m^3$). A pipeline is proposed for super-resolution appearance transfer from high-resolution static capture to dynamic video performance capture to produce super-resolution dynamic textures. This addresses two key problems: colour mapping between different camera systems; and dynamic texture map super-resolution using a learnt model. Comparative evaluation demonstrates a significant qualitative and quantitative improvement in rendering the 4D performance capture with super-resolution dynamic texture appearance. The proposed approach reproduces the high-resolution detail of the static capture whilst maintaining the appearance dynamics of the captured video. </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 241、复杂噪声下的无监督单幅图像超分辨率
- [ ] Unsupervised Single Image Super-resolution Under Complex Noise 
时间：2021年08月30日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 240、通过对抗鲁棒性实现广义真实世界超分辨率
- [ ] Generalized Real-World Super-Resolution through Adversarial Robustness 
时间：2021年08月25日                         第一作者：Angela Castillo                       [链接](https://arxiv.org/abs/2108.11505).                     
## 摘要：传统上，解决真实世界超分辨率（SR）问题的方法是首先学习一种特定的退化模型，该模型类似于低分辨率图像中的噪声和腐败伪影。因此，当前的方法缺乏泛化性，在对看不见的腐败类型进行测试时会失去准确性。与传统方案相比，我们提出了鲁棒超分辨率（RSR），这是一种利用对抗性攻击的泛化能力来处理真实世界SR的方法。我们的新框架为真实世界SR方法的发展带来了范式转变。我们没有学习特定于数据集的降级，而是使用对抗性攻击来创建针对模型弱点的困难示例。之后，我们在训练期间使用这些对抗性示例来提高模型处理噪声输入的能力。我们在合成图像和真实图像上进行了大量实验，并通过经验证明，我们的RSR方法在数据集上具有良好的通用性，无需对特定的噪声先验进行重新训练。通过使用单一的稳健模型，我们在现实世界的基准测试中的表现优于最先进的专门方法。
<details>	<summary>英文摘要</summary>	Real-world Super-Resolution (SR) has been traditionally tackled by first learning a specific degradation model that resembles the noise and corruption artifacts in low-resolution imagery. Thus, current methods lack generalization and lose their accuracy when tested on unseen types of corruption. In contrast to the traditional proposal, we present Robust Super-Resolution (RSR), a method that leverages the generalization capability of adversarial attacks to tackle real-world SR. Our novel framework poses a paradigm shift in the development of real-world SR methods. Instead of learning a dataset-specific degradation, we employ adversarial attacks to create difficult examples that target the model's weaknesses. Afterward, we use these adversarial examples during training to improve our model's capacity to process noisy inputs. We perform extensive experimentation on synthetic and real-world images and empirically demonstrate that our RSR method generalizes well across datasets without re-training for specific noise priors. By using a single robust model, we outperform state-of-the-art specialized methods on real-world benchmarks. </details>
<details>	<summary>注释</summary>	ICCV Workshops, 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月27日</details>

# 239、基于记忆增强的视频超分辨率非局部注意
- [ ] Memory-Augmented Non-Local Attention for Video Super-Resolution 
时间：2021年08月25日                         第一作者：Jiyang Yu                       [链接](https://arxiv.org/abs/2108.11048).                     
## 摘要：在本文中，我们提出了一种新的视频超分辨率方法，旨在从低分辨率（LR）视频生成高保真高分辨率（HR）视频。以前的方法主要利用时间相邻帧来辅助当前帧的超分辨率。这些方法的性能有限，因为它们在空间帧对齐方面面临挑战，并且缺乏来自相似LR相邻帧的有用信息。相比之下，我们设计了一种跨帧非局部注意机制，允许视频超分辨率而无需帧对齐，从而对视频中的大运动更具鲁棒性。此外，为了获取相邻帧以外的信息，我们设计了一种新的记忆增强注意模块，用于在超分辨率训练中记忆一般的视频细节。实验结果表明，与现有的无帧对齐方法相比，我们的方法在大运动视频上可以获得更好的性能。我们的源代码将被发布。
<details>	<summary>英文摘要</summary>	In this paper, we propose a novel video super-resolution method that aims at generating high-fidelity high-resolution (HR) videos from low-resolution (LR) ones. Previous methods predominantly leverage temporal neighbor frames to assist the super-resolution of the current frame. Those methods achieve limited performance as they suffer from the challenge in spatial frame alignment and the lack of useful information from similar LR neighbor frames. In contrast, we devise a cross-frame non-local attention mechanism that allows video super-resolution without frame alignment, leading to be more robust to large motions in the video. In addition, to acquire the information beyond neighbor frames, we design a novel memory-augmented attention module to memorize general video details during the super-resolution training. Experimental results indicate that our method can achieve superior performance on large motion videos comparing to the state-of-the-art methods without aligning frames. Our source code will be released. </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 238、用于单图像超分辨率的高效变换器
- [ ] Efficient Transformer for Single Image Super-Resolution 
时间：2021年08月25日                         第一作者：Zhisheng Lu                       [链接](https://arxiv.org/abs/2108.11084).                     
## 摘要：随着深度学习的发展，单幅图像的超分辨率任务有了长足的发展。然而，现有的大多数研究都集中在构建一个具有大量层的更复杂的神经网络，这会带来沉重的计算成本和内存存储。近年来，随着Transformer在NLP任务中取得了辉煌的成果，越来越多的研究者开始探索Transformer在计算机视觉任务中的应用。但由于视觉转换器的计算量大、GPU内存占用率高，网络设计不能太深。为了解决这个问题，我们提出了一种新的高效超分辨率转换器（ESRT），用于快速准确的图像超分辨率。ESRT是一种混合变压器，首先在前端设计基于CNN的SR网络以提取深层特征。具体来说，有两个主干用于格式化ESRT：轻型CNN主干（LCB）和轻型转换器主干（LTB）。其中，LCB是一种轻量级SR网络，通过动态调整特征映射的大小，以较低的计算成本提取深层SR特征。LTB由一个高效转换器（ET）组成，具有较小的GPU内存占用，这得益于新型高效多头注意（EMHA）。在EMHA中，提出了一种特征分割模块（FSM）将长序列分割成子段，然后通过注意操作应用这些子段。该模块可以显著减少GPU内存占用。大量实验表明，我们的ESRT取得了有竞争力的结果。与原变压器占用16057M GPU内存相比，该变压器仅占用4191M GPU内存，性能更好。
<details>	<summary>英文摘要</summary>	Single image super-resolution task has witnessed great strides with the development of deep learning. However, most existing studies focus on building a more complex neural network with a massive number of layers, bringing heavy computational cost and memory storage. Recently, as Transformer yields brilliant results in NLP tasks, more and more researchers start to explore the application of Transformer in computer vision tasks. But with the heavy computational cost and high GPU memory occupation of the vision Transformer, the network can not be designed too deep. To address this problem, we propose a novel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image super-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is first designed in the front to extract deep features. Specifically, there are two backbones for formatting the ESRT: lightweight CNN backbone (LCB) and lightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR network to extract deep SR features at a low computational cost by dynamically adjusting the size of the feature map. LTB is made up of an efficient Transformer (ET) with a small GPU memory occupation, which benefited from the novel efficient multi-head attention (EMHA). In EMHA, a feature split module (FSM) is proposed to split the long sequence into sub-segments and then these sub-segments are applied by attention operation. This module can significantly decrease the GPU memory occupation. Extensive experiments show that our ESRT achieves competitive results. Compared with the original Transformer which occupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with better performance. </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 237、多属性结构化文本到人脸合成
- [ ] Multi-Attributed and Structured Text-to-Face Synthesis 
时间：2021年08月25日                         第一作者：Rohan Wadhawan                       [链接](https://arxiv.org/abs/2108.11100).                     
## 摘要：生成性对抗网络（GAN）通过人脸生成、照片编辑和图像超分辨率等许多应用，彻底改变了图像合成。使用GANs的图像合成主要是单峰的，很少有方法可以从文本或其他数据模式合成图像。文本到图像的合成，特别是文本到人脸的合成，在从目击者的叙述中生成健壮的人脸和用视觉线索增强阅读体验方面有着很好的应用前景。然而，只有几个数据集为文本到面合成提供了整合的面数据和文本描述。此外，这些文本注释不太广泛和描述性，这减少了从中生成的面的多样性。本文实证证明，增加每个文本描述中人脸属性的数量有助于生成更加多样化和真实的人脸。为了证明这一点，我们提出了一种新的方法，重点是使用结构化的文本描述。我们还整合了一个多属性和结构化文本到人脸（MAST）数据集，该数据集由具有结构化文本注释的高质量图像组成，并可供研究人员进行实验和构建。最后，我们报告了MAST数据集的基准Frechet起始距离（FID）、面部语义相似性（FSS）和面部语义距离（FSD）分数。
<details>	<summary>英文摘要</summary>	Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Frechet's Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset. </details>
<details>	<summary>注释</summary>	Accepted by IEEE TEMSMET 2020, Camera Ready Version </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 236、一种高效的CNN图像超分辨率双参考训练数据采集方法
- [ ] An Efficient Dual-reference Training Data Acquisition Method for CNN Image Super-Resolution 
时间：2021年08月24日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 235、边缘SR：质量的超分辨率
- [ ] edge-SR: Super-Resolution For The Masses 
时间：2021年08月23日                         第一作者：Pablo Navarrete Michelini                       [链接](https://arxiv.org/abs/2108.10335).                     
## 摘要：经典的图像缩放（如双三次）可以看作是一个卷积层和一个放大滤波器。它的实现在所有显示设备和图像处理软件中无处不在。在过去的十年中，深度学习系统已经被引入到图像超分辨率（SR）的任务中，使用了几个卷积层和许多滤波器。这些方法已经取代了放大任务的图像质量基准。是否有可能用显示面板、平板电脑、笔记本电脑等边缘设备上的深度学习体系结构取代经典的升级。？一方面，随着能够高效运行深度学习任务的硬件的快速发展，Edge AI芯片的当前趋势显示了这一方向的良好前景。另一方面，在image-SR中，只有少数体系结构将极限推到了可以在边缘设备上实时运行的极小尺寸。我们探索这个问题的可能解决方案，旨在填补经典升级和小型深度学习配置之间的差距。作为从经典到深度学习放大的过渡，我们提出了edge SR（eSR），这是一组使用可解释机制来放大图像的单层体系结构。当然，单层体系结构无法达到深度学习系统的质量。然而，我们发现，对于高速需求，eSR在权衡图像质量和运行时性能方面变得更好。填补经典体系结构和深度学习体系结构之间的差距，实现图像的放大，对于大规模采用这项技术至关重要。同样重要的是，有一个可解释的系统，可以揭示解决这个问题的内在策略，并指导我们未来的改进和更好地理解更大的网络。
<details>	<summary>英文摘要</summary>	Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks. </details>
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 234、SwinIR：使用Swin变压器进行图像恢复
- [ ] SwinIR: Image Restoration Using Swin Transformer 
时间：2021年08月23日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.10257).                     
## 摘要：图像恢复是一个长期存在的低级视觉问题，其目的是从低质量图像（例如缩小的、有噪声的和压缩的图像）恢复高质量图像。虽然最先进的图像恢复方法是基于卷积神经网络的，但很少有人尝试使用变压器，因为变压器在高级视觉任务中表现出令人印象深刻的性能。在本文中，我们提出了一种基于Swin变换的强基线图像恢复模型SwinIR。SwinIR由三部分组成：浅层特征提取、深层特征提取和高质量图像重建。具体地说，深度特征提取模块由几个剩余的Swin变压器块（RSTB）组成，每个块都有几个Swin变压器层和一个剩余连接。我们在三个具有代表性的任务上进行了实验：图像超分辨率（包括经典、轻量级和真实世界的图像超分辨率）、图像去噪（包括灰度和彩色图像去噪）和JPEG压缩伪影减少。实验结果表明，在不同的任务上，SwinIR的性能优于最先进的方法$\textbf{高达0.14$\sim$0.45dB}$，而参数总数可以减少$\textbf{高达67%}$。
<details>	<summary>英文摘要</summary>	Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\textbf{up to 0.14$\sim$0.45dB}$, while the total number of parameters can be reduced by $\textbf{up to 67%}$. </details>
<details>	<summary>注释</summary>	Sota results on classical/lightweight/real-world image SR, image denoising and JPEG compression artifact reduction. Code: https://github.com/JingyunLiang/SwinIR </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 233、Lucas Kanade重新加载：原始图像突发的端到端超分辨率
- [ ] Lucas-Kanade Reloaded: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年08月23日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 232、野外非配对深度超分辨率
- [ ] Unpaired Depth Super-Resolution in the Wild 
时间：2021年08月23日                         第一作者：Aleks                       [链接](https://arxiv.org/abs/2105.12038).                     
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 231、用局部属性图解释超分辨率网络
- [ ] Interpreting Super-Resolution Networks with Local Attribution Maps 
时间：2021年08月22日                         第一作者：Jinjin Gu                       [链接](https://arxiv.org/abs/2011.11036).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 230、用神经网络结构和剪枝搜索实现移动实时超分辨率
- [ ] Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search 
时间：2021年08月18日                         第一作者：Zheng Zhan                       [链接](https://arxiv.org/abs/2108.08910).                     
## 摘要：尽管近年来随着深度神经网络（DNN）的蓬勃发展，单图像超分辨率（SISR）任务取得了显著进展，但深度学习方法在实际应用中面临着计算和内存消耗问题，特别是对于资源有限的平台，如移动设备。为了克服这一挑战并便于在移动设备上实时部署SISR任务，我们将神经结构搜索与剪枝搜索相结合，提出了一种自动搜索框架，该框架在满足实时推理要求的同时，导出具有高图像质量的稀疏超分辨率（SR）模型。为了降低搜索成本，我们通过引入超网来利用权重共享策略，并将搜索问题分解为三个阶段，包括超网构造、编译器感知体系结构和剪枝搜索以及编译器感知剪枝比率搜索。利用所提出的框架，我们是第一个实现实时SR推断（每帧仅数十毫秒）的公司，用于在移动平台（三星Galaxy S20）上实现720p分辨率和具有竞争力的图像质量（PSNR和SSIM）。
<details>	<summary>英文摘要</summary>	Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20). </details>
<details>	<summary>邮件日期</summary>	2021年08月23日</details>

# 229、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年08月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes are available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 228、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年08月19日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 227、盲视频超分辨率的时间核一致性
- [ ] Temporal Kernel Consistency for Blind Video Super-Resolution 
时间：2021年08月18日                         第一作者：Lichuan Xiang                       [链接](https://arxiv.org/abs/2108.08305).                     
## 摘要：基于深度学习的盲超分辨率（SR）方法最近在未知退化的放大帧中取得了前所未有的性能。这些模型能够从给定的低分辨率（LR）图像中准确估计未知的降尺度核，以便在恢复过程中利用核。尽管这些方法在很大程度上取得了成功，但它们主要基于图像，因此不利用多个视频帧中内核的时间特性。在本文中，我们研究了核的时间特性，并强调了它在盲视频超分辨率任务中的重要性。具体地说，我们测量了真实世界视频的内核时间一致性，并说明了在场景及其对象的动态性不同的视频中，估计的内核在每帧中是如何变化的。有了这一新的见解，我们回顾了以前流行的视频SR方法，并表明以前在整个恢复过程中使用固定内核的假设在放大真实世界的视频时会导致视觉伪影。为了解决这个问题，我们定制了现有的单图像和视频SR技术，以在内核估计和视频放大过程中利用内核一致性。对合成视频和真实视频的大量实验表明，从数量和质量上都有很大的恢复收益，实现了盲视频SR的最新技术，并强调了利用内核时间一致性的潜力。
<details>	<summary>英文摘要</summary>	Deep learning-based blind super-resolution (SR) methods have recently achieved unprecedented performance in upscaling frames with unknown degradation. These models are able to accurately estimate the unknown downscaling kernel from a given low-resolution (LR) image in order to leverage the kernel during restoration. Although these approaches have largely been successful, they are predominantly image-based and therefore do not exploit the temporal properties of the kernels across multiple video frames. In this paper, we investigated the temporal properties of the kernels and highlighted its importance in the task of blind video super-resolution. Specifically, we measured the kernel temporal consistency of real-world videos and illustrated how the estimated kernels might change per frame in videos of varying dynamicity of the scene and its objects. With this new insight, we revisited previous popular video SR approaches, and showed that previous assumptions of using a fixed kernel throughout the restoration process can lead to visual artifacts when upscaling real-world videos. In order to counteract this, we tailored existing single-image and video SR techniques to leverage kernel consistency during both kernel estimation and video upscaling processes. Extensive experiments on synthetic and real-world videos show substantial restoration gains quantitatively and qualitatively, achieving the new state-of-the-art in blind video SR and underlining the potential of exploiting kernel temporal consistency. </details>
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 226、基于物理启发的深度网络的热图像处理
- [ ] Thermal Image Processing via Physics-Inspired Deep Networks 
时间：2021年08月18日                         第一作者：Vishwanath Saragadam                       [链接](https://arxiv.org/abs/2108.07973).                     
## 摘要：我们介绍了DeepIR，一种新的热图像处理框架，它将物理精确的传感器建模与基于深度网络的图像表示相结合。我们的关键使能观测是，热传感器捕获的图像可以分解为缓慢变化、场景无关的传感器不均匀性（可以使用物理精确建模）和场景特定的辐射通量（使用基于深度网络的正则化器很好地表示）。DeepIR既不需要训练数据，也不需要定期对已知黑体目标进行地面真实性校准，因此非常适合实际的计算机视觉任务。我们通过开发新的去噪和超分辨率算法，利用相机抖动拍摄的多幅场景图像，展示了深入红外的威力。仿真和实际数据实验表明，DeepIR可以用三幅图像进行高质量的非均匀性校正，与其他方法相比，峰值信噪比（PSNR）提高了10分贝。
<details>	<summary>英文摘要</summary>	We introduce DeepIR, a new thermal image processing framework that combines physically accurate sensor modeling with deep network-based image representation. Our key enabling observations are that the images captured by thermal sensors can be factored into slowly changing, scene-independent sensor non-uniformities (that can be accurately modeled using physics) and a scene-specific radiance flux (that is well-represented using a deep network-based regularizer). DeepIR requires neither training data nor periodic ground-truth calibration with a known black body target--making it well suited for practical computer vision tasks. We demonstrate the power of going DeepIR by developing new denoising and super-resolution algorithms that exploit multiple images of the scene captured with camera jitter. Simulated and real data experiments demonstrate that DeepIR can perform high-quality non-uniformity correction with as few as three images, achieving a 10dB PSNR improvement over competing approaches. </details>
<details>	<summary>注释</summary>	Accepted to 2nd ICCV workshop on Learning for Computational Imaging (LCI) </details>
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 225、多帧超分辨率深度重参数化与去噪
- [ ] Deep Reparametrization of Multi-Frame Super-Resolution and Denoising 
时间：2021年08月18日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2108.08286).                     
## 摘要：我们对多帧图像恢复任务中常用的最大后验公式提出了一种深度再参数化方法。我们的方法是通过引入学习的误差度量和目标图像的潜在表示，将地图目标转换为深层特征空间。深度再参数化允许我们直接在潜在空间中建模图像形成过程，并将学习到的图像先验知识集成到预测中。因此，我们的方法利用了深度学习的优势，同时也受益于经典MAP公式提供的原则性多帧融合。我们通过对突发去噪和突发超分辨率数据集的综合实验来验证我们的方法。我们的方法为这两项任务设定了一个新的最先进水平，证明了所提议的公式的通用性和有效性。
<details>	<summary>英文摘要</summary>	We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation. </details>
<details>	<summary>注释</summary>	ICCV 2021 Oral </details>
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 224、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年08月18日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 223、带变压器的光场图像超分辨率
- [ ] Light Field Image Super-Resolution with Transformers 
时间：2021年08月17日                         第一作者：Zhengyu Liang                       [链接](https://arxiv.org/abs/2108.07597).                     
## 摘要：光场（LF）图像超分辨率（SR）旨在从低分辨率图像重建高分辨率LF图像。虽然基于CNN的方法在LF图像SR中取得了显著的效果，但这些方法不能完全模拟4D LF数据的非局部特性。在本文中，我们提出了一种简单但有效的基于变换器的LF图像SR方法。在我们的方法中，设计了一个角度变换器来整合不同视图之间的互补信息，并开发了一个空间变换器来捕获每个子孔径图像中的局部和长程相关性。利用所提出的角度和空间变换器，可以充分利用LF中的有益信息，提高SR性能。我们通过广泛的烧蚀研究验证了我们的角度和空间变换器的有效性，并在五个公共LF数据集上将我们的方法与最新的方法进行了比较。我们的方法以较小的模型尺寸和较低的计算成本实现了优越的SR性能。
<details>	<summary>英文摘要</summary>	Light field (LF) image super-resolution (SR) aims at reconstructing high-resolution LF images from their low-resolution counterparts. Although CNN-based methods have achieved remarkable performance in LF image SR, these methods cannot fully model the non-local properties of the 4D LF data. In this paper, we propose a simple but effective Transformer-based method for LF image SR. In our method, an angular Transformer is designed to incorporate complementary information among different views, and a spatial Transformer is developed to capture both local and long-range dependencies within each sub-aperture image. With the proposed angular and spatial Transformers, the beneficial information in an LF can be fully exploited and the SR performance is boosted. We validate the effectiveness of our angular and spatial Transformers through extensive ablation studies, and compare our method to recent state-of-the-art methods on five public LF datasets. Our method achieves superior SR performance with a small model size and low computational cost. </details>
<details>	<summary>注释</summary>	The first two authors contribute equally to this work. Code is available at https://github.com/ZhengyuLiang24/LFT </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 222、spectrai：光谱数据的深度学习框架
- [ ] spectrai: A deep learning framework for spectral data 
时间：2021年08月17日                         第一作者：Conor C. Horgan                        [链接](https://arxiv.org/abs/2108.07595).                     
## 摘要：近年来，深入学习计算机视觉技术在许多成像领域取得了许多成功。然而，由于需要增强例程、光谱数据的特定体系结构和显著的内存需求，对光谱数据的深度学习应用仍然是一项复杂的任务。在这里，我们介绍了spectrai，这是一个开源的深度学习框架，旨在促进对光谱数据的神经网络训练，并使不同方法之间能够进行比较。Spectrai提供了许多内置的光谱数据预处理和增强方法、光谱数据的神经网络，包括光谱（图像）去噪、光谱（图像）分类、光谱图像分割和光谱图像超分辨率。Spectrai包括命令行和图形用户界面（GUI），旨在指导用户通过各种应用程序的模型和超参数决策。
<details>	<summary>英文摘要</summary>	Deep learning computer vision techniques have achieved many successes in recent years across numerous imaging domains. However, the application of deep learning to spectral data remains a complex task due to the need for augmentation routines, specific architectures for spectral data, and significant memory requirements. Here we present spectrai, an open-source deep learning framework designed to facilitate the training of neural networks on spectral data and enable comparison between different methods. Spectrai provides numerous built-in spectral data pre-processing and augmentation methods, neural networks for spectral data including spectral (image) denoising, spectral (image) classification, spectral image segmentation, and spectral image super-resolution. Spectrai includes both command line and graphical user interfaces (GUI) designed to guide users through model and hyperparameter decisions for a wide range of applications. </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 221、真实ESRGAN：用纯合成数据训练真实世界的盲超分辨率
- [ ] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data 
时间：2021年08月17日                         第一作者：Xintao Wang                       [链接](https://arxiv.org/abs/2107.10833).                     
<details>	<summary>注释</summary>	Tech Report. Training/testing codes and executable files are in https://github.com/xinntao/Real-ESRGAN </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 220、端到端自适应蒙特卡罗去噪和超分辨率
- [ ] End-to-End Adaptive Monte Carlo Denoising and Super-Resolution 
时间：2021年08月16日                         第一作者：Xinyue Wei                       [链接](https://arxiv.org/abs/2108.06915).                     
## 摘要：经典的蒙特卡罗路径跟踪可以在大量计算的代价下实现高质量的渲染。最近的工作利用深度神经网络来加速这一过程，通过在后处理中使用超分辨率或去噪神经网络来改善低分辨率或更少的样本渲染。然而，在以前的工作中，去噪和超分辨率仅被单独考虑。我们在这项工作中表明，在后处理中，联合超分辨率和去噪（SRD）可以进一步加速蒙特卡罗路径跟踪。这种新型的联合滤波仅允许通过路径跟踪渲染低分辨率和较少样本（因此有噪声）的图像，然后将路径跟踪馈入深度神经网络以生成高分辨率和干净的图像。这项工作的主要贡献是一种新的端到端网络体系结构，专门为SRD任务设计。它包含两个具有共享组件的级联级。我们发现，去噪和超分辨率需要非常不同的感受野，这是导致在网络设计中引入可变形卷积的关键洞察。大量的实验表明，该方法的性能优于以前的SRD任务所采用的方法及其变体。
<details>	<summary>英文摘要</summary>	The classic Monte Carlo path tracing can achieve high quality rendering at the cost of heavy computation. Recent works make use of deep neural networks to accelerate this process, by improving either low-resolution or fewer-sample rendering with super-resolution or denoising neural networks in post-processing. However, denoising and super-resolution have only been considered separately in previous work. We show in this work that Monte Carlo path tracing can be further accelerated by joint super-resolution and denoising (SRD) in post-processing. This new type of joint filtering allows only a low-resolution and fewer-sample (thus noisy) image to be rendered by path tracing, which is then fed into a deep neural network to produce a high-resolution and clean image. The main contribution of this work is a new end-to-end network architecture, specifically designed for the SRD task. It contains two cascaded stages with shared components. We discover that denoising and super-resolution require very different receptive fields, a key insight that leads to the introduction of deformable convolution into the network design. Extensive experiments show that the proposed method outperforms previous methods and their variants adopted for the SRD task. </details>
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 219、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年08月16日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 218、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年08月15日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 217、分层条件流：图像超分辨率和图像重缩放的统一框架
- [ ] Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling 
时间：2021年08月11日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.05301).                     
## 摘要：规范化流程最近在低级别视觉任务中显示了有希望的结果。对于图像超分辨率（SR），它学习从低分辨率（LR）图像预测不同的照片真实高分辨率（HR）图像，而不是学习确定性映射。对于图像重缩放，它通过联合建模降尺度和升尺度过程来实现高精度。虽然现有的方法对这两项任务使用专门的技术，但我们开始将它们统一到一个公式中。在本文中，我们提出了分层条件流（HCFlow）作为图像SR和图像重缩放的统一框架。更具体地说，HCFlow通过同时建模LR图像和其余高频分量的分布来学习HR和LR图像对之间的双射映射。具体地，高频分量以分层方式取决于LR图像。为了进一步提高性能，将感知损失和GAN损失等其他损失与训练中常用的负对数似然损失相结合。对普通图像SR、人脸图像SR和图像重缩放的大量实验表明，所提出的HCFlow在定量度量和视觉质量方面都达到了最先进的性能。
<details>	<summary>英文摘要</summary>	Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality. </details>
<details>	<summary>注释</summary>	Accepted by ICCV2021. Code: https://github.com/JingyunLiang/HCFlow </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 216、互仿射网络在盲图像超分辨率空间变异核估计中的应用
- [ ] Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution 
时间：2021年08月11日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.05302).                     
## 摘要：现有的盲图像超分辨率（SR）方法大多假设模糊核在整个图像中具有空间不变性。然而，这样的假设很少适用于真实图像，其模糊核通常由于对象运动和失焦等因素而在空间上变化。因此，现有的盲SR方法在实际应用中不可避免地会导致性能下降。为了解决这个问题，本文提出了一种用于空间变异核估计的互仿射网络（MANet）。具体来说，MANet有两个显著的特点。首先，它有一个适度的感受野，以保持退化的位置。其次，它涉及一个新的互仿射卷积（MAConv）层，该层在不增加感受野、模型大小和计算负担的情况下增强了特征表达能力。这是通过利用通道相互依赖性实现的，它使用仿射变换模块应用每个通道分割，仿射变换模块的输入是剩余通道分割。在合成图像和真实图像上的大量实验表明，所提出的MANet不仅具有良好的空间变异和不变核估计性能，而且在与非盲SR方法相结合时还具有最先进的盲SR性能。
<details>	<summary>英文摘要</summary>	Existing blind image super-resolution (SR) methods mostly assume blur kernels are spatially invariant across the whole image. However, such an assumption is rarely applicable for real images whose blur kernels are usually spatially variant due to factors such as object motion and out-of-focus. Hence, existing blind SR methods would inevitably give rise to poor performance in real applications. To address this issue, this paper proposes a mutual affine network (MANet) for spatially variant kernel estimation. Specifically, MANet has two distinctive features. First, it has a moderate receptive field so as to keep the locality of degradation. Second, it involves a new mutual affine convolution (MAConv) layer that enhances feature expressiveness without increasing receptive field, model size and computation burden. This is made possible through exploiting channel interdependence, which applies each channel split with an affine transformation module whose input are the rest channel splits. Extensive experiments on synthetic and real images show that the proposed MANet not only performs favorably for both spatially variant and invariant kernel estimation, but also leads to state-of-the-art blind SR performance when combined with non-blind SR methods. </details>
<details>	<summary>注释</summary>	Accepted by ICCV2021. Code: https://github.com/JingyunLiang/MANet </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 215、FA-GAN：用于MRI图像超分辨率的融合注意生成对抗网络
- [ ] FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution 
时间：2021年08月09日                         第一作者：Mingfeng Jiang                       [链接](https://arxiv.org/abs/2108.03920).                     
## 摘要：高分辨率磁共振图像可以提供精细的解剖信息，但获取这些数据需要很长的扫描时间。本文提出了一种称为融合注意生成对抗网络（FA-GAN）的框架，用于从低分辨率磁共振图像生成超分辨率磁共振图像，该框架可有效缩短扫描时间，但具有高分辨率磁共振图像。在FA-GAN框架下，提出了一种局部融合特征块，该特征块由不同的卷积核组成的三通网络组成，用于提取不同尺度下的图像特征。设计了全局特征融合模块，包括通道注意模块、自我注意模块和融合操作，以增强MR图像的重要特征。此外，为了使鉴别器网络稳定，还引入了频谱归一化过程。使用40组3D磁共振图像（每组图像包含256个切片）对网络进行训练，并使用10组图像对所提出的方法进行测试。实验结果表明，所提出的FA-GAN方法生成的超分辨率磁共振图像的PSNR和SSIM值高于现有的重建方法。
<details>	<summary>英文摘要</summary>	High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super-resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods. </details>
<details>	<summary>注释</summary>	27 pages, 12 figures, accepted by CMIG </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 214、基于空间角密集网络的高效光场重建
- [ ] Efficient Light Field Reconstruction via Spatio-Angular Dense Network 
时间：2021年08月08日                         第一作者：Zexi Hu                       [链接](https://arxiv.org/abs/2108.03635).                     
## 摘要：作为一种图像传感仪器，光场图像可以提供比单目图像更多的角度信息，并促进了广泛的测量应用。光场图像捕获设备通常在角度分辨率和空间分辨率之间存在固有的权衡。为了解决这一问题，人们提出了几种方法，如光场重建和光场超分辨率，但没有解决两个问题，即域不对称和有效信息流。在本文中，我们提出了一种用于光场重建的端到端空间角度密集网络（SADenseNet），该网络具有两个新的组件，即相关块和空间角度密集跳过连接来解决它们。前者以符合域不对称性的方式对相关信息进行有效建模。后者包括三种连接，增强了两个领域内的信息流。在真实世界和合成数据集上进行了大量实验，以证明所提出的SADenseNet以显著降低内存和计算成本的方式实现了最先进的性能。定性结果表明，重建的光场图像清晰，细节正确，可以作为预处理，提高相关测量应用的精度。
<details>	<summary>英文摘要</summary>	As an image sensing instrument, light field images can supply extra angular information compared with monocular images and have facilitated a wide range of measurement applications. Light field image capturing devices usually suffer from the inherent trade-off between the angular and spatial resolutions. To tackle this problem, several methods, such as light field reconstruction and light field super-resolution, have been proposed but leaving two problems unaddressed, namely domain asymmetry and efficient information flow. In this paper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for light field reconstruction with two novel components, namely correlation blocks and spatio-angular dense skip connections to address them. The former performs effective modeling of the correlation information in a way that conforms with the domain asymmetry. And the latter consists of three kinds of connections enhancing the information flow within two domains. Extensive experiments on both real-world and synthetic datasets have been conducted to demonstrate that the proposed SADenseNet's state-of-the-art performance at significantly reduced costs in memory and computation. The qualitative results show that the reconstructed light field images are sharp with correct details and can serve as pre-processing to improve the accuracy of related measurement applications. </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 213、Ada-VSR：具有元学习的自适应视频超分辨率
- [ ] Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning 
时间：2021年08月05日                         第一作者：Akash Gupta                       [链接](https://arxiv.org/abs/2108.02832).                     
## 摘要：现有的有监督时空视频超分辨率（STVSR）研究大多依赖于由成对的低分辨率低帧速率（LR-LFR）和高分辨率高帧速率（HR-HFR）视频组成的大规模外部数据集。尽管这些方法具有显著的性能，但它们预先假设低分辨率视频是通过使用已知的降级内核（在实际设置中不适用）对高分辨率视频进行降级来获得的。这些方法的另一个问题是，它们无法在测试时利用特定于实例的视频内部信息。最近，深层内部学习方法因其利用视频实例特定统计数据的能力而受到关注。然而，这些方法需要大量的推理时间，因为它们需要数千次梯度更新来学习数据的内在结构。在这项工作中，我们提出了AdaptiveVideoSuper Resolution（Ada VSR），它分别通过元迁移学习和内部学习利用外部和内部信息。具体而言，元学习用于使用大规模外部数据集获得自适应参数，该参数可在内部学习任务期间快速适应给定测试视频的新条件（退化模型），从而利用视频的外部和内部信息实现超分辨率。使用我们的方法训练的模型可以快速适应特定的视频条件，只需少量的梯度更新，从而显著减少了推理时间。在标准数据集上的大量实验表明，我们的方法与各种最先进的方法相比具有良好的性能。
<details>	<summary>英文摘要</summary>	Most of the existing works in supervised spatio-temporal video super-resolution (STVSR) heavily rely on a large-scale external dataset consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these methods make a prior assumption that the low-resolution video is obtained by down-scaling the high-resolution video using a known degradation kernel, which does not hold in practical settings. Another problem with these methods is that they cannot exploit instance-specific internal information of video at testing time. Recently, deep internal learning approaches have gained attention due to their ability to utilize the instance-specific statistics of a video. However, these methods have a large inference time as they require thousands of gradient updates to learn the intrinsic structure of the data. In this work, we presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as well as internal, information through meta-transfer learning and internal learning, respectively. Specifically, meta-learning is employed to obtain adaptive parameters, using a large-scale external dataset, that can adapt quickly to the novel condition (degradation model) of the given test video during the internal learning task, thereby exploiting external and internal information of a video for super-resolution. The model trained using our approach can quickly adapt to a specific video condition with only a few gradient updates, which reduces the inference time significantly. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches. </details>
<details>	<summary>邮件日期</summary>	2021年08月09日</details>

# 212、用于图像超分辨率的双参考训练数据采集和CNN构建
- [ ] Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution 
时间：2021年08月05日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
## 摘要：对于图像超分辨率的深度学习方法，最关键的问题是用于训练的成对低分辨率和高分辨率图像是否准确反映真实相机的采样过程。现有退化模型（如双三次下采样）合成的低分辨率和高分辨率（LR$\sim$HR）图像对与现实中的图像对不同；因此，由这些合成的LR$\sim$HR图像对训练的超分辨率CNN在应用于真实图像时表现不佳。在本文中，我们提出了一种新的方法，使用真实的摄像机捕获大量真实的LR$\sim$HR图像对。数据采集是在可控的实验室条件下以最少的人为干预和高吞吐量（大约每小时500个图像对）进行的。高度自动化使得为每台摄像机生成一组真实的LR$\sim$HR训练图像对变得容易。我们的创新在于以不同的分辨率拍摄显示在超高质量屏幕上的图像。我们的方法有三个独特的优点，允许我们收集高质量的训练数据集，以实现图像超分辨率。首先，由于LR和HR图像是从3D平面（屏幕）上拍摄的，因此配准问题完全符合单应模型。第二，我们可以在图像边缘显示特殊标记，以进一步提高配准精度。第三，可以利用显示的数字图像文件作为参考，优化恢复图像的高频内容。实验结果表明，在推理阶段，使用LR$\sim$HR数据集训练超分辨率CNN比使用现有数据集训练CNN具有更好的恢复性能。
<details>	<summary>英文摘要</summary>	For deep learning methods of image super-resolution, the most critical issue is whether the paired low and high resolution images for training accurately reflect the sampling process of real cameras. Low and high resolution (LR$\sim$HR) image pairs synthesized by existing degradation models (\eg, bicubic downsampling) deviate from those in reality; thus the super-resolution CNN trained by these synthesized LR$\sim$HR image pairs does not perform well when being applied to real images. In this paper, we propose a novel method to capture a large set of realistic LR$\sim$HR image pairs using real cameras.The data acquisition is carried out under controllable lab conditions with minimum human intervention and at high throughput (about 500 image pairs per hour). The high level of automation makes it easy to produce a set of real LR$\sim$HR training image pairs for each camera. Our innovation is to shoot images displayed on an ultra-high quality screen at different resolutions.There are three distinctive advantages with our method that allow us to collect high-quality training datasets for image super-resolution. First, as the LR and HR images are taken of a 3D planar surface (the screen) the registration problem fits exactly to a homography model. Second, we can display special markers on the image margin to further improve the registration precision.Third, the displayed digital image file can be exploited as a reference to optimize the high frequency content of the restored image. Experimental results show that training a super-resolution CNN by our LR$\sim$HR dataset has superior restoration performance than training it by existing datasets on real world images at the inference stage. </details>
<details>	<summary>邮件日期</summary>	2021年08月06日</details>

# 211、在盲超分辨中寻找特定退化的鉴别滤波器
- [ ] Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution 
时间：2021年08月02日                         第一作者：Liangbin Xie                       [链接](https://arxiv.org/abs/2108.01070).                     
## 摘要：最近的盲超分辨率（SR）方法通常由两个分支组成，一个用于退化预测，另一个用于条件恢复。然而，我们的实验表明，单分支网络可以实现与双分支方案相当的性能。然后我们想知道：单分支网络如何自动学会区分退化？为了找到答案，我们提出了一种新的诊断工具——基于积分梯度的滤波归因方法（FAIG）。与以前的积分梯度方法不同，我们的FAIG旨在寻找最具辨别力的滤波器，而不是用于盲SR网络中去除退化的输入像素/特征。利用所发现的滤波器，我们进一步发展了一种简单而有效的方法来预测输入图像的退化。基于FAIG，我们证明，在单分支盲SR网络中，1）我们能够为每个特定退化找到非常少的（1%）鉴别滤波器；2） 发现的过滤器的权重、位置和连接对于确定特定网络功能都很重要。3） 退化预测的任务可以通过这些判别滤波器隐式地实现，而无需显式的监督学习。我们的发现不仅可以帮助我们更好地理解单分支盲SR网络中的网络行为，而且可以为设计更有效的结构和诊断盲SR网络提供指导。
<details>	<summary>英文摘要</summary>	Recent blind super-resolution (SR) methods typically consist of two branches, one for degradation prediction and the other for conditional restoration. However, our experiments show that a one-branch network can achieve comparable performance to the two-branch scheme. Then we wonder: how can one-branch networks automatically learn to distinguish degradations? To find the answer, we propose a new diagnostic tool -- Filter Attribution method based on Integral Gradient (FAIG). Unlike previous integral gradient methods, our FAIG aims at finding the most discriminative filters instead of input pixels/features for degradation removal in blind SR networks. With the discovered filters, we further develop a simple yet effective method to predict the degradation of an input image. Based on FAIG, we show that, in one-branch blind SR networks, 1) we are able to find a very small number of (1%) discriminative filters for each specific degradation; 2) The weights, locations and connections of the discovered filters are all important to determine the specific network function. 3) The task of degradation prediction can be implicitly realized by these discriminative filters without explicit supervised learning. Our findings can not only help us better understand network behaviors inside one-branch blind SR networks, but also provide guidance on designing more efficient architectures and diagnosing networks for blind SR. </details>
<details>	<summary>注释</summary>	Tech report </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 210、基于互Dirichlet网的无监督非注册高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年08月02日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	IEEE Transactions on Remote Sensing and Geoscience DOI: 10.1109/TGRS.2021.3079518 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 209、超分辨网络中的语义发现
- [ ] Discovering "Semantics" in Super-Resolution Networks 
时间：2021年08月01日                         第一作者：Yihao Liu                       [链接](https://arxiv.org/abs/2108.00406).                     
## 摘要：超分辨率（SR）是低水平视觉领域的一项基础性和代表性任务。一般认为，从SR网络中提取的特征没有特定的语义信息，网络只是简单地学习从输入到输出的复杂非线性映射。我们能在SR网络中找到任何“语义”吗？在本文中，我们对这个问题给出了肯定的回答。通过对特征表示进行降维和可视化分析，我们成功地发现了与图像退化类型和程度相关的SR网络中的深层语义表示、深层退化表示（DDR）。我们还揭示了分类和SR网络在表示语义上的差异。通过大量的实验和分析，我们得出了一系列的观察结果和结论，这些观察结果和结论对今后的工作具有重要意义，例如解释低层CNN网络的内在机制，开发新的盲SR评估方法。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is a fundamental and representative task of low-level vision area. It is generally thought that the features extracted from the SR network have no specific semantic information, and the network simply learns complex non-linear mappings from input to output. Can we find any "semantics" in SR networks? In this paper, we give affirmative answers to this question. By analyzing the feature representations with dimensionality reduction and visualization, we successfully discover the deep semantic representations in SR networks, \textit{i.e.}, deep degradation representations (DDR), which relate to the image degradation types and degrees. We also reveal the differences in representation semantics between classification and SR networks. Through extensive experiments and analysis, we draw a series of observations and conclusions, which are of great significance for future work, such as interpreting the intrinsic mechanisms of low-level CNN networks and developing new evaluation approaches for blind SR. </details>
<details>	<summary>注释</summary>	discovering and interpreting deep degradation representations (DDR) in super-resolution networks </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 208、利用可变感受野二阶通道注意的热图像超分辨率
- [ ] Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields 
时间：2021年07月30日                         第一作者：Nolan B. Gutierrez                       [链接](https://arxiv.org/abs/2108.00094).                     
## 摘要：热图像模拟了电磁光谱的长红外范围，即使在没有可见光照明的情况下也能提供有意义的信息。然而，与代表可见光连续体辐射的图像不同，由于硬件限制，红外图像固有的低分辨率。热图像的恢复对于涉及安全、搜索和救援以及军事行动的应用至关重要。在本文中，我们介绍了一个系统，以有效地重建热图像。具体来说，我们探讨了如何有效地处理对比感受野（RFs），在这种情况下，增加网络的RFs在计算上可能非常昂贵。为此，我们对可变感受野网络（AVRFN）进行了深入的研究。我们提供了一个门控卷积层，其中包含从不同的RFs中提取的高阶信息，其中RF由膨胀率参数化。通过这种方式，可以调整扩张率以使用较少的参数，从而提高AVRFN的疗效。我们的实验结果表明，与竞争对手的热图像超分辨率方法相比，我们的技术水平有所提高。
<details>	<summary>英文摘要</summary>	Thermal images model the long-infrared range of the electromagnetic spectrum and provide meaningful information even when there is no visible illumination. Yet, unlike imagery that represents radiation from the visible continuum, infrared images are inherently low-resolution due to hardware constraints. The restoration of thermal images is critical for applications that involve safety, search and rescue, and military operations. In this paper, we introduce a system to efficiently reconstruct thermal images. Specifically, we explore how to effectively attend to contrasting receptive fields (RFs) where increasing the RFs of a network can be computationally expensive. For this purpose, we introduce a deep attention to varying receptive fields network (AVRFN). We supply a gated convolutional layer with higher-order information extracted from disparate RFs, whereby an RF is parameterized by a dilation rate. In this way, the dilation rate can be tuned to use fewer parameters thus increasing the efficacy of AVRFN. Our experimental results show an improvement over the state of the art when compared against competing thermal image super-resolution methods. </details>
<details>	<summary>注释</summary>	To be published in the 2021 13th International Conference on Computer Vision Systems (ICVS) </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 207、低分辨率扫描病理图像的多尺度超分辨率生成
- [ ] Multi-scale super-resolution generation of low-resolution scanned pathological images 
时间：2021年07月30日                         第一作者：Kai Sun (1)                       [链接](https://arxiv.org/abs/2105.07200).                     
<details>	<summary>注释</summary>	27 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 206、基于傅里叶级数展开的等变卷积滤波器参数化
- [ ] Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions 
时间：2021年07月30日                         第一作者：Qi Xie                        [链接](https://arxiv.org/abs/2107.14519).                     
## 摘要：已经证明，等变卷积对于许多类型的计算机视觉任务都非常有用。近年来，二维滤波器参数化技术在设计等变卷积时发挥了重要作用。然而，目前的滤波器参数化方法仍存在明显的缺陷，其中最关键的是滤波器表示的精度问题。针对这个问题，本文修改了二维滤波器的经典傅里叶级数展开，提出了一组新的原子基函数用于滤波器参数化。所提出的滤波器参数化方法不仅能很好地表示滤波器未旋转时误差为零的二维滤波器，而且能显著地减轻旋转滤波器时由于栅栏效应引起的质量下降。因此，我们提出了一种新的基于共变函数的离散化方法，称之为共变函数。在共变函数的基础上，我们提出了一种新的基于共变函数的离散化方法。大量实验表明了该方法的优越性。特别地，我们将旋转等变卷积方法应用于图像超分辨率任务中，F-Conv在该任务中的性能明显优于以前基于滤波器参数化的方法，反映了其在局部图像特征中忠实保持旋转对称性的内在能力。
<details>	<summary>英文摘要</summary>	It has been shown that equivariant convolution is very helpful for many types of computer vision tasks. Recently, the 2D filter parametrization technique plays an important role when designing equivariant convolutions. However, the current filter parametrization method still has its evident drawbacks, where the most critical one lies in the accuracy problem of filter representation. Against this issue, in this paper we modify the classical Fourier series expansion for 2D filters, and propose a new set of atomic basis functions for filter parametrization. The proposed filter parametrization method not only finely represents 2D filters with zero error when the filter is not rotated, but also substantially alleviates the fence-effect-caused quality degradation when the filter is rotated. Accordingly, we construct a new equivariant convolution method based on the proposed filter parametrization method, named F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the continuous domain, which becomes approximate only after discretization. Extensive experiments show the superiority of the proposed method. Particularly, we adopt rotation equivariant convolution methods to image super-resolution task, and F-Conv evidently outperforms previous filter parametrization based method in this task, reflecting its intrinsic capability of faithfully preserving rotation symmetries in local image features. </details>
<details>	<summary>注释</summary>	27 pages, 19 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月02日</details>

# 205、基于像素自适应核注意的内容感知定向传播网络
- [ ] Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention 
时间：2021年07月28日                         第一作者：Min-Cheol Sagong                       [链接](https://arxiv.org/abs/2107.13144).                     
## 摘要：卷积神经网络（CNN）不仅得到了广泛的应用，而且在图像分类、恢复和生成等众多应用领域也取得了显著的成果。虽然卷积的权重共享特性使其广泛应用于各种任务中，但其内容不可知的特性也可以被认为是一个主要缺点。为了解决这个问题，本文提出了一种新的操作，称为像素自适应核注意（PAKA）。PAKA通过将来自可学习特征的空间变化注意力相乘，为过滤器权重提供方向性。该方法分别沿通道和空间方向推断像素自适应注意图，以较少的参数处理分解模型。我们的方法可以以端到端的方式进行培训，并且适用于任何基于CNN的模型。此外，我们还提出了一种改进的带有PAKA的信息聚合模块，称为层次PAKA模块（HPM）。与传统的信息聚合模块相比，我们展示了HPM在语义分割方面的最新性能，从而证明了HPM的优越性。我们通过额外的消融研究和可视化PAKA提供卷积权重方向性的效果来验证所提出的方法。我们还通过将该方法应用于多模态任务，特别是彩色引导深度图超分辨率任务，证明了该方法的通用性。
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) have been not only widespread but also achieved noticeable results on numerous applications including image classification, restoration, and generation. Although the weight-sharing property of convolutions makes them widely adopted in various tasks, its content-agnostic characteristic can also be considered a major drawback. To solve this problem, in this paper, we propose a novel operation, called pixel adaptive kernel attention (PAKA). PAKA provides directivity to the filter weights by multiplying spatially varying attention from learnable features. The proposed method infers pixel-adaptive attention maps along the channel and spatial directions separately to address the decomposed model with fewer parameters. Our method is trainable in an end-to-end manner and applicable to any CNN-based models. In addition, we propose an improved information aggregation module with PAKA, called the hierarchical PAKA module (HPM). We demonstrate the superiority of our HPM by presenting state-of-the-art performance on semantic segmentation compared to the conventional information aggregation modules. We validate the proposed method through additional ablation studies and visualizing the effect of PAKA providing directivity to the weights of convolutions. We also show the generalizability of the proposed method by applying it to multi-modal tasks especially color-guided depth map super-resolution. </details>
<details>	<summary>注释</summary>	submitted to IEEE transactions on Neural Networks and Learning System </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 204、通过超分辨率提高多视点立体感
- [ ] Improving Multi-View Stereo via Super-Resolution 
时间：2021年07月28日                         第一作者：Eugenio Lomurno                       [链接](https://arxiv.org/abs/2107.13261).                     
## 摘要：如今，多视图立体技术能够重建健壮而详细的三维模型，特别是从高分辨率图像开始时。然而，在某些情况下，输入图像的分辨率相对较低，例如，在处理旧照片时，或者在硬件限制可以获取的数据量时。在本文中，我们研究了通过超分辨率技术提高此类输入图像的分辨率是否、如何以及在多大程度上反映了重建三维模型的质量改进，尽管有时可能会产生伪影。我们表明，在大多数情况下，在恢复深度贴图之前应用超分辨率步骤可以在基于补丁匹配和基于深度学习的算法中获得更好的三维模型。超分辨率的使用特别提高了重建模型的完整性，并且在纹理场景的情况下特别有效。
<details>	<summary>英文摘要</summary>	Today, Multi-View Stereo techniques are able to reconstruct robust and detailed 3D models, especially when starting from high-resolution images. However, there are cases in which the resolution of input images is relatively low, for instance, when dealing with old photos, or when hardware constrains the amount of data that can be acquired. In this paper, we investigate if, how, and how much increasing the resolution of such input images through Super-Resolution techniques reflects in quality improvements of the reconstructed 3D models, despite the artifacts that sometimes this may generate. We show that applying a Super-Resolution step before recovering the depth maps in most cases leads to a better 3D model both in the case of PatchMatch-based and deep-learning-based algorithms. The use of Super-Resolution improves especially the completeness of reconstructed models and turns out to be particularly effective in the case of textured scenes. </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 203、BridgeNet：深度图超分辨率和单目深度估计的联合学习网络
- [ ] BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation 
时间：2021年07月27日                         第一作者：Qi Tang                       [链接](https://arxiv.org/abs/2107.12541).                     
## 摘要：深度图超分辨率是一项实际应用要求很高的任务。现有的彩色引导深度图超分辨率重建方法通常需要一个额外的分支从RGB图像中提取高频细节信息来指导低分辨率深度图的重建。然而，由于两种模式之间仍然存在一些差异，在特征维度或边缘贴图维度中的直接信息传输无法达到令人满意的结果，甚至可能在RGB-D对的结构不一致的区域触发纹理复制。受多任务学习的启发，我们提出了一种深度图超分辨率（DSR）和单目深度估计（MDE）的联合学习网络，无需引入额外的监督标签。对于两个子网之间的相互作用，我们采用了差异化的引导策略，并相应地设计了两个网桥。一种是为特征编码过程设计的高频注意桥（HABdg），它学习MDE任务的高频信息来指导DSR任务。另一个是为深度图重建过程设计的内容指导桥（CGBdg），它为MDE任务提供从DSR任务中学习到的内容指导。整个网络体系结构具有高度的可移植性，可以为DSR和MDE任务的关联提供范例。在基准数据集上的大量实验表明，我们的方法达到了有竞争力的性能。我们的型号和代码都有https://rmcong.github.io/proj_BridgeNet.html.
<details>	<summary>英文摘要</summary>	Depth map super-resolution is a task with high practical application requirements in the industry. Existing color-guided depth map super-resolution methods usually necessitate an extra branch to extract high-frequency detail information from RGB image to guide the low-resolution depth map reconstruction. However, because there are still some differences between the two modalities, direct information transmission in the feature dimension or edge map dimension cannot achieve satisfactory result, and may even trigger texture copying in areas where the structures of the RGB-D pair are inconsistent. Inspired by the multi-task learning, we propose a joint learning network of depth map super-resolution (DSR) and monocular depth estimation (MDE) without introducing additional supervision labels. For the interaction of two subnetworks, we adopt a differentiated guidance strategy and design two bridges correspondingly. One is the high-frequency attention bridge (HABdg) designed for the feature encoding process, which learns the high-frequency information of the MDE task to guide the DSR task. The other is the content guidance bridge (CGBdg) designed for the depth map reconstruction process, which provides the content guidance learned from DSR task for MDE task. The entire network architecture is highly portable and can provide a paradigm for associating the DSR and MDE tasks. Extensive experiments on benchmark datasets demonstrate that our method achieves competitive performance. Our code and models are available at https://rmcong.github.io/proj_BridgeNet.html. </details>
<details>	<summary>注释</summary>	10 pages, 7 figures, Accepted by ACM MM 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 202、局部自适应卷积用于图像融合
- [ ] LAConv: Local Adaptive Convolution for Image Fusion 
时间：2021年07月24日                         第一作者：Zi-Rong Jin                        [链接](https://arxiv.org/abs/2107.11617).                     
## 摘要：卷积运算是一种强有力的特征提取工具，在计算机视觉领域占有重要地位。然而，在针对图像融合等像素级任务时，如果在不同的面片上使用均匀卷积核，则无法充分感知图像中每个像素的特殊性。在本文中，我们提出了一种局部自适应卷积（LAConv），它是动态调整到不同的空间位置。LAConv使网络能够关注学习过程中的每一个特定的局部区域。此外，引入动态偏差（DYB），为特征描述提供了更多的可能性，使网络更加灵活。我们进一步设计了一个包含LAConv和DYB模块的残差结构网络，并将其应用于两个图像融合任务中。通过对泛锐化和高光谱图像超分辨率（HISR）的实验，证明了该方法的优越性。值得一提的是，LAConv还可以以较少的计算量胜任其他超分辨率任务。
<details>	<summary>英文摘要</summary>	The convolution operation is a powerful tool for feature extraction and plays a prominent role in the field of computer vision. However, when targeting the pixel-wise tasks like image fusion, it would not fully perceive the particularity of each pixel in the image if the uniform convolution kernel is used on different patches. In this paper, we propose a local adaptive convolution (LAConv), which is dynamically adjusted to different spatial locations. LAConv enables the network to pay attention to every specific local area in the learning process. Besides, the dynamic bias (DYB) is introduced to provide more possibilities for the depiction of features and make the network more flexible. We further design a residual structure network equipped with the proposed LAConv and DYB modules, and apply it to two image fusion tasks. Experiments for pansharpening and hyperspectral image super-resolution (HISR) demonstrate the superiority of our method over other state-of-the-art methods. It is worth mentioning that LAConv can also be competent for other super-resolution tasks with less computation effort. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 201、任意尺度超分辨率的单网络学习
- [ ] Learning A Single Network for Scale-Arbitrary Super-Resolution 
时间：2021年07月23日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2004.03791).                     
<details>	<summary>注释</summary>	Accepted by ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月26日</details>

# 200、联合隐式图像函数在制导深度超分辨中的应用
- [ ] Joint Implicit Image Function for Guided Depth Super-Resolution 
时间：2021年07月23日                         第一作者：Jiaxiang Tang                       [链接](https://arxiv.org/abs/2107.08717).                     
<details>	<summary>注释</summary>	Accepted by ACM MM 2021 DOI: 10.1145/3474085.3475584 </details>
<details>	<summary>邮件日期</summary>	2021年07月26日</details>

# 199、真实ESRGAN：用纯合成数据训练真实世界的盲超分辨率
- [ ] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data 
时间：2021年07月22日                         第一作者：Xintao Wang                       [链接](https://arxiv.org/abs/2107.10833).                     
## 摘要：尽管人们在盲超分辨率技术中已经进行了许多尝试，以恢复具有未知和复杂退化的低分辨率图像，但它们仍然远远不能解决一般的真实退化图像。在这项工作中，我们将强大的ESRGAN扩展到一个实际的恢复应用程序（即真实的ESRGAN），它是用纯合成数据训练的。特别地，引入了高阶退化建模过程来更好地模拟复杂的真实退化。我们还考虑了合成过程中常见的振铃和过冲伪影。此外，我们还采用了一个具有谱归一化的U-Net鉴别器，以提高鉴别器的性能并稳定训练动态。广泛的比较表明，它的视觉性能优于以往的工作对各种真实数据集。我们还提供了有效的实现来动态合成训练对。
<details>	<summary>英文摘要</summary>	Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. </details>
<details>	<summary>注释</summary>	Tech Report. Training codes, testing codes, and executable files are in https://github.com/xinntao/Real-ESRGAN </details>
<details>	<summary>邮件日期</summary>	2021年07月23日</details>

# 198、基于注意和循环丢失的生成对抗网络的高分辨率骨盆MRI重建
- [ ] High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss 
时间：2021年07月21日                         第一作者：Guangyuan Li                       [链接](https://arxiv.org/abs/2107.09989).                     
## 摘要：磁共振成像（MRI）是一种重要的医学成像手段，但由于生理限制，其采集速度较慢。近年来，超分辨率方法在磁共振成像加速方面表现出了优异的性能。在某些情况下，即使扫描时间延长，也很难获得高分辨率图像。因此，我们提出了一种新的超分辨率方法，它使用具有循环丢失和注意机制的生成对抗网络（GAN）从低分辨率的MR图像中生成高分辨率的MR图像，因子为2。我们将我们的模型应用于健康受试者的骨盆图像作为训练和验证数据，而这些来自患者的数据被用于检测。使用不同的成像序列获得MR数据集，包括T2、T2W-SPAIR和mDIXON-W。采用双三次、SRCNN、SRGAN和EDSR四种方法进行比较。以结构相似性、峰值信噪比、均方根误差和方差膨胀因子为计算指标，评价了该方法的性能。实验结果表明，与其他方法相比，该方法能更好地恢复高分辨率MR图像的细节。此外，重建的高分辨率MR图像可以为肿瘤患者提供更好的病变纹理，在临床诊断中具有广阔的应用前景。
<details>	<summary>英文摘要</summary>	Magnetic resonance imaging (MRI) is an important medical imaging modality, but its acquisition speed is quite slow due to the physiological limitations. Recently, super-resolution methods have shown excellent performance in accelerating MRI. In some circumstances, it is difficult to obtain high-resolution images even with prolonged scan time. Therefore, we proposed a novel super-resolution method that uses a generative adversarial network (GAN) with cyclic loss and attention mechanism to generate high-resolution MR images from low-resolution MR images by a factor of 2. We implemented our model on pelvic images from healthy subjects as training and validation data, while those data from patients were used for testing. The MR dataset was obtained using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison. Structural similarity, peak signal to noise ratio, root mean square error, and variance inflation factor were used as calculation indicators to evaluate the performances of the proposed method. Various experimental results showed that our method can better restore the details of the high-resolution MR image as compared to the other methods. In addition, the reconstructed high-resolution MR image can provide better lesion textures in the tumor patients, which is promising to be used in clinical diagnosis. </details>
<details>	<summary>注释</summary>	21 pages, 7 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年07月22日</details>

# 197、RankSRGAN：学习排名的超分辨率生成对抗网络
- [ ] RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank 
时间：2021年07月20日                         第一作者：Wenlong Zhang                       [链接](https://arxiv.org/abs/2107.09427).                     
## 摘要：生成对抗网络（GAN）已经证明了在单图像超分辨率（SISR）中恢复真实细节的潜力。为了进一步提高超分辨结果的视觉质量，PIRM2018-SR Challenge采用了感知度量来评估感知质量，如PI、NIQE和Ma。然而，现有的方法无法直接优化这些与人类评分高度相关的无差别感知指标。为了解决这个问题，我们提出了一种基于Ranker（RankSRGAN）的超分辨率生成对抗网络，在不同感知度量的方向上对生成元进行优化。具体地说，我们首先训练一个能够学习感知度量行为的等级者，然后引入一种新的等级内容损失来优化感知质量。最吸引人的部分是，该方法可以结合不同SR方法的优点，产生更好的结果。此外，我们将我们的方法扩展到多个ranker，为生成器提供多维约束。大量的实验表明，RankSRGAN在视觉上达到了令人满意的效果，并且在感知度量和质量方面达到了最先进的水平。项目页面：https://wenlongzhang0517.github.io/Projects/RankSRGAN
<details>	<summary>英文摘要</summary>	Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of different perceptual metrics. Specifically, we first train a Ranker which can learn the behaviour of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Furthermore, we extend our method to multiple Rankers to provide multi-dimension constraints for the generator. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics and quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN </details>
<details>	<summary>注释</summary>	IEEE PAMI accepted. arXiv admin note: substantial text overlap with arXiv:1908.06382 </details>
<details>	<summary>邮件日期</summary>	2021年07月21日</details>

# 196、联合隐式图像函数在制导深度超分辨中的应用
- [ ] Joint Implicit Image Function for Guided Depth Super-Resolution 
时间：2021年07月19日                         第一作者：Jiaxiang Tang                       [链接](https://arxiv.org/abs/2107.08717).                     
## 摘要：引导深度超分辨率是一个实际的任务，低分辨率和噪声的输入深度地图恢复到一个高分辨率版本的帮助下，一个高分辨率的RGB引导图像。现有的方法通常把这一任务看作是一个依赖于设计显式滤波器和目标函数的广义引导滤波问题，或是一个通过深度神经网络直接预测目标图像的稠密回归问题。这些方法要么有模型能力，要么有解释能力。受隐式神经表示的启发，我们提出将引导超分辨率问题描述为一个神经隐式图像插值问题，其中我们采用了一般图像插值的形式，但使用了一种新的联合隐式图像函数（JIIF）表示来学习插值权重和插值值。JIIF用从输入图像和引导图像中提取的空间分布的局部潜码来表示目标图像域，并利用图形注意机制在一个统一的深隐函数中同时学习插值权值。我们证明了我们的JIIF表示在引导深度超分辨率任务上的有效性，在三个公共基准上显著优于最先进的方法。可以在\url中找到代码{https://git.io/JC2sU}.
<details>	<summary>英文摘要</summary>	Guided depth super-resolution is a practical task where a low-resolution and noisy input depth map is restored to a high-resolution version, with the help of a high-resolution RGB guide image. Existing methods usually view this task as a generalized guided filtering problem that relies on designing explicit filters and objective functions, or a dense regression problem that directly predicts the target image via deep neural networks. These methods suffer from either model capability or interpretability. Inspired by the recent progress in implicit neural representation, we propose to formulate the guided super-resolution as a neural implicit image interpolation problem, where we take the form of a general image interpolation but use a novel Joint Implicit Image Function (JIIF) representation to learn both the interpolation weights and values. JIIF represents the target image domain with spatially distributed local latent codes extracted from the input image and the guide image, and uses a graph attention mechanism to learn the interpolation weights at the same time in one unified deep implicit function. We demonstrate the effectiveness of our JIIF representation on guided depth super-resolution task, significantly outperforming state-of-the-art methods on three public benchmarks. Code can be found at \url{https://git.io/JC2sU}. </details>
<details>	<summary>注释</summary>	Accepted by ACM MM 2021 DOI: 10.1145/3474085.3475584 </details>
<details>	<summary>邮件日期</summary>	2021年07月20日</details>

# 195、混合对抗性高斯域自适应联合半监督三维超分辨率分割
- [ ] Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation 
时间：2021年07月16日                         第一作者：Nicolo Savioli                       [链接](https://arxiv.org/abs/2107.07975).                     
## 摘要：优化心脏结构和功能的分析需要精确的形状和运动的三维表示。然而，心脏磁共振成像等技术通常局限于获取连续的横截面切片，这些切片具有低的穿透平面分辨率和潜在的层间空间错位。医学成像中的超分辨率是为了提高图像的分辨率，但传统的超分辨率训练方法是基于低分辨率数据集的特征，而不是对相应的分割进行超分辨率。本文提出了一种半监督的多任务生成对抗网络（Gemini-GAN），该网络利用高分辨率3D电影和分割的地面真值，对图像及其标签进行联合超分辨率处理，而非监督的变分对抗混合自动编码器（V-AMA）用于连续域自适应。我们提出的方法被广泛评价在两个跨国多民族人口的1331和205分别成年人，提供了一个改进的国家的最先进的方法方面的骰子指数，峰值信噪比和结构相似性指数的措施。这个框架在外部验证方面也超过了最新的生成域适应模型的性能（左心室的Dice指数为0.81 vs 0.74）。这说明了如何联合超分辨率和分割，训练三维地面真相数据与跨领域推广，使稳健的精度表型在不同的人口。
<details>	<summary>英文摘要</summary>	Optimising the analysis of cardiac structure and function requires accurate 3D representations of shape and motion. However, techniques such as cardiac magnetic resonance imaging are conventionally limited to acquiring contiguous cross-sectional slices with low through-plane resolution and potential inter-slice spatial misalignment. Super-resolution in medical imaging aims to increase the resolution of images but is conventionally trained on features from low resolution datasets and does not super-resolve corresponding segmentations. Here we propose a semi-supervised multi-task generative adversarial network (Gemini-GAN) that performs joint super-resolution of the images and their labels using a ground truth of high resolution 3D cines and segmentations, while an unsupervised variational adversarial mixture autoencoder (V-AMA) is used for continuous domain adaptation. Our proposed approach is extensively evaluated on two transnational multi-ethnic populations of 1,331 and 205 adults respectively, delivering an improvement on state of the art methods in terms of Dice index, peak signal to noise ratio, and structural similarity index measure. This framework also exceeds the performance of state of the art generative domain adaptation models on external validation (Dice index 0.81 vs 0.74 for the left ventricle). This demonstrates how joint super-resolution and segmentation, trained on 3D ground-truth data with cross-domain generalization, enables robust precision phenotyping in diverse populations. </details>
<details>	<summary>邮件日期</summary>	2021年07月19日</details>

# 194、关卡生成与风格提升——游戏开发深度学习概述
- [ ] Level generation and style enhancement -- deep learning for game development overview 
时间：2021年07月15日                         第一作者：Piotr Migda{\l}                       [链接](https://arxiv.org/abs/2107.07397).                     
## 摘要：我们提出了使用深度学习来创建和增强视频游戏（桌面、移动和网络）的水平贴图和纹理的实用方法。我们的目标是为游戏开发者和水平艺术家提供新的可能性。设计关卡并用细节填充关卡的任务很有挑战性。这既费时又费劲，使水平丰富，复杂，并有一种自然的感觉。幸运的是，最近在深度学习方面取得的进展为高级设计师和视觉艺术家提供了新的工具。此外，他们还提供了一种方法来生成无限的游戏可重复性世界，并根据玩家的需要调整教育游戏。我们提出了七种创建层次图的方法，每种方法都使用统计方法、机器学习或深度学习。特别是，我们包括：-从现有示例（如ProGAN）创建新图像的生成性对抗网络在保留清晰细节的同时放大图像的超分辨率技术（如ESRGAN）改变视觉主题的神经风格转换图像翻译-将语义地图转换为图像（如GauGAN）用于将图像转换为语义掩码的语义分割（例如U-Net）用于提取语义特征（例如Tile2Vec）的无监督语义分割-纹理合成-创建一个较小的样本（如英根）为基础的大模式。
<details>	<summary>英文摘要</summary>	We present practical approaches of using deep learning to create and enhance level maps and textures for video games -- desktop, mobile, and web. We aim to present new possibilities for game developers and level artists. The task of designing levels and filling them with details is challenging. It is both time-consuming and takes effort to make levels rich, complex, and with a feeling of being natural. Fortunately, recent progress in deep learning provides new tools to accompany level designers and visual artists. Moreover, they offer a way to generate infinite worlds for game replayability and adjust educational games to players' needs. We present seven approaches to create level maps, each using statistical methods, machine learning, or deep learning. In particular, we include: - Generative Adversarial Networks for creating new images from existing examples (e.g. ProGAN). - Super-resolution techniques for upscaling images while preserving crisp detail (e.g. ESRGAN). - Neural style transfer for changing visual themes. - Image translation - turning semantic maps into images (e.g. GauGAN). - Semantic segmentation for turning images into semantic masks (e.g. U-Net). - Unsupervised semantic segmentation for extracting semantic features (e.g. Tile2Vec). - Texture synthesis - creating large patterns based on a smaller sample (e.g. InGAN). </details>
<details>	<summary>注释</summary>	16 pages, 10 figures, submitted to the 52nd International Simulation and Gaming Association (ISAGA) Conference 2021 ACM-class: I.2.10; I.4.3; J.5 </details>
<details>	<summary>邮件日期</summary>	2021年07月16日</details>

# 193、遥感图像超分辨率的多注意生成对抗网络
- [ ] Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution 
时间：2021年07月14日                         第一作者：Meng Xu                       [链接](https://arxiv.org/abs/2107.06536).                     
## 摘要：图像超分辨率（SR）方法可以在不增加成本的前提下生成高空间分辨率的遥感图像，从而为获取高分辨率遥感图像提供了一种可行的方法，而高分辨率遥感图像由于采集设备成本高、天气复杂而难以获取。显然，图像超分辨率是一个严重的不适定问题。幸运的是，随着深度学习的发展，深度神经网络强大的拟合能力在一定程度上解决了这一问题。本文提出了一种基于生成对抗网络（GAN）的高分辨率遥感图像生成网络，称为多注意生成对抗网络（MA-GAN）。我们首先设计了一个基于GAN的图像SR任务框架。完成SR任务的核心是我们设计的带后上采样的图像发生器。发电机主体包括两个模块；一种是剩余密集块中的金字塔卷积（PCRDB），另一种是基于注意的上采样（AUP）块。PCRDB块中的注意金字塔卷积（AttPConv）是一个将多尺度卷积和信道注意相结合的模块，用于自动学习和调整残差的比例以获得更好的结果。AUP块是一个模块，它结合像素注意（PA）来执行任意倍数的上采样。这两个块一起工作，以帮助生成更好的图像质量。对于损失函数，我们设计了一个基于像素损失的损失函数，并引入对抗损失和特征损失来指导生成器的学习。在一个遥感场景图像数据集上，我们将该方法与现有的几种方法进行了比较，实验结果一致地证明了该方法的有效性。
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost, thereby providing a feasible way to acquire high-resolution remote sensing images, which are difficult to obtain due to the high cost of acquisition equipment and complex weather. Clearly, image super-resolution is a severe ill-posed problem. Fortunately, with the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. In this paper, we propose a network based on the generative adversarial network (GAN) to generate high resolution remote sensing images, named the multi-attention generative adversarial network (MA-GAN). We first designed a GAN-based framework for the image SR task. The core to accomplishing the SR task is the image generator with post-upsampling that we designed. The main body of the generator contains two blocks; one is the pyramidal convolution in the residual-dense block (PCRDB), and the other is the attention-based upsample (AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB block is a module that combines multi-scale convolution and channel attention to automatically learn and adjust the scaling of the residuals for better results. The AUP block is a module that combines pixel attention (PA) to perform arbitrary multiples of upsampling. These two blocks work together to help generate better quality images. For the loss function, we design a loss function based on pixel loss and introduce both adversarial loss and feature loss to guide the generator learning. We have compared our method with several state-of-the-art methods on a remote sensing scene image dataset, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 192、基于深度学习的4K视频实时超分辨率系统
- [ ] Real-Time Super-Resolution System of 4K-Video Based on Deep Learning 
时间：2021年07月14日                         第一作者：Yanpeng Cao                       [链接](https://arxiv.org/abs/2107.05307).                     
<details>	<summary>注释</summary>	8 pages, 7 figures, ASAP </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 191、基于深度学习的单幅图像超分辨率研究综述
- [ ] A Comprehensive Review of Deep Learning-based Single Image Super-resolution 
时间：2021年07月13日                         第一作者：Syed Muhammad Arsalan Bashir                       [链接](https://arxiv.org/abs/2102.09351).                     
<details>	<summary>注释</summary>	56 Pages, 11 Figures, 5 Tables Journal-ref: PeerJ Computer Science 7:e621, 2021 DOI: 10.7717/peerj-cs.621 </details>
<details>	<summary>邮件日期</summary>	2021年07月14日</details>

# 190、学习超分辨率超声改善乳腺病变特征
- [ ] Learned super resolution ultrasound for improved breast lesion characterization 
时间：2021年07月12日                         第一作者：Or Bar-Shira                       [链接](https://arxiv.org/abs/2107.05270).                     
## 摘要：乳腺癌是女性最常见的恶性肿瘤。微钙化和肿块等乳房X线表现以及超声扫描中肿块的形态特征是肿瘤检测的主要诊断目标。然而，需要提高这些成像方式的特异性。一个主要的替代靶点是新生血管生成。病理学上，它有助于许多类型的肿瘤的发展和转移的形成。因此，通过微血管的可视化显示新生血管可能是非常重要的。超分辨超声定位显微术可以在毛细血管水平成像微血管。然而，将超分辨率超声转化为临床需要解决的挑战包括重建时间长、依赖于系统点扩散函数（PSF）的先验知识以及超声造影剂（uca）的可分性。在这项工作中，我们使用了一个深层的神经网络架构，有效地利用信号结构来应对这些挑战。我们介绍了三种不同乳腺病变的活体人体结果。通过利用我们训练的网络，微血管结构可以在短时间内恢复，而无需事先了解PSF，也不需要uca的可分性。每种回收物都显示出与已知组织结构相对应的不同结构。这项研究证明了基于临床扫描仪的活体人体超分辨技术的可行性，以提高超声对不同乳腺病变的特异性，并促进超声在乳腺疾病诊断中的应用。
<details>	<summary>英文摘要</summary>	Breast cancer is the most common malignancy in women. Mammographic findings such as microcalcifications and masses, as well as morphologic features of masses in sonographic scans, are the main diagnostic targets for tumor detection. However, improved specificity of these imaging modalities is required. A leading alternative target is neoangiogenesis. When pathological, it contributes to the development of numerous types of tumors, and the formation of metastases. Hence, demonstrating neoangiogenesis by visualization of the microvasculature may be of great importance. Super resolution ultrasound localization microscopy enables imaging of the microvasculature at the capillary level. Yet, challenges such as long reconstruction time, dependency on prior knowledge of the system Point Spread Function (PSF), and separability of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation of super-resolution US into the clinic. In this work we use a deep neural network architecture that makes effective use of signal structure to address these challenges. We present in vivo human results of three different breast lesions acquired with a clinical US scanner. By leveraging our trained network, the microvasculature structure is recovered in a short time, without prior PSF knowledge, and without requiring separability of the UCAs. Each of the recoveries exhibits a different structure that corresponds with the known histological structure. This study demonstrates the feasibility of in vivo human super resolution, based on a clinical scanner, to increase US specificity for different breast lesions and promotes the use of US in the diagnosis of breast pathologies. </details>
<details>	<summary>注释</summary>	to be published in MICCAI 2021 proceedings </details>
<details>	<summary>邮件日期</summary>	2021年07月13日</details>

# 189、基于深度学习的4K视频实时超分辨率系统
- [ ] Real-Time Super-Resolution System of 4K-Video Based on Deep Learning 
时间：2021年07月12日                         第一作者：Yanpeng Cao                       [链接](https://arxiv.org/abs/2107.05307).                     
## 摘要：视频超分辨率（VSR）技术在重建低质量视频方面有着突出的优势，避免了基于插值的算法带来的令人不快的模糊效果。然而，在实际应用中，特别是在大规模VSR任务中，庞大的计算复杂度和内存占用严重影响了系统的可靠性和运行时推理。本文探讨了实时VSR系统的可能性，并设计了一种高效通用的VSR网络EGVSR。提出了一种基于时空对抗学习的EGVSR算法。为了在4K分辨率下追求更快的VSR处理能力，本文尝试在保证高视觉质量的前提下，选择轻量级的网络结构和高效的上采样方法来减少EGVSR网络所需的计算量。此外，在实际硬件平台上实现了批量归一化计算融合、卷积加速算法等神经网络加速技术，优化了EGVSR网络的推理过程。最后，我们的EGVSR达到了实时处理的能力4K@29.61FPS. 与目前最先进的VSR网络TecoGAN相比，计算密度降低了85.04%，性能提高了7.92倍。在视觉质量方面，所提出的EGVSR在公共测试数据集Vid4的大多数度量（如LPIPS、tOF、tLP等）中名列前茅，在总体性能得分上超过了其他最先进的方法。这个项目的源代码可以在https://github.com/Thmen/EGVSR.
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92x performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on https://github.com/Thmen/EGVSR. </details>
<details>	<summary>注释</summary>	8 pages, 7 figures, ASAP </details>
<details>	<summary>邮件日期</summary>	2021年07月13日</details>

# 188、区域差分信息熵在超分辨率图像质量评价中的应用
- [ ] Regional Differential Information Entropy for Super-Resolution Image Quality Assessment 
时间：2021年07月08日                         第一作者：Ningyuan Xu                       [链接](https://arxiv.org/abs/2107.03642).                     
## 摘要：PSNR和SSIM是超分辨率问题中应用最广泛的指标，因为它们易于使用，并且可以评估生成图像和参考图像之间的相似性。然而，单幅图像的超分辨率是一个不适定问题，同一幅低分辨率图像对应多幅高分辨率图像。相似性不能完全反映修复效果。生成的图像的感知质量也很重要，但是PSNR和SSIM不能很好地反映感知质量。为了解决这个问题，我们提出了一种区域差分信息熵的方法来度量相似度和感知质量。针对传统的图像信息熵不能反映图像的结构信息的问题，提出了用滑动窗口来度量各区域的信息熵。考虑到人类视觉系统对低亮度下的亮度差异更为敏感，我们采用$\gamma$量化而不是线性量化。为了加快计算速度，我们用神经网络重新组织了信息熵的计算过程。通过在我们的IQA数据集和PIPAL上的实验，证明了RDIE能够更好地量化图像尤其是基于GAN的图像的感知质量。
<details>	<summary>英文摘要</summary>	PSNR and SSIM are the most widely used metrics in super-resolution problems, because they are easy to use and can evaluate the similarities between generated images and reference images. However, single image super-resolution is an ill-posed problem, there are multiple corresponding high-resolution images for the same low-resolution image. The similarities can't totally reflect the restoration effect. The perceptual quality of generated images is also important, but PSNR and SSIM do not reflect perceptual quality well. To solve the problem, we proposed a method called regional differential information entropy to measure both of the similarities and perceptual quality. To overcome the problem that traditional image information entropy can't reflect the structure information, we proposed to measure every region's information entropy with sliding window. Considering that the human visual system is more sensitive to the brightness difference at low brightness, we take $\gamma$ quantization rather than linear quantization. To accelerate the method, we reorganized the calculation procedure of information entropy with a neural network. Through experiments on our IQA dataset and PIPAL, this paper proves that RDIE can better quantify perceptual quality of images especially GAN-based images. </details>
<details>	<summary>注释</summary>	8 pages, 9 figures, 4 tables ACM-class: I.4.3; I.4.4 </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 187、基于潜在优化的联合运动校正和超分辨率心脏分割
- [ ] Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation 
时间：2021年07月08日                         第一作者：Shuo Wang                       [链接](https://arxiv.org/abs/2107.03887).                     
## 摘要：在心脏磁共振成像（CMR）中，心脏的三维高分辨率分割对于详细描述其解剖结构至关重要。然而，由于采集时间和呼吸/心脏运动的限制，临床上通常需要采集多层二维图像。这些图像的分割提供了心脏解剖结构的低分辨率表示，其中可能包含由运动引起的人工制品。在这里，我们提出了一个新的潜在优化框架，联合执行运动校正和超分辨率心脏图像分割。在给定低分辨率分割作为输入的情况下，该框架考虑了心脏MR成像中的层间运动，并将输入超分辨为与输入一致的高分辨率分割。一个多视图的损失是结合利用信息从短轴和长轴视图的心脏成像。为了解决反问题，在一个潜在空间中进行迭代优化，以确保解剖学上的合理性。这减轻了有监督学习对低分辨率和高分辨率图像的需要。在两个心脏MR数据集上的实验表明，该框架具有很高的性能，与最新的超分辨率方法相当，并且具有更好的跨域通用性和解剖学合理性。
<details>	<summary>英文摘要</summary>	In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution segmentation of the heart is essential for detailed description of its anatomical structures. However, due to the limit of acquisition duration and respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in clinical routine. The segmentation of these images provides a low-resolution representation of cardiac anatomy, which may contain artefacts caused by motion. Here we propose a novel latent optimisation framework that jointly performs motion correction and super resolution for cardiac image segmentations. Given a low-resolution segmentation as input, the framework accounts for inter-slice motion in cardiac MR imaging and super-resolves the input into a high-resolution segmentation consistent with input. A multi-view loss is incorporated to leverage information from both short-axis view and long-axis view of cardiac imaging. To solve the inverse problem, iterative optimisation is performed in a latent space, which ensures the anatomical plausibility. This alleviates the need of paired low-resolution and high-resolution images for supervised learning. Experiments on two cardiac MR datasets show that the proposed framework achieves high performance, comparable to state-of-the-art super-resolution approaches and with better cross-domain generalisability and anatomical plausibility. </details>
<details>	<summary>注释</summary>	The paper is early accepted to MICCAI 2021. The codes are available at https://github.com/shuowang26/SRHeart </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 186、盲图像超分辨率研究综述及展望
- [ ] Blind Image Super-Resolution: A Survey and Beyond 
时间：2021年07月07日                         第一作者：Anran Liu                       [链接](https://arxiv.org/abs/2107.03055).                     
## 摘要：盲图像超分辨率（SR）技术是一种针对低分辨率图像的超分辨率技术，具有重要的现实意义。近年来，人们提出了许多新颖有效的解决方案，特别是借助于强大的深度学习技术。尽管经过多年的努力，它仍然是一个具有挑战性的研究问题。本文系统地回顾了近年来盲图像SR的研究进展，提出了一种分类方法，根据退化建模的方法和求解SR模型所用的数据，将现有的方法分为三类。这种分类法有助于总结和区分现有的方法。我们希望能对当前的研究状况提供一些见解，同时也能揭示出值得探索的新的研究方向。此外，本文还对常用的数据集和以往与盲图像相关的比赛进行了总结。最后，对各种方法进行了比较，并用合成图像和真实测试图像详细分析了它们的优缺点。
<details>	<summary>英文摘要</summary>	Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with the powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used for solving the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as to reveal novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images. </details>
<details>	<summary>邮件日期</summary>	2021年07月08日</details>

# 185、一种用于多域图像超分辨率的深剩余恒星生成对抗网络
- [ ] A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution 
时间：2021年07月07日                         第一作者：Rao Muhammad Umer                       [链接](https://arxiv.org/abs/2107.03145).                     
## 摘要：近年来，利用深度卷积神经网络（DCNNs）实现的最新单图像超分辨率（SISR）方法取得了令人瞩目的性能。现有的SR方法由于固定的退化设置（即通常是低分辨率（LR）图像的双三次缩小）而性能有限。然而，在现实环境中，LR退化过程是未知的，可以是双三次LR、双线性LR、最近邻LR或真实LR。因此，大多数SR方法在处理单个网络中的多个降级设置时是无效和低效的。为了解决多重退化问题，即多域图像的超分辨率问题，我们提出了一种深度超分辨率残差StarGAN（SR2*GAN）算法，该算法只需一个模型就可以对多个LR域的LR图像进行超分辨率处理。该方案是在一个类似StarGAN的网络拓扑结构中训练的，该拓扑结构由一个生成器和一个鉴别器网络组成。与其他先进的方法相比，我们在定量和定性实验中证明了我们提出的方法的有效性。
<details>	<summary>英文摘要</summary>	Recently, most of state-of-the-art single image super-resolution (SISR) methods have attained impressive performance by using deep convolutional neural networks (DCNNs). The existing SR methods have limited performance due to a fixed degradation settings, i.e. usually a bicubic downscaling of low-resolution (LR) image. However, in real-world settings, the LR degradation process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR, or real LR. Therefore, most SR methods are ineffective and inefficient in handling more than one degradation settings within a single network. To handle the multiple degradation, i.e. refers to multi-domain image super-resolution, we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and scalable approach that super-resolves the LR images for the multiple LR domains using only a single model. The proposed scheme is trained in a StarGAN like network topology with a single generator and discriminator networks. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments compared to other state-of-the-art methods. </details>
<details>	<summary>注释</summary>	5 pages, 6th International Conference on Smart and Sustainable Technologies 2021. arXiv admin note: text overlap with arXiv:2009.03693, arXiv:2005.00953 </details>
<details>	<summary>邮件日期</summary>	2021年07月08日</details>

# 184、从一般到特殊：盲超分辨率的在线更新
- [ ] From General to Specific: Online Updating for Blind Super-Resolution 
时间：2021年07月06日                         第一作者：Shang Li                       [链接](https://arxiv.org/abs/2107.02398).                     
## 摘要：大多数基于深度学习的超分辨率（SR）方法都不是图像特定的：1）它们在由预定义模糊核（如双三次）合成的数据集上进行穷尽训练，而不考虑与测试图像的域间距。2） 它们的模型权值在测试过程中是固定的，这意味着具有不同退化的测试图像被同一组权值所超分辨。然而，真实图像的退化是多种多样且未知的（\ie blind SR）。单个模型很难在所有情况下都表现良好。为了解决这些问题，我们提出了一种在线超分辨率（ONSR）方法。它不依赖于预定义的模糊核，并且允许根据测试图像的退化更新模型权重。具体来说，ONSR由两个分支组成，即内部分支（IB）和外部分支（EB）。IB可以学习给定测试LR图像的特定退化，EB可以学习由所学习的退化所退化的图像的超分辨率。通过这种方式，ONSR可以为每个测试图像定制一个特定的模型，从而在实际应用中对各种退化具有更高的容忍度。在合成图像和真实图像上的大量实验表明，ONSR能产生更为直观的随机共振结果，并在盲随机共振中获得最先进的性能。
<details>	<summary>英文摘要</summary>	Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are exhaustively trained on datasets synthesized by predefined blur kernels (\eg bicubic), regardless of the domain gap with test images. 2) Their model weights are fixed during testing, which means that test images with various degradations are super-resolved by the same set of weights. However, degradations of real images are various and unknown (\ie blind SR). It is hard for a single model to perform well in all cases. To address these issues, we propose an online super-resolution (ONSR) method. It does not rely on predefined blur kernels and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus could be more tolerant with various degradations in real applications. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR. </details>
<details>	<summary>注释</summary>	Submitted to Pattern Recognition </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 183、基于深度学习的图像超分辨率对二值信号检测的影响
- [ ] Impact of deep learning-based image super-resolution on binary signal detection 
时间：2021年07月06日                         第一作者：Xiaohui Zhang                       [链接](https://arxiv.org/abs/2107.02338).                     
## 摘要：基于深度学习的图像超分辨率（DL-SR）在医学成像领域有着广阔的应用前景。迄今为止，大多数提出的DL-SR方法仅通过使用计算机视觉领域中常用的传统图像质量（IQ）度量进行评估。然而，这些方法对与医学成像任务相关的图像质量的客观度量的影响仍然很大程度上未被探索。在这项研究中，我们研究了DL-SR方法对二进制信号检测性能的影响。利用模拟医学图像数据训练了两种常用的DL-SR方法：超分辨率卷积神经网络（SRCNN）和超分辨率生成对抗网络（SRGAN）。提出了背景统计已知二值信号（SKE/BKS）和背景统计已知信号（SKS/BKS）检测任务。数值观察器包括神经网络逼近理想观察器和普通线性数值观察器，用于评估DL-SR对任务绩效的影响。量化了DL-SR网络结构的复杂性对任务性能的影响。此外，还研究了DL-SR在改善次优观测器任务性能方面的作用。我们的数值实验证实，DL-SR可以改善传统的智商测量方法。然而，对于所考虑的许多研究设计，DL-SR方法对任务绩效的改善很少或没有改善，甚至可能降低任务绩效。结果表明，在一定条件下，DL-SR可以提高次优观测器的任务性能。本研究强调了客观评估DL-SR方法的迫切需要，并提出了提高其在医学成像应用中的有效性的途径。
<details>	<summary>英文摘要</summary>	Deep learning-based image super-resolution (DL-SR) has shown great promise in medical imaging applications. To date, most of the proposed methods for DL-SR have only been assessed by use of traditional measures of image quality (IQ) that are commonly employed in the field of computer vision. However, the impact of these methods on objective measures of image quality that are relevant to medical imaging tasks remains largely unexplored. In this study, we investigate the impact of DL-SR methods on binary signal detection performance. Two popular DL-SR methods, the super-resolution convolutional neural network (SRCNN) and the super-resolution generative adversarial network (SRGAN), were trained by use of simulated medical image data. Binary signal-known-exactly with background-known-statistically (SKE/BKS) and signal-known-statistically with background-known-statistically (SKS/BKS) detection tasks were formulated. Numerical observers, which included a neural network-approximated ideal observer and common linear numerical observers, were employed to assess the impact of DL-SR on task performance. The impact of the complexity of the DL-SR network architectures on task-performance was quantified. In addition, the utility of DL-SR for improving the task-performance of sub-optimal observers was investigated. Our numerical experiments confirmed that, as expected, DL-SR could improve traditional measures of IQ. However, for many of the study designs considered, the DL-SR methods provided little or no improvement in task performance and could even degrade it. It was observed that DL-SR could improve the task-performance of sub-optimal observers under certain conditions. The presented study highlights the urgent need for the objective assessment of DL-SR methods and suggests avenues for improving their efficacy in medical imaging applications. </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 182、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年07月05日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
<details>	<summary>注释</summary>	10 pages, 3 figures Journal-ref: International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2021) </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 181、用于关节MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年07月05日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 180、复杂噪声下的无监督单幅图像超分辨率
- [ ] Unsupervised Single Image Super-resolution Under Complex Noise 
时间：2021年07月02日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
## 摘要：近年来，单图像超分辨率（SISR）的研究，特别是基于深度神经网络（DNNs）的单图像超分辨率（SISR）的研究取得了巨大的成就，但仍存在两大局限性。首先，真实的图像退化通常是未知的，并且在不同的情况下变化很大，因此很难训练单一的模型来处理一般的SISR任务。其次，现有的方法大多集中在退化的降采样过程，而忽略或低估了不可避免的噪声污染。例如，通常使用的独立同分布（i.i.d.）高斯噪声分布总是很大程度上偏离真实图像噪声（例如相机传感器噪声），这限制了它们在真实场景中的性能。针对这些问题，本文提出了一种基于模型的无监督SISR方法来处理具有未知退化的一般SISR任务。提出了一种新的基于面片的非i.i.d.噪声建模方法，代替了传统的i.i.d.高斯噪声假设。此外，利用DNN参数化的深度发生器将潜变量映射到高分辨率图像中，并将传统的超拉普拉斯先验知识嵌入到深度发生器中，进一步约束图像梯度。最后，设计了一个montecarlo-EM算法来求解我们的模型，它提供了一个通用的推理框架来更新图像发生器的w.r.t.潜变量和网络参数。综合实验表明，该方法不仅模型较细（0.34M比2.40M），而且速度较快，明显优于现有的SotA方法（峰值信噪比1dB左右）。
<details>	<summary>英文摘要</summary>	While the researches on single image super-resolution (SISR), especially equipped with deep neural networks (DNNs), have achieved tremendous successes recently, they still suffer from two major limitations. Firstly, the real image degradation is usually unknown and highly variant from one to another, making it extremely hard to train a single model to handle the general SISR task. Secondly, most of current methods mainly focus on the downsampling process of the degradation, but ignore or underestimate the inevitable noise contamination. For example, the commonly-used independent and identically distributed (i.i.d.) Gaussian noise distribution always largely deviates from the real image noise (e.g., camera sensor noise), which limits their performance in real scenarios. To address these issues, this paper proposes a model-based unsupervised SISR method to deal with the general SISR task with unknown degradations. Instead of the traditional i.i.d. Gaussian noise assumption, a novel patch-based non-i.i.d. noise modeling method is proposed to fit the complex real noise. Besides, a deep generator parameterized by a DNN is used to map the latent variable to the high-resolution image, and the conventional hyper-Laplacian prior is also elaborately embedded into such generator to further constrain the image gradients. Finally, a Monte Carlo EM algorithm is designed to solve our model, which provides a general inference framework to update the image generator both w.r.t. the latent variable and the network parameters. Comprehensive experiments demonstrate that the proposed method can evidently surpass the current state of the art (SotA) method (about 1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster speed. </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 179、深部图像先验光谱偏差的测量与控制
- [ ] On Measuring and Controlling the Spectral Bias of the Deep Image Prior 
时间：2021年07月02日                         第一作者：Zenglin Shi                       [链接](https://arxiv.org/abs/2107.01125).                     
## 摘要：深度图像先验证明了未经训练的网络能够通过对单个退化图像进行优化来解决逆成像问题，如去噪、修复和超分辨率。尽管它有承诺，但它有两个局限性。首先，除了网络架构的选择之外，人们如何控制先验知识还不清楚。第二，当性能达到峰值后下降时，需要oracle确定何时停止优化。为了解决这些问题，本文从光谱偏移的角度研究了深部图像的先验知识。通过引入频带对应度量，我们观察到用于逆成像的深度图像先验在优化过程中表现出光谱偏差，其中低频图像信号比高频噪声信号学习得更快更好。这就明确了当优化在适当的时间停止时，为什么退化的图像可以去噪或修复。基于我们的观察，我们建议在深度图像中控制光谱偏差，以防止性能下降和加速优化收敛。我们在两种核心层类型的逆成像网络中实现：卷积层和上采样层。我们提出了卷积的Lipschitz控制方法和上采样层的高斯控制方法。为了避免多余的计算，我们进一步引入了停止准则。去噪、修复和超分辨率实验表明，该方法在优化过程中不再出现性能下降的问题，从而避免了oracle准则的提前终止。我们进一步概述了停止标准，以避免多余的计算。最后，我们证明了与现有方法相比，我们的方法在所有任务中都取得了良好的恢复效果。
<details>	<summary>英文摘要</summary>	The deep image prior has demonstrated the remarkable ability that untrained networks can address inverse imaging problems, such as denoising, inpainting and super-resolution, by optimizing on just a single degraded image. Despite its promise, it suffers from two limitations. First, it remains unclear how one can control the prior beyond the choice of the network architecture. Second, it requires an oracle to determine when to stop the optimization as the performance degrades after reaching a peak. In this paper, we study the deep image prior from a spectral bias perspective to address these problems. By introducing a frequency-band correspondence measure, we observe that deep image priors for inverse imaging exhibit a spectral bias during optimization, where low-frequency image signals are learned faster and better than high-frequency noise signals. This pinpoints why degraded images can be denoised or inpainted when the optimization is stopped at the right time. Based on our observations, we propose to control the spectral bias in the deep image prior to prevent performance degradation and to speed up optimization convergence. We do so in the two core layer types of inverse imaging networks: the convolution layer and the upsampling layer. We present a Lipschitz-controlled approach for the convolution and a Gaussian-controlled approach for the upsampling layer. We further introduce a stopping criterion to avoid superfluous computation. The experiments on denoising, inpainting and super-resolution show that our method no longer suffers from performance degradation during optimization, relieving us from the need for an oracle criterion to stop early. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable restoration results compared to current approaches, across all tasks. </details>
<details>	<summary>注释</summary>	Spectral bias; Deep image prior; 18 pages </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 178、基于对比表征学习的盲图像超分辨
- [ ] Blind Image Super-Resolution via Contrastive Representation Learning 
时间：2021年07月01日                         第一作者：Jiahui Zhang                       [链接](https://arxiv.org/abs/2107.00708).                     
## 摘要：近年来，由于卷积神经网络（CNNs）的发展，图像超分辨率（SR）的研究取得了令人瞩目的进展。然而，大多数现有的SR方法是非盲的，并且假设退化具有单一的固定的和已知的分布（例如，双三次分布），在处理通常遵循多模态、空间变化和未知分布的真实世界数据中的退化时，这些分布很困难。最近的盲SR研究通过退化估计来解决这一问题，但它们不能很好地推广到多源退化，也不能处理空间变异退化。我们设计了CRL-SR，这是一个对比表征学习网络，主要研究具有多模态和空间变异分布的图像的盲SR。CRL-SR从两个角度解决了盲SR挑战。第一种是对比解耦编码，在双向对比丢失的指导下，引入对比学习来提取分辨率不变的嵌入和丢弃分辨率可变的嵌入。第二种是对比特征提取，在条件对比丢失的指导下产生丢失或损坏的高频细节。在合成数据集和真实图像上的大量实验表明，该方法能有效地处理盲环境下的多模态和空间变异退化问题，并且在定性和定量上都优于现有的随机共振方法。
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) research has witnessed impressive progress thanks to the advance of convolutional neural networks (CNNs) in recent years. However, most existing SR methods are non-blind and assume that degradation has a single fixed and known distribution (e.g., bicubic) which struggle while handling degradation in real-world data that usually follows a multi-modal, spatially variant, and unknown distribution. The recent blind SR studies address this issue via degradation estimation, but they do not generalize well to multi-source degradation and cannot handle spatially variant degradation. We design CRL-SR, a contrastive representation learning network that focuses on blind SR of images with multi-modal and spatially variant distributions. CRL-SR addresses the blind SR challenges from two perspectives. The first is contrastive decoupling encoding which introduces contrastive learning to extract resolution-invariant embedding and discard resolution-variant embedding under the guidance of a bidirectional contrastive loss. The second is contrastive feature refinement which generates lost or corrupted high-frequency details under the guidance of a conditional contrastive loss. Extensive experiments on synthetic datasets and real images show that the proposed CRL-SR can handle multi-modal and spatially variant degradation effectively under blind settings and it also outperforms state-of-the-art SR methods qualitatively and quantitatively. </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 177、文本优先引导场景文本图像超分辨率
- [ ] Text Prior Guided Scene Text Image Super-resolution 
时间：2021年06月30日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2106.15368).                     
<details>	<summary>注释</summary>	Code has been released on https://github.com/mjq11302010044/TPGSR </details>
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 176、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年06月30日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 175、应用自监督学习实现动态胎儿MRI的超分辨率
- [ ] STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning 
时间：2021年06月30日                         第一作者：Junshen Xu                       [链接](https://arxiv.org/abs/2106.12407).                     
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 174、文本优先引导场景文本图像超分辨率
- [ ] Text Prior Guided Scene Text Image Super-resolution 
时间：2021年06月29日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2106.15368).                     
## 摘要：场景文本图像超分辨率（STISR）旨在提高低分辨率（LR）场景文本图像的分辨率和视觉质量，从而提高文本识别的性能。然而，现有的STISR方法大多将文本图像视为自然场景图像，忽略了文本的分类信息。在本文中，我们做了一个鼓舞人心的尝试，将分类文本嵌入到STISR模型训练中。具体地说，我们采用字符概率序列作为文本先验，这可以方便地从文本识别模型中获得。文本优先提供分类指导，以恢复高分辨率（HR）文本图像。另一方面，重构后的HR图像可以对文本进行细化。最后，我们提出了一个多阶段的文本优先引导超分辨率（TPGSR）框架。在基准TextZoom数据集上的实验表明，TPGSR不仅能有效地提高场景文本图像的视觉质量，而且比现有的STISR方法显著提高了文本识别的准确率。在TextZoom上训练的模型对其它数据集中的LR图像也具有一定的泛化能力。
<details>	<summary>英文摘要</summary>	Scene text image super-resolution (STISR) aims to improve the resolution and visual quality of low-resolution (LR) scene text images, and consequently boost the performance of text recognition. However, most of existing STISR methods regard text images as natural scene images, ignoring the categorical information of text. In this paper, we make an inspiring attempt to embed categorical text prior into STISR model training. Specifically, we adopt the character probability sequence as the text prior, which can be obtained conveniently from a text recognition model. The text prior provides categorical guidance to recover high-resolution (HR) text images. On the other hand, the reconstructed HR image can refine the text prior in return. Finally, we present a multi-stage text prior guided super-resolution (TPGSR) framework for STISR. Our experiments on the benchmark TextZoom dataset show that TPGSR can not only effectively improve the visual quality of scene text images, but also significantly improve the text recognition accuracy over existing STISR methods. Our model trained on TextZoom also demonstrates certain generalization capability to the LR images in other datasets. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 173、IREM：通过隐式神经表示的高分辨率磁共振（MR）图像重建
- [ ] IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation 
时间：2021年06月29日                         第一作者：Qing Wu                       [链接](https://arxiv.org/abs/2106.15097).                     
## 摘要：为了采集高质量的高分辨率（HR）MR图像，我们提出了一种新的图像重建网络IREM，该网络对多幅低分辨率（LR）MR图像进行训练，实现了任意上采样率的HR图像重建。在这项工作中，我们假设期望的HR图像是3D图像空间坐标的隐式连续函数，而厚切片LR图像是该函数的几个稀疏离散采样。然后，超分辨率（SR）任务是使用完全连接的神经网络结合Fourier特征位置编码从有限的观测值中学习连续体积函数。通过简单地最小化网络预测和每个成像平面上获得的LR图像强度之间的误差，IREM被训练成表示观察到的组织解剖结构的连续模型。实验结果表明，IREM成功地表达了高频图像特征，在真实场景数据采集中，IREM缩短了扫描时间，在信噪比和局部图像细节方面实现了高质量的高分辨率MR成像。
<details>	<summary>英文摘要</summary>	For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 172、一种混合监督多级GAN图像质量增强框架
- [ ] A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement 
时间：2021年06月29日                         第一作者：Uddeshya Upadhyay                       [链接](https://arxiv.org/abs/2106.15575).                     
## 摘要：用于图像质量增强的深度神经网络通常需要大量由一对低质量图像及其相应的高质量图像组成的高度精确的训练数据。虽然高质量图像采集通常昂贵且耗时，但中等质量图像的采集速度更快，设备成本更低，并且可以大量获取。因此，我们提出了一种新的生成性对抗网络（GAN），它可以在多个质量级别（例如，高质量和中等质量）上利用训练数据来提高性能，同时限制数据管理的成本。我们将我们的混合监督GAN应用于（i）超分辨率组织病理学图像和（ii）结合超分辨率和外科消烟来增强腹腔镜图像。在大量临床和临床前数据集上的结果表明，我们的混合监督机制优于现有技术。
<details>	<summary>英文摘要</summary>	Deep neural networks for image quality enhancement typically need large quantities of highly-curated training data comprising pairs of low-quality images and their corresponding high-quality images. While high-quality image acquisition is typically expensive and time-consuming, medium-quality images are faster to acquire, at lower equipment costs, and available in larger quantities. Thus, we propose a novel generative adversarial network (GAN) that can leverage training data at multiple levels of quality (e.g., high and medium quality) to improve performance while limiting costs of data curation. We apply our mixed-supervision GAN to (i) super-resolve histopathology images and (ii) enhance laparoscopy images by combining super-resolution and surgical smoke removal. Results on large clinical and pre-clinical datasets show the benefits of our mixed-supervision GAN over the state of the art. </details>
<details>	<summary>注释</summary>	MICCAI 2019 </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 171、“零炮”点云上采样
- [ ] "Zero Shot" Point Cloud Upsampling 
时间：2021年06月25日                         第一作者：Kaiyue Zhou                       [链接](https://arxiv.org/abs/2106.13765).                     
## 摘要：在过去的几年中，利用深度学习进行点云上采样已经付出了各种努力。最近的有监督深度学习方法局限于训练数据的大小，并且局限于覆盖所有形状的点云。此外，获取如此数量的数据是不现实的，而且网络在看不见的记录上的性能通常不如预期。在本文中，我们提出了一种无监督的点云上采样方法，内部称为“零炮”点云上采样（ZSPU）在整体水平。我们的方法完全基于特定点云提供的内部信息，而不是在自我训练和测试阶段进行修补。这种单流设计通过学习低分辨率（LR）点云和高分辨率（HR）点云之间的关系，显著减少了上采样任务的训练时间。当原始点云作为输入加载时，此关联将提供超分辨率（SR）输出。与其他上采样方法相比，我们在基准点云数据集上展示了具有竞争力的性能。此外，ZSPU在局部细节复杂或曲率较大的形状上取得了较好的定性结果。
<details>	<summary>英文摘要</summary>	Point cloud upsampling using deep learning has been paid various efforts in the past few years. Recent supervised deep learning methods are restricted to the size of training data and is limited in terms of covering all shapes of point clouds. Besides, the acquisition of such amount of data is unrealistic, and the network generally performs less powerful than expected on unseen records. In this paper, we present an unsupervised approach to upsample point clouds internally referred as "Zero Shot" Point Cloud Upsampling (ZSPU) at holistic level. Our approach is solely based on the internal information provided by a particular point cloud without patching in both self-training and testing phases. This single-stream design significantly reduces the training time of the upsampling task, by learning the relation between low-resolution (LR) point clouds and their high (original) resolution (HR) counterparts. This association will provide super-resolution (SR) outputs when original point clouds are loaded as input. We demonstrate competitive performance on benchmark point cloud datasets when compared to other upsampling methods. Furthermore, ZSPU achieves superior qualitative results on shapes with complex local details or high curvatures. </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 170、基于长时自样本的视频超分辨率
- [ ] Video Super-Resolution with Long-Term Self-Exemplars 
时间：2021年06月24日                         第一作者：Guotao Meng                       [链接](https://arxiv.org/abs/2106.12778).                     
## 摘要：现有的视频超分辨率方法通常利用几个相邻帧为每一帧生成更高分辨率的图像。然而，这些方法并没有充分利用帧间的冗余信息：同一实例对应的补丁在不同尺度的帧间出现。基于这一观察，我们提出了一种长期跨尺度聚集的视频超分辨率方法，该方法利用了跨远帧的相似块（自样本）。我们的模型还包括一个多参考对齐模块，用于融合来自相似面片的特征：我们融合来自远处参考的特征来执行高质量的超分辨率。提出了一种新颖实用的基于参考的超分辨率训练策略。为了评估我们提出的方法的性能，我们在收集的CarCam数据集和Waymo开放数据集上进行了大量的实验，结果表明我们的方法优于现有的方法。我们的源代码将公开。
<details>	<summary>英文摘要</summary>	Existing video super-resolution methods often utilize a few neighboring frames to generate a higher-resolution image for each frame. However, the redundant information between distant frames has not been fully exploited in these methods: corresponding patches of the same instance appear across distant frames at different scales. Based on this observation, we propose a video super-resolution method with long-term cross-scale aggregation that leverages similar patches (self-exemplars) across distant frames. Our model also consists of a multi-reference alignment module to fuse the features derived from similar patches: we fuse the features of distant references to perform high-quality super-resolution. We also propose a novel and practical training strategy for referenced-based super-resolution. To evaluate the performance of our proposed method, we conduct extensive experiments on our collected CarCam dataset and the Waymo Open dataset, and the results demonstrate our method outperforms state-of-the-art methods. Our source code will be publicly available. </details>
<details>	<summary>邮件日期</summary>	2021年06月25日</details>

# 169、通过深入学习提高生物超分辨显微镜：一个简要回顾
- [ ] Advancing biological super-resolution microscopy through deep learning: a brief review 
时间：2021年06月24日                         第一作者：Tianjie Yang                       [链接](https://arxiv.org/abs/2106.13064).                     
## 摘要：超分辨显微镜克服了传统光学显微镜在空间分辨率上的衍射限制。通过提供具有分子特异性的纳米尺度生物过程的时空信息，它在生命科学中发挥着越来越重要的作用。然而，它的技术局限性要求权衡其空间分辨率、时间分辨率和样品的光照。近年来，深度学习在许多图像处理和计算机视觉任务中取得了突破性的进展。它在推动超分辨显微镜的性能包络方面也显示出巨大的前景。在这篇简短的综述中，我们综述了利用深度学习提高超分辨显微镜性能的最新进展。我们主要关注深度学习如何促进超分辨率图像的重建。讨论了相关的关键技术挑战。尽管面临挑战，深度学习将在超分辨显微镜的发展中发挥不可或缺的变革性作用。最后，我们展望了深度学习如何影响新一代光学显微镜技术的未来。
<details>	<summary>英文摘要</summary>	Super-resolution microscopy overcomes the diffraction limit of conventional light microscopy in spatial resolution. By providing novel spatial or spatio-temporal information on biological processes at nanometer resolution with molecular specificity, it plays an increasingly important role in life sciences. However, its technical limitations require trade-offs to balance its spatial resolution, temporal resolution, and light exposure of samples. Recently, deep learning has achieved breakthrough performance in many image processing and computer vision tasks. It has also shown great promise in pushing the performance envelope of super-resolution microscopy. In this brief Review, we survey recent advances in using deep learning to enhance performance of super-resolution microscopy. We focus primarily on how deep learning ad-vances reconstruction of super-resolution images. Related key technical challenges are discussed. Despite the challenges, deep learning is set to play an indispensable and transformative role in the development of super-resolution microscopy. We conclude with an outlook on how deep learning could shape the future of this new generation of light microscopy technology. </details>
<details>	<summary>邮件日期</summary>	2021年06月25日</details>

# 168、应用自监督学习实现动态胎儿MRI的超分辨率
- [ ] STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning 
时间：2021年06月23日                         第一作者：Junshen Xu                       [链接](https://arxiv.org/abs/2106.12407).                     
## 摘要：在常规MR扫描时间范围内，胎动是不可预测和快速的。因此，动态胎儿磁共振成像（dynamic fetal MRI）仅限于图像质量和分辨率较差的快速成像技术，其目的是捕捉胎儿运动和胎儿功能的动态变化。动态胎儿磁共振成像的超分辨率仍然是一个挑战，特别是当用于过采样的多方向图像切片堆栈不可用并且需要用于记录胎儿或胎盘动态的高时间分辨率时。此外，胎儿运动使得在有监督学习方法中很难获得高分辨率图像。为了解决这个问题，在这项工作中，我们提出了STRESS（时空分辨率增强与模拟扫描），一个自我监督的超分辨率框架，用于动态胎儿MRI的交错切片采集。我们提出的方法在原始采集的数据上模拟沿高分辨率轴的交错切片采集，以生成低分辨率和高分辨率图像对。然后，利用MR时间序列的时空相关性训练超分辨率网络，提高原始数据的分辨率。对模拟数据和子宫内数据的评价表明，该方法优于其他自监督超分辨率方法，提高了图像质量，有利于后续任务和评价。
<details>	<summary>英文摘要</summary>	Fetal motion is unpredictable and rapid on the scale of conventional MR scan times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and dynamics of fetal function, is limited to fast imaging techniques with compromises in image quality and resolution. Super-resolution for dynamic fetal MRI is still a challenge, especially when multi-oriented stacks of image slices for oversampling are not available and high temporal resolution for recording the dynamics of the fetus or placenta is desired. Further, fetal motion makes it difficult to acquire high-resolution images for supervised learning methods. To address this problem, in this work, we propose STRESS (Spatio-Temporal Resolution Enhancement with Simulated Scans), a self-supervised super-resolution framework for dynamic fetal MRI with interleaved slice acquisitions. Our proposed method simulates an interleaved slice acquisition along the high-resolution axis on the originally acquired data to generate pairs of low- and high-resolution images. Then, it trains a super-resolution network by exploiting both spatial and temporal correlations in the MR time series, which is used to enhance the resolution of the original data. Evaluations on both simulated and in utero data show that our proposed method outperforms other self-supervised super-resolution methods and improves image quality, which is beneficial to other downstream tasks and evaluations. </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 167、基于条件像元合成的卫星图像时空超分辨率分析
- [ ] Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis 
时间：2021年06月22日                         第一作者：Yutong He                       [链接](https://arxiv.org/abs/2106.11485).                     
## 摘要：高分辨率卫星图像已被证明对广泛的任务有用，包括测量全球人口、当地经济生计和生物多样性等。不幸的是，高分辨率图像收集的频率很低，购买成本也很高，因此很难在时间和空间上有效地扩展这些下游任务。我们提出了一种新的条件像素合成模型，该模型利用丰富、低成本、低分辨率的图像在不可用的时间和地点生成精确的高分辨率图像。我们的研究表明，我们的模型在一个关键的下游任务——物体计数——上达到了照片真实的样本质量，并且优于竞争基线，特别是在地面条件变化迅速的地理位置上。
<details>	<summary>英文摘要</summary>	High-resolution satellite imagery has proven useful for a broad range of tasks, including measurement of global human population, local economic livelihoods, and biodiversity, among many others. Unfortunately, high-resolution imagery is both infrequently collected and expensive to purchase, making it hard to efficiently and effectively scale these downstream tasks over both time and space. We propose a new conditional pixel synthesis model that uses abundant, low-cost, low-resolution imagery to generate accurate high-resolution imagery at locations and times in which it is unavailable. We show that our model attains photo-realistic sample quality and outperforms competing baselines on a key downstream task -- object counting -- particularly in geographic locations where conditions on the ground are changing rapidly. </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 166、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年06月22日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
<details>	<summary>注释</summary>	ICME2021, https://ieeexplore.ieee.org/abstract/document/9428136 DOI: 10.1109/ICME51207.2021.9428136 </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 165、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年06月22日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 164、科学数据降阶与可视化的深层超分辨率方法
- [ ] Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization 
时间：2021年05月30日                         第一作者：Skylar W. Wurster                       [链接](https://arxiv.org/abs/2107.00462).                     
## 摘要：提出了一种基于八叉树数据表示的神经网络层次超分辨方法。我们训练了一个神经网络层次结构，每个神经网络能够在两个细节层次之间的每个空间维度上进行2倍的放大，并将这些网络串联使用，以促进大尺度因子超分辨率，随着训练网络的数量进行缩放。我们利用这些网络在一个分层超分辨率算法，提高多分辨率数据到一个统一的高分辨率，而不引入接缝伪影八叉树节点的边界。我们通过将输入数据动态降尺度到基于八叉树的数据结构来表示多分辨率数据，然后压缩以减少额外的存储空间，来评估该算法在数据简化框架中的应用。我们证明了我们的方法避免了多分辨率数据格式中常见的seam伪影，并且说明了在相同的压缩比下，神经网络超分辨率辅助数据约简如何比单独使用压缩器更好地保留全局特征。
<details>	<summary>英文摘要</summary>	We present an approach for hierarchical super resolution (SR) using neural networks on an octree data representation. We train a hierarchy of neural networks, each capable of 2x upscaling in each spatial dimension between two levels of detail, and use these networks in tandem to facilitate large scale factor super resolution, scaling with the number of trained networks. We utilize these networks in a hierarchical super resolution algorithm that upscales multiresolution data to a uniform high resolution without introducing seam artifacts on octree node boundaries. We evaluate application of this algorithm in a data reduction framework by dynamically downscaling input data to an octree-based data structure to represent the multiresolution data before compressing for additional storage reduction. We demonstrate that our approach avoids seam artifacts common to multiresolution data formats, and show how neural network super resolution assisted data reduction can preserve global features better than compressors alone at the same compression ratios. </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 163、用于高保真图像生成的级联扩散模型
- [ ] Cascaded Diffusion Models for High Fidelity Image Generation 
时间：2021年05月30日                         第一作者：Jonathan Ho                       [链接](https://arxiv.org/abs/2106.15282).                     
## 摘要：我们证明了级联扩散模型能够在类条件ImageNet生成挑战上生成高保真图像，而不需要任何辅助图像分类器的帮助来提高样本质量。级联扩散模型包括多个扩散模型的管道，这些扩散模型生成分辨率不断提高的图像，首先是最低分辨率的标准扩散模型，然后是一个或多个超分辨率扩散模型，这些模型依次对图像进行上采样并添加更高分辨率的细节。我们发现级联管道的样本质量主要依赖于条件增强，我们提出的方法是将低分辨率条件输入数据增强到超分辨率模型中。我们的实验表明，条件增强可以防止级联模型采样过程中的复合误差，帮助我们训练级联管道，在64x64、128x128和256x256分辨率下的FID分数分别达到1.48、3.52和4.88，优于BigGAN-deep。
<details>	<summary>英文摘要</summary>	We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 162、在聚焦二十面体网格上应用VertexShuffle实现360度视频超分辨率
- [ ] Applying VertexShuffle Toward 360-Degree Video Super-Resolution on Focused-Icosahedral-Mesh 
时间：2021年06月21日                         第一作者：Na Li                        [链接](https://arxiv.org/abs/2106.11253).                     
## 摘要：随着360度图像/视频、增强现实（AR）和虚拟现实（VR）的出现，人们对球形信号的分析和处理的需求越来越大。然而，大量的研究工作都集中在从球面信号投影出来的平面信号上，导致了像素浪费、失真等问题。球形CNN的最新进展为直接分析球形信号提供了可能。然而，他们关注的是全网格，这使得在实际应用中由于对带宽的要求非常大而难以处理。为了解决360度视频流的带宽浪费问题和节省计算量，我们利用聚焦二十面体网格来表示一个小区域，并构造矩阵将球形内容旋转到聚焦网格区域。我们还提出了一种新的顶点洗牌操作，与UGSCNN中引入的MeshConv转置操作相比，该操作可以显著提高性能和效率。我们进一步将所提出的方法应用于超分辨率模型，这是第一个提出一个直接操作360度数据的球形像素网格表示的球形超分辨率模型。为了评估我们的模型，我们还收集了一组高分辨率的360度视频来生成一个球形图像数据集。我们的实验表明，与使用简单MeshConv转置操作的基线球面超分辨率模型相比，我们提出的球面超分辨率模型在性能和推理时间方面都取得了显著的优势。综上所述，我们的模型在360度输入上取得了很好的超分辨率性能，在对网格上的16x顶点进行超分辨率处理时，平均达到32.79db的PSNR。
<details>	<summary>英文摘要</summary>	With the emerging of 360-degree image/video, augmented reality (AR) and virtual reality (VR), the demand for analysing and processing spherical signals get tremendous increase. However, plenty of effort paid on planar signals that projected from spherical signals, which leading to some problems, e.g. waste of pixels, distortion. Recent advances in spherical CNN have opened up the possibility of directly analysing spherical signals. However, they pay attention to the full mesh which makes it infeasible to deal with situations in real-world application due to the extremely large bandwidth requirement. To address the bandwidth waste problem associated with 360-degree video streaming and save computation, we exploit Focused Icosahedral Mesh to represent a small area and construct matrices to rotate spherical content to the focused mesh area. We also proposed a novel VertexShuffle operation that can significantly improve both the performance and the efficiency compared to the original MeshConv Transpose operation introduced in UGSCNN. We further apply our proposed methods on super resolution model, which is the first to propose a spherical super-resolution model that directly operates on a mesh representation of spherical pixels of 360-degree data. To evaluate our model, we also collect a set of high-resolution 360-degree videos to generate a spherical image dataset. Our experiments indicate that our proposed spherical super-resolution model achieves significant benefits in terms of both performance and inference time compared to the baseline spherical super-resolution model that uses the simple MeshConv Transpose operation. In summary, our model achieves great super-resolution performance on 360-degree inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices on the mesh. </details>
<details>	<summary>注释</summary>	This paper introduce a new mesh representation and a new upsampling method on a mesh </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 161、基于深度度量学习的生成建模对手流形匹配
- [ ] Adversarial Manifold Matching via Deep Metric Learning for Generative Modeling 
时间：2021年06月20日                         第一作者：Mengyu Dai                        [链接](https://arxiv.org/abs/2106.10777).                     
## 摘要：我们提出了一种生成模型的流形匹配方法，它包括一个分布生成器（或数据生成器）和一个度量生成器。在我们的框架中，我们将真实的数据集看作嵌入高维欧氏空间的流形。分布生成器的目标是生成样本，这些样本遵循围绕真实数据流形压缩的某种分布。它是通过使用两组点的几何形状描述符（例如质心和直径）和学习的距离度量来匹配两组点来实现的；度量生成器利用真实数据和生成的样本来学习距离度量，该距离度量接近真实数据流形上的某个固有测地距离。生成的距离度量进一步用于流形匹配。在训练过程中，两个网络同时学习。我们将该方法应用于无监督学习和有监督学习任务中：在无条件图像生成任务中，与已有的生成模型相比，该方法取得了较好的效果；在超分辨率任务中，我们将该框架融入到基于感知的模型中，通过生成具有更自然纹理的样本来提高视觉质量。理论分析和实际数据实验都证明了该框架的可行性和有效性。
<details>	<summary>英文摘要</summary>	We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and $p$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Both theoretical analysis and real data experiments guarantee the feasibility and effectiveness of the proposed framework. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 160、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年06月19日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
## 摘要：超分辨率（SR）是一个一对多的任务，有多种可能的解决方案。然而，以往的研究并不关注这一特征。对于一对多管道，生成器应该能够生成重建的多个估计，并且不会因为生成相似且同样真实的图像而受到惩罚。为了实现这一点，我们建议在剩余密集块（RRDB）中的每一个残差之后加入加权像素噪声，以使生成器能够生成各种图像。我们修改了严格的内容丢失，只要内容一致，就不会惩罚重建图像中的随机变化。此外，我们观察到，在DIV2K和DIV8K数据集中，有一些没有焦点的区域提供了毫无帮助的指导方针。我们使用[10]的方法过滤训练数据中的模糊区域。最后，我们修改鉴别器以接收低解析度影像作为参考影像与目标影像，以提供更好的回馈给产生器。使用我们提出的方法，我们能够提高ESRGAN在x4知觉SR中的性能，并在x16知觉极限SR中获得最先进的LPIPS分数。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is a one-to-many task with multiple possible solutions. However, previous works were not concerned about this characteristic. For a one-to-many pipeline, the generator should be able to generate multiple estimates of the reconstruction, and not be penalized for generating similar and equally realistic images. To achieve this, we propose adding weighted pixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable the generator to generate various images. We modify the strict content loss to not penalize the stochastic variation in reconstructed images as long as it has consistent content. Additionally, we observe that there are out-of-focus regions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We filter blurry regions in the training data using the method of [10]. Finally, we modify the discriminator to receive the low-resolution image as a reference image along with the target image to provide better feedback to the generator. Using our proposed methods, we were able to improve the performance of ESRGAN in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual extreme SR. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 159、基于主观评价的真实图像增强方法
- [ ] Debiased Subjective Assessment of Real-World Image Enhancement 
时间：2021年06月18日                         第一作者：Cao Peibei. Wang Zhangyang                       [链接](https://arxiv.org/abs/2106.10080).                     
## 摘要：在实际图像增强中，获取地面真实数据往往是一个挑战（如果不是不可能的话），这就妨碍了采用距离度量进行客观质量评估。因此，人们常常求助于主观质量评估，这是评估图像增强最直接和可靠的方法。传统的主观测试需要人工预选一小部分视觉样本，这可能会产生三种偏差：1）由于所选样本在图像空间的分布极为稀疏，导致采样偏差；2） 由于所选样本的潜在过拟合导致的算法偏差；3） 由于进一步潜在的樱桃采摘试验结果而产生的主观偏见。这最终使得现实世界中的图像增强领域更像是一门艺术，而不是一门科学。在这里，我们采取步骤，通过自动采样一组自适应和多样性的图像进行后续测试，来削弱传统的主观评估。这是通过将样本选择转化为增强子之间的差异和所选输入图像之间的多样性的联合最大化来实现的。仔细的视觉检查得到的增强图像提供了一个增强算法的排名。我们展示了我们的主观评估方法使用三个流行的和实际要求的图像增强任务：去杂波，超分辨率和弱光增强。
<details>	<summary>英文摘要</summary>	In real-world image enhancement, it is often challenging (if not impossible) to acquire ground-truth data, preventing the adoption of distance metrics for objective quality assessment. As a result, one often resorts to subjective quality assessment, the most straightforward and reliable means of evaluating image enhancement. Conventional subjective testing requires manually pre-selecting a small set of visual examples, which may suffer from three sources of biases: 1) sampling bias due to the extremely sparse distribution of the selected samples in the image space; 2) algorithmic bias due to potential overfitting the selected samples; 3) subjective bias due to further potential cherry-picking test results. This eventually makes the field of real-world image enhancement more of an art than a science. Here we take steps towards debiasing conventional subjective assessment by automatically sampling a set of adaptive and diverse images for subsequent testing. This is achieved by casting sample selection into a joint maximization of the discrepancy between the enhancers and the diversity among the selected input images. Careful visual inspection on the resulting enhanced images provides a debiased ranking of the enhancement algorithms. We demonstrate our subjective assessment method using three popular and practically demanding image enhancement tasks: dehazing, super-resolution, and low-light enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月21日</details>

# 158、从模糊中恢复形状：快速移动对象的纹理三维形状和运动
- [ ] Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects 
时间：2021年06月16日                         第一作者：Denys Rozumnyi                       [链接](https://arxiv.org/abs/2106.08762).                     
## 摘要：我们提出了一个新的任务，联合重建三维形状，纹理和运动的物体从一个单一的运动模糊图像。虽然以前的方法只在二维图像域解决去模糊问题，但我们提出的在三维域中对所有对象属性的严格建模能够正确描述任意对象的运动。这将导致更好的图像分解和更清晰的去模糊结果。我们将观察到的运动模糊物体的外观建模为背景和具有恒定平移和旋转的三维物体的组合。我们的方法通过使用合适的正则化器进行可微渲染来最小化重建输入图像的损失。这使得能够以高保真度估计模糊对象的纹理三维网格。在快速运动物体去模糊的几个基准上，我们的方法明显优于其他方法。定性结果表明，重建的三维网格生成了高质量的时间超分辨率和新颖的图像。
<details>	<summary>英文摘要</summary>	We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to significantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high fidelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object. </details>
<details>	<summary>注释</summary>	15 pages, 8 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年06月17日</details>

# 157、受感知启发的压缩视频超分辨率
- [ ] Perceptually-inspired super-resolution of compressed videos 
时间：2021年06月15日                         第一作者：Di Ma                       [链接](https://arxiv.org/abs/2106.08147).                     
## 摘要：空间分辨率自适应是视频压缩中常用的一种提高编码效率的技术。这种方法对输入视频的低分辨率版本进行编码，并在解码过程中重建原始分辨率。为了进一步提高重建质量，最近的工作采用了基于卷积神经网络（CNNs）的先进的超分辨率方法来代替传统的上采样滤波器。这些方法通常被训练来最小化基于像素的损失，例如均方误差（MSE），尽管这种类型的损失度量与主观观点没有很好的相关性。本文提出了一种基于感知启发的超分辨率方法（M-SRGAN），该方法利用一种改进的CNN模型对压缩视频进行空间上采样，该模型是在具有感知损失函数的压缩内容上用生成对抗网络（GAN）训练的。该方法与HEVC-hm16.20相结合，并在JVET通用测试条件（UHD测试序列）上使用随机访问配置进行了评估。结果表明，与原来的hm16.20相比，感知质量有了明显的改善，基于感知质量度量VMAF的平均比特率节省了35.6%（Bj{\o}ntegaard Delta度量）。
<details>	<summary>英文摘要</summary>	Spatial resolution adaptation is a technique which has often been employed in video compression to enhance coding efficiency. This approach encodes a lower resolution version of the input video and reconstructs the original resolution during decoding. Instead of using conventional up-sampling filters, recent work has employed advanced super-resolution methods based on convolutional neural networks (CNNs) to further improve reconstruction quality. These approaches are usually trained to minimise pixel-based losses such as Mean-Squared Error (MSE), despite the fact that this type of loss metric does not correlate well with subjective opinions. In this paper, a perceptually-inspired super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of compressed video using a modified CNN model, which has been trained using a generative adversarial network (GAN) on compressed content with perceptual loss functions. The proposed method was integrated with HEVC HM 16.20, and has been evaluated on the JVET Common Test Conditions (UHD test sequences) using the Random Access configuration. The results show evident perceptual quality improvement over the original HM 16.20, with an average bitrate saving of 35.6% (Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF. </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 156、SinIR：单图像重建的高效通用图像处理
- [ ] SinIR: Efficient General Image Manipulation with Single Image Reconstruction 
时间：2021年06月14日                         第一作者：Jihyeong Yoo                        [链接](https://arxiv.org/abs/2106.07140).                     
## 摘要：我们提出了一个基于SinIR的高效重建框架，它训练在单个自然图像上进行一般的图像处理，包括超分辨率、编辑、协调、绘制到图像、照片真实感风格转换和艺术风格转换。我们通过级联多尺度学习在单个图像上训练我们的模型，每个尺度上的每个网络负责图像重建。与GAN目标相比，该重建目标大大降低了训练的复杂度和运行时间。然而，重建目标也加剧了产出质量。因此，为了解决这个问题，我们进一步利用简单的随机像素洗牌，这也提供了控制操作，受去噪自动编码器的启发。通过定量评估，我们发现SinIR在各种图像处理任务上都有很强的竞争力。此外，使用更简单的训练目标（即重建），SinIR的训练速度是SinGAN（500 X 500图像）的33.5倍，后者可以解决类似的任务。我们的代码可在github.com/YooJiHyeong/SinIR上公开获取。
<details>	<summary>英文摘要</summary>	We propose SinIR, an efficient reconstruction-based framework trained on a single natural image for general image manipulation, including super-resolution, editing, harmonization, paint-to-image, photo-realistic style transfer, and artistic style transfer. We train our model on a single image with cascaded multi-scale learning, where each network at each scale is responsible for image reconstruction. This reconstruction objective greatly reduces the complexity and running time of training, compared to the GAN objective. However, the reconstruction objective also exacerbates the output quality. Therefore, to solve this problem, we further utilize simple random pixel shuffling, which also gives control over manipulation, inspired by the Denoising Autoencoder. With quantitative evaluation, we show that SinIR has competitive performance on various image manipulation tasks. Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our code is publicly available at github.com/YooJiHyeong/SinIR. </details>
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 155、基于群的双向递归小波神经网络在视频超分辨率中的应用
- [ ] Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video Super-Resolution 
时间：2021年06月14日                         第一作者：Young-Ju Choi                       [链接](https://arxiv.org/abs/2106.07190).                     
## 摘要：视频超分辨率（VSR）的目标是从低分辨率（LR）帧中估计出高分辨率（HR）帧。VSR的关键挑战在于有效地利用帧内空间相关性和连续帧间的时间相关性。然而，以往的方法大多对不同类型的空间特征进行统一处理，从分离的模块中提取空间和时间特征。这导致缺乏获得有意义的信息和加强细节。在VSR中，有三种时态建模框架：二维卷积神经网络（CNN）、三维CNN和递归神经网络（RNN）。其中，基于RNN的方法适用于序列数据。因此，利用相邻帧的隐藏状态可以大大提高SR性能。然而，在递归结构中的每一个时间步，基于RNN的以往工作都限制性地利用相邻特征。由于每个时间步的可访问运动范围很窄，因此对于动态或大运动，恢复丢失的细节仍然存在限制。本文提出了一种基于群的双向递归小波神经网络（GBR-WNN）来有效地挖掘VSR的序列数据和时空信息。提出了一种基于组的双向RNN（GBR）时序建模框架，该框架建立在具有图片组（GOP）的结构良好的过程之上。我们提出了一个时间小波注意（TWA）模块，其中注意被用于空间和时间特征。实验结果表明，与现有方法相比，该方法在定量和定性评价方面都取得了较好的效果。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame from a low-resolution (LR) frames. The key challenge for VSR lies in the effective exploitation of spatial correlation in an intra-frame and temporal dependency between consecutive frames. However, most of the previous methods treat different types of the spatial features identically and extract spatial and temporal features from the separated modules. It leads to lack of obtaining meaningful information and enhancing the fine details. In VSR, there are three types of temporal modeling frameworks: 2D convolutional neural networks (CNN), 3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach is suitable for sequential data. Thus the SR performance can be greatly improved by using the hidden states of adjacent frames. However, at each of time step in a recurrent structure, the RNN-based previous works utilize the neighboring features restrictively. Since the range of accessible motion per time step is narrow, there are still limitations to restore the missing details for dynamic or large motion. In this paper, we propose a group-based bi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the sequential data and spatio-temporal information effectively for VSR. The proposed group-based bi-directional RNN (GBR) temporal modeling framework is built on the well-structured process with the group of pictures (GOP). We propose a temporal wavelet attention (TWA) module, in which attention is adopted for both spatial and temporal features. Experimental results demonstrate that the proposed method achieves superior performance compared with state-of-the-art methods in both of quantitative and qualitative evaluations. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 154、单幅图像超分辨率的反馈金字塔注意网络
- [ ] Feedback Pyramid Attention Networks for Single Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06966).                     
## 摘要：近年来，基于卷积神经网络（CNN）的图像超分辨率（SR）方法取得了显著的性能改进。然而，大多数基于CNN的方法主要集中在前馈结构设计上，而忽略了人类视觉系统中普遍存在的反馈机制。在本文中，我们提出了反馈金字塔注意网络（FPAN）来充分利用特征之间的相互依赖性。具体地说，本文提出了一种新的反馈连接结构，利用高层信息增强低层特征表达。在我们的方法中，每一层在第一阶段的输出也被用作下一阶段对应层的输入，以重新更新先前的低层滤波器。此外，我们还引入了金字塔非局部结构来模拟不同尺度下的全局上下文信息，提高了网络的区分性。在各种数据集上的大量实验结果证明了我们的FPAN与最先进的SR方法相比的优越性。
<details>	<summary>英文摘要</summary>	Recently, convolutional neural network (CNN) based image super-resolution (SR) methods have achieved significant performance improvement. However, most CNN-based methods mainly focus on feed-forward architecture design and neglect to explore the feedback mechanism, which usually exists in the human visual system. In this paper, we propose feedback pyramid attention networks (FPAN) to fully exploit the mutual dependencies of features. Specifically, a novel feedback connection structure is developed to enhance low-level feature expression with high-level information. In our method, the output of each layer in the first stage is also used as the input of the corresponding layer in the next state to re-update the previous low-level filters. Moreover, we introduce a pyramid non-local structure to model global contextual information in different scales and improve the discriminative representation of the network. Extensive experimental results on various datasets demonstrate the superiority of our FPAN in comparison with the state-of-the-art SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 153、用于超分辨率轻量化图像的金字塔密集注意网络
- [ ] Pyramidal Dense Attention Networks for Lightweight Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06996).                     
## 摘要：近年来，深度卷积神经网络方法在图像超分辨率（SR）方面取得了很好的效果，但由于存储开销大，不易应用于嵌入式设备。为了解决这一问题，本文提出了一种用于轻量化图像超分辨率的金字塔密集注意网络（PDAN）。在我们的方法中，所提出的金字塔密集学习可以逐渐增加金字塔密集块内部密集连接层的宽度，从而有效地提取深层特征。同时，引入了组数随卷积层数线性增长的自适应组卷积，以减少参数爆炸。此外，我们还提出了一种新的联合注意方法，以有效地捕捉空间维度和通道维度之间的跨维度交互，从而提供丰富的区分性特征表示。大量的实验结果表明，与目前最先进的轻量级SR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural network methods have achieved an excellent performance in image superresolution (SR), but they can not be easily applied to embedded devices due to large memory cost. To solve this problem, we propose a pyramidal dense attention network (PDAN) for lightweight image super-resolution in this paper. In our method, the proposed pyramidal dense learning can gradually increase the width of the densely connected layer inside a pyramidal dense block to extract deep features efficiently. Meanwhile, the adaptive group convolution that the number of groups grows linearly with dense convolutional layers is introduced to relieve the parameter explosion. Besides, we also present a novel joint attention to capture cross-dimension interaction between the spatial dimensions and channel dimension in an efficient way for providing rich discriminative feature representations. Extensive experimental results show that our method achieves superior performance in comparison with the state-of-the-art lightweight SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 152、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年06月13日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
<details>	<summary>注释</summary>	10 pages, 3 figures Journal-ref: International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2021) </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 151、用于关节MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年06月12日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
## 摘要：磁共振成像（MRI）的核心问题是加速度和图像质量之间的权衡。图像重建和超分辨率是磁共振成像（MRI）的两项关键技术。当前的方法被设计为分别执行这些任务，而忽略了它们之间的相关性。在这项工作中，我们提出了一个端到端的任务变压器网络（T$^2$Net）用于关节MRI重建和超分辨率，它允许在多个任务之间共享表示和特征传输，从而从高度欠采样和退化的MRI数据中获得更高质量、超分辨率和无运动伪影的图像。我们的框架结合了重建和超分辨率，分为两个子分支，其特征表示为查询和键。具体来说，我们鼓励两个任务之间的联合特征学习，从而传递准确的任务信息。我们首先使用两个独立的CNN分支来提取特定于任务的特征。然后，设计了一个任务转换器模块来嵌入和合成两个任务之间的相关性。实验结果表明，我们的多任务模型在定量和定性上都明显优于先进的序贯方法。
<details>	<summary>英文摘要</summary>	The core problem of Magnetic Resonance Imaging (MRI) is the trade off between acceleration and image quality. Image reconstruction and super-resolution are two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are designed to perform these tasks separately, ignoring the correlations between them. In this work, we propose an end-to-end task transformer network (T$^2$Net) for joint MRI reconstruction and super-resolution, which allows representations and feature transmission to be shared between multiple task to achieve higher-quality, super-resolved and motion-artifacts-free images from highly undersampled and degenerated MRI data. Our framework combines both reconstruction and super-resolution, divided into two sub-branches, whose features are expressed as queries and keys. Specifically, we encourage joint feature learning between the two tasks, thereby transferring accurate task information. We first use two separate CNN branches to extract task-specific features. Then, a task transformer module is designed to embed and synthesize the relevance between the two tasks. Experimental results show that our multi-task model significantly outperforms advanced sequential methods, both quantitatively and qualitatively. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 150、视频超分辨率转换器
- [ ] Video Super-Resolution Transformer 
时间：2021年06月12日                         第一作者：Jiezhang Cao                       [链接](https://arxiv.org/abs/2106.06847).                     
## 摘要：视频超分辨率（VSR）是一个时空序列预测问题，其目的是从相应的低分辨率视频中恢复出高分辨率的视频。近年来，Transformer以其对序列间建模的并行计算能力得到了广泛的应用。因此，应用视觉变换器来求解VSR似乎是很简单的。然而，由于以下两个原因，具有全连接自关注层和令牌前馈层的变压器的典型块设计不适合VSR。首先，完全连通的自注意层由于依赖于线性层来计算注意图而忽略了对数据局部性的利用。其次，令牌前馈层缺乏特征对齐，这对于VSR很重要，因为该层独立地处理每个输入令牌嵌入，它们之间没有任何交互。本文对VSR用变压器进行了首次尝试。具体来说，为了解决第一个问题，我们提出了一个时空卷积自我注意层的理论理解，以利用局部信息。对于第二个问题，我们设计了一个基于双向光流的前馈层来发现不同视频帧之间的相关性并对齐特征。在多个基准数据集上的大量实验证明了该方法的有效性。代码将在https://github.com/caojiezhang/VSR-Transformer.
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 149、用于GAN自动设计的自适应超分辨结构框架
- [ ] A self-adapting super-resolution structures framework for automatic design of GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.06011).                     
## 摘要：随着深度学习的发展，单一的超分辨率图像重建网络模型变得越来越复杂。模型超参数的微小变化对模型性能的影响较大。在现有的工作中，专家们已经逐渐探索出一套基于经验值或进行暴力搜索的最优模型参数。本文介绍了一种新的超分辨率图像重建生成对抗性网络框架，并用贝叶斯优化方法对发生器和鉴别器的超参数进行了优化。发生器采用自校正卷积法，鉴别器采用卷积法。定义了网络层数、神经元数等超参数。该方法采用贝叶斯优化作为GAN的优化策略。不仅可以自动找到最优超参数解，而且可以构造超分辨率图像重建网络，减少人工工作量。实验表明，贝叶斯优化算法比其他两种优化算法能更早地搜索到最优解。
<details>	<summary>英文摘要</summary>	With the development of deep learning, the single super-resolution image reconstruction network models are becoming more and more complex. Small changes in hyperparameters of the models have a greater impact on model performance. In the existing works, experts have gradually explored a set of optimal model parameters based on empirical values or performing brute-force search. In this paper, we introduce a new super-resolution image reconstruction generative adversarial network framework, and a Bayesian optimization method used to optimizing the hyperparameters of the generator and discriminator. The generator is made by self-calibrated convolution, and discriminator is made by convolution lays. We have defined the hyperparameters such as the number of network layers and the number of neurons. Our method adopts Bayesian optimization as a optimization policy of GAN in our model. Not only can find the optimal hyperparameter solution automatically, but also can construct a super-resolution image reconstruction network, reducing the manual workload. Experiments show that Bayesian optimization can search the optimal solution earlier than the other two optimization algorithms. </details>
<details>	<summary>注释</summary>	9 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月14日</details>

# 148、基于自标定卷积GAN的超分辨率图像重建
- [ ] Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.05545).                     
## 摘要：随着深度学习在计算机视觉中的有效应用，超分辨率图像重建的研究取得了突破性进展。然而，许多研究指出，神经网络对图像特征提取的不足可能会导致新重建图像的恶化。另一方面，由于过度平滑，生成的图片有时过于人工。为了解决上述问题，我们提出了一种新的自校正卷积生成对抗网络。该发生器由特征提取和图像重建两部分组成。特征提取使用自校正卷积，卷积包含四个部分，每个部分都有特定的功能。它不仅可以扩大感受野的范围，而且可以获得长距离的空间依赖性和通道间依赖性。然后进行图像重建，最后重建超分辨率图像。在SSIM评估方法下，我们对set5、set14和BSD100等不同的数据集进行了深入的实验。实验结果证明了该网络的有效性。
<details>	<summary>英文摘要</summary>	With the effective application of deep learning in computer vision, breakthroughs have been made in the research of super-resolution images reconstruction. However, many researches have pointed out that the insufficiency of the neural network extraction on image features may bring the deteriorating of newly reconstructed image. On the other hand, the generated pictures are sometimes too artificial because of over-smoothing. In order to solve the above problems, we propose a novel self-calibrated convolutional generative adversarial networks. The generator consists of feature extraction and image reconstruction. Feature extraction uses self-calibrated convolutions, which contains four portions, and each portion has specific functions. It can not only expand the range of receptive fields, but also obtain long-range spatial and inter-channel dependencies. Then image reconstruction is performed, and finally a super-resolution image is reconstructed. We have conducted thorough experiments on different datasets including set5, set14 and BSD100 under the SSIM evaluation method. The experimental results prove the effectiveness of the proposed network. </details>
<details>	<summary>注释</summary>	8 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月11日</details>

# 147、基于参考图像超分辨率的变分自动编码器
- [ ] Variational AutoEncoder for Reference based Image Super-Resolution 
时间：2021年06月08日                         第一作者：Zhi-Song Liu                        [链接](https://arxiv.org/abs/2106.04090).                     
## 摘要：本文提出了一种新的基于参考的变分自编码器（RefVAE）图像超分辨率方法。现有的超分辨率方法主要集中在单幅图像的超分辨率上，对于较大的上采样因子（如8$\times$）效果不佳。提出了一种基于参考的图像超分辨率方法，任意一幅图像都可以作为参考进行超分辨率处理。即使使用随机映射或低分辨率图像本身，该方法也能将参考图像中的知识转化为超分辨率图像。根据不同的参考文献，该方法可以从隐藏的超分辨率空间生成不同版本的超分辨率图像。除了使用不同的数据集对PSNR和SSIM进行一些标准评估外，我们还参加了NTIR2021 SR空间挑战赛，并提供了我们方法的随机性评估结果。与其他最先进的方法相比，我们的方法获得了更高的多样性分数。
<details>	<summary>英文摘要</summary>	In this paper, we propose a novel reference based image super-resolution approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8$\times$. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space. Besides using different datasets for some standard evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space challenge and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores. </details>
<details>	<summary>注释</summary>	10 pages, 6 figures Journal-ref: 2021 IEEE Conference on Computer Vision and Pattern Recognition Workshop </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 146、NTIRE 2021突发超分辨率挑战：方法和结果
- [ ] NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results 
时间：2021年06月07日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2106.03839).                     
## 摘要：本文回顾了NTIRE2021对突发超分辨率的挑战。给定一个原始噪声脉冲作为输入，挑战中的任务是生成一个分辨率高出4倍的干净RGB图像。挑战包含两个轨道；轨道1评估综合生成的数据，轨道2使用移动摄像机的真实世界爆发。在最后的测试阶段，6个团队使用不同的解决方案提交了结果。性能最好的方法为突发超分辨率任务设置了一个新的最新技术。
<details>	<summary>英文摘要</summary>	This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a RAW noisy burst as input, the task in the challenge was to generate a clean RGB image with 4 times higher resolution. The challenge contained two tracks; Track 1 evaluating on synthetically generated data, and Track 2 using real-world bursts from mobile camera. In the final testing phase, 6 teams submitted results using a diverse set of solutions. The top-performing methods set a new state-of-the-art for the burst super-resolution task. </details>
<details>	<summary>注释</summary>	NTIRE 2021 Burst Super-Resolution challenge report </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 145、基于深度神经网络的图像和视频流增强技术：综述和未来发展方向
- [ ] Deep Neural Network-based Enhancement for Image and Video Streaming Systems: A Survey and Future Directions 
时间：2021年06月07日                         第一作者：Royson Lee                       [链接](https://arxiv.org/abs/2106.03727).                     
## 摘要：支持互联网的智能手机和超宽显示器正在将各种视频应用程序从点播电影和360度视频转变为视频会议和流媒体直播。然而，在不稳定的网络条件下，在具有不同功能的设备上可靠地交付可视内容仍然是一个开放的问题。近年来，在诸如超分辨率和图像增强等任务的深度学习领域取得的进展使得从低质量图像生成高质量图像的性能达到了前所未有的水平，我们称之为神经增强。在本文中，我们调查了最先进的内容交付系统，这些系统将神经增强作为实现快速响应时间和高视觉质量的关键组件。我们首先介绍现有内容交付系统的组件和体系结构，强调它们面临的挑战，并鼓励使用神经增强模型作为对策。然后，我们将讨论这些模型的部署挑战，并分析现有系统及其设计决策，以有效地克服这些技术挑战。此外，我们强调了针对不同用例的跨系统的关键趋势和通用方法。最后，基于深度学习研究的最新见解，我们提出了未来有希望的方向，以进一步提高内容交付系统的体验质量。
<details>	<summary>英文摘要</summary>	Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360{\deg} videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the components and architecture of existing content delivery systems, highlighting their challenges and motivating the use of neural enhancement models as a countermeasure. We then cover the deployment challenges of these models and analyze existing systems and their design decisions in efficiently overcoming these technical challenges. Additionally, we underline the key trends and common approaches across systems that target diverse use-cases. Finally, we present promising future directions based on the latest insights from deep learning research to further boost the quality of experience of content delivery systems. </details>
<details>	<summary>注释</summary>	Accepted for publication at the ACM Computing Surveys (CSUR) journal, 2021. arXiv admin note: text overlap with arXiv:2010.05838 </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 144、深度学习使体积荧光显微镜无参考各向同性超分辨率成为可能
- [ ] Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy 
时间：2021年06月07日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 143、学习超分辨空间的噪声条件流模型
- [ ] Noise Conditional Flow Model for Learning the Super-Resolution Space 
时间：2021年06月06日                         第一作者：Younggeun Kim                       [链接](https://arxiv.org/abs/2106.04428).                     
## 摘要：从根本上说，超分辨率是一个病态的问题，因为低分辨率图像可以从许多高分辨率图像。最近对超分辨率的研究不能产生多样化的超分辨率图像。尽管SRFlow试图通过预测给定低分辨率图像的多幅高分辨率图像来解释超分辨率的不适定性，但仍有提高多样性和视觉质量的空间。本文提出了噪声条件流超分辨率模型NCSR，通过噪声条件层提高图像的视觉质量和多样性。为了了解更多不同的数据分布，我们在训练数据中加入噪声。然而，低质量的图像是由于添加噪声造成的。我们提出了噪声条件层来克服这一现象。噪声条件层使得我们的模型生成的图像更加多样化，视觉质量也更高。此外，我们还证明了该层可以克服数据分布不匹配的问题，这是在规范化流模型时出现的一个问题。基于这些优点，NCSR在多样性和视觉质量方面优于基线，并且比基于GAN的传统模型获得更好的视觉质量。在NTIRE 2021挑战赛中，我们也获得了优异的成绩。
<details>	<summary>英文摘要</summary>	Fundamentally, super-resolution is ill-posed problem because a low-resolution image can be obtained from many high-resolution images. Recent studies for super-resolution cannot create diverse super-resolution images. Although SRFlow tried to account for ill-posed nature of the super-resolution by predicting multiple high-resolution images given a low-resolution image, there is room to improve the diversity and visual quality. In this paper, we propose Noise Conditional flow model for Super-Resolution, NCSR, which increases the visual quality and diversity of images through noise conditional layer. To learn more diverse data distribution, we add noise to training data. However, low-quality images are resulted from adding noise. We propose the noise conditional layer to overcome this phenomenon. The noise conditional layer makes our model generate more diverse images with higher visual quality than other works. Furthermore, we show that this layer can overcome data distribution mismatch, a problem that arises in normalizing flow models. With these benefits, NCSR outperforms baseline in diversity and visual quality and achieves better visual quality than traditional GAN-based models. We also get outperformed scores at NTIRE 2021 challenge. </details>
<details>	<summary>注释</summary>	Final CVPR2021 workshop version </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 142、基于参考的超分辨率图像匹配加速与空间自适应
- [ ] MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution 
时间：2021年06月04日                         第一作者：Liying Lu                       [链接](https://arxiv.org/abs/2106.02299).                     
## 摘要：基于参考的图像超分辨率（RefSR）在利用外部参考图像（Ref）恢复高频细节方面取得了成功。在这个任务中，纹理细节从参考图像传输到低分辨率（LR）图像根据其点或面片的对应关系。因此，高质量的匹配是至关重要的。它还要求计算效率高。此外，现有的RefSR方法往往忽略了LR图像和Ref图像在分布上可能存在的较大差异，影响了信息利用的有效性。在本文中，我们提出了RefSR的MASA网络，其中设计了两个新的模块来解决这些问题。该匹配与提取模块通过粗到细的匹配方案显著降低了计算量。空间自适应模块学习LR和Ref图像之间的分布差异，并以空间自适应的方式将Ref特征的分布重新映射到LR特征的分布。该方案使得网络对不同的参考图像具有很强的鲁棒性。大量的定量和定性实验验证了该模型的有效性。
<details>	<summary>英文摘要</summary>	Reference-based image super-resolution (RefSR) has shown promising success in recovering high-frequency details by utilizing an external reference image (Ref). In this task, texture details are transferred from the Ref image to the low-resolution (LR) image according to their point- or patch-wise correspondence. Therefore, high-quality correspondence matching is critical. It is also desired to be computationally efficient. Besides, existing RefSR methods tend to ignore the potential large disparity in distributions between the LR and Ref images, which hurts the effectiveness of the information utilization. In this paper, we propose the MASA network for RefSR, where two novel modules are designed to address these problems. The proposed Match & Extraction Module significantly reduces the computational cost by a coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module learns the difference of distribution between the LR and Ref images, and remaps the distribution of Ref features to that of LR features in a spatially adaptive way. This scheme makes the network robust to handle different reference images. Extensive quantitative and qualitative experiments validate the effectiveness of our proposed model. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 141、SOUP-GAN：基于生成对抗网络的超分辨率MRI
- [ ] SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks 
时间：2021年06月04日                         第一作者：Kuan Zhang                       [链接](https://arxiv.org/abs/2106.02599).                     
## 摘要：在临床和科研应用中，对高分辨率医学图像的需求越来越大。为了获得更好的患者舒适度、更低的检查成本、更低的剂量和更少的运动诱发伪影，图像质量不可避免地要与采集时间进行权衡。对于许多基于图像的任务，通常使用增加垂直平面上的视分辨率来生成多平面重新格式化或三维图像。单图像超分辨率（SR）是一种很有前途的基于无监督学习的HR图像提供技术，可以提高2D图像的分辨率，但是关于3D SR的报道很少。此外，文献中提出的感知损失比使用像素损失函数更好地捕捉文本细节和边缘，通过比较预先训练的二维网络（如VGG）在高维特征空间中的语义距离。然而，目前尚不清楚如何将其推广到三维医学图像中，随之而来的影响仍不清楚。在本文中，我们提出了一个称为SOUP-GAN的框架：使用感知调谐生成对抗网络（GAN）优化的超分辨率，以产生具有抗混叠和去模糊功能的更薄切片（例如，Z平面上的高分辨率）医学图像。通过定性和定量的比较，该方法优于其它常规的分辨率增强方法和以往的SR方法。具体地说，我们检验了该模型在各种SR比值和成像模式下的泛化。通过解决这些限制，我们的模型显示了作为一种新的三维SR插值技术的前景，在临床和研究环境中提供了潜在的应用。
<details>	<summary>英文摘要</summary>	There is a growing demand for high-resolution (HR) medical images in both the clinical and research applications. Image quality is inevitably traded off with the acquisition time for better patient comfort, lower examination costs, dose, and fewer motion-induced artifacts. For many image-based tasks, increasing the apparent resolution in the perpendicular plane to produce multi-planar reformats or 3D images is commonly used. Single image super-resolution (SR) is a promising technique to provide HR images based on unsupervised learning to increase resolution of a 2D image, but there are few reports on 3D SR. Further, perceptual loss is proposed in the literature to better capture the textual details and edges than using pixel-wise loss functions, by comparing the semantic distances in the high-dimensional feature space of a pre-trained 2D network (e.g., VGG). However, it is not clear how one should generalize it to 3D medical images, and the attendant implications are still unclear. In this paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using Perceptual-tuned Generative Adversarial Network (GAN), in order to produce thinner slice (e.g., high resolution in the 'Z' plane) medical images with anti-aliasing and deblurring. The proposed method outperforms other conventional resolution-enhancement methods and previous SR work on medical images upon both qualitative and quantitative comparisons. Specifically, we examine the model in terms of its generalization for various SR ratios and imaging modalities. By addressing those limitations, our model shows promise as a novel 3D SR interpolation technique, providing potential applications in both clinical and research settings. </details>
<details>	<summary>注释</summary>	10 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 140、基于C2匹配的鲁棒参考超分辨方法
- [ ] Robust Reference-based Super-Resolution via C2-Matching 
时间：2021年06月03日                         第一作者：Yuming Jiang                       [链接](https://arxiv.org/abs/2106.01863).                     
## 摘要：基于参考的超分辨率（Ref-SR）通过引入额外的高分辨率（HR）参考图像来增强低分辨率（LR）输入图像是一种很有前途的方法。现有的Ref-SR方法大多依靠隐式对应匹配从参考图像中借用HR纹理来补偿输入图像中的信息丢失。然而，由于输入图像和参考图像之间存在两个间隙：变换间隙（例如缩放和旋转）和分辨率间隙（例如HR和LR），因此执行局部传输是困难的。为了应对这些挑战，我们在这项工作中提出了C2匹配，它产生了显式的鲁棒匹配交叉变换和分辨率。1） 对于变换间隙，我们提出了一种对比对应网络，该网络利用输入图像的增广视图学习变换鲁棒对应。2） 对于分辨率差距，我们采用了师生相关提取的方法，从比较容易的HR-HR匹配中提取知识，指导比较模糊的LR-HR匹配。3） 最后，我们设计了一个动态聚合模块来解决潜在的失调问题。此外，为了真实地评估Ref-SR在现实环境下的性能，我们模拟实际使用场景，提供了web引用SR（WR-SR）数据集。大量的实验表明，我们提出的C2匹配明显优于国家的艺术超过1dB的标准CUFED5基准。值得注意的是，它在WR-SR数据集上具有很强的通用性，并且在大规模变换和旋转变换中具有很强的鲁棒性。
<details>	<summary>英文摘要</summary>	Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g. scale and rotation) and the resolution gap (e.g. HR and LR). To tackle these challenges, we propose C2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C2-Matching significantly outperforms state of the arts by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation transformations. </details>
<details>	<summary>注释</summary>	To appear in CVPR2021. The source code is available at https://github.com/yumingj/C2-Matching </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 139、互增强立体图像超分辨率和视差估计的反馈网络
- [ ] Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation 
时间：2021年06月02日                         第一作者：Qinyan Dai                       [链接](https://arxiv.org/abs/2106.00985).                     
## 摘要：在立体背景下，图像超分辨率（SR）和视差估计问题是相互关联的，每个问题的结果可以帮助解决另一个问题。有效地利用不同视图之间的对应关系有利于SR性能的提高，而细节更丰富的高分辨率特征有利于对应关系的估计。基于这一动机，我们提出了一种立体图像超分辨率和视差估计反馈网络（ssrdefnet），它在统一的框架下同时处理立体图像的超分辨率和视差估计，并将它们相互作用以进一步提高它们的性能。具体而言，SSRDE-FNet由两个用于左视图和右视图的双重递归子网络组成。除了在低分辨率（LR）空间中利用交叉视图信息外，还利用SR过程产生的HR表示来进行更高精度的HR视差估计，通过该方法可以聚集HR特征以产生更精细的SR结果。然后，提出的HR视差信息反馈（HRDIF）机制将HR视差所携带的信息反馈给前一层，进一步细化SR图像重建。大量实验证明了SSRDE-FNet的有效性和先进性。
<details>	<summary>英文摘要</summary>	Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 138、有效感知图像超分辨率的傅里叶空间损失
- [ ] Fourier Space Losses for Efficient Perceptual Image Super-Resolution 
时间：2021年06月01日                         第一作者：Dario Fuoli                       [链接](https://arxiv.org/abs/2106.00783).                     
## 摘要：许多超分辨率（SR）模型只针对高性能进行优化，因此由于模型复杂度大而缺乏效率。由于大模型在实际应用中往往不实用，我们研究并提出了新的损失函数，以使SR从更有效的模型中获得更高的感知质量。一个给定的低复杂度发电网络的代表功率只有通过对最优参数集的有力指导才能得到充分利用。我们证明了仅应用我们提出的损失函数就可以提高最近引入的高效发电机结构的性能。特别地，我们使用傅立叶空间监督损失来改进从地面真实图像中恢复丢失的高频（HF）内容，并设计了一种直接在傅立叶域工作的鉴别器结构，以更好地匹配目标HF分布。我们发现，我们的损失直接强调在傅立叶空间的频率显着提高了感知图像质量，同时保持了较高的恢复质量相比，以前提出的损失函数的任务。由于两种表示在训练期间提供互补信息，因此通过利用空间域和频域损失的组合来进一步改进性能。除此之外，经过训练的生成器与最新的感知SR方法RankSRGAN和SRFlow相比，分别快2.4倍和48倍。
<details>	<summary>英文摘要</summary>	Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4x and 48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 137、CTSpine1K：一个用于ct脊柱分割的大规模数据集
- [ ] CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography 
时间：2021年05月31日                         第一作者：Yang Deng                       [链接](https://arxiv.org/abs/2105.14711).                     
## 摘要：脊柱相关疾病发病率高，造成巨大的社会成本负担。脊柱成像是无创可视化和评估脊柱病理学的重要工具。CT图像中椎体的分割是脊柱疾病临床诊断和手术计划的定量医学图像分析的基础。目前公开获得的脊柱注释数据集规模较小。由于缺乏大规模的带注释脊柱图像数据集，目前主流的基于深度学习的数据驱动分割方法受到很大的限制。在本文中，我们介绍了一个大型脊柱CT数据集CTSpine1K，该数据集来自多个来源，用于椎体分割，包含1005个CT体积，超过11100个标记的椎体属于不同的脊柱状况。基于此数据集，我们进行了多个脊椎分割实验来建立第一个基准。我们相信，这个大规模的数据集将有助于进一步研究许多与脊柱相关的图像分析任务，包括但不限于椎骨分割、标记、双平面x线片的三维脊柱重建、图像超分辨率和增强。
<details>	<summary>英文摘要</summary>	Spine-related diseases have high morbidity and cause a huge burden of social cost. Spine imaging is an essential tool for noninvasively visualizing and assessing spinal pathology. Segmenting vertebrae in computed tomography (CT) images is the basis of quantitative medical image analysis for clinical diagnosis and surgery planning of spine diseases. Current publicly available annotated datasets on spinal vertebrae are small in size. Due to the lack of a large-scale annotated spine image dataset, the mainstream deep learning-based segmentation methods, which are data-driven, are heavily restricted. In this paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated from multiple sources for vertebra segmentation, which contains 1,005 CT volumes with over 11,100 labeled vertebrae belonging to different spinal conditions. Based on this dataset, we conduct several spinal vertebrae segmentation experiments to set the first benchmark. We believe that this large-scale dataset will facilitate further research in many spine-related image analysis tasks, including but not limited to vertebrae segmentation, labeling, 3D spine reconstruction from biplanar radiographs, image super-resolution, and enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 136、SNIPS：随机求解含噪反问题
- [ ] SNIPS: Solving Noisy Inverse Problems Stochastically 
时间：2021年05月31日                         第一作者：Bahjat Kawar                       [链接](https://arxiv.org/abs/2105.14951).                     
## 摘要：在这项工作中，我们介绍了一种新的随机算法称为SNIPS，它从任何线性逆问题的后验分布中提取样本，假设观测值被加性高斯白噪声污染。我们的解决方案结合了朗之万动力学和牛顿法的思想，并利用预先训练的最小均方误差（MMSE）高斯去噪器。所提出的方法依赖于后验分数函数的复杂推导，该后验分数函数包括退化算子的奇异值分解（SVD），以便获得所需采样的易于处理的迭代算法。由于其随机性，该算法可以为同一噪声观测产生多个高感知质量的样本。我们证明了所提出的模式的能力，图像去模糊，超分辨率，压缩传感。我们发现，所产生的样本是尖锐的，详细的，并与给定的测量一致，其多样性暴露了所解决的反问题固有的不确定性。
<details>	<summary>英文摘要</summary>	In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 135、光谱之外：通过再合成检测假货
- [ ] Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis 
时间：2021年05月29日                         第一作者：Yang He                        [链接](https://arxiv.org/abs/2105.14376).                     
## 摘要：在过去的几年里，深度生成模型的快速发展已经导致了高度真实的媒体，称为深度赝品，通常从真实的眼睛到人类的眼睛是无法区分的。这些进步使得评估视觉数据的真实性变得越来越困难，并对视觉内容的可信性构成了错误信息的威胁。尽管最近的工作已经显示出这种深度假货的强检测精度，但成功在很大程度上依赖于识别生成图像中的频率伪影，这将不会产生一种可持续的检测方法，因为生成模型继续发展并缩小与真实图像的差距。为了克服这一问题，我们提出了一种新的伪检测方法，该方法通过对检测图像的重新合成和提取视觉线索进行检测。再合成过程是灵活的，允许我们纳入一系列的视觉任务-我们采用超分辨率，去噪和彩色作为再合成。我们在CelebA HQ、FFHQ和LSUN数据集上的多个生成器的各种检测场景中展示了改进的有效性、交叉通用性和抗干扰的鲁棒性。源代码位于https://github.com/SSAW14/BeyondtheSpectrum.
<details>	<summary>英文摘要</summary>	The rapid advances in deep generative models over the past years have led to highly {realistic media, known as deepfakes,} that are commonly indistinguishable from real to human eyes. These advances make assessing the authenticity of visual data increasingly difficult and pose a misinformation threat to the trustworthiness of visual content in general. Although recent work has shown strong detection accuracy of such deepfakes, the success largely relies on identifying frequency artifacts in the generated images, which will not yield a sustainable detection approach as generative models continue evolving and closing the gap to real images. In order to overcome this issue, we propose a novel fake detection that is designed to re-synthesize testing images and extract visual cues for detection. The re-synthesis procedure is flexible, allowing us to incorporate a series of visual tasks - we adopt super-resolution, denoising and colorization as the re-synthesis. We demonstrate the improved effectiveness, cross-GAN generalization, and robustness against perturbations of our approach in a variety of detection scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN datasets. Source code is available at https://github.com/SSAW14/BeyondtheSpectrum. </details>
<details>	<summary>注释</summary>	To appear in IJCAI2021. Source code at https://github.com/SSAW14/BeyondtheSpectrum </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 134、动态时空学习满足静态图像理解的盲运动去模糊超分辨率方法
- [ ] Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding 
时间：2021年05月27日                         第一作者：Wenjia Niu                       [链接](https://arxiv.org/abs/2105.13077).                     
## 摘要：单帧超分辨率（SR）和多帧超分辨率（SR）是超分辨率低分辨率图像的两种方法。单个图像SR通常独立地处理每个图像，但忽略连续帧中隐含的时间信息。多帧SR能够通过捕获运动信息来建模时间相关性。然而，它依赖于在现实世界中并不总是可用的相邻帧。同时，轻微的相机抖动容易导致长距离拍摄低分辨率图像的运动模糊。针对这些问题，提出了一种盲运动去模糊超分辨率网络BMDSRNet，用于从单个静态运动模糊图像中学习动态时空信息。运动模糊图像是相机曝光过程中随时间的积累，而BMDSRNet学习反向过程，并基于精心设计的重建损失函数，使用三个流来学习双向时空信息，以恢复干净的高分辨率图像。大量实验表明，该算法的性能优于现有的图像去模糊和随机共振处理方法。
<details>	<summary>英文摘要</summary>	Single-image super-resolution (SR) and multi-frame SR are two ways to super resolve low-resolution images. Single-Image SR generally handles each image independently, but ignores the temporal information implied in continuing frames. Multi-frame SR is able to model the temporal dependency via capturing motion information. However, it relies on neighbouring frames which are not always available in the real world. Meanwhile, slight camera shake easily causes heavy motion blur on long-distance-shot low-resolution images. To address these problems, a Blind Motion Deblurring Super-Reslution Networks, BMDSRNet, is proposed to learn dynamic spatio-temporal information from single static motion-blurred images. Motion-blurred images are the accumulation over time during the exposure of cameras, while the proposed BMDSRNet learns the reverse process and uses three-streams to learn Bidirectional spatio-temporal information based on well designed reconstruction loss functions to recover clean high-resolution images. Extensive experiments demonstrate that the proposed BMDSRNet outperforms recent state-of-the-art methods, and has the ability to simultaneously deal with image deblurring and SR. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 133、CogView：通过变压器控制文本到图像的生成
- [ ] CogView: Mastering Text-to-Image Generation via Transformers 
时间：2021年05月26日                         第一作者：Ming Ding                       [链接](https://arxiv.org/abs/2105.13290).                     
## 摘要：文本到图像的生成在一般领域一直是一个开放的问题，它需要生成模型和跨模态理解。我们提出了CogView，一个带有VQ-VAE标记器的40亿参数变压器来解决这个问题。我们还展示了各种下游任务的微调策略，例如风格学习、超分辨率、文本图像排序和时装设计，以及稳定预训练的方法，例如消除NaN损失。CogView（zero-shot）在COCO上实现了一种新的最先进的FID，优于以前基于GAN的模型和最近的类似工作DALL-E。
<details>	<summary>英文摘要</summary>	Text-to-Image generation in the general domain has long been an open problem, which requires both generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 132、低分辨率信息也很重要：学习多分辨率表示以重新识别人
- [ ] Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification 
时间：2021年05月26日                         第一作者：Guoqing Zhang                       [链接](https://arxiv.org/abs/2105.12684).                     
## 摘要：人员再识别（re-ID）是视频监控和取证领域的一项重要任务，其目的是对非重叠摄像机拍摄到的人员图像进行匹配。在不受约束的场景中，人物图像通常会遇到分辨率不匹配的问题，即\emph{Cross resolution person Re ID}。为了克服这一问题，现有的大多数方法都是通过超分辨率（SR）将低分辨率（LR）图像恢复到高分辨率（HR）。然而，它们只关注于HR特征的提取，而忽略了原始LR图像的有效信息。在这项工作中，我们探讨了分辨率对特征提取的影响，并提出了一种新的交叉分辨率人物识别方法，称为多分辨率表示法。该方法由分辨率重建网络（RRN）和双特征融合网络（DFFN）组成。RRN使用一个输入图像构造一个HR版本和一个LR版本，其中包含一个编码器和两个解码器，而DFFN采用双分支结构从多分辨率图像生成人物表示。在五个基准上的综合实验验证了所提出的MRJL方法相对于现有方法的优越性。
<details>	<summary>英文摘要</summary>	As a prevailing task in video surveillance and forensics field, person re-identification (re-ID) aims to match person images captured from non-overlapped cameras. In unconstrained scenarios, person images often suffer from the resolution mismatch problem, i.e., \emph{Cross-Resolution Person Re-ID}. To overcome this problem, most existing methods restore low resolution (LR) images to high resolution (HR) by super-resolution (SR). However, they only focus on the HR feature extraction and ignore the valid information from original LR images. In this work, we explore the influence of resolutions on feature extraction and develop a novel method for cross-resolution person re-ID called \emph{\textbf{M}ulti-Resolution \textbf{R}epresentations \textbf{J}oint \textbf{L}earning} (\textbf{MRJL}). Our method consists of a Resolution Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN uses an input image to construct a HR version and a LR version with an encoder and two decoders, while the DFFN adopts a dual-branch structure to generate person representations from multi-resolution images. Comprehensive experiments on five benchmarks verify the superiority of the proposed MRJL over the relevent state-of-the-art methods. </details>
<details>	<summary>注释</summary>	accepted by IJCAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 131、多时相图像超分辨率的排列不变性和不确定性
- [ ] Permutation invariance and uncertainty in multitemporal image super-resolution 
时间：2021年05月26日                         第一作者：Diego Valsesia                       [链接](https://arxiv.org/abs/2105.12409).                     
## 摘要：最近的进展表明，从低分辨率图像的多时相采集开始，深层神经网络在超分辨率遥感图像方面是多么的有效。然而，现有的模型忽略了时间排列的问题，即输入图像的时间顺序不携带任何与超分辨率任务相关的信息，并且导致这种模型对于可用于训练的、通常稀缺的地面真实数据是低效的。因此，模型不应该学习依赖于时间顺序的特征提取器。在本文中，我们展示了如何建立一个对时间排列完全不变的模型来显著地提高性能和数据效率。此外，我们还研究了如何量化超分辨率图像的不确定性，以便最终用户了解产品的局部质量。我们展示了不确定性如何与序列中的时间变化相关，以及量化它如何进一步提高模型性能。在Proba-V挑战数据集上的实验表明，在不需要自我感知的情况下，比现有技术有了显著的改进，并且提高了数据效率，仅用25%的训练数据就达到了挑战胜利者的表现。
<details>	<summary>英文摘要</summary>	Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of low-resolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data. </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 130、基于对比自蒸馏的紧凑型单幅图像超分辨率研究
- [ ] Towards Compact Single Image Super-Resolution via Contrastive Self-distillation 
时间：2021年05月25日                         第一作者：Yanbo Wang                       [链接](https://arxiv.org/abs/2105.11683).                     
## 摘要：卷积神经网络（CNNs）是一种非常成功的超分辨率（SR）网络，但其结构复杂，内存开销大，计算开销大，极大地限制了其在资源有限的设备上的实际应用。在本文中，我们提出了一个新的对比自蒸馏（CSD）框架来同时压缩和加速各种现成的SR模型。特别地，信道分裂超分辨率网络可以首先从目标教师网络构造为紧凑的学生网络。然后，我们提出了一种新的对比度损失方法，通过显式知识转移来提高SR图像和PSNR/SSIM的质量。大量实验表明，该方案有效地压缩和加速了EDSR、RCAN和CARN等标准SR模型。代码位于https://github.com/Booooooooooo/CSD.
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) are highly successful for super-resolution (SR) but often require sophisticated architectures with heavy memory cost and computational overhead, significantly restricts their practical deployments on resource-limited devices. In this paper, we proposed a novel contrastive self-distillation (CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models. In particular, a channel-splitting super-resolution network can first be constructed from a target teacher network as a compact student network. Then, we propose a novel contrastive loss to improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer. Extensive experiments demonstrate that the proposed CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN. Code is available at https://github.com/Booooooooooo/CSD. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI-21 </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 129、高频感知图像增强
- [ ] High-Frequency aware Perceptual Image Enhancement 
时间：2021年05月25日                         第一作者：Hyungmin Roh                        [链接](https://arxiv.org/abs/2105.11711).                     
## 摘要：本文介绍了一种适用于多尺度分析的深度神经网络，并提出了一种有效的模型不可知方法，帮助该网络从高频域提取信息，重建出更清晰的图像。该模型可应用于多尺度图像增强问题，包括去噪、去模糊和单图像超分辨率。在SIDD、Flickr2K、DIV2K和REDS数据集上的实验表明，我们的方法在每个任务上都达到了最先进的性能。此外，我们还证明了我们的模型可以克服现有面向PSNR的方法中普遍存在的过度平滑问题，并通过对抗性训练生成更自然的高分辨率图像。
<details>	<summary>英文摘要</summary>	In this paper, we introduce a novel deep neural network suitable for multi-scale analysis and propose efficient model-agnostic methods that help the network extract information from high-frequency domains to reconstruct clearer images. Our model can be applied to multi-scale image enhancement problems including denoising, deblurring and single image super-resolution. Experiments on SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves state-of-the-art performance on each task. Furthermore, we show that our model can overcome the over-smoothing problem commonly observed in existing PSNR-oriented methods and generate more natural high-resolution images by applying adversarial training. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 128、基于快速RCNN检测模型的无人机RGB图像玉米密度估计：空间分辨率的影响
- [ ] Estimates of maize plant density from UAV RGB images using Faster-RCNN detection model: impact of the spatial resolution 
时间：2021年05月25日                         第一作者：Kaaviya Velumani                       [链接](https://arxiv.org/abs/2105.11857).                     
## 摘要：在给定的环境条件和管理措施下，早期植株密度是决定基因型命运的重要性状。使用从无人机上拍摄的RGB图像可以取代传统的现场视觉计数，从而提高吞吐量、精确度和工厂定位。然而，需要高分辨率（HR）图像来检测早期存在的小植物。研究了图像地面采样距离（GSD）对快速RCNN在3-5叶期玉米植株检测性能的影响。在HR（GSD=0.3cm）收集的6个对比部位的数据用于模型训练。另外两个具有高分辨率和低分辨率（GSD=0.6cm）图像的位置用于模型评估。结果表明，当本地HR图像同时用于训练和验证时，更快的RCNN可以获得非常好的植物检测和计数性能（rRMSE=0.08）。同样地，当模型在通过对本地训练HR图像下采样获得的合成低分辨率（LR）图像上训练并应用于合成LR验证图像时，观察到良好的性能（rRMSE=0.11）。相反，当模型在给定的空间分辨率上训练并应用到另一个空间分辨率上时，会获得较差的性能。混合HR和LR图像的训练允许在本地HR（rRMSE=0.06）和合成LR（rRMSE=0.10）图像上获得非常好的性能。然而，在本地LR图像（rRMSE=0.48）上仍然观察到非常低的性能，主要是由于本地LR图像的质量差。最后，提出了一种基于生成对抗网络（generativediscountarial network，GAN）的改进的超分辨率方法，该方法引入了来自本地HR图像的额外纹理信息，并应用于本地LR验证图像。结果表明，与双三次上采样方法相比，该方法有一些显著的改进（rRMSE=0.22）。
<details>	<summary>英文摘要</summary>	Early-stage plant density is an essential trait that determines the fate of a genotype under given environmental conditions and management practices. The use of RGB images taken from UAVs may replace traditional visual counting in fields with improved throughput, accuracy and access to plant localization. However, high-resolution (HR) images are required to detect small plants present at early stages. This study explores the impact of image ground sampling distance (GSD) on the performances of maize plant detection at 3-5 leaves stage using Faster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used for model training. Two additional sites with images acquired both at high and low (GSD=0.6cm) resolution were used for model evaluation. Results show that Faster-RCNN achieved very good plant detection and counting (rRMSE=0.08) performances when native HR images are used both for training and validation. Similarly, good performances were observed (rRMSE=0.11) when the model is trained over synthetic low-resolution (LR) images obtained by down-sampling the native training HR images, and applied to the synthetic LR validation images. Conversely, poor performances are obtained when the model is trained on a given spatial resolution and applied to another spatial resolution. Training on a mix of HR and LR images allows to get very good performances on the native HR (rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low performances are still observed over the native LR images (rRMSE=0.48), mainly due to the poor quality of the native LR images. Finally, an advanced super-resolution method based on GAN (generative adversarial network) that introduces additional textural information derived from the native HR images was applied to the native LR validation images. Results show some significant improvement (rRMSE=0.22) compared to bicubic up-sampling approach. </details>
<details>	<summary>注释</summary>	16 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 127、野外非配对深度增强与超分辨率研究进展
- [ ] Towards Unpaired Depth Enhancement and Super-Resolution in the Wild 
时间：2021年05月25日                         第一作者：Aleks                       [链接](https://arxiv.org/abs/2105.12038).                     
## 摘要：商品传感器获取的深度图通常质量和分辨率较低；这些地图需要增强才能在许多应用中使用。最先进的深度图超分辨率数据驱动方法依赖于同一场景的低分辨率和高分辨率深度图的注册对。获取真实世界的成对数据需要专门的设置。另一种替代方法是，通过子采样、添加噪声和其他人工退化方法从高分辨率地图生成低分辨率地图，这种方法不能完全捕捉现实世界中低分辨率图像的特征。因此，在这种人工配对数据上训练的有监督学习方法在现实世界的低分辨率输入上可能表现不好。我们考虑一种基于未配对数据学习的深度图增强方法。虽然已经提出了许多非配对图像到图像的转换技术，但大多数技术并不直接适用于深度图。我们提出了一种同时进行深度增强和超分辨率的非配对学习方法，该方法基于可学习退化模型和表面法线估计作为特征来生成更精确的深度图。我们证明了我们的方法优于现有的非配对方法，并在我们开发的新的非配对学习基准上与配对方法相当。
<details>	<summary>英文摘要</summary>	Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth map enhancement based on learning from unpaired data. While many techniques for unpaired image-to-image translation have been proposed, most are not directly applicable to depth maps. We propose an unpaired learning method for simultaneous depth enhancement and super-resolution, which is based on a learnable degradation model and surface normal estimates as features to produce more accurate depth maps. We demonstrate that our method outperforms existing unpaired methods and performs on par with paired methods on a new benchmark for unpaired learning that we developed. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 126、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月23日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 125、MIASSR：一种医学图像任意尺度超分辨率处理方法
- [ ] MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution 
时间：2021年05月22日                         第一作者：Jin Zhu                       [链接](https://arxiv.org/abs/2105.10738).                     
## 摘要：单幅图像超分辨率（Single-image super-resolution，SISR）的目标是从一幅低分辨率图像中获得高分辨率的输出。目前，基于深度学习的SISR方法在医学图像处理中得到了广泛的讨论，因为它们可以在不需要额外扫描的情况下获得高质量、高空间分辨率的图像。然而，大多数现有的方法是针对特定规模的SR任务而设计的，无法在放大规模上推广。本文提出了一种医学图像任意尺度超分辨率（MIASSR）方法，该方法将元学习与生成对抗网络（GANs）相结合，实现了（1，4）中任意尺度的医学图像超分辨率。与单模态磁共振（MR）脑图像（OASIS-brains）和多模态MR脑图像（BraTS）上最先进的SISR算法相比，MIASSR以最小的模型尺寸获得了相当的保真度性能和最佳的感知质量。我们还利用转移学习使MIASSR能够处理新医学模式的SR任务，如心脏MR图像（ACDC）和胸部CT图像（COVID-CT）。我们工作的源代码也是公开的。因此，MIASSR有可能成为临床图像分析任务（如重建、图像质量增强和分割）中一个新的基础前/后处理步骤。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalise over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation. </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 124、用卷积鉴别器组合变压器发生器
- [ ] Combining Transformer Generators with Convolutional Discriminators 
时间：2021年05月21日                         第一作者：Ricard Durall                       [链接](https://arxiv.org/abs/2105.10189).                     
## 摘要：变压器模型最近引起了计算机视觉研究人员的极大兴趣，并已成功地应用于一些传统的卷积神经网络解决的问题。同时，在过去的几年中，使用生成性对抗网络（GANs）的图像合成有了很大的改进。最近提出的TransGAN是第一个只使用基于变压器的结构的GAN，并且与卷积GAN相比取得了竞争性的结果。然而，由于变压器是数据饥渴的架构，TransGAN需要数据扩充、训练期间的辅助超分辨率任务以及引导自我注意机制之前的掩蔽。在本文中，我们研究了基于变压器的发生器和卷积鉴别器的组合，成功地消除了上述设计选择的需要。我们通过对著名的CNN鉴别器进行基准测试来评估我们的方法，烧蚀基于变压器的发电机的大小，并表明将两种结构元素结合到一个混合模型中可以得到更好的结果。此外，我们研究了生成图像的频谱特性，并观察到我们的模型保留了基于注意的生成器的优点。
<details>	<summary>英文摘要</summary>	Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator. </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 123、线性组合像素自适应回归网络用于单图像超分辨率及更高分辨率
- [ ] LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond 
时间：2021年05月21日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2105.10422).                     
## 摘要：单图像超分辨率（SISR）是将低分辨率（LR）图像上采样为高分辨率（HR）图像的一个基本问题。在过去的几年里，在深度学习方法的推动下取得了令人瞩目的进步。然而，现有方法面临的一个关键挑战是如何找到一个深度模型复杂性和由此产生的SISR质量的最佳点。本文提出了一种线性组合像素自适应回归网络（LAPAR），将LR到HR的直接映射学习转化为多个预定义滤波器基字典上的线性系数回归任务。这样的参数表示使得我们的模型具有高度的轻量级和易于优化，同时在SISR基准上获得最先进的结果。此外，基于同样的思想，将LAPAR算法扩展到图像去噪和JPEG图像去块等恢复任务中，同样获得了很好的性能。代码可在https://github.com/dvlab-research/Simple-SR.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) deals with a fundamental problem of upsampling a low-resolution (LR) image to its high-resolution (HR) version. Last few years have witnessed impressive progress propelled by deep learning methods. However, one critical challenge faced by existing methods is to strike a sweet spot of deep model complexity and resulting SISR quality. This paper addresses this pain point by proposing a linearly-assembled pixel-adaptive regression network (LAPAR), which casts the direct LR to HR mapping learning into a linear coefficient regression task over a dictionary of multiple predefined filter bases. Such a parametric representation renders our model highly lightweight and easy to optimize while achieving state-of-the-art results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended to tackle other restoration tasks, e.g., image denoising and JPEG image deblocking, and again, yields strong performance. The code is available at https://github.com/dvlab-research/Simple-SR. </details>
<details>	<summary>注释</summary>	NeurIPS2020 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 122、用于图像去模糊和超分辨率的特征空间图卷积网络
- [ ] Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution 
时间：2021年05月21日                         第一作者：Boyan Xu                        [链接](https://arxiv.org/abs/2105.10465).                     
## 摘要：图卷积网络（GCNs）在处理非欧几里德结构的数据方面取得了巨大的成功。它们的成功直接归功于将图形结构有效地与社交媒体和知识数据库中的数据相匹配。对于图像处理应用，图形结构和gcn的使用还没有得到充分的探讨。本文提出了一种新的加图卷积的编解码网络，通过将特征映射转换为预生成图的顶点来综合构造图结构数据。通过这样做，我们莫名其妙地应用图拉普拉斯正则化的特征地图，使他们更结构化。实验结果表明，该算法能显著提高图像恢复的性能，包括去模糊和超分辨率。我们相信它为更多应用中基于GCN的方法提供了机会。
<details>	<summary>英文摘要</summary>	Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications. </details>
<details>	<summary>注释</summary>	Accepted by IJCNN 2021 (Oral) Journal-ref: International Joint Conference on Neural Networks (IJCNN) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 121、快速图像超分辨率的内容自适应表示学习
- [ ] Content-adaptive Representation Learning for Fast Image Super-resolution 
时间：2021年05月20日                         第一作者：Yukai Shi                       [链接](https://arxiv.org/abs/2105.09645).                     
## 摘要：深度卷积网络在图像恢复和增强中受到了广泛的关注。通常，通过构造更多的卷积块来提高恢复质量。然而，这些方法大多学习一个特定的模型来处理所有的图像，忽略了难度的多样性。换言之，图像中的高频区域在压缩过程中往往丢失更多信息，而低频区域往往丢失更少信息。本文针对图像检索中的效率问题，提出了一种基于分片滚动网络（PRN）的内容自适应恢复算法。与现有的忽略难度多样性的研究不同，本文采用不同阶段的神经网络进行图像恢复。此外，我们还提出了一种滚动策略，使得每一阶段的参数都更加灵活。大量的实验表明，我们的模型不仅显示了显著的加速，而且保持了最先进的性能。
<details>	<summary>英文摘要</summary>	Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance. </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 120、基于锚网的移动图像超分辨率平面网
- [ ] Anchor-based Plain Net for Mobile Image Super-Resolution 
时间：2021年05月20日                         第一作者：Zongcai Du                       [链接](https://arxiv.org/abs/2105.09750).                     
## 摘要：随着实际应用的快速发展，对图像超分辨率（SR）的精度和效率提出了更高的要求。现有的方法虽然取得了显著的成功，但大多需要大量的计算资源和大量的RAM，不能很好地应用于移动设备。本文旨在设计高效的8位量化体系结构，并将其部署到移动设备上。首先，我们通过分解轻量级SR架构来进行关于元节点延迟的实验，这决定了我们可以利用的可移植操作。在此基础上，深入探讨了什么样的体系结构有利于8位量化，并提出了基于锚的平面网（ABPN）。最后，我们采用量化感知训练策略来进一步提升绩效。该模型在满足实际需要的同时，PSNR性能比8位量化FSRCNN提高了近2dB。代码可在https://github.com/NJU- Jet/SR\移动\量化。
<details>	<summary>英文摘要</summary>	Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization. </details>
<details>	<summary>注释</summary>	accepted by CVPR2021 MAI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 119、XCycles反投影声学超分辨率
- [ ] XCycles Backprojection Acoustic Super-Resolution 
时间：2021年05月19日                         第一作者：Feras Almasri                       [链接](https://arxiv.org/abs/2105.09128).                     
## 摘要：利用深度神经网络（DNNs）进行可见光图像超分辨率（SR）的研究已引起计算机视觉界的广泛关注，并取得了令人瞩目的成果。声成像传感器等非可见光传感器的发展引起了人们的广泛关注，因为它们可以使人们直观地看到可见光谱以外的声波强度。然而，由于获取声学数据的局限性，需要新的方法来提高声学图像的分辨率。目前，还没有为SR问题设计的声学成像数据集。本文提出了一种新的用于声像超分辨率问题的反投影模型体系结构，并与声地图成像VUB-ULB数据集（AMIVU）相结合。数据集提供了不同分辨率的大型模拟和真实捕获图像。与前馈模型方法相比，本文提出的XCycles反投影模型（XCBP）充分利用了每个周期的迭代校正过程，在低分辨率和高分辨率空间中重建编码特征的残差校正。在数据集上对所提出的方法进行了评估，结果表明，与经典的插值算子和最新的前馈模型相比，该方法具有更高的性能。这也大大减少了数据采集过程中产生的次采样误差。
<details>	<summary>英文摘要</summary>	The computer vision community has paid much attention to the development of visible image super-resolution (SR) using deep neural networks (DNNs) and has achieved impressive results. The advancement of non-visible light sensors, such as acoustic imaging sensors, has attracted much attention, as they allow people to visualize the intensity of sound waves beyond the visible spectrum. However, because of the limitations imposed on acquiring acoustic data, new methods for improving the resolution of the acoustic images are necessary. At this time, there is no acoustic imaging dataset designed for the SR problem. This work proposed a novel backprojection model architecture for the acoustic image super-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset (AMIVU). The dataset provides large simulated and real captured images at different resolutions. The proposed XCycles BackProjection model (XCBP), in contrast to the feedforward model approach, fully uses the iterative correction procedure in each cycle to reconstruct the residual error correction for the encoded features in both low- and high-resolution space. The proposed approach was evaluated on the dataset and showed high outperformance compared to the classical interpolation operators and to the recent feedforward state-of-the-art models. It also contributed to a drastically reduced sub-sampling error produced during the data acquisition. </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 118、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年05月19日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
## 摘要：超分辨率（SR）对提高磁共振成像（MRI）的图像质量起着至关重要的作用。磁共振成像产生多对比度图像，可以提供软组织的清晰显示。然而，目前的超分辨率方法仅采用单一对比度，或者采用简单的多对比度融合机制，忽略了不同对比度之间丰富的关系，这对提高磁共振成像的分辨率是有价值的，它明确地建模了不同阶段的多对比度图像之间的依赖关系以指导图像SR。特别地，我们的MINet首先从多个卷积阶段学习不同对比度图像的分层特征表示。随后，我们引入了一个多级集成模块来挖掘多对比度图像表示之间的综合关系。具体来说，该模块将每个表示与所有其他特征相匹配，这些特征根据其相似性进行集成，以获得丰富的表示。在fastMRI和真实临床数据集上的大量实验表明：1）我们的MINet在各种指标方面优于最先进的多对比度SR方法；2）我们的多阶段集成模块能够挖掘不同阶段多对比度特征之间的复杂交互作用，从而提高目标图像质量。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) plays a crucial role in improving the image quality of magnetic resonance imaging (MRI). MRI produces multi-contrast images and can provide a clear display of soft tissues. However, current super-resolution methods only employ a single contrast, or use a simple multi-contrast fusion mechanism, ignoring the rich relations among different contrasts, which are valuable for improving SR. In this work, we propose a multi-stage integration network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the dependencies between multi-contrast images at different stages to guide image SR. In particular, our MINet first learns a hierarchical feature representation from multiple convolutional stages for each of different-contrast image. Subsequently, we introduce a multi-stage integration module to mine the comprehensive relations between the representations of the multi-contrast images. Specifically, the module matches each representation with all other features, which are integrated in terms of their similarities to obtain an enriched representation. Extensive experiments on fastMRI and real-world clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art multi-contrast SR methods in terms of various metrics and 2) our multi-stage integration module is able to excavate complex interactions among multi-contrast features at different stages, leading to improved target-image quality. </details>
<details>	<summary>注释</summary>	10 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 117、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年05月19日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 116、道路网序列中小目标检测的改进方法
- [ ] Improved detection of small objects in road network sequences 
时间：2021年05月18日                         第一作者：Iv\'an Garc\'ia                       [链接](https://arxiv.org/abs/2105.08416).                     
## 摘要：当前道路网络中现有的大量IP摄像机为利用捕获的数据、分析视频和检测任何重大事件提供了机会。为此，有必要检测移动的车辆，直到几年前，这项任务一直是使用经典的人工视觉技术来完成的。如今，深度学习网络已经取得了显著的进步。尽管如此，目标检测仍然被认为是计算机视觉领域的主要开放性问题之一。目前的情况是不断发展的，新的模型和技术正在出现，试图改善这一领域。特别地，在检测小目标方面出现了新的问题和缺点，这些小目标主要对应于道路场景中出现的车辆。所有这一切意味着，新的解决方案，试图提高低检出率的小元素是必不可少的。在众多新兴的研究领域中，本文主要研究小目标的检测。特别是，我们的建议旨在从视频监控摄像头捕获的图像中检测车辆。本文提出了一种基于卷积神经网络（CNN）检测的超分辨过程检测小尺度目标的新方法。将神经网络与提高图像分辨率的过程相结合，提高目标检测性能。通过对一组包含不同尺度元素的交通图像的测试，验证了该方法的有效性，表明该方法在多种情况下都取得了良好的效果。
<details>	<summary>英文摘要</summary>	The vast number of existing IP cameras in current road networks is an opportunity to take advantage of the captured data and analyze the video and detect any significant events. For this purpose, it is necessary to detect moving vehicles, a task that was carried out using classical artificial vision techniques until a few years ago. Nowadays, significant improvements have been obtained by deep learning networks. Still, object detection is considered one of the leading open issues within computer vision. The current scenario is constantly evolving, and new models and techniques are appearing trying to improve this field. In particular, new problems and drawbacks appear regarding detecting small objects, which correspond mainly to the vehicles that appear in the road scenes. All this means that new solutions that try to improve the low detection rate of small elements are essential. Among the different emerging research lines, this work focuses on the detection of small objects. In particular, our proposal aims to vehicle detection from images captured by video surveillance cameras. In this work, we propose a new procedure for detecting small-scale objects by applying super-resolution processes based on detections performed by convolutional neural networks \emph{(CNN)}. The neural network is integrated with processes that are in charge of increasing the resolution of the images to improve the object detection performance. This solution has been tested for a set of traffic images containing elements of different scales to test the efficiency according to the detections obtained by the model, thus demonstrating that our proposal achieves good results in a wide range of situations. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 115、超网络在固定触发器计数下的过参数化实现了快速的神经图像增强
- [ ] Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast Neural Image Enhancement 
时间：2021年05月18日                         第一作者：Lorenz K. Muller                       [链接](https://arxiv.org/abs/2105.08470).                     
## 摘要：深度卷积神经网络可以增强小型移动相机传感器拍摄的图像，擅长去噪、去噪和超分辨率等任务。然而，在移动设备上的实际应用中，这些网络往往需要太多的触发器，减少了卷积层的触发器，也减少了其参数计数。鉴于最近的研究发现，过度参数化的神经网络通常是泛化效果最好的网络，这是有问题的。在本文中，我们建议使用超网络来打破固定比例的触发器参数的标准卷积。这使我们能够在苏黎世原始到DSLR（ZRR）数据集上以低于10倍的浮点计数超过SSIM和MS-SSIM中以前最先进的体系结构。在ZRR上，我们进一步观察到在大图像极限下，在固定的FLOP计数下与“双下降”行为一致的泛化曲线。最后，我们证明了同样的技术可以应用于一个现有的网络（VDN），以降低其计算成本，同时保持对智能手机图像去噪数据集（SIDD）的保真度。附录中给出了关键功能的代码。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks can enhance images taken with small mobile camera sensors and excel at tasks like demoisaicing, denoising and super-resolution. However, for practical use on mobile devices these networks often require too many FLOPs and reducing the FLOPs of a convolution layer, also reduces its parameter count. This is problematic in view of the recent finding that heavily over-parameterized neural networks are often the ones that generalize best. In this paper we propose to use HyperNetworks to break the fixed ratio of FLOPs to parameters of standard convolutions. This allows us to exceed previous state-of-the-art architectures in SSIM and MS-SSIM on the Zurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we further observe generalization curves consistent with 'double-descent' behavior at fixed FLOP-count, in the large image limit. Finally we demonstrate the same technique can be applied to an existing network (VDN) to reduce its computational cost while maintaining fidelity on the Smartphone Image Denoising Dataset (SIDD). Code for key functions is given in the appendix. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 114、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年05月18日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 113、深度学习智能手机上的实时视频超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Video Super-Resolution on Smartphones with Deep Learning, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.08826).                     
## 摘要：随着视频通信和流媒体服务的兴起，视频超分辨率已经成为移动领域的一个重要问题。虽然已经有许多解决方案被提出来完成这项任务，但是大多数解决方案的计算成本太高，无法在硬件资源有限的便携式设备上运行。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的视频超分辨率解决方案，可以在移动gpu上实现实时性能。向参与者提供了REDS数据集，并训练他们的模型进行有效的4X视频放大。所有模型的运行时间都在OPPO Find X2智能手机上进行了评估，Snapdragon 865 SoC能够加速Adreno GPU上的浮点网络。所提出的解决方案完全兼容任何移动GPU，可以将视频放大到高清分辨率，分辨率高达80帧/秒，同时显示高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial text overlap with arXiv:2105.07825. substantial text overlap with arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.08630 </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 112、移动NPU上的实时量化图像超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.07825).                     
## 摘要：图像超分辨率是计算机视觉领域的一个热点问题，在移动设备中有着重要的应用。虽然已经有许多解决方案被提出用于这项任务，但它们通常甚至没有针对常见的智能手机AI硬件进行优化，更不用说通常只支持INT8推理的更受约束的智能电视平台了。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的图像超分辨率解决方案，可以在移动或边缘NPU上展示实时性能。为此，参与者被提供了DIV2K数据集和经过训练的量化模型来进行有效的3X图像放大。所有模型的运行时间均在Synaptics VS680智能家居板上进行评估，该智能家居板配有一个能够加速量化神经网络的专用NPU。所提出的解决方案与所有主要的移动人工智能加速器完全兼容，能够在40-60毫秒的时间内重建全高清图像，同时获得高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/ </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 111、用于高保真材料标签传输的无监督超分辨率卫星图像
- [ ] Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer 
时间：2021年05月16日                         第一作者：Arthita Ghosh                       [链接](https://arxiv.org/abs/2105.07322).                     
## 摘要：遥感图像中的城市物质识别是一个高度相关但极具挑战性的问题，尤其是在低分辨率卫星图像上，由于人类注释的获取非常困难。为此，我们提出了一种基于对抗学习的无监督领域自适应方法。我们的目标是从少量的高分辨率数据（源域）中获取信息，并利用这些数据对低分辨率图像（目标域）进行超分辨率处理。这可能有助于语义以及材料标签从带丰富注释的源到目标域的转移。
<details>	<summary>英文摘要</summary>	Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain. </details>
<details>	<summary>注释</summary>	Published in the proceedings of the 2019 IEEE International Geoscience and Remote Sensing Symposium Journal-ref: IGARSS (2019), 5144-5147 DOI: 10.1109/IGARSS.2019.8900639 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 110、图像超分辨率质量评估：结构保真度与统计自然度
- [ ] Image Super-Resolution Quality Assessment: Structural Fidelity Versus Statistical Naturalness 
时间：2021年05月15日                         第一作者：Wei Zhou                       [链接](https://arxiv.org/abs/2105.07139).                     
## 摘要：单图像超分辨率（SISR）算法用低分辨率（LR）重建高分辨率（HR）图像。发展图像质量评价（IQA）方法不仅可以评价和比较SISR算法，而且可以指导其未来的发展。在本文中，我们评估了在结构保真度与统计自然度的二维（2D）空间中SISR生成图像的质量。这使得我们能够观察不同SISR算法在2D空间中的行为。具体地说，SISR方法传统上是为了获得高的结构保真度而设计的，但往往牺牲了统计的自然性，而最近基于生成性对抗网络（GAN）的算法倾向于创建更自然的结果，但在结构保真度上损失很大。此外，这样的2D评估可以容易地与标量质量预测融合。有趣的是，我们发现一个简单的线性组合一个简单的局部结构保真度和一个全球性的统计自然度措施产生令人惊讶的准确预测SISR图像质量时，测试使用公共科目评分SISR图像数据集。建议的SFSN模型的代码可在\url上公开获取{https://github.com/weizhou-geek/SFSN}.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) algorithms reconstruct high-resolution (HR) images with their low-resolution (LR) counterparts. It is desirable to develop image quality assessment (IQA) methods that can not only evaluate and compare SISR algorithms, but also guide their future development. In this paper, we assess the quality of SISR generated images in a two-dimensional (2D) space of structural fidelity versus statistical naturalness. This allows us to observe the behaviors of different SISR algorithms as a tradeoff in the 2D space. Specifically, SISR methods are traditionally designed to achieve high structural fidelity but often sacrifice statistical naturalness, while recent generative adversarial network (GAN) based algorithms tend to create more natural-looking results but lose significantly on structural fidelity. Furthermore, such a 2D evaluation can be easily fused to a scalar quality prediction. Interestingly, we find that a simple linear combination of a straightforward local structural fidelity and a global statistical naturalness measures produce surprisingly accurate predictions of SISR image quality when tested using public subject-rated SISR image datasets. Code of the proposed SFSN model is publicly available at \url{https://github.com/weizhou-geek/SFSN}. </details>
<details>	<summary>注释</summary>	Accepted by QoMEX 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 109、低分辨率扫描病理图像的多尺度超分辨率生成
- [ ] Multi-scale super-resolution generation of low-resolution scanned pathological images 
时间：2021年05月15日                         第一作者：Yanhua Gao (1)                       [链接](https://arxiv.org/abs/2105.07200).                     
## 摘要：数字化病理切片易于存储和管理，便于浏览和传输。然而，由于数字化过程中的高分辨率扫描（例如40倍放大率），每张幻灯片图像的文件大小超过1G字节，最终导致巨大的存储容量和非常缓慢的网络传输。我们设计了一种低分辨率（5X）幻灯片扫描策略，并提出了一种超分辨率方法来恢复诊断时的图像细节。该方法基于多尺度生成对抗网络，依次生成10X、20X和40X三幅高分辨率图像。在三种图像分辨率下比较了生成图像和真实图像的感知损失、发生器损失，并用鉴别器来评价最高分辨率生成图像和真实图像的差异。一个由10万张人体组织病理图像组成的数据集被用来训练和测试网络。生成的图像具有较高的峰值信噪比（PSNR）和结构相似性指数（SSIM）。10X-40X图像的峰值信噪比分别为24.16、22.27和20.44，SSIM分别为0.845、0.680和0.512，优于DBPN、ESPCN、RDN、EDSR和MDSR等超分辨率网络。此外，视觉检测表明，我们的网络生成的高分辨率图像具有足够的诊断细节，色彩再现性好，接近真实图像，而其他五个网络严重模糊，局部变形或遗漏重要细节。而且，基于生成的图像和真实图像的病理诊断没有显著差异。提出的多尺度网络能够生成高分辨率的病理图像，为数字病理学提供了一种低成本的存储（约15MB/5X图像），更快的图像共享方法。
<details>	<summary>英文摘要</summary>	Digital pathology slide is easy to store and manage, convenient to browse and transmit. However, because of the high-resolution scan for example 40 times magnification(40X) during the digitization, the file size of each whole slide image exceeds 1Gigabyte, which eventually leads to huge storage capacity and very slow network transmission. We design a strategy to scan slides with low resolution (5X) and a super-resolution method is proposed to restore the image details when in diagnosis. The method is based on a multi-scale generative adversarial network, which sequentially generate three high-resolution images such as 10X, 20X and 40X. The perceived loss, generator loss of the generated images and real images are compared on three image resolutions, and a discriminator is used to evaluate the difference of highest-resolution generated image and real image. A dataset consisting of 100,000 pathological images from 10 types of human tissues is performed for training and testing the network. The generated images have high peak-signal-to-noise-ratio (PSNR) and structural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16, 22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than other super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR. Moreover, visual inspections show that the generated high-resolution images by our network have enough details for diagnosis, good color reproduction and close to real images, while other five networks are severely blurred, local deformation or miss important details. Moreover, no significant differences can be found on pathological diagnosis based on the generated and real images. The proposed multi-scale network can generate good high-resolution pathological images, and will provide a low-cost storage (about 15MB/image on 5X), faster image sharing method for digital pathology. </details>
<details>	<summary>注释</summary>	27 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 108、盲超分辨的端到端交替优化
- [ ] End-to-end Alternating Optimization for Blind Super Resolution 
时间：2021年05月14日                         第一作者：Zhengxiong Luo                       [链接](https://arxiv.org/abs/2105.06878).                     
## 摘要：以前的方法将盲超分辨率（SR）问题分解为两个连续的步骤：\textit{i}从给定的低分辨率（LR）图像中估计模糊核和\textit{ii}基于估计核恢复SR图像。这两个步骤的解决方案涉及两个独立训练的模型，这两个模型可能不太兼容。第一步的微小估计误差可能导致第二步的性能严重下降。另一方面，第一步只能利用有限的LR图像信息，难以预测出高精度的模糊核。针对这两个问题，本文采用交替优化算法，在单一模型下估计模糊核，恢复SR图像，而不是单独考虑这两个步骤。具体来说，我们设计了两个卷积神经模块，即\textit{Restorer}和\textit{Estimator}\textit{Restorer}基于预测核对SR图像进行恢复，而\textit{Estimator}借助恢复的SR图像对模糊核进行估计。我们反复交替这两个模块，展开这个过程，形成一个端到端的培训网络。通过这种方式，\textit{Estimator}利用来自LR和SR图像的信息，这使得模糊核的估计更容易。更重要的是，用估计核来训练恢复核，而不是用真核来训练恢复核，因此恢复核对估计核的估计误差有更大的容忍度。在合成数据集和真实图像上的大量实验表明，我们的模型在很大程度上优于最先进的方法，并以更高的速度产生更直观的结果。源代码位于\url{https://github.com/greatlog/DAN.git}.
<details>	<summary>英文摘要</summary>	Previous methods decompose the blind super-resolution (SR) problem into two sequential steps: \textit{i}) estimating the blur kernel from given low-resolution (LR) image and \textit{ii}) restoring the SR image based on the estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. A small estimation error of the first step could cause a severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from the LR image, which makes it difficult to predict a highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate the blur kernel and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the predicted kernel, and \textit{Estimator} estimates the blur kernel with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of the blur kernel easier. More importantly, \textit{Restorer} is trained with the kernel estimated by \textit{Estimator}, instead of the ground-truth kernel, thus \textit{Restorer} could be more tolerant to the estimation error of \textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at a much higher speed. The source code is available at \url{https://github.com/greatlog/DAN.git}. </details>
<details>	<summary>注释</summary>	Submited to PAMI. arXiv admin note: substantial text overlap with arXiv:2010.02631 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 107、合成X射线图像超分辨率的频域约束
- [ ] A Frequency Domain Constraint for Synthetic X-ray Image Super Resolution 
时间：2021年05月14日                         第一作者：Qing Ma                       [链接](https://arxiv.org/abs/2105.06887).                     
## 摘要：合成的X射线图像有助于图像引导系统和虚拟现实仿真。然而，由于CT扫描分辨率有限、计算资源要求高或算法复杂等原因，很难实时生成高质量的任意视点合成X射线图像。我们的目标是通过对低分辨率图像进行上采样，实时生成高分辨率的合成X射线图像。基于参考的超分辨率（RefSR）近年来得到了广泛的研究，并被证明比传统的单图像超分辨率（SISR）更强大。RefSR可以利用参考图像产生精细的细节，但仍不可避免地产生一些伪影和噪声。本文提出了一种基于频域的纹理变换器超分辨率（TTSR-FD）。我们引入频域损耗作为约束条件，进一步提高了RefSR结果的质量，细节清晰，无明显伪影。这使得实时合成X射线图像引导程序VR仿真系统成为可能。据我们所知，这是第一篇利用频域作为超分辨率领域损失函数的论文。我们在合成X射线图像数据集上评估了TTSR-FD，并取得了最新的结果。
<details>	<summary>英文摘要</summary>	Synthetic X-ray images can be helpful for image guiding systems and VR simulations. However, it is difficult to produce high-quality arbitrary view synthetic X-ray images in real-time due to limited CT scanning resolution, high computation resource demand or algorithm complexity. Our goal is to generate high-resolution synthetic X-ray images in real-time by upsampling low-resolution im-ages. Reference-based Super Resolution (RefSR) has been well studied in recent years and has been proven to be more powerful than traditional Single Image Su-per-Resolution (SISR). RefSR can produce fine details by utilizing the reference image but it still inevitably generates some artifacts and noise. In this paper, we propose texture transformer super-resolution with frequency domain (TTSR-FD). We introduce frequency domain loss as a constraint to further improve the quality of the RefSR results with fine details and without obvious artifacts. This makes a real-time synthetic X-ray image-guided procedure VR simulation system possible. To the best of our knowledge, this is the first paper utilizing the frequency domain as part of the loss functions in the field of super-resolution. We evaluated TTSR-FD on our synthetic X-ray image dataset and achieved state-of-the-art results. </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 106、用于视频超分辨率的流引导可变形对准网络FDAN
- [ ] FDAN: Flow-guided Deformable Alignment Network for Video Super-Resolution 
时间：2021年05月12日                         第一作者：Jiayi Lin                       [链接](https://arxiv.org/abs/2105.05640).                     
## 摘要：大多数视频超分辨率（VSR）方法通过对齐相邻帧并挖掘这些帧上的信息来增强视频参考帧。近年来，可变形对齐技术以其能自适应地将相邻帧和参考帧对齐的优异性能，引起了VSR界的广泛关注。然而，我们在实验中发现，可变形对齐方法仍然会因局部损失驱动的偏移预测而受到快速运动的影响，并且缺乏明确的运动约束。因此，我们提出了一个基于匹配的流量估计（MFE）模块来进行全局语义特征匹配，并将光流估计为每个位置的粗偏移量。提出了一种流引导可变形模块（FDM），将光流集成到可变形卷积中。FDM首先利用光流对相邻帧进行扭曲。然后，利用扭曲的相邻帧和参考帧对每个粗偏移量预测一组精细偏移量。一般来说，我们提出了一种端到端的深度网络，称为流引导可变形对齐网络（FDAN），它在两个基准数据集上达到了最先进的性能，同时在计算和内存消耗方面仍然具有竞争力。
<details>	<summary>英文摘要</summary>	Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption. </details>
<details>	<summary>邮件日期</summary>	2021年05月13日</details>

# 105、增强的深金字塔模糊图像恢复网络EDPN
- [ ] EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration 
时间：2021年05月11日                         第一作者：Ruikang Xu                       [链接](https://arxiv.org/abs/2105.04872).                     
## 摘要：随着深度神经网络的发展，图像去模糊技术有了很大的发展。然而，在实际应用中，模糊图像常常会受到诸如缩小尺度和压缩之类的额外退化。为了解决这些问题，我们提出了一种改进的深度金字塔网络（EDPN），通过充分利用退化图像的自相似性和跨尺度相似性，从多个退化图像中恢复模糊图像。，金字塔递进转移（PPT）模块和金字塔自我注意（PSA）模块作为该网络的主要组成部分。PPT模块以多幅复制的模糊图像作为输入，以渐进的方式传输同一退化图像的自相似性和跨尺度相似性信息。然后，PSA模块利用自我和空间注意机制将上述特征融合起来进行后续恢复。实验结果表明，该方法在模糊图像超分辨率和模糊图像去块方面的性能明显优于现有的方法。在NTIRE 2021图像去模糊挑战中，EDPN在第1轨（低分辨率）中获得最佳PSNR/SSIM/LPIPS分数，在第2轨（JPEG伪影）中获得最佳SSIM/LPIPS分数。
<details>	<summary>英文摘要</summary>	Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image.Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts). </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Ruikang and Zeyu contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年05月12日</details>

# 104、超低分辨率印刷文本图像的端到端光学字符识别方法
- [ ] An end-to-end Optical Character Recognition approach for ultra-low-resolution printed text images 
时间：2021年05月10日                         第一作者：Julian D. Gilbey                       [链接](https://arxiv.org/abs/2105.04515).                     
## 摘要：一些历史和较新的打印文档以非常低的分辨率（如60 dpi）进行扫描或存储。尽管这样的扫描相对容易让人阅读，但对于光学字符识别（OCR）系统来说仍然是一个巨大的挑战。目前的技术水平是使用超分辨率重建原始高分辨率图像的近似值，并将其输入标准OCR系统。我们新的端到端方法绕过了超分辨率步骤，产生了更好的OCR结果。这种方法的灵感来自我们对人类视觉系统的理解，并建立在已建立的用于执行OCR的神经网络的基础上。我们的实验表明，在60 dpi扫描的英文文本图像上进行OCR是可能的，这是一个显著低于最新技术的分辨率，并且在一组1000页左右的60 dpi文本中，在广泛的字体范围内，我们实现了99.7%的平均字符级准确率（CLA）和98.9%的单词级准确率（WLA）。对于75dpi图像，同一文本样本的平均CLA为99.9%，平均WLA为99.4%。我们公开我们的代码和数据（包括一组低分辨率图像及其基本事实），作为该领域未来工作的基准。
<details>	<summary>英文摘要</summary>	Some historical and more recent printed documents have been scanned or stored at very low resolutions, such as 60 dpi. Though such scans are relatively easy for humans to read, they still present significant challenges for optical character recognition (OCR) systems. The current state-of-the art is to use super-resolution to reconstruct an approximation of the original high-resolution image and to feed this into a standard OCR system. Our novel end-to-end method bypasses the super-resolution step and produces better OCR results. This approach is inspired from our understanding of the human visual system, and builds on established neural networks for performing OCR. Our experiments have shown that it is possible to perform OCR on 60 dpi scanned images of English text, which is a significantly lower resolution than the state-of-the-art, and we achieved a mean character level accuracy (CLA) of 99.7% and word level accuracy (WLA) of 98.9% across a set of about 1000 pages of 60 dpi text in a wide range of fonts. For 75 dpi images, the mean CLA was 99.9% and the mean WLA was 99.4% on the same sample of texts. We make our code and data (including a set of low-resolution images with their ground truths) publicly available as a benchmark for future work in this field. </details>
<details>	<summary>注释</summary>	8 pages MSC-class: 68T10 ACM-class: I.7.5 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 103、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年05月10日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 102、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年05月10日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 101、基于分层可微神经搜索的轻量级图像超分辨率
- [ ] Lightweight Image Super-Resolution with Hierarchical and Differentiable Neural Architecture Search 
时间：2021年05月09日                         第一作者：Han Huang                       [链接](https://arxiv.org/abs/2105.03939).                     
## 摘要：单图像超分辨率（SISR）任务在深度神经网络中取得了显著的性能。然而，在基于CNN的SISR任务处理方法中，大量的参数需要大量的计算。虽然最近提出了几种有效的SISR模型，但大多数都是手工制作的，因此缺乏灵活性。在这项工作中，我们提出了一种新的可微神经结构搜索（NAS）方法，在细胞和网络两个层面上搜索轻量级SISR模型。具体来说，单元级搜索空间是基于信息提取机制设计的，关注轻量级操作的组合，旨在构建更轻量级、更精确的SR结构。网络级搜索空间的设计考虑了小区间的特征联系，目的是找出哪些信息流对小区最有利，从而提高小区的性能。与现有的基于强化学习（RL）或进化算法（EA）的SISR任务NAS方法不同，我们的搜索管道是完全可微的，并且轻量级SISR模型可以在单个GPU上同时在单元级和网络级进行有效搜索。实验表明，我们的方法在PSNR、SSIM和模型复杂度方面可以在基准数据集上达到最先进的性能，而$\乘以2$的任务只需要68G多次加法，而$\乘以4$的任务只需要18G多次加法。代码将在\url处提供{https://github.com/DawnHH/DLSR-PyTorch}.
<details>	<summary>英文摘要</summary>	Single Image Super-Resolution (SISR) tasks have achieved significant performance with deep neural networks. However, the large number of parameters in CNN-based methods for SISR tasks require heavy computations. Although several efficient SISR models have been recently proposed, most are handcrafted and thus lack flexibility. In this work, we propose a novel differentiable Neural Architecture Search (NAS) approach on both the cell-level and network-level to search for lightweight SISR models. Specifically, the cell-level search space is designed based on an information distillation mechanism, focusing on the combinations of lightweight operations and aiming to build a more lightweight and accurate SR structure. The network-level search space is designed to consider the feature connections among the cells and aims to find which information flow benefits the cell most to boost the performance. Unlike the existing Reinforcement Learning (RL) or Evolutionary Algorithm (EA) based NAS methods for SISR tasks, our search pipeline is fully differentiable, and the lightweight SISR models can be efficiently searched on both the cell-level and network-level jointly on a single GPU. Experiments show that our methods can achieve state-of-the-art performance on the benchmark datasets in terms of PSNR, SSIM, and model complexity with merely 68G Multi-Adds for $\times 2$ and 18G Multi-Adds for $\times 4$ SR tasks. Code will be available at \url{https://github.com/DawnHH/DLSR-PyTorch}. </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 100、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月08日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
## 摘要：近年来，高时间分辨率卫星在各种实际应用中引起了广泛的关注。然而，由于带宽和硬件成本的限制，这类卫星的空间分辨率相当低，很大程度上限制了它们在需要空间明确信息的情况下的潜力。为了提高图像分辨率，人们提出了许多基于训练低-高分辨率对的方法来解决超分辨率问题。然而，尽管它们取得了成功，但在具有高时间分辨率的卫星中通常很难获得低/高空间分辨率对，这使得在SR中使用这种方法不切实际。在本文中，我们提出了一种新的无监督学习框架，称为MIP，它可以在没有低/高分辨率图像对的情况下完成SR任务。首先，将随机噪声映射输入到设计的生成对抗网络（GAN）中进行重构。然后，该方法将参考图像转换为潜空间作为偏移图像的先验信息。最后，利用隐式方法对输入噪声进行更新，进一步传递参考图像的纹理和结构信息。在Draper数据集上的大量实验结果表明，MIP在数量和质量上都比最先进的方法有显著的改进。提议的MIP是开源的http://github.com/jiaming-wang/MIP.
<details>	<summary>英文摘要</summary>	Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. Despite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called "MIP", which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at http://github.com/jiaming-wang/MIP. </details>
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 99、用消噪器中的先验隐式法求解线性反问题
- [ ] Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser 
时间：2021年05月07日                         第一作者：Zahra Kadkhodaie                        [链接](https://arxiv.org/abs/2007.13640).                     
<details>	<summary>注释</summary>	19 pages, 12 figures. Changes: more detailed description of relationships to previous literature, including empirical comparisons for super-resolution, debarring, and compressive sensing </details>
<details>	<summary>邮件日期</summary>	2021年05月10日</details>

# 98、基于局部推理和全局参数联合估计的实时视频超分辨率
- [ ] Real-Time Video Super-Resolution by Joint Local Inference and Global Parameter Estimation 
时间：2021年05月06日                         第一作者：Noam Elron                       [链接](https://arxiv.org/abs/2105.02794).                     
## 摘要：视频超分辨率（SR）是基于深度学习的技术，但在现实世界的视频中表现不佳（见图1）。原因是训练图像对通常是通过缩小高分辨率图像的比例来产生低分辨率的对应图像。因此，深度模型被训练为撤消缩小尺度，而不是推广到超分辨率的真实世界图像。最近的一些出版物提出了改进基于学习的随机共振泛化的技术，但都不适合实时应用。我们提出了一种新的方法来合成训练数据，通过模拟两个不同尺度的数码相机图像捕获过程。我们的方法产生图像对，其中两幅图像都具有自然图像的特性。使用这些数据训练SR模型可以更好地推广到真实世界的图像和视频。此外，深视频SR模型的特点是每像素运算量大，这使得其无法实时应用。我们提出了一种有效的CNN架构，使得视频SR能够在低功耗边缘设备上实时应用。我们将SR任务分为两个子任务：一个控制流，用于估计输入视频的全局属性，并调整执行实际处理的CNN的权重和偏差。由于CNN过程是根据输入的统计信息定制的，因此它的容量保持较低，同时保持了有效性。另外，由于视频统计发展缓慢，控制流以远低于视频帧速率的速率操作。这减少了多达两个数量级的总体计算负载。这种将算法的自适应性与像素处理解耦的框架，可以应用于一大类实时视频增强应用中，如视频去噪、局部色调映射、稳定等。
<details>	<summary>英文摘要</summary>	The state of the art in video super-resolution (SR) are techniques based on deep learning, but they perform poorly on real-world videos (see Figure 1). The reason is that training image-pairs are commonly created by downscaling a high-resolution image to produce a low-resolution counterpart. Deep models are therefore trained to undo downscaling and do not generalize to super-resolving real-world images. Several recent publications present techniques for improving the generalization of learning-based SR, but are all ill-suited for real-time application. We present a novel approach to synthesizing training data by simulating two digital-camera image-capture processes at different scales. Our method produces image-pairs in which both images have properties of natural images. Training an SR model using this data leads to far better generalization to real-world images and videos. In addition, deep video-SR models are characterized by a high operations-per-pixel count, which prohibits their application in real-time. We present an efficient CNN architecture, which enables real-time application of video SR on low-power edge-devices. We split the SR task into two sub-tasks: a control-flow which estimates global properties of the input video and adapts the weights and biases of a processing-CNN that performs the actual processing. Since the process-CNN is tailored to the statistics of the input, its capacity kept low, while retaining effectivity. Also, since video-statistics evolve slowly, the control-flow operates at a much lower rate than the video frame-rate. This reduces the overall computational load by as much as two orders of magnitude. This framework of decoupling the adaptivity of the algorithm from the pixel processing, can be applied in a large family of real-time video enhancement applications, e.g., video denoising, local tone-mapping, stabilization, etc. </details>
<details>	<summary>注释</summary>	Technical report; accompanying a poster appearing in ICCP 2021 Journal-ref: ICCP 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月07日</details>

# 97、FC$^2$N：用于单图像超分辨率的全通道连接网络
- [ ] FC$^2$N: Fully Channel-Concatenated Network for Single Image Super-Resolution 
时间：2021年05月05日                         第一作者：Xiaole Zhao                       [链接](https://arxiv.org/abs/1907.03221).                     
<details>	<summary>注释</summary>	17 pages, 8 figures and 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 96、COMISR：压缩信息视频超分辨率
- [ ] COMISR: Compression-Informed Video Super-Resolution 
时间：2021年05月04日                         第一作者：Yinxiao Li                       [链接](https://arxiv.org/abs/2105.01237).                     
## 摘要：大多数视频超分辨率方法的重点是从低分辨率视频中恢复高分辨率的视频帧，而不考虑压缩。然而，网络或移动设备上的大多数视频都是压缩的，当带宽有限时，压缩会很严重。本文提出了一种新的基于压缩信息的视频超分辨率模型，在不引入压缩伪影的情况下恢复高分辨率的视频内容。该模型由三个视频超分辨率模块组成：双向递归扭曲、细节保持流估计和拉普拉斯增强。所有这三个模块都用于处理压缩特性，例如输入帧中帧内的位置和输出帧中的平滑度。为了进行全面的性能评估，我们在标准数据集上进行了广泛的实验，包括许多真实的视频用例。结果表明，该方法不仅能从广泛使用的基准数据集中恢复未压缩帧上的高分辨率内容，而且在基于大量量化指标的超分辨率压缩视频中也取得了最新的性能。我们还通过模拟YouTube上的流媒体来评估该方法的有效性和鲁棒性。
<details>	<summary>英文摘要</summary>	Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compression-informed video super-resolution model to restore high-resolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. </details>
<details>	<summary>注释</summary>	14 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 95、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年05月03日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 94、基于对抗图神经网络的脑图超分辨及其在脑功能连接中的应用
- [ ] Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity 
时间：2021年05月02日                         第一作者：Megi Isallari                        [链接](https://arxiv.org/abs/2105.00425).                     
## 摘要：近年来，随着以不同分辨率获取的神经影像数据的激增，脑图像分析得到了长足的发展。近年来，脑图像超分辨率的研究得到了迅速的发展，但由于非欧氏图数据的复杂性，脑图像超分辨率的研究还很薄弱。在本文中，我们提出了有史以来第一个深度图超分辨率（GSR）框架，该框架试图从具有N个节点的低分辨率（LR）图（其中N<N'）自动生成具有N'节点的高分辨率（HR）脑图（即解剖感兴趣区域（roi））。首先，我们将GSR问题形式化为一个节点特征嵌入学习任务。一旦学习了HR节点的嵌入，就可以通过基于一种新的图形U-Net结构的聚合规则来获得大脑roi之间的成对连接强度。图U-Net是一种典型的以节点为中心的结构，其中图的嵌入主要依赖于节点的属性，我们提出了一种以节点为中心的结构，其中节点特征的嵌入基于图的拓扑结构。其次，受图谱理论的启发，我们打破了U-Net结构的对称性，用一个GSR层和两个图卷积网络层对低分辨率的脑图结构和节点内容进行了超分辨，进一步了解了HR图中的节点嵌入。第三，为了处理基本真值和预测的HR脑图之间的域转移，我们结合了对抗正则化来对齐它们各自的分布。我们提出的AGSR网络框架在从低分辨率脑功能图预测高分辨率脑功能图方面优于其变体。我们的AGSR网络代码可在GitHub上获得https://github.com/basiralab/AGSR-Net.
<details>	<summary>英文摘要</summary>	Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2009.11080 </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 93、基于无监督深度学习的磁共振弥散加权成像超分辨率和运动伪影同步去除
- [ ] Simultaneous super-resolution and motion artifact removal in diffusion-weighted MRI using unsupervised deep learning 
时间：2021年05月01日                         第一作者：Hyungjin Chung                       [链接](https://arxiv.org/abs/2105.00240).                     
## 摘要：磁共振弥散加权成像由于其预后的能力，现在常规进行，但扫描质量往往不令人满意，这可能会阻碍临床应用。为了克服这些局限性，本文提出了一种完全无监督的质量增强方案，该方案在提高分辨率的同时去除了运动伪影。这一过程是通过使用具有随机退化块的最优传输驱动cycleGAN训练网络来实现的，cycleGAN学习去除混叠伪影并提高分辨率，然后在测试阶段使用训练好的网络，利用bootstrap子采样和聚集来抑制运动伪影。进一步证明了在推理阶段通过控制bootstrap子采样率可以控制伪影校正量和分辨率之间的折衷。据我们所知，提出的方法是第一个解决超分辨率和运动伪影校正，同时在磁共振背景下使用无监督学习。我们通过将该方法应用于模拟研究的定量评价和活体扩散加权MR扫描，证明了该方法的有效性，这表明该方法优于目前最先进的方法。该方法具有灵活性，可以应用于其他类型MR扫描的各种质量增强方案，也可以直接应用于表观扩散系数图的质量增强。
<details>	<summary>英文摘要</summary>	Diffusion-weighted MRI is nowadays performed routinely due to its prognostic ability, yet the quality of the scans are often unsatisfactory which can subsequently hamper the clinical utility. To overcome the limitations, here we propose a fully unsupervised quality enhancement scheme, which boosts the resolution and removes the motion artifact simultaneously. This process is done by first training the network using optimal transport driven cycleGAN with stochastic degradation block which learns to remove aliasing artifacts and enhance the resolution, then using the trained network in the test stage by utilizing bootstrap subsampling and aggregation for motion artifact suppression. We further show that we can control the trade-off between the amount of artifact correction and resolution by controlling the bootstrap subsampling ratio at the inference stage. To the best of our knowledge, the proposed method is the first to tackle super-resolution and motion artifact correction simultaneously in the context of MRI using unsupervised learning. We demonstrate the efficiency of our method by applying it to both quantitative evaluation using simulation study, and to in vivo diffusion-weighted MR scans, which shows that our method is superior to the current state-of-the-art methods. The proposed method is flexible in that it can be applied to various quality enhancement schemes in other types of MR scans, and also directly to the quality enhancement of apparent diffusion coefficient maps. </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 92、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年04月30日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
## 摘要：超分辨率（Super-Resolution，SR）是一项基本的计算机视觉任务，其目标是从给定的低分辨率图像中获得高分辨率的清晰图像。本文回顾了NTIRE 2021在视频超分辨率方面的挑战。我们给出了两条赛道的评估结果以及建议的解决方案。track1的目标是开发传统的视频SR方法，重点是恢复质量。轨道2假设一个具有较低帧速率的更具挑战性的环境，投射时空SR问题。在每项比赛中，分别有247名和223名参赛者报名。在最后的测试阶段，14个团队在每条赛道上进行比赛，以实现视频SR任务的最先进性能。
<details>	<summary>英文摘要</summary>	Super-Resolution (SR) is a fundamental computer vision task that aims to obtain a high-resolution clean image from the given low-resolution counterpart. This paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We present evaluation results from two competition tracks as well as the proposed solutions. Track 1 aims to develop conventional video SR methods focusing on the restoration quality. Track 2 assumes a more challenging environment with lower frame rates, casting spatio-temporal SR problem. In each competition, 247 and 223 participants have registered, respectively. During the final testing phase, 14 teams competed in each track to achieve state-of-the-art performance on video SR tasks. </details>
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 91、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年04月30日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是从给定的低分辨率（low-resolution，LR）图像重建出高分辨率（high-resolution，HR）图像，由于一幅LR图像对应于多幅HR图像，因此这是一个病态问题。近年来，基于学习的SISR方法在性能上明显优于传统的SISR方法，而面向PSNR、GAN驱动和基于流的SISR方法存在过平滑、模式崩溃和模型占用大等问题。为了解决这些问题，我们提出了一种新的单图像超分辨率扩散概率模型（SRDiff），这是第一个基于扩散的SISR模型。SRDiff利用数据似然的变分界变量进行优化，通过马尔可夫链将高斯噪声逐步转化为LR输入条件下的超分辨率（SR）图像，可以提供多样化和真实的SR预测。另外，在整个框架中引入残差预测，加快了收敛速度。我们在面部和一般基准测试（CelebA和DIV2K数据集）上的大量实验表明：1）SRDiff只需一个LR输入，就可以生成丰富细节的不同SR结果，具有最先进的性能；2） SRDiff易于训练，占地面积小；SRDiff可以进行灵活的图像处理，包括潜在空间插值和内容融合。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion. </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 90、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月30日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
<details>	<summary>注释</summary>	This paper is accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 89、BasicVSR++：通过增强传播和对齐提高视频超分辨率
- [ ] BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment 
时间：2021年04月27日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2104.13371).                     
## 摘要：递归结构是视频超分辨率处理的常用框架选择。最先进的方法BasicVSR采用双向传播和特征对齐，有效地利用整个输入视频中的信息。在这项研究中，我们提出二阶网格传播和流动导向的变形对准来重新设计BasicVSR。我们表明，通过增强传播和对齐的循环框架，可以更有效地利用跨错位视频帧的时空信息。在类似的计算约束下，新的组件可以提高性能。特别是，我们的模型BasicVSR++在参数数目相近的情况下，PSNR比BasicVSR高出0.82db。除了视频超分辨率，BasicVSR++还可以很好地推广到其他视频恢复任务，如压缩视频增强。2021年，BasicVSR++在视频超分辨率和压缩视频增强挑战中获得三个冠军和一个亚军。代码和模型将发布给MMEditing。
<details>	<summary>英文摘要</summary>	A recurrent structure is a popular framework choice for the task of video super-resolution. The state-of-the-art method BasicVSR adopts bidirectional propagation with feature alignment to effectively exploit information from the entire input video. In this study, we redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment. We show that by empowering the recurrent framework with the enhanced propagation and alignment, one can exploit spatiotemporal information across misaligned video frames more effectively. The new components lead to an improved performance under a similar computational constraint. In particular, our model BasicVSR++ surpasses BasicVSR by 0.82 dB in PSNR with similar number of parameters. In addition to video super-resolution, BasicVSR++ generalizes well to other video restoration tasks such as compressed video enhancement. In NTIRE 2021, BasicVSR++ obtains three champions and one runner-up in the Video Super-Resolution and Compressed Video Enhancement Challenges. Codes and models will be released to MMEditing. </details>
<details>	<summary>注释</summary>	3 champions and 1 runner-up in NTIRE 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月28日</details>

# 88、好的艺术家复制，伟大的艺术家窃取：模型提取攻击图像翻译生成对抗网络
- [ ] Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks 
时间：2021年04月26日                         第一作者：Sebastian Szyller                       [链接](https://arxiv.org/abs/2104.12623).                     
## 摘要：机器学习模型通常通过推理api提供给潜在的客户机用户。当恶意客户端使用从查询中收集到的信息到受害者模型$F_V$的推理API来构建具有类似功能的代理模型$F_a$时，就会发生模型提取攻击。最近的研究表明，针对图像分类和NLP模型的模型提取攻击是成功的。在本文中，我们展示了第一个针对真实世界生成性对抗网络（GAN）图像翻译模型的模型提取攻击。我们提出了一个对图像翻译模型进行模型提取攻击的框架，并证明了对手可以成功地提取功能代理模型。敌方不需要知道$F\V$的体系结构或任何其他超出其预期图像翻译任务的信息，并且使用从与$F\V$的训练数据相同的域中提取的数据来查询$F\V$的推理接口。我们使用两种流行的图像翻译类型的三个不同实例来评估我们的攻击的有效性：（1）自拍到动画，（2）莫奈到照片（图像样式转换），和（3）超分辨率（超分辨率）。使用GANs的标准性能指标，我们证明了我们的攻击在这三种情况下都是有效的，$F_V$和$F_A$与目标之间的差异在以下范围内：自拍到动画：FID$13.36-68.66$，莫奈到照片：FID$3.57-4.40$，超分辨率：SSIM:$0.06-0.08$，PSNR:$1.43-4.46$。此外，我们进行了一项大规模（125名参与者）的用户研究，从自拍到动画，从莫奈到照片，以表明人类对受害者和代理模型产生的图像的感知是等效的，在科恩的$d=0.3$的等效范围内。
<details>	<summary>英文摘要</summary>	Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ that has comparable functionality. Recent research has shown successful model extraction attacks against image classification, and NLP models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting model extraction attacks against image translation models, and show that the adversary can successfully extract functional surrogate models. The adversary is not required to know $F_V$'s architecture or any other information about it beyond its intended image translation task, and queries $F_V$'s inference interface using data drawn from the same domain as the training data for $F_V$. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective in each of the three cases -- the differences between $F_V$ and $F_A$, compared to the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$, Monet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and PSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by the victim and surrogate models can be considered equivalent, within an equivalence bound of Cohen's $d=0.3$. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 87、意向性深度过度适应学习（IDOL）：一种新的适应性放射治疗深度学习策略
- [ ] Intentional Deep Overfit Learning (IDOL): A Novel Deep Learning Strategy for Adaptive Radiation Therapy 
时间：2021年04月23日                         第一作者：Jaehee Chun (3)                       [链接](https://arxiv.org/abs/2104.11401).                     
## 摘要：在这项研究中，我们提出了一个针对患者特定表现的定制DL框架，该框架利用了一个模型的行为，该模型故意过度拟合患者特定的训练数据集，该数据集是从ART工作流中可用的先验信息扩充而来的——我们称之为故意深度过度拟合学习（IDOL）。在放射治疗的任何任务中实施IDOL框架包括两个训练阶段：1）训练一个具有N个患者的不同训练数据集的广义模型，就像传统的DL方法一样，2）有意地将该一般模型过度拟合到特定于感兴趣的患者（N+1）的小训练数据集，该数据集是通过对可用的特定于任务和患者的先验信息的扰动和增强而生成的，以建立个性化的偶像模型。IDOL框架本身是任务无关的，因此广泛适用于ART工作流的许多组件，其中三个我们在这里用作概念证明：用于传统ART的重新规划CT的自动轮廓任务，用于MRI引导ART的MRI超分辨率（SR）任务，以及仅用于MRI ART的合成CT（sCT）重建任务。在重新规划CT自动轮廓任务中，采用Dice相似系数测量的精度由一般模型的0.847提高到采用IDOL模型的0.935。在MRI-SR的情况下，使用IDOL框架比传统模型的平均绝对误差（MAE）提高了40%。最后，在sCT重建任务中，利用IDOL框架将MAE从68降到22hu。
<details>	<summary>英文摘要</summary>	In this study, we propose a tailored DL framework for patient-specific performance that leverages the behavior of a model intentionally overfitted to a patient-specific training dataset augmented from the prior information available in an ART workflow - an approach we term Intentional Deep Overfit Learning (IDOL). Implementing the IDOL framework in any task in radiotherapy consists of two training stages: 1) training a generalized model with a diverse training dataset of N patients, just as in the conventional DL approach, and 2) intentionally overfitting this general model to a small training dataset-specific the patient of interest (N+1) generated through perturbations and augmentations of the available task- and patient-specific prior information to establish a personalized IDOL model. The IDOL framework itself is task-agnostic and is thus widely applicable to many components of the ART workflow, three of which we use as a proof of concept here: the auto-contouring task on re-planning CTs for traditional ART, the MRI super-resolution (SR) task for MRI-guided ART, and the synthetic CT (sCT) reconstruction task for MRI-only ART. In the re-planning CT auto-contouring task, the accuracy measured by the Dice similarity coefficient improves from 0.847 with the general model to 0.935 by adopting the IDOL model. In the case of MRI SR, the mean absolute error (MAE) is improved by 40% using the IDOL framework over the conventional model. Finally, in the sCT reconstruction task, the MAE is reduced from 68 to 22 HU by utilizing the IDOL framework. </details>
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 86、基于三层神经网络结构的高效单幅图像超分辨率搜索
- [ ] Trilevel Neural Architecture Search for Efficient Single Image Super-Resolution 
时间：2021年04月23日                         第一作者：Yan Wu                       [链接](https://arxiv.org/abs/2101.06658).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 85、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年04月23日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 84、SRWarp：任意变换下的广义图像超分辨率
- [ ] SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation 
时间：2021年04月21日                         第一作者：Sanghyun Son                        [链接](https://arxiv.org/abs/2104.10325).                     
## 摘要：深部CNNs在图像处理和应用方面取得了巨大的成功，包括单图像超分辨率（SR）。然而，传统方法仍然求助于一些预定的整数比例因子，例如x2或x4。因此，当需要任意的目标分辨率时，它们很难被应用。最近的方法将范围扩展到实值上采样因子，甚至使用不同的纵横比来处理限制。在本文中，我们提出SRWarp框架来进一步将SR任务推广到任意图像变换。我们将传统的图像扭曲任务，特别是当输入被放大时，解释为一个空间变化的SR问题。我们还提出了一些新的公式，包括自适应翘曲层和多尺度混合，重建视觉上良好的结果在转换过程中。与以前的方法相比，我们没有将SR模型限制在规则的网格上，而是允许多种可能的变形，以实现灵活多样的图像编辑。大量的实验和烧蚀研究证明了该方法的必要性，并证明了该方法在各种变换下的优越性。
<details>	<summary>英文摘要</summary>	Deep CNNs have achieved significant successes in image processing and its applications, including single image super-resolution (SR). However, conventional methods still resort to some predetermined integer scaling factors, e.g., x2 or x4. Thus, they are difficult to be applied when arbitrary target resolutions are required. Recent approaches extend the scope to real-valued upsampling factors, even with varying aspect ratios to handle the limitation. In this paper, we propose the SRWarp framework to further generalize the SR tasks toward an arbitrary image transformation. We interpret the traditional image warping task, specifically when the input is enlarged, as a spatially-varying SR problem. We also propose several novel formulations, including the adaptive warping layer and multiscale blending, to reconstruct visually favorable results in the transformation process. Compared with previous methods, we do not constrain the SR model on a regular grid but allow numerous possible deformations for flexible and diverse image editing. Extensive experiments and ablation studies justify the necessity and demonstrate the advantage of the proposed SRWarp method under various transformations. </details>
<details>	<summary>注释</summary>	Accepted to CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 83、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月21日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
## 摘要：时空视频超分辨率（STVSR）旨在提高低分辨率、低帧速率视频的时空分辨率。近年来，基于可变形卷积的STVSR方法取得了很好的性能，但它们只能在训练阶段推断出预先定义的中间帧。此外，这些方法低估了相邻帧之间的短期运动线索。本文提出了一种时间调制网络（TMNet）来插值任意中间帧，实现精确的高分辨率重建。具体地说，我们提出了一种时间调制块（TMB）来调制可变形卷积核以实现可控的特征插值。为了更好地利用视频中的时间信息，我们提出了一种局部时间特征比较（LFC）模块和双向可变形ConvLSTM来提取视频中的短期和长期运动线索。在三个基准数据集上的实验表明，我们的TMNet算法优于以前的STVSR算法。代码可在https://github.com/CS-GangXu/TMNet.
<details>	<summary>英文摘要</summary>	Space-time video super-resolution (STVSR) aims to increase the spatial and temporal resolutions of low-resolution and low-frame-rate videos. Recently, deformable convolution based methods have achieved promising STVSR performance, but they could only infer the intermediate frame pre-defined in the training stage. Besides, these methods undervalued the short-term motion cues among adjacent frames. In this paper, we propose a Temporal Modulation Network (TMNet) to interpolate arbitrary intermediate frame(s) with accurate high-resolution reconstruction. Specifically, we propose a Temporal Modulation Block (TMB) to modulate deformable convolution kernels for controllable feature interpolation. To well exploit the temporal information, we propose a Locally-temporal Feature Comparison (LFC) module, along with the Bi-directional Deformable ConvLSTM, to extract short-term and long-term motion cues in videos. Experiments on three benchmark datasets demonstrate that our TMNet outperforms previous STVSR methods. The code is available at https://github.com/CS-GangXu/TMNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 82、单图像超分辨率的两级注意网络
- [ ] A Two-Stage Attentive Network for Single Image Super-Resolution 
时间：2021年04月21日                         第一作者：Jiqing Zhang                       [链接](https://arxiv.org/abs/2104.10488).                     
## 摘要：近年来，深度卷积神经网络（CNNs）在单幅图像超分辨率（SISR）中得到了广泛的应用，并取得了显著的进展。然而，现有的基于CNNs的SISR方法大多在特征提取阶段没有充分挖掘背景信息，对最终的高分辨率图像重建步骤关注较少，从而影响了理想的SR性能。为了解决上述两个问题，本文提出了一种从粗到精的两级注意网络（TSAN）方法。具体来说，我们设计了一个新的多上下文注意块（MCAB），使网络关注更多的信息上下文特征。此外，我们提出了一个必要的精细注意块（RAB），它可以在HR空间中寻找有用的线索来重建精细的HR图像。对四个基准数据集的广泛评估显示了我们提出的TSAN在定量指标和视觉效果方面的有效性。代码位于https://github.com/Jee-King/TSAN.
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at https://github.com/Jee-King/TSAN. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 81、TWIST-GAN：面向小波变换和传输GAN的时空单图像超分辨率研究
- [ ] TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatio-Temporal Single Image Super Resolution 
时间：2021年04月20日                         第一作者：Fayaz Ali Dharejo                       [链接](https://arxiv.org/abs/2104.10268).                     
## 摘要：单图像超分辨率（Single-Image Super-resolution，SISR）是从低空间分辨率的遥感图像中提取高分辨率、高空间分辨率的图像。近年来，针对具有挑战性的单图像超分辨率（SISR）问题，深入学习和生成对抗网络（GANs）取得了突破性进展。然而，生成的图像仍然存在一些不需要的伪影，例如缺少纹理特征表示和高频信息。提出了一种基于频域的时空遥感单幅图像超分辨率重建技术，结合不同频段的生成性对抗网络（GANs）对HR图像进行重建。介绍了一种结合小波变换（WT）特性和传递生成对抗网络的新方法。利用小波变换将LR图像分解为多个频段，而传递生成对抗网络则通过一种新的结构来预测高频分量。最后，通过小波变换的逆变换得到超分辨率的重建图像。该模型首先在一个外部DIV2 Kdataset上进行训练，并用UC-Merceed Landsat遥感数据集和Set14进行验证，每个图像大小为256x256。然后，为了减小计算量的差异，提高纹理信息的质量，利用传递的GANs对时空遥感图像进行处理。对研究结果进行了定性和定性的比较，并与现有的研究方法进行了比较。此外，我们在训练期间节省了大约43%的GPU内存，并通过消除批处理规范化层加快了简化版本的执行。
<details>	<summary>英文摘要</summary>	Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from aremotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks(GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, thegenerated image still suffers from undesirable artifacts such as, the absence of texture-feature representationand high-frequency information. We propose a frequency domain-based spatio-temporal remote sensingsingle image super-resolution technique to reconstruct the HR image combined with generative adversarialnetworks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporatingWavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image hasbeen split into various frequency bands by using the WT, whereas, the transfer generative adversarial networkpredicts high-frequency components via a proposed architecture. Finally, the inverse transfer of waveletsproduces a reconstructed image with super-resolution. The model is first trained on an external DIV2 Kdataset and validated with the UC Merceed Landsat remote sensing dataset and Set14 with each image sizeof 256x256. Following that, transferred GANs are used to process spatio-temporal remote sensing images inorder to minimize computation cost differences and improve texture information. The findings are comparedqualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of theGPU memory during training and accelerated the execution of our simplified version by eliminating batchnormalization layers. </details>
<details>	<summary>注释</summary>	Accepted: ACM TIST (10-03-2021) </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 80、立体图像超分辨率的对称视差注意
- [ ] Symmetric Parallax Attention for Stereo Image Super-Resolution 
时间：2021年04月20日                         第一作者：Yingqian Wang                       [链接](https://arxiv.org/abs/2011.03802).                     
<details>	<summary>注释</summary>	Accepted to NTIRE workshop at CVPR 2021. The first two authors contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 79、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月20日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 78、核不可知的真实图像超分辨率
- [ ] Kernel Agnostic Real-world Image Super-resolution 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2104.09008).                     
## 摘要：近年来，深度神经网络模型在各个研究领域取得了令人瞩目的成果。随着深度超分辨（SR）技术的发展，它引起了越来越多的关注。许多现有的方法都试图从直接下采样的低分辨率图像中恢复高分辨率图像，或者由于其简单性而假设高斯退化核具有加性噪声。然而，在真实场景中，即使失真图像在视觉上与清晰图像相似，也可能涉及高度复杂的核和非加性噪声。在这种情况下，现有的SR模型很难处理真实世界的图像。本文提出了一种新的核不可知SR框架来处理现实世界中的图像SR问题。这个框架可以无缝地挂接到多个主流模型上。在该框架中，退化核和噪声被自适应地建模而不是显式地指定。此外，我们还从正交的角度提出了一个迭代监督过程和频率参与目标，以进一步提高性能。实验验证了该框架在多个真实数据集上的有效性。
<details>	<summary>英文摘要</summary>	Recently, deep neural network models have achieved impressive results in various research fields. Come with it, an increasing number of attentions have been attracted by deep super-resolution (SR) approaches. Many existing methods attempt to restore high-resolution images from directly down-sampled low-resolution images or with the assumption of Gaussian degradation kernels with additive noises for their simplicities. However, in real-world scenarios, highly complex kernels and non-additive noises may be involved, even though the distorted images are visually similar to the clear ones. Existing SR models are facing difficulties to deal with real-world images under such circumstances. In this paper, we introduce a new kernel agnostic SR framework to deal with real-world image SR problem. The framework can be hanged seamlessly to multiple mainstream models. In the proposed framework, the degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and frequency-attended objective from orthogonal perspectives to further boost the performance. The experiments validate the effectiveness of the proposed framework on multiple real-world datasets. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 77、基于稠密搜索空间的神经网络超分辨率搜索：DeCoNAS
- [ ] Neural Architecture Search for Image Super-Resolution Using Densely Constructed Search Space: DeCoNAS 
时间：2021年04月19日                         第一作者：Joon Young Ahn                        [链接](https://arxiv.org/abs/2104.09048).                     
## 摘要：近年来，深度卷积神经网络在单幅图像超分辨率（SISR）和其他许多视觉任务中取得了巨大的成功。通过深化网络和开发更复杂的网络结构，它们的性能也在提高。然而，为给定的问题找到一个最优的结构是一项困难的任务，即使是对人类专家来说也是如此。为此，人们引入了神经结构搜索（NAS）方法，使结构的构建过程自动化。在本文中，我们将NAS扩展到超分辨率域，找到了一个轻量级的密集连接网络decoasnet。我们使用分层搜索策略来寻找与局部和全局特征的最佳连接。在这个过程中，我们定义了一个基于复杂度的惩罚来解决图像的超分辨率问题，这可以看作是一个多目标问题。实验结果表明，我们的decoasnet比现有的基于NAS的设计和手工设计的轻量级超分辨率网络具有更好的性能。
<details>	<summary>英文摘要</summary>	The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 76、基于无监督深度学习的三维荧光显微镜轴向-横向超分辨分析
- [ ] Axial-to-lateral super-resolution for 3D fluorescence microscopy using unsupervised deep learning 
时间：2021年04月19日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
## 摘要：与横向分辨率相比，荧光显微镜的体积成像通常受到轴向分辨率较低的各向异性空间分辨率的限制。为了解决这一问题，本文提出了一种基于深度学习的无监督超分辨技术来增强体荧光显微镜中的各向异性图像。与现有的需要匹配高分辨率目标体图像的深度学习方法相比，我们的方法大大减少了投入实践的工作量，因为网络的训练只需要一个3D图像堆栈，而不需要图像形成过程的先验知识、训练数据的配准或单独的训练获取目标数据。这是基于最优传输驱动循环一致生成对抗网络实现的，该网络从横向图像平面的高分辨率二维图像和其他平面的低分辨率二维图像之间的不成对匹配中学习。利用荧光共焦显微镜和光片显微镜，我们证明了训练的网络不仅提高了轴向分辨率超过衍射极限，而且增强了成像平面之间被抑制的视觉细节，消除了成像伪影。
<details>	<summary>英文摘要</summary>	Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution from inferior axial resolution compared to the lateral resolution. To address this problem, here we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target volume images, our method greatly reduces the effort to put into practice as the training of a network requires as little as a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in lateral image plane and low-resolution 2D images in the other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution beyond the diffraction limit, but also enhances suppressed visual details between the imaging planes and removes imaging artifacts. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 75、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年04月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
## 摘要：在过去的十年中，卷积神经网络在单幅图像超分辨率（SISR）方面取得了显著的进展。在SISR的最新进展中，注意机制是高性能SR模型的关键。然而，很少有作品真正讨论注意力为什么起作用以及它是如何起作用的。在这项工作中，我们试图量化和可视化的静态注意机制，并表明并非所有的注意模块都是同样有益的。然后，我们提出注意网络中的注意（A$^2$N）来获得高精度的图像SR。具体来说，我们的A$^2$N由一个非注意分支和一个耦合注意分支组成。提出了一种基于输入特征的动态注意权值提取模块，该模块能够有效地抑制不必要的注意调整。这允许注意模块专门化有益的例子，而无需其他惩罚，从而大大提高了注意网络的容量，而参数开销很小。实验表明，与现有的轻量级网络相比，该模型具有更好的折衷性能。在局部属性图上的实验也证明了注意力结构（attention-in-attention，A$^2$）可以从更广的范围内提取特征。
<details>	<summary>英文摘要</summary>	Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high performance SR models. However, few works really discuss why attention works and how it works. In this work, we attempt to quantify and visualize the static attention mechanisms and show that not all attention modules are equally beneficial. We then propose attention in attention network (A$^2$N) for highly accurate image SR. Specifically, our A$^2$N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. Experiments have demonstrated that our model could achieve superior trade-off performances comparing with state-of-the-art lightweight networks. Experiments on local attribution maps also prove attention in attention (A$^2$) structure can extract features from a wider range. </details>
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes will be available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 74、全量化图像超分辨率网络
- [ ] Fully Quantized Image Super-Resolution Networks 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2011.14265).                     
<details>	<summary>注释</summary>	Results updated </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 73、VSpSR：基于变分稀疏表示的可探索超分辨率
- [ ] VSpSR: Explorable Super-Resolution via Variational Sparse Representation 
时间：2021年04月17日                         第一作者：Hangqi Zhou                       [链接](https://arxiv.org/abs/2104.08575).                     
## 摘要：超分辨率（SR）是一个不适定问题，它意味着无限多幅高分辨率（HR）图像可以退化为同一幅低分辨率（LR）图像。为了研究一对多随机SR映射，我们隐式地表示了自然图像的非局部自相似性，并通过神经网络建立了一个变分稀疏的超分辨率框架（VSpSR）。由于HR图像的每一小块都可以很好地用原子在超完备字典中的稀疏表示来逼近，因此我们设计了一个双分支模块VSpM来探索SR空间。具体地说，VSpM的一个分支从LR输入中提取面片级基，另一个分支根据稀疏系数推断像素级的变分分布。通过重复采样系数，我们可以得到无限的稀疏表示，从而产生不同的HR图像。根据NTIRE 2021学习SR空间挑战赛的初步结果，我们团队（FUDANMIC21）的发布分数排名第7位。VSpSR的实现发布于https://zmiclab.github.io/。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 72、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月16日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
## 摘要：最新的视频编码标准，称为多功能视频编码（VVC），在编码链的不同层次上包含了几种新颖而精细的编码工具。与以前的高效视频编码（HEVC）标准相比，这些工具带来了显著的编码增益。然而，编码器仍然可以引入可见的编码伪影，主要由应用于将比特率调整到可用带宽的编码决策引起。因此，通常将预处理和后处理技术添加到编码管道以提高解码视频的质量。由于近年来在深度学习方面的进步，与传统方法相比，这些方法最近显示出了突出的效果。通常，多个神经网络被独立地训练来执行不同的任务，因此忽略了模型之间存在的冗余。在本文中，我们研究了一种基于学习的解决方案作为后处理步骤，以提高解码的VVC视频质量。我们的方法依赖于多任务学习来执行质量增强和超分辨率使用一个单一的共享网络优化多个退化水平。与传统的专用体系结构相比，该方案在减少编码伪影和超分辨率方面具有良好的性能，且网络参数较少。
<details>	<summary>英文摘要</summary>	The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 71、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年04月15日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年04月19日</details>

# 70、缩放SlowMo：一种高效的单级时空视频超分辨率框架
- [ ] Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution 
时间：2021年04月15日                         第一作者：Xiaoyu Xiang                       [链接](https://arxiv.org/abs/2104.07473).                     
## 摘要：本文提出了一种基于低分辨率（LR）和低帧速率（LFR）视频序列的高分辨率（HR）慢动作视频超分辨率算法。一种简单的方法是将其分解为两个子任务：视频帧插值（VFI）和视频超分辨率（VSR）。然而，在这个问题中，时间插值和空间上缩放是相互关联的。两阶段方法不能充分利用这一自然属性。另外，现有的VFI或VSR深度网络为了获得高质量的真实感视频帧，通常需要一个较大的帧重建模块，这使得两阶段的方法具有较大的模型，因而相对耗时。为了克服这一问题，我们提出了一种单级时空视频超分辨率框架，该框架可以直接从输入的LR和LFR视频中重建HR慢动作视频序列。我们不象VFI模型那样重建丢失的LR中间帧，而是通过特征时间插值模块对捕获局部时间上下文的丢失LR帧的LR帧特征进行时间插值。在广泛使用的基准测试上的大量实验表明，该框架不仅在干净和有噪声的LR帧上实现了更好的定性和定量性能，而且比最新的两级网络快数倍。源代码在中发布https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.
<details>	<summary>英文摘要</summary>	In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na\"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 . </details>
<details>	<summary>注释</summary>	Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 69、BAM：一种轻量级高效的单图像超分辨率均衡注意机制
- [ ] BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution 
时间：2021年04月15日                         第一作者：Fanyi Wang                       [链接](https://arxiv.org/abs/2104.07566).                     
## 摘要：单图像超分辨率（SISR）是计算机视觉领域中最具挑战性的问题之一。在基于深度卷积神经网络的方法中，注意机制显示出巨大的潜力。然而，由于网络结构的多样性，SISR任务缺乏一种通用的注意机制。本文提出了一种轻量级、高效的平衡注意机制（BAM），该机制可广泛适用于不同的SISR网络。它由Avgpool通道注意模块（ACAM）和Maxpool空间注意模块（MSAM）组成。这两个模块是并联的，以尽量减少误差积累和串扰。为了减少冗余信息对注意力产生的不良影响，我们仅将Avgpool应用于通道注意，因为Maxpool可以在空间维度上提取特征图中的虚幻极值点，我们只将Maxpool应用于空间注意，因为通道维度上的有用特征通常以最大值的形式存在于SISR任务中。为了验证BAM的有效性和鲁棒性，我们将其应用于12个最先进的SISR网络，其中8个没有注意，因此我们插入了BAM，4个有注意，因此我们用BAM替换了原有的注意模块。我们在Set5、Set14和BSD100基准数据集上进行了实验，其标度因子为x2、x3和x4。实验结果表明，BAM可以普遍提高网络性能。此外，我们还进行了烧蚀实验来证明BAM的极简性。结果表明，BAM的并行结构能够更好地平衡信道和空间注意，从而优于传统卷积块注意模块（CBAM）的串行结构。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) is one of the most challenging problems in the field of computer vision. Among the deep convolutional neural network based methods, attention mechanism has shown the enormous potential. However, due to the diverse network architectures, there is a lack of a universal attention mechanism for the SISR task. In this paper, we propose a lightweight and efficient Balanced Attention Mechanism (BAM), which can be generally applicable for different SISR networks. It consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM). These two modules are connected in parallel to minimize the error accumulation and the crosstalk. To reduce the undesirable effect of redundant information on the attention generation, we only apply Avgpool for channel attention because Maxpool could pick up the illusive extreme points in the feature map across the spatial dimensions, and we only apply Maxpool for spatial attention because the useful features along the channel dimension usually exist in the form of maximum values for SISR task. To verify the efficiency and robustness of BAM, we apply it to 12 state-of-the-art SISR networks, among which eight were without attention thus we plug BAM in and four were with attention thus we replace its original attention module with BAM. We experiment on Set5, Set14 and BSD100 benchmark datasets with the scale factor of x2 , x3 and x4 . The results demonstrate that BAM can generally improve the network performance. Moreover, we conduct the ablation experiments to prove the minimalism of BAM. Our results show that the parallel structure of BAM can better balance channel and spatial attentions, thus outperforming the series structure of prior Convolutional Block Attention Module (CBAM). </details>
<details>	<summary>注释</summary>	13 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 68、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年04月15日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
## 摘要：我们提出了SR3，一种通过重复细化实现图像超分辨率的方法。SR3采用去噪扩散概率模型生成条件图像，并通过随机去噪过程进行超分辨率处理。推理从纯高斯噪声开始，并使用在不同噪声水平下进行去噪训练的U网络模型迭代地细化噪声输出。SR3在不同放大倍数的超分辨率任务、人脸和自然图像上表现出很强的性能。我们在CelebA HQ上对标准8X人脸超分辨率任务进行了人体评估，并与SOTA-GAN方法进行了比较。SR3实现了接近50%的傻瓜率，这表明照片逼真的输出，而GANs不超过34%的傻瓜率。我们进一步证明了SR3在级联图像生成中的有效性，其中生成模型与超分辨率模型相链接，在ImageNet上产生了11.3的竞争性FID分数。
<details>	<summary>英文摘要</summary>	We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 67、用于制导深度图超分辨率的离散余弦变换网络
- [ ] Discrete Cosine Transform Network for Guided Depth Map Super-Resolution 
时间：2021年04月14日                         第一作者：Zixiang Zhao                       [链接](https://arxiv.org/abs/2104.06977).                     
## 摘要：制导深度超分辨率（GDSR）是多模图像处理中的一个热点问题。目标是使用高分辨率（HR）RGB图像提供关于边缘和对象轮廓的额外信息，以便低分辨率深度贴图可以向上采样到HR贴图。针对现有方法中存在的RGB纹理过度传输、跨模态特征提取困难、模块工作机制不明确等问题，提出了一种改进的离散余弦变换网络（DCTNet）。首先，将成对的RGB/深度图像输入到半耦合特征提取模块。共享卷积核分别提取跨模态的公共特征，私有核分别提取各自的独特特征。然后将RGB特征输入到边缘注意机制中，以突出显示对上采样有用的边缘。随后，在离散余弦变换（DCT）模块中，采用DCT来解决图像域GDSR的优化问题。将该方法推广到多通道RGB/depth特征上采样，提高了DCTNet的合理性，比传统方法更灵活有效。最终的深度预测由重建模块输出。大量的定性和定量实验证明了该方法的有效性，它可以生成准确的HR深度图，超过了现有的方法。同时，通过烧蚀实验验证了模块设计的合理性。
<details>	<summary>英文摘要</summary>	Guided depth super-resolution (GDSR) is a hot topic in multi-modal image processing. The goal is to use high-resolution (HR) RGB images to provide extra information on edges and object contours, so that low-resolution depth maps can be upsampled to HR ones. To solve the issues of RGB texture over-transferred, cross-modal feature extraction difficulty and unclear working mechanism of modules in existing methods, we propose an advanced Discrete Cosine Transform Network (DCTNet), which is composed of four components. Firstly, the paired RGB/depth images are input into the semi-coupled feature extraction module. The shared convolution kernels extract the cross-modal common features, and the private kernels extract their unique features, respectively. Then the RGB features are input into the edge attention mechanism to highlight the edges useful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT) module, where DCT is employed to solve the optimization problem designed for image domain GDSR. The solution is then extended to implement the multi-channel RGB/depth features upsampling, which increases the rationality of DCTNet, and is more flexible and effective than conventional methods. The final depth prediction is output by the reconstruction module. Numerous qualitative and quantitative experiments demonstrate the effectiveness of our method, which can generate accurate and HR depth maps, surpassing state-of-the-art methods. Meanwhile, the rationality of modules is also proved by ablation experiments. </details>
<details>	<summary>邮件日期</summary>	2021年04月15日</details>

# 66、SRR-Net：一种高分辨率MR图像的超分辨率重建方法
- [ ] SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging 
时间：2021年04月13日                         第一作者：Wenqi Huang                       [链接](https://arxiv.org/abs/2104.05901).                     
## 摘要：提高磁共振成像（MRI）的图像分辨率和采集速度是一个具有挑战性的问题。主要有两种策略来处理速度-分辨率的折衷：（1）$k$空间欠采样和高分辨率采集；（2）低分辨率图像重建和图像超分辨率流水线。然而，这些方法要么在某些高加速因子下性能有限，要么存在两级结构的误差积累。本文将MR重建和图像超分辨率的思想结合起来，直接从低分辨率的$k$空间采样数据中恢复HR图像。特别地，将SR重建问题描述为一个变分问题，并提出了一种从求解算法中展开的可学习网络。为了提高细节细化性能，引入了鉴别器。在体HR多线圈脑数据的实验结果表明，该SRR网络能够恢复高分辨率的脑图像，具有良好的视觉质量和感知质量。
<details>	<summary>英文摘要</summary>	Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 65、走向快速准确的真实世界深度超分辨率：基准数据集和基线
- [ ] Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline 
时间：2021年04月13日                         第一作者：Lingzhi He                       [链接](https://arxiv.org/abs/2104.06174).                     
## 摘要：商业深度传感器获取的深度图分辨率较低，难以用于各种计算机视觉任务。因此，深度图超分辨率（SR）是一项实用而有价值的工作，它可以将深度图提升到高分辨率（HR）空间。然而，由于缺乏真实世界的成对低分辨率（LR）和HR深度图，现有的方法大多采用降采样来获得成对的训练样本。为此，我们首先构建了一个名为RGB-D-D的大规模数据集，它可以极大地促进深度图SR的研究，甚至可以促进更多与深度相关的实际任务。数据集中的“D-D”表示从手机和Lucid Helios分别捕获的成对LR和HR深度图，范围从室内场景到具有挑战性的室外场景。此外，我们提供了一个快速的深度图超分辨率（FDSR）基线，其中高频分量从RGB图像自适应分解来指导深度图超分辨率。在现有公共数据集上的大量实验证明了我们的网络与现有方法相比的有效性和效率。此外，对于真实的LR深度图，我们的算法可以生成更精确的HR深度图，边界更清晰，并且在一定程度上修正了深度值误差。
<details>	<summary>英文摘要</summary>	Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which upscales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use downsampling to obtain paired training samples. To this end, we first construct a large-scale dataset named "RGB-D-D", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The "D-D" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 64、混叠是你的盟友：从原始图像突发端到端的超分辨率
- [ ] Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年04月13日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
## 摘要：本演示解决了从空间和时间上稍微不同的视点捕获的多个低分辨率快照重建高分辨率图像的问题。解决这个问题的关键挑战包括（i）以亚像素精度对齐输入图片，（ii）处理原始（噪声）图像以最大程度地忠实于本地相机数据，以及（iii）设计/学习非常适合该任务的图像先验（正则化器）。基于Wronski等人的见解，我们采用一种混合算法来解决这三个难题。在这种情况下，混叠是一个盟友，参数可以端到端地学习，同时保留了反问题经典方法的可解释性。我们的方法在合成和真实图像突发上的有效性得到了证明，在几个基准上建立了一个新的技术状态，并在智能手机和prosumer相机捕获的真实原始突发上提供了极好的定性结果。
<details>	<summary>英文摘要</summary>	This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 63、利用低分辨率流和掩模上采样实现高效的时空视频超分辨率
- [ ] Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling 
时间：2021年04月12日                         第一作者：Saikat Dutta                       [链接](https://arxiv.org/abs/2104.05778).                     
## 摘要：针对低分辨率、低帧速率的慢动作视频，提出了一种高效的时空超分辨率解决方案。一个简单的解决方案是连续运行视频超分辨率和视频帧插值模型。然而，这类解的内存效率低，推理时间长，不能充分利用时空关系的性质。为此，我们首先利用二次模型在LR空间中进行插值。使用最先进的视频超分辨率方法对输入LR帧进行超分辨率。利用双线性上采样技术，在HR空间重用用于合成LR插值帧的流程图和混合模板。这导致HR中间帧的粗略估计，该中间帧通常包含沿运动边界的伪影。通过残差学习，利用细化网络提高HR中间帧的质量。我们的模型是轻量级的，在REDS-STSR验证集中的性能比目前最先进的模型要好。
<details>	<summary>英文摘要</summary>	This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set. </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Project page: https://github.com/saikatdutta/FMU_STSR </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 62、基于深度学习的超分辨率网络边缘感知图像压缩
- [ ] Edge-Aware Image Compression using Deep Learning-based Super-resolution Network 
时间：2021年04月11日                         第一作者：Dipti Mishra                       [链接](https://arxiv.org/abs/2104.04926).                     
## 摘要：我们提出了一种基于学习的压缩方案，在预处理和后处理的深度cnn之间封装一个标准的编解码器。具体地说，我们通过引入：（a）一个边缘感知损失函数来防止在以前的工作中经常出现的模糊&（b）一个用于后处理的超分辨率卷积神经网络（CNN）以及一个相应的预处理网络，来展示对使用压缩-解压缩网络的先前方法的改进在低速率下提高率失真性能。该算法在从低分辨率到高分辨率的各种数据集上进行评估，即Set 5、Set 7、Classic 5、Set 14、Live 1、Kodak、General 100、CLIC 2019。与JPEG、JPEG2000、BPG和最近的CNN方法相比，该算法在低码率和高码率下的峰值信噪比分别提高了20.75%、8.47%、3.22%、3.23%和24.59%、14.46%、10.14%和8.57%。同样，在低比特率和高比特率下，MS-SSIM的这种改进分别约为71.43%、50%、36.36%、23.08%、64.70%和64.47%、61.29%、47.06%、51.52%和16.28%。使用CLIC 2019数据集，在低比特率和高比特率下，PSNR分别约为16.67%、10.53%、6.78%和24.62%、17.39%和14.08%，优于JPEG2000、BPG和最近的CNN方法。同样，与相同的方法相比，MS-SSIM在低比特率和高比特率下的性能分别约为72%、45.45%、39.13%、18.52%和71.43%、50%、41.18%和17.07%。其他数据集也实现了类似的改进。
<details>	<summary>英文摘要</summary>	We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%, 61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%, 10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%, and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively, compared to the same approaches. A similar type of improvement is achieved with other datasets also. </details>
<details>	<summary>注释</summary>	13 pages, 9 figures, 16 tables </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 61、CoPE：使用多项式展开的条件图像生成
- [ ] CoPE: Conditional image generation using Polynomial Expansions 
时间：2021年04月11日                         第一作者：Grigorios G Chrysos                       [链接](https://arxiv.org/abs/2104.05077).                     
## 摘要：生成建模已经发展成为机器学习的一个重要领域。深度多项式神经网络（PNNs）在无监督图像生成中取得了令人印象深刻的结果，其任务是将输入向量（即噪声）映射到合成图像。然而，PNNs的成功还没有在超分辨率等条件生成任务中得到推广。现有的pnn主要集中在单变量多项式展开上，对于两个变量的输入，即噪声变量和条件变量，pnn表现不好。在这项工作中，我们引入了一个通用的框架，称为CoPE，它可以对两个输入变量进行多项式展开，并捕捉它们的自相关和互相关。我们展示了CoPE如何被简单地扩充以接受任意数量的输入变量。CoPE分为五个任务（类条件生成、反问题、边缘到图像的转换、图像到图像的转换、属性引导生成），涉及八个数据集。全面评估表明，CoPE可用于处理各种条件生成任务。
<details>	<summary>英文摘要</summary>	Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 60、作物类型语义切分的语境自对比预训练
- [ ] Context-self contrastive pretraining for crop type semantic segmentation 
时间：2021年04月09日                         第一作者：Michail Tarasiou                       [链接](https://arxiv.org/abs/2104.04310).                     
## 摘要：本文提出了一种基于对比学习的全监督预训练方案，特别适合于密集分类任务。提出的上下文自对比丢失（CSCL）算法利用训练样本中每个位置与其局部上下文之间的相似性度量，学习一个使语义边界弹出的嵌入空间。对于卫星图像中作物类型的语义分割，我们发现包裹边界的性能是一个关键的瓶颈，并解释了CSCL如何解决该问题的根本原因，从而提高该任务的最新性能。此外，利用Sentinel-2（S2）卫星任务的图像，我们编制了据我们所知最大的卫星图像时间序列数据集，这些数据集由作物类型和包裹标识密集标注，我们与数据生成管道一起公开。利用这些数据，我们发现CSCL，即使在最小的预训练下，也可以改善所有的基线，并提出了一个超分辨率的语义分割过程，以获得更细粒度的作物类。该方法在二维和三维立体图像的语义分割任务中得到了进一步的验证，结果表明，该方法在竞争性基线下的性能得到了一致的提高。
<details>	<summary>英文摘要</summary>	In this paper we propose a fully-supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in an training sample and its local context. For crop type semantic segmentation from satellite images we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, dataset of satellite image timeseries densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The proposed method is further validated on the task of semantic segmentation on 2D and 3D volumetric images showing consistent performance improvements upon competitive baselines. </details>
<details>	<summary>注释</summary>	11 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 59、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月09日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 58、加法器：迈向节能图像超分辨率
- [ ] AdderSR: Towards Energy Efficient Image Super-Resolution 
时间：2021年04月09日                         第一作者：Dehua Song                       [链接](https://arxiv.org/abs/2009.08891).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 57、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月08日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
## 摘要：虽然单图像超分辨率（single-image super-resolution，SISR）方法在单次退化方面取得了很大的成功，但在实际应用中仍存在性能下降和多重退化的问题。近年来，人们对多重降解的盲模型和非盲模型进行了研究。然而，由于训练数据和测试数据之间的分布变化，这些方法通常会显著降低性能。为此，我们首次提出了一个条件元网络框架（CMDSR），帮助SR框架学习如何适应输入分布的变化。我们利用所提出的条件网在任务级提取退化先验，以适应基本SR网络（BaseNet）的参数。具体地说，我们框架的ConditionNet首先从一个支持集学习退化先验知识，该支持集由来自同一任务的一系列退化图像块组成。然后自适应基网根据条件特征快速地改变其参数。此外，为了更好地提取退化先验信息，我们提出了一种任务对比损失的方法来减小任务内部的距离，增加任务级特征之间的跨任务距离。在不预先定义退化映射的情况下，我们的盲框架可以进行单参数更新，从而产生可观的SR结果。大量的实验证明了CMDSR在各种盲甚至非盲方法中的有效性。灵活的基本网结构也表明CMDSR可以作为一个大系列SISR模型的通用框架。
<details>	<summary>英文摘要</summary>	Although single-image super-resolution (SISR) methods have achieved great success on single degradation, they still suffer performance drop with multiple degrading effects in real scenarios. Recently, some blind and non-blind models for multiple degradations have been explored. However, those methods usually degrade significantly for distribution shifts between the training and test data. Towards this end, we propose a conditional meta-network framework (named CMDSR) for the first time, which helps SR framework learn how to adapt to changes in input distribution. We extract degradation prior at task-level with the proposed ConditionNet, which will be used to adapt the parameters of the basic SR network (BaseNet). Specifically, the ConditionNet of our framework first learns the degradation prior from a support set, which is composed of a series of degraded image patches from the same task. Then the adaptive BaseNet rapidly shifts its parameters according to the conditional features. Moreover, in order to better extract degradation prior, we propose a task contrastive loss to decrease the inner-task distance and increase the cross-task distance between task-level features. Without predefining degradation maps, our blind framework can conduct one single parameter update to yield considerable SR results. Extensive experiments demonstrate the effectiveness of CMDSR over various blind, even non-blind methods. The flexible BaseNet structure also reveals that CMDSR can be a general framework for large series of SISR models. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 56、太阳电池检测的联合超分辨与校正
- [ ] Joint Super-Resolution and Rectification for Solar Cell Inspection 
时间：2021年04月07日                         第一作者：Mathis Hoffmann                       [链接](https://arxiv.org/abs/2011.05003).                     
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 55、BasicVSR：寻找视频超分辨率及更高分辨率的关键组件
- [ ] BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond 
时间：2021年04月07日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2012.02181).                     
<details>	<summary>注释</summary>	CVPR 2021 camera-ready </details>
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 54、基于内容自适应超分辨率的高效视频压缩
- [ ] Efficient Video Compression via Content-Adaptive Super-Resolution 
时间：2021年04月06日                         第一作者：Mehrdad Khani                       [链接](https://arxiv.org/abs/2104.02322).                     
## 摘要：视频压缩是互联网视频传输的重要组成部分。最近的研究表明，深度学习技术可以与人类设计的算法相媲美或优于人类设计的算法，但这些方法的计算效率和功耗明显低于现有的编解码器。本文提出了一种新的方法，通过一个小的、内容自适应的超分辨率模型来增强现有的编解码器，从而显著提高视频质量。我们的方法，SRVC，将视频编码成两个比特流：（i）内容流，通过使用现有的编解码器压缩下采样的低分辨率视频产生，（ii）模型流，对为视频的短片段定制的轻量级超分辨率神经网络的周期性更新进行编码。SRVC通过将解压缩后的低分辨率视频帧通过（时变）超分辨率模型来重构高分辨率视频帧，从而对视频进行解码。我们的结果表明，为了获得相同的PSNR，SRVC在慢模式下需要H.265每像素16%的比特，而在DVC（一种新的基于深度学习的视频压缩方案）中需要2%的每像素比特。SRVC在nvidiav100gpu上以每秒90帧的速度运行。
<details>	<summary>英文摘要</summary>	Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 53、超分辨率的测试时间调整：你只需要在更多的图像上过度调整
- [ ] Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a Few More Images 
时间：2021年04月06日                         第一作者：Mohammad Saeed Rad                       [链接](https://arxiv.org/abs/2104.02663).                     
## 摘要：现有的基于参考（RF）的超分辨率（SR）模型试图在高分辨率RF图像与低分辨率（LR）输入匹配的假设下提高SR的感知质量。由于射频图像在内容、颜色、对比度等方面应与测试图像相似，这妨碍了其在实际场景中的适用性。其他提高图像感知质量的方法，包括感知损失和对抗损失，往往通过显著降低PSNR/SSIM来显著降低对地面真实感的保真度。针对这两个问题，我们提出了一种简单而通用的方法，通过进一步微调训练数据集中具有与初始HR预测相似激活模式的图像子集上的SR网络，来提高预先训练的SR网络对给定LR输入的HR预测的感知质量，关于特征提取器的过滤器。特别地，我们从感知质量和PSNR/SSIM值方面展示了微调对这些图像的影响。与感知驱动的方法相反，我们证明了微调网络产生的HR预测具有更高的感知质量和相对于初始HR预测的PSNR/SSIM的最小变化。此外，我们还提出了有关随机共振网络滤波器的新的数值实验，通过滤波器相关性，我们表明，与基线网络或随机图像上微调的网络相比，我们方法中微调网络的滤波器更接近“理想”滤波器。
<details>	<summary>英文摘要</summary>	Existing reference (RF)-based super-resolution (SR) models try to improve perceptual quality in SR under the assumption of the availability of high-resolution RF images paired with low-resolution (LR) inputs at testing. As the RF images should be similar in terms of content, colors, contrast, etc. to the test image, this hinders the applicability in a real scenario. Other approaches to increase the perceptual quality of images, including perceptual loss and adversarial losses, tend to dramatically decrease fidelity to the ground-truth through significant decreases in PSNR/SSIM. Addressing both issues, we propose a simple yet universal approach to improve the perceptual quality of the HR prediction from a pre-trained SR network on a given LR input by further fine-tuning the SR network on a subset of images from the training dataset with similar patterns of activation as the initial HR prediction, with respect to the filters of a feature extractor. In particular, we show the effects of fine-tuning on these images in terms of the perceptual quality and PSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate that the fine-tuned network produces a HR prediction with both greater perceptual quality and minimal changes to the PSNR/SSIM with respect to the initial HR prediction. Further, we present novel numerical experiments concerning the filters of SR networks, where we show through filter correlation, that the filters of the fine-tuned network from our method are closer to "ideal" filters, than those of the baseline network or a network fine-tuned on random images. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 52、深脉冲超分辨率
- [ ] Deep Burst Super-Resolution 
时间：2021年04月06日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2101.10997).                     
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 51、基于注意的层次多模态融合高分辨率深度图成像
- [ ] High-resolution Depth Maps Imaging via Attention-based Hierarchical Multi-modal Fusion 
时间：2021年04月04日                         第一作者：Zhiwei Zhong                       [链接](https://arxiv.org/abs/2104.01530).                     
## 摘要：深度图记录了场景中视点和物体之间的距离，在许多实际应用中起着至关重要的作用。然而，消费者级RGB-D相机拍摄的深度图空间分辨率较低。引导深度图超分辨率（DSR）是解决这一问题的一种常用方法，它试图从输入的低分辨率（LR）深度及其作为引导的耦合HR-RGB图像恢复高分辨率（HR）深度图。如何正确地选择和传播一致性结构，正确地处理不一致性结构是指导DSR最具挑战性的问题。本文提出了一种新的基于注意的分层多模态融合（AHMF）网络。具体来说，为了有效地从LR深度和HR引导中提取和组合相关信息，我们提出了一种分层卷积层的多模式基于注意的融合（MMAF）策略，包括一个特征增强块，用于选择有价值的特征；一个特征重新校准块，用于统一具有不同外观特征的模式的相似性度量。在此基础上，提出了一种双向分层特征协作（BHFC）模型，充分利用多尺度特征间的低层空间信息和高层结构信息。实验结果表明，该方法在重建精度、运行速度和存储效率等方面均优于现有方法。
<details>	<summary>英文摘要</summary>	Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance. The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 50、具有光谱混合和异构数据集的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution with Spectral Mixup and Heterogeneous Datasets 
时间：2021年04月03日                         第一作者：Ke Li                       [链接](https://arxiv.org/abs/2101.07589).                     
<details>	<summary>注释</summary>	16 pages, 14 tables, 5 figures; Code available at https://github.com/kli8996/HSISR </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 49、用于学习失调光学变焦的平方变形对准网络
- [ ] SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom 
时间：2021年04月02日                         第一作者：Kangfu Mei                       [链接](https://arxiv.org/abs/2104.00848).                     
## 摘要：基于深度神经网络（DNN）的超分辨率算法大大提高了生成图像的质量。然而，由于学习失调光学变焦的困难，这些算法在处理真实世界的超分辨率问题时往往会产生明显的伪影。为了解决这一问题，本文提出了一种平方变形对准网络（SDAN）。我们的网络学习卷积核的每点平方偏移量，然后根据偏移量在校正的卷积窗口中对齐特征。因此，通过提取对齐的特征，可以最大限度地减少不对齐。与普通可变形卷积网络（DCN）中的逐点偏移不同，本文提出的平方偏移不仅加快了偏移学习，而且在参数较少的情况下提高了生成质量。此外，我们进一步提出一个有效的交叉堆积注意层来提高学习偏移量的准确性。它利用打包和解包操作来扩大偏移量学习的接受域，增强低分辨率图像与参考图像之间空间联系的提取能力。综合实验表明，该方法在计算效率和真实感细节方面均优于其他先进方法。
<details>	<summary>英文摘要</summary>	Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details. </details>
<details>	<summary>注释</summary>	ICME21. Code is available at https://github.com/MKFMIKU/SDAN </details>
<details>	<summary>邮件日期</summary>	2021年04月05日</details>

# 48、盲超分辨的无监督退化表示学习
- [ ] Unsupervised Degradation Representation Learning for Blind Super-Resolution 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2104.00416).                     
## 摘要：大多数现有的基于CNN的超分辨率（SR）方法都是在假设退化是固定的和已知的（例如双三次下采样）的基础上发展起来的。然而，当实际性能下降与假设不同时，这些方法的性能会严重下降。在实际应用中，为了处理各种未知的退化，以往的方法都是依靠退化估计来重建SR图像。然而，退化估计方法通常是耗时的，并且可能由于较大的估计误差而导致SR失效。本文提出了一种无监督退化表示学习方案，用于盲随机共振，无需显式退化估计。具体来说，我们学习抽象表示来区分表示空间中的各种退化，而不是像素空间中的显式估计。此外，我们还提出了一种基于学习表示的退化感知SR（DASR）网络，该网络能够灵活地适应各种退化。实验结果表明，我们的退化表征学习方法可以提取有区别的表征来获得精确的退化信息。在合成图像和真实图像上的实验表明，我们的网络对于盲SR任务达到了最先进的性能。代码位于：https://github.com/LongguangWang/DASR。
<details>	<summary>英文摘要</summary>	Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To handle various unknown degradations in real-world applications, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation estimation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this paper, we propose an unsupervised degradation representation learning scheme for blind SR without explicit degradation estimation. Specifically, we learn abstract representations to distinguish various degradations in the representation space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 47、探索图像超分辨率中的稀疏性实现高效推理
- [ ] Exploring Sparsity in Image Super-Resolution for Efficient Inference 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2006.09603).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 46、通过学习匹配内部斑块分布来估计MR切片轮廓
- [ ] MR Slice Profile Estimation by Learning to Match Internal Patch Distributions 
时间：2021年03月31日                         第一作者：Shuo Han                       [链接](https://arxiv.org/abs/2104.00100).                     
## 摘要：为了超分辨多层面二维磁共振（MR）图像的通平面方向，在训练监督算法时，可以将其切片选择剖面作为高分辨率（HR）到低分辨率（LR）的退化模型来生成成对数据。现有的超分辨率算法对切片选择轮廓进行了假设，因为给定的图像不容易知道它。在这项工作中，我们通过学习匹配其内部补丁分布来估计给定特定图像的切片选择剖面。具体来说，我们假设在应用正确的切片选择轮廓之后，沿着HR平面内方向的图像面片分布应该与沿着LR平面内方向的分布相匹配。因此，我们将切片选择轮廓的估计作为生成对抗网络（GAN）中学习生成器的一部分。这样，就可以在没有任何外部数据的情况下学习切片选择轮廓。我们的算法通过各向同性MR图像的模拟进行了测试，并将其与一种通过平面的超分辨率算法结合起来，以证明其优越性，同时也被用作测量图像分辨率的工具。我们的密码是https://github.com/shuohan/espreso2。
<details>	<summary>英文摘要</summary>	To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, accepted by Information Processing in Medical Imaging (IPMI) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 45、视频探索通过视频特定的自动编码器
- [ ] Video Exploration via Video-Specific Autoencoders 
时间：2021年03月31日                         第一作者：Kevin Wang                        [链接](https://arxiv.org/abs/2103.17261).                     
## 摘要：我们提出了简单的视频特定的自动编码器，使人类可控的视频探索。这包括各种各样的分析任务，例如（但不限于）空间和时间超分辨率、空间和时间编辑、对象移除、视频纹理、平均视频探索以及视频内部和跨视频的对应估计。以前的工作已经独立地研究了这些问题，并提出了不同的公式。在这项工作中，我们观察到一个简单的自动编码器训练（从头开始）对多个帧的特定视频，使一个人能够执行各种各样的视频处理和编辑任务。我们的任务是通过两个关键的观察来实现的：（1）由自动编码器学习的潜在代码捕获视频的空间和时间特性；（2）自动编码器可以将样本输入投射到视频特定的流形上。例如：（1）内插潜在代码实现了时间超分辨率和用户可控的视频纹理；（2）流形重投影实现了空间超分辨率、对象移除和去噪，而无需对任何任务进行训练。重要的是，通过主成分分析的潜在代码的二维可视化可以作为用户可视化和直观控制视频编辑的工具。最后，我们将我们的方法与现有技术进行定量对比，发现在没有任何监督和任务特定知识的情况下，我们的方法可以与专门为任务训练的监督方法进行比较。
<details>	<summary>英文摘要</summary>	We present simple video-specific autoencoders that enables human-controllable video exploration. This includes a wide variety of analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, object removal, video textures, average video exploration, and correspondence estimation within and across videos. Prior work has independently looked at each of these problems and proposed different formulations. In this work, we observe that a simple autoencoder trained (from scratch) on multiple frames of a specific video enables one to perform a large variety of video processing and editing tasks. Our tasks are enabled by two key observations: (1) latent codes learned by the autoencoder capture spatial and temporal properties of that video and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. For e.g. (1) interpolating latent codes enables temporal super-resolution and user-controllable video textures; (2) manifold reprojection enables spatial super-resolution, object removal, and denoising without training for any of the tasks. Importantly, a two-dimensional visualization of latent codes via principal component analysis acts as a tool for users to both visualize and intuitively control video edits. Finally, we quantitatively contrast our approach with the prior art and found that without any supervision and task-specific knowledge, our approach can perform comparably to supervised approaches specifically trained for a task. </details>
<details>	<summary>注释</summary>	Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/ </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 44、用于人脸超分辨率的边缘和身份保持网络
- [ ] Edge and Identity Preserving Network for Face Super-Resolution 
时间：2021年03月31日                         第一作者：Jonghyun Kim                       [链接](https://arxiv.org/abs/2008.11977).                     
<details>	<summary>注释</summary>	Neurocomputing'2021 DOI: 10.1016/j.neucom.2021.03.048 </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 43、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月31日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	The first two authors contributed equally to this work. Accepted to CVPR 2021 (camera-ready version) </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 42、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月30日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 41、传递学习：探索盲超分辨退化的传递性
- [ ] Transitive Learning: Exploring the Transitivity of Degradations for Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2103.15290).                     
## 摘要：现有的盲超分辨率（SR）方法由于对数据或模型的迭代估计和校正的依赖性，通常耗时且效率较低。针对这一问题，本文提出了一种基于端到端网络的盲随机共振传递学习方法。首先，我们分析并论证了退化的传递性，包括广泛使用的加法退化和卷积退化。在此基础上，提出了一种新的传递学习方法，通过自适应地推导传递变换函数来求解未知退化问题，而不需要任何迭代操作。具体而言，端到端TLSR网络由传递度（DoT）估计网络、齐次特征提取网络和传递学习模块组成。对盲SR任务的定量和定性评价表明，与现有的盲SR方法相比，本文提出的TLSR具有更好的性能和更少的时间消耗。代码可在https://github.com/YuanfeiHuang/TLSR。
<details>	<summary>英文摘要</summary>	Being extremely dependent on the iterative estimation and correction of data or models, the existing blind super-resolution (SR) methods are generally time-consuming and less effective. To address it, this paper proposes a transitive learning method for blind SR using an end-to-end network without any additional iterations in inference. To begin with, we analyze and demonstrate the transitivity of degradations, including the widely used additive and convolutive degradations. We then propose a novel Transitive Learning method for blind Super-Resolution on transitive degradations (TLSR), by adaptively inferring a transitive transformation function to solve the unknown degradations without any iterative operations in inference. Specifically, the end-to-end TLSR network consists of a degree of transitivity (DoT) estimation network, a homogeneous feature extraction network, and a transitive learning module. Quantitative and qualitative evaluations on blind SR tasks demonstrate that the proposed TLSR achieves superior performance and consumes less time against the state-of-the-art blind SR methods. The code is available at https://github.com/YuanfeiHuang/TLSR. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 40、高细节图像超分辨率的最佳伙伴
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年03月29日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
## 摘要：我们考虑了单图像超分辨率（SISR）问题，即基于低分辨率（LR）输入生成高分辨率（HR）图像。近年来，生成性对抗网络（generativediscountarialnetworks，GANs）开始流行于制造幻觉。沿着这条路线的大多数方法依赖于预定义的单个LR-single-HR映射，这对于SISR任务来说不够灵活。而且，甘生成的虚假细节往往会破坏整个图像的真实感。我们通过为丰富的细节SISR提出bestbuddygan（bebygan）来解决这些问题。放松了不可变的一对一约束，使得估计出的补丁在训练过程中动态地寻求最佳监督，有利于产生更合理的细节。此外，我们提出了一种区域感知的对抗式学习策略，使我们的模型能够自适应地生成纹理区域的细节。大量的实验证明了我们方法的有效性。同时还构建了一个超高分辨率4K数据集，为今后的超分辨率研究提供了便利。
<details>	<summary>英文摘要</summary>	We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the immutable one-to-one constraint, we allow the estimated patches to dynamically seek the best supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 39、全知视频超分辨率
- [ ] Omniscient Video Super-Resolution 
时间：2021年03月29日                         第一作者：Peng Yi                        [链接](https://arxiv.org/abs/2103.15683).                     
## 摘要：最新的视频超分辨率（SR）方法要么采用迭代的方式来处理来自时间滑动窗口的低分辨率（LR）帧，要么利用先前估计的SR输出来帮助循环地重构当前帧。一些研究试图将这两种结构结合起来形成一个混合框架，但未能充分发挥其作用。在本文中，我们提出了一个全知的框架，不仅可以利用前面的SR输出，还可以利用现在和将来的SR输出。全知框架更具通用性，因为迭代框架、循环框架和混合框架可以看作它的特例。提出的全知框架使得生成器比其他框架下的生成器表现得更好。在公共数据集上的大量实验表明，该方法在客观度量、主观视觉效果和复杂度方面均优于现有的方法。我们的代码将会公开。
<details>	<summary>英文摘要</summary>	Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity. Our code will be made public. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 38、交叉MPI：利用多平面图像实现图像超分辨率的交叉尺度立体成像
- [ ] Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images 
时间：2021年03月29日                         第一作者：Yuemei Zhou                       [链接](https://arxiv.org/abs/2011.14631).                     
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 37、基于隐式场景表示的现场场景标注与理解
- [ ] In-Place Scene Labelling and Understanding with Implicit Scene Representation 
时间：2021年03月29日                         第一作者：Shuaifeng Zhi                       [链接](https://arxiv.org/abs/2103.15875).                     
## 摘要：语义标注与几何和辐射重建高度相关，因为形状和外观相似的场景实体更可能来自相似的类。最近的隐式神经重建技术是有吸引力的，因为他们不需要事先训练数据，但同样的完全自我监督的方法是不可能的语义，因为标签是人类定义的属性。我们扩展了神经辐射场（NeRF）技术，将语义与外观和几何信息进行联合编码，从而使用少量的特定场景的就地标注就可以得到完整、准确的二维语义标注。NeRF内在的多视图一致性和平滑性使得稀疏标签能够有效地传播，从而有利于语义。我们展示了这种方法的好处，当标签是稀疏或非常嘈杂的房间规模的场景。在视觉语义映射系统中，我们展示了它在各种有趣的应用中的优势，如高效的场景标注工具、新颖的语义视图合成、标签去噪、超分辨率、标签插值和多视图语义标签融合。
<details>	<summary>英文摘要</summary>	Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems. </details>
<details>	<summary>注释</summary>	Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/ </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 36、基于流的核先验算法及其在盲超分辨中的应用
- [ ] Flow-based Kernel Prior with Application to Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2103.15977).                     
## 摘要：核估计通常是盲图像超分辨率（SR）的关键问题之一。最近，Double-DIP提出通过网络结构对核进行建模，KernelGAN则采用深度线性网络和一些正则化损失来约束核空间。然而，他们没有充分利用一般的SR核假设，即各向异性高斯核对图像SR是足够的。为了解决这个问题，本文提出了一种基于归一化流的核先验（FKP）核建模方法。通过学习各向异性高斯核分布和可处理的潜在分布之间的可逆映射，FKP可以很容易地代替双倾角和KernelGAN的核建模模块。具体地说，FKP在隐空间而不是网络参数空间中对核进行优化，从而生成合理的核初始化，遍历学习到的核流形，提高优化的稳定性。在合成图像和真实图像上的大量实验表明，所提出的FKP算法可以在较少的参数、运行时间和内存使用的情况下显著提高核估计精度，从而得到最新的盲SR结果。
<details>	<summary>英文摘要</summary>	Kernel estimation is generally one of the key problems for blind image super-resolution (SR). Recently, Double-DIP proposes to model the kernel via a network architecture prior, while KernelGAN employs the deep linear network and several regularization losses to constrain the kernel space. However, they fail to fully exploit the general SR kernel assumption that anisotropic Gaussian kernels are sufficient for image SR. To address this issue, this paper proposes a normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an invertible mapping between the anisotropic Gaussian kernel distribution and a tractable latent distribution, FKP can be easily used to replace the kernel modeling modules of Double-DIP and KernelGAN. Specifically, FKP optimizes the kernel in the latent space rather than the network parameter space, which allows it to generate reasonable kernel initialization, traverse the learned kernel manifold and improve the optimization stability. Extensive experiments on synthetic and real-world images demonstrate that the proposed FKP can significantly improve the kernel estimation accuracy with less parameters, runtime and memory usage, leading to state-of-the-art blind SR results. </details>
<details>	<summary>注释</summary>	Accepted by CVPR2021. Code: https://github.com/JingyunLiang/FKP </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 35、去噪去噪超分辨率流水线的再思考
- [ ] Rethinking the Pipeline of Demosaicing, Denoising and Super-Resolution 
时间：2021年03月29日                         第一作者：Guocheng Qian                        [链接](https://arxiv.org/abs/1905.02538).                     
<details>	<summary>注释</summary>	Code is available at: https://github.com/guochengqian/TENet </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 34、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年03月28日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 33、D2C-SR：一种从散度到收敛的图像超分辨率方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Image Super-Resolution 
时间：2021年03月26日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
## 摘要：本文提出了一种新的图像超分辨率（SR）框架D2C-SR。作为一个不适定问题，超分辨率相关任务的关键挑战是对于给定的低分辨率输入可以有多个预测。大多数经典的方法和早期的基于深度学习的方法忽略了这一基本事实，将这个问题建模为一个确定性的过程，这往往导致不满意的结果。受最近SRFlow等工作的启发，我们采用半概率的方法解决了这一问题，并提出了一种两阶段流水线：一个发散阶段用于学习离散形式的高分辨率输出的分布，然后一个收敛阶段用于将学习到的预测融合成最终的输出。更具体地说，我们提出了一种基于树结构的深度网络，其中每个分支被设计来学习可能的高分辨率预测。在发散阶段，对每个分支分别进行训练以拟合地面真值，并用三重损失来增强发散分支的输出。随后，我们添加一个保险丝模块来组合多个预测，因为第一阶段的输出可能是次优的。可以训练引信模块以端到端的方式将w.r.t收敛到最终的高分辨率图像。我们对几个基准进行了评估，包括一个新提出的具有8倍放大因子的数据集。我们的实验表明，D2C-SR可以在峰值信噪比和SSIM上实现最先进的性能，并且计算量显著减少。
<details>	<summary>英文摘要</summary>	In this paper, we present D2C-SR, a novel framework for the task of image super-resolution(SR). As an ill-posed problem, the key challenge for super-resolution related tasks is there can be multiple predictions for a given low-resolution input. Most classical methods and early deep learning based approaches ignored this fundamental fact and modeled this problem as a deterministic processing which often lead to unsatisfactory results. Inspired by recent works like SRFlow, we tackle this problem in a semi-probabilistic manner and propose a two-stage pipeline: a divergence stage is used to learn the distribution of underlying high-resolution outputs in a discrete form, and a convergence stage is followed to fuse the learned predictions into a final output. More specifically, we propose a tree-based structure deep network, where each branch is designed to learn a possible high-resolution prediction. At the divergence stage, each branch is trained separately to fit ground truth, and a triple loss is used to enforce the outputs from different branches divergent. Subsequently, we add a fuse module to combine the multiple predictions as the outputs from the first stage can be sub-optimal. The fuse module can be trained to converge w.r.t the final high-resolution image in an end-to-end manner. We conduct evaluations on several benchmarks, including a new proposed dataset with 8x upscaling factor. Our experiments demonstrate that D2C-SR can achieve state-of-the-art performance on PSNR and SSIM, with a significantly less computational cost. </details>
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 32、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月25日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
## 摘要：近五年来，深度卷积神经网络（CNNs）被广泛应用于低层视觉。根据不同应用的特点，设计合适的CNN结构。然而，定制的体系结构通过对所有像素点的等价处理来聚集不同的特征，从而提高了应用的性能，忽略了局部功率像素点的影响，导致训练效率低下。在本文中，我们提出了一个非对称CNN（ACNet）包括一个非对称块（AB），一个mem？图像超分辨率增强块（MEB）和高频特征增强块（HFFEB）。该算法利用一维非对称卷积，在水平方向和垂直方向上增强平方卷积核，以增强局部显著特征对SISR的影响。MEB通过残差学习（RL）技术融合AB的所有分层低频特征，以解决长期依赖问题，并转换得到的低频fea？转换成高频特性。HFFEB利用低频和高频特征来获得更健壮的超分辨率特征，并解决过多的特征增强问题。广告？另外，它还负责重建高分辨率（HR）图像。大量实验表明，该网络能有效地解决单图像超分辨率（SISR）、盲SISR和盲噪声的盲SISR问题。ACNet的代码如所示https://github.com/hellloxiaotian/ACNet。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks (CNNs) have been widely applied for low-level vision over the past five years. According to nature of different applications, designing appropriate CNN architectures is developed. However, customized architectures gather different features via treating all pixel points as equal to improve the performance of given application, which ignores the effects of local power pixel points and results in low training efficiency. In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric block (AB), a mem?ory enhancement block (MEB) and a high-frequency feature enhancement block (HFFEB) for image super-resolution. The AB utilizes one-dimensional asymmetric convolutions to intensify the square convolution kernels in horizontal and vertical directions for promoting the influences of local salient features for SISR. The MEB fuses all hierarchical low-frequency features from the AB via residual learning (RL) technique to resolve the long-term dependency problem and transforms obtained low-frequency fea?tures into high-frequency features. The HFFEB exploits low- and high-frequency features to obtain more robust super-resolution features and address excessive feature enhancement problem. Ad?ditionally, it also takes charge of reconstructing a high-resolution (HR) image. Extensive experiments show that our ACNet can effectively address single image super-resolution (SISR), blind SISR and blind SISR of blind noise problems. The code of the ACNet is shown at https://github.com/hellloxiaotian/ACNet. </details>
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 31、JDSR-GAN：构建联合协作的蒙面超分辨率学习网络
- [ ] JDSR-GAN: Constructing A Joint and Collaborative Learning Network for Masked Face Super-Resolution 
时间：2021年03月25日                         第一作者：Guangwei Gao                       [链接](https://arxiv.org/abs/2103.13676).                     
## 摘要：随着预防COVID-19病毒的重要性日益增强，在大多数视频监控场景中获得的人脸图像都是低分辨率的。然而，以往的人脸超分辨率算法大多不能在一个模型中同时处理两个任务。本文将遮罩遮挡视为图像噪声，构建了一个联合协作学习网络JDSR-GAN，用于遮罩人脸的超分辨率识别。给定一幅以掩模为输入的低质量人脸图像，由去噪模块和超分辨率模块组成的发生器的作用是获取高质量的高分辨率人脸图像。鉴别器利用了一些精心设计的损失函数来保证恢复的人脸图像的质量。此外，我们将身份信息和注意机制融入到我们的网络中，以实现可行的相关特征表达和信息性特征学习。通过联合进行去噪和人脸超分辨率处理，这两个任务可以相互补充，获得良好的性能。大量的定性和定量结果表明，我们提出的JDSR-GAN方法优于一些分别执行前两个任务的可比较方法。
<details>	<summary>英文摘要</summary>	With the growing importance of preventing the COVID-19 virus, face images obtained in most video surveillance scenarios are low resolution with mask simultaneously. However, most of the previous face super-resolution solutions can not handle both tasks in one model. In this work, we treat the mask occlusion as image noise and construct a joint and collaborative learning network, called JDSR-GAN, for the masked face super-resolution task. Given a low-quality face image with the mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some comparable methods which perform the previous two tasks separately. </details>
<details>	<summary>注释</summary>	24 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 30、噪声数据的多帧超分辨率分析
- [ ] Multi-frame Super-resolution from Noisy Data 
时间：2021年03月25日                         第一作者：Kireeti Bodduna                        [链接](https://arxiv.org/abs/2103.13778).                     
## 摘要：由于问题的病态性，从低分辨率数据中获得高分辨率图像在算法上具有挑战性。到目前为止，这类问题几乎没有得到解决，现有的一些方法使用了简单的正则化方法。我们证明了两种基于各向异性扩散思想的自适应正则化方法的有效性：除了评估经典的边缘增强各向异性扩散正则化方法外，我们还引入了一种新的非局部正则化方法。这被称为部门扩散。我们将其与经典超分辨率观测模型的所有六种变体结合起来，这些变体是由其三种扭曲、模糊和下采样算子的排列产生的。令人惊讶的是，在实际相关的噪声场景中进行的评估产生的排名与我们之前工作（SSVM 2017）中在无噪声环境中的排名不同。
<details>	<summary>英文摘要</summary>	Obtaining high resolution images from low resolution data with clipped noise is algorithmically challenging due to the ill-posed nature of the problem. So far such problems have hardly been tackled, and the few existing approaches use simplistic regularisers. We show the usefulness of two adaptive regularisers based on anisotropic diffusion ideas: Apart from evaluating the classical edge-enhancing anisotropic diffusion regulariser, we introduce a novel non-local one with one-sided differences and superior performance. It is termed sector diffusion. We combine it with all six variants of the classical super-resolution observational model that arise from permutations of its three operators for warping, blurring, and downsampling. Surprisingly, the evaluation in a practically relevant noisy scenario produces a different ranking than the one in the noise-free setting in our previous work (SSVM 2017). </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 29、一种实用的深盲图像超分辨率退化模型的设计
- [ ] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution 
时间：2021年03月25日                         第一作者：Kai Zhang                       [链接](https://arxiv.org/abs/2103.14006).                     
## 摘要：人们普遍认为，如果假设的退化模型与真实图像中的退化模型相背离，单图像超分辨率（SISR）方法将无法取得很好的效果。虽然有几种退化模型考虑了模糊等其他因素，但它们仍然不能有效地覆盖真实图像的各种退化。针对这一问题，本文提出了一种更为复杂但实用的退化模型，该模型由随机混洗模糊、下采样和噪声退化三部分组成。具体地说，模糊由两个具有各向同性和各向异性高斯核的卷积来逼近；下采样从最近点、双线性和双三次插值中随机选择；噪声由不同噪声级的高斯噪声叠加而成，采用不同质量因子的JPEG压缩，通过逆前向摄像机图像信号处理（ISP）流水线模型和原始图像噪声模型生成处理后的摄像机传感器噪声。为了验证新退化模型的有效性，我们训练了一个深度盲ESRGAN超分解器，并将其应用于不同退化程度的合成图像和真实图像的超分辨。实验结果表明，新的退化模型有助于提高深超旋转变压器的实用性，为实际SISR应用提供了一种强有力的替代方案。
<details>	<summary>英文摘要</summary>	It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ESRGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications. </details>
<details>	<summary>注释</summary>	Code: https://github.com/cszn/BSRGAN </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 28、基于物理激励下采样核的内窥镜零炮超分辨
- [ ] Zero-shot super-resolution with a physically-motivated downsampling kernel for endomicroscopy 
时间：2021年03月25日                         第一作者：Agnieszka Barbara Szczotka                       [链接](https://arxiv.org/abs/2103.14015).                     
## 摘要：随着卷积神经网络（CNNs）的发展，超分辨率（SR）方法得到了长足的发展。CNNs已被成功应用于提高内镜成像质量。然而，内窥镜下SR研究的固有局限性仍然是缺乏地面真实高分辨率（HR）图像，通常用于监督训练和基于参考的图像质量评估（IQA）。因此，替代方法，如无监督SR正在探索中。为了解决非参考图像质量改善的需要，我们设计了一种新的零炮超分辨率（ZSSR）方法，该方法仅依赖于内窥镜数据，不需要地面真实图像，而是以自我监督的方式进行处理。我们根据内窥镜的特殊性定制了建议的管道，引入了两种方法：一种物理激励的Voronoi降尺度核，用于解释内窥镜基于不规则纤维的采样模式和真实的噪声模式。我们还利用视频序列来开发一个图像序列，以提高自监督零拍图像的质量。我们进行了烧蚀研究，以评估我们在缩小核尺度和噪声模拟方面的贡献。我们在合成数据和原始数据上验证了我们的方法。合成实验用基于参考的IQA进行评估，而我们对原始图像的结果则在由专家和非专家观察者进行的用户研究中进行评估。结果表明，ZSSR重建的图像质量优于基线方法。与有监督的单幅图像SR相比，ZSSR具有很强的竞争力，尤其是作为专家们首选的重建技术。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) methods have seen significant advances thanks to the development of convolutional neural networks (CNNs). CNNs have been successfully employed to improve the quality of endomicroscopy imaging. Yet, the inherent limitation of research on SR in endomicroscopy remains the lack of ground truth high-resolution (HR) images, commonly used for both supervised training and reference-based image quality assessment (IQA). Therefore, alternative methods, such as unsupervised SR are being explored. To address the need for non-reference image quality improvement, we designed a novel zero-shot super-resolution (ZSSR) approach that relies only on the endomicroscopy data to be processed in a self-supervised manner without the need for ground-truth HR images. We tailored the proposed pipeline to the idiosyncrasies of endomicroscopy by introducing both: a physically-motivated Voronoi downscaling kernel accounting for the endomicroscope's irregular fibre-based sampling pattern, and realistic noise patterns. We also took advantage of video sequences to exploit a sequence of images for self-supervised zero-shot image quality improvement. We run ablation studies to assess our contribution in regards to the downscaling kernel and noise simulation. We validate our methodology on both synthetic and original data. Synthetic experiments were assessed with reference-based IQA, while our results for original images were evaluated in a user study conducted with both expert and non-expert observers. The results demonstrated superior performance in image quality of ZSSR reconstructions in comparison to the baseline method. The ZSSR is also competitive when compared to supervised single-image SR, especially being the preferred reconstruction technique by experts. </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 27、基于跨任务知识转移的单深度超分辨率场景结构引导学习
- [ ] Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for Single Depth Super-Resolution 
时间：2021年03月24日                         第一作者：Baoli Sun                       [链接](https://arxiv.org/abs/2103.12955).                     
## 摘要：现有的颜色引导深度超分辨率（DSR）方法需要成对的RGB-D数据作为训练样本，利用RGB图像的几何相似性作为结构引导来恢复退化的深度图。然而，在实际测试环境中，成对数据的收集可能有限或昂贵。因此，我们第一次探索在训练阶段学习跨模态知识，在训练阶段RGB和深度模态都可用，但在目标数据集上测试，只有单一的深度模态存在。我们的核心思想是在不改变网络结构的前提下，将场景结构制导知识从RGB模态提取到单个DSR任务。具体地说，我们构造了一个以RGB图像为输入的辅助深度估计（DE）任务来估计深度图，并协同训练DSR任务和DE任务来提高DSR的性能。在此基础上，提出了一个跨任务交互模块来实现双边跨任务知识转移。首先，我们设计了一个跨任务的提炼方案，鼓励DSR和DE网络以师生角色交换的方式相互学习。然后，我们提出了一个结构预测（SP）任务，该任务提供额外的结构正则化，以帮助DSR和DE网络学习更多信息的结构表示，以便进行深度恢复。大量实验表明，与其他DSR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 26、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年03月24日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
## 摘要：近年来，采用深度复杂卷积神经网络结构的单图像超分辨率（SISR）方法取得了良好的效果。然而，这些方法以较高的内存消耗为代价来提高性能，难以应用于存储和计算资源有限的移动设备。为了解决这个问题，我们提出了一个轻量级的多尺度特征交互网络（MSFIN）。对于轻量级SISR，MSFIN扩展了接收域，充分利用了低分辨率观测图像的信息特征，这些特征来自不同的尺度和交互连接。此外，我们还设计了一个轻量级的循环剩余信道注意块（RRCAB），使得网络能够在充分轻量级的同时受益于信道注意机制。在一些基准上的大量实验已经证实，我们提出的MSFIN可以通过更轻量化的模型实现与现有技术相当的性能。
<details>	<summary>英文摘要</summary>	Recently, the single image super-resolution (SISR) approaches with deep and complex convolutional neural network structures have achieved promising performance. However, those methods improve the performance at the cost of higher memory consumption, which is difficult to be applied for some mobile devices with limited storage and computing resources. To solve this problem, we present a lightweight multi-scale feature interaction network (MSFIN). For lightweight SISR, MSFIN expands the receptive field and adequately exploits the informative features of the low-resolution observed images from various scales and interactive connections. In addition, we design a lightweight recurrent residual channel attention block (RRCAB) so that the network can benefit from the channel attention mechanism while being sufficiently lightweight. Extensive experiments on some benchmarks have confirmed that our proposed MSFIN can achieve comparable performance against the state-of-the-arts with a more lightweight model. </details>
<details>	<summary>注释</summary>	Accepted by ICME2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 25、UltraSR：空间编码是基于隐式图像函数的任意尺度超分辨率的关键
- [ ] UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution 
时间：2021年03月23日                         第一作者：Xingqian Xu                       [链接](https://arxiv.org/abs/2103.12716).                     
## 摘要：NeRF和其他相关隐式神经表示方法的成功为连续图像表示开辟了一条新的途径，即不再需要从存储的离散二维阵列中查找像素值，而是可以从连续空间域上的神经网络模型中推断像素值。尽管最近的研究表明，这种新的方法在任意尺度的超分辨率任务中都能取得很好的效果，但由于高频纹理的错误预测，放大后的图像往往会出现结构性失真。在这项工作中，我们提出了一种简单而有效的基于隐式图像函数的新网络设计方法UltraSR，其中空间坐标和周期编码与隐式神经表示深度结合。我们通过大量的实验和研究表明，空间编码确实是下一阶段高精度隐式图像函数的关键。与以前最先进的方法相比，我们的UltraSR在DIV2K基准上设置了所有超分辨率标度下的最新性能。UltraSR在其他标准基准数据集上也取得了优异的性能，在几乎所有的实验中都优于以前的工作。我们的代码将在https://github.com/SHI-Labs/UltraSR-arbitral-Scale-Super-Resolution。
<details>	<summary>英文摘要</summary>	The recent success of NeRF and other related implicit neural representation methods has opened a new path for continuous image representation, where pixel values no longer need to be looked up from stored discrete 2D arrays but can be inferred from neural network models on a continuous spatial domain. Although the recent work LIIF has demonstrated that such novel approach can achieve good performance on the arbitrary-scale super-resolution task, their upscaled images frequently show structural distortion due to the faulty prediction on high-frequency textures. In this work, we propose UltraSR, a simple yet effective new network design based on implicit image functions in which spatial coordinates and periodic encoding are deeply integrated with the implicit neural representation. We show that spatial encoding is indeed a missing key towards the next-stage high-accuracy implicit image function through extensive experiments and ablation studies. Our UltraSR sets new state-of-the-art performance on the DIV2K benchmark under all super-resolution scales comparing to previous state-of-the-art methods. UltraSR also achieves superior performance on other standard benchmark datasets in which it outperforms prior works in almost all experiments. Our code will be released at https://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution. </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 24、双子网多级通信上采样大运动视频超分辨率
- [ ] Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling 
时间：2021年03月22日                         第一作者：Hongying Liu                       [链接](https://arxiv.org/abs/2103.11744).                     
## 摘要：视频超分辨率（VSR）的目标是恢复低分辨率（LR）的视频，并将其提高到更高的分辨率（HR）。由于视频任务的特点，在VSR算法中，对帧间运动信息的关注、总结和利用是非常重要的。特别是当视频包含大运动时，传统的方法容易产生非相干的结果或伪影。提出了一种新的双子网多级通信上采样深度神经网络（DSMC），用于大运动视频的超分辨率处理。设计了一个新的三维卷积U形残差密集网络（U3D-RDN）模块，用于精细隐式运动估计和运动补偿（MEMC）以及粗空间特征提取。提出了一种新的多级通信上采样（MSCU）模块，充分利用上采样的中间结果指导VSR。此外，本文还设计了一种新的双子网来辅助DSMC的训练，它的双子网损失有助于减少解空间和提高泛化能力。实验结果表明，与现有的方法相比，该方法在大运动视频上具有更好的性能。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月23日</details>

# 23、一种新的单图像超分辨率公共Alsat-2B数据集
- [ ] A new public Alsat-2B dataset for single-image super-resolution 
时间：2021年03月21日                         第一作者：Achraf Djerida                       [链接](https://arxiv.org/abs/2103.12547).                     
## 摘要：目前，当有可靠的训练数据集可用时，深度学习方法是图像超分辨率的主要解决方案。然而，对于遥感基准来说，获取高空间分辨率的图像是非常昂贵的。大多数超分辨率方法都采用下采样技术来模拟低分辨率和高分辨率的空间对，并构造训练样本。为了解决这一问题，本文提出了一种新的低分辨率和高分辨率（分别为10m和2.5m）的公共遥感数据集（Alsat2B），用于单幅图像的超分辨率处理。通过平移锐化得到高分辨率图像。此外，基于通用准则对数据集上一些超分辨率方法的性能进行了评估。结果表明，该方法是有前途的，并突出了数据集的挑战，这表明需要先进的方法来掌握低分辨率和高分辨率斑块之间的关系。
<details>	<summary>英文摘要</summary>	Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches. </details>
<details>	<summary>注释</summary>	This paper has been Accepted for publication in the International Geoscience and Remote Sensing Symposium (IGARSS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 22、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年03月19日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
## 摘要：高光谱图像具有较窄的光谱波段，能够获取丰富的光谱信息，适合于许多计算机视觉任务。HSI的一个基本限制是它的低空间分辨率，最近一些关于超分辨率（SR）的工作被提出来解决这个问题。然而，由于HSI摄像机的多样性，不同的摄像机捕捉到的图像具有不同的光谱响应函数和总通道数。现有的HSI数据集通常很小，因此不足以建模。提出了一种基于元学习的超分辨率（MLSR）模型，该模型可以在任意多个输入波段的峰值波长处获取HSI图像，并生成任意多个输出波段的峰值波长的超分辨率HSI。我们通过对NTIRE2020和ICVL数据集的波段进行采样，人工创建子数据集，模拟交叉数据集设置，并对其进行谱内插和外推的HSI SR。我们为所有子数据集训练单个MLSR模型，并为每个子数据集训练专用的基线模型。结果表明，与现有的HSI-SR方法相比，该模型具有相同的水平或更好的性能。
<details>	<summary>英文摘要</summary>	Hyperspectral images (HSIs) with narrow spectral bands can capture rich spectral information, making them suitable for many computer vision tasks. One of the fundamental limitations of HSI is its low spatial resolution, and several recent works on super-resolution(SR) have been proposed to tackle this challenge. However, due to HSI cameras' diversity, different cameras capture images with different spectral response functions and the number of total channels. The existing HSI datasets are usually small and consequently insufficient for modeling. We propose a Meta-Learning-Based Super-Resolution(MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate super-resolved HSIs with an arbitrary number of output bands' peak wavelengths. We artificially create sub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate the cross-dataset settings and perform HSI SR with spectral interpolation and extrapolation on them. We train a single MLSR model for all sub-datasets and train dedicated baseline models for each sub-dataset. The results show the proposed model has the same level or better performance compared to the-state-of-the-art HSI SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 21、视频超分辨率的自监督自适应算法
- [ ] Self-Supervised Adaptation for Video Super-Resolution 
时间：2021年03月18日                         第一作者：Jinsu Yoo                        [链接](https://arxiv.org/abs/2103.10081).                     
## 摘要：近年来，单图像超分辨率（single-image super-resolution，SISR）网络通过利用输入数据中的信息和大量外部数据集，使网络参数适应特定的输入图像，取得了良好的效果。然而，这些自监督SISR方法在视频处理中的扩展还有待研究。因此，我们提出了一种新的学习算法，使得传统的视频超分辨率（VSR）网络能够在不使用地面真实数据集的情况下调整参数来测试视频帧。通过利用空间和时间上的许多自相似块，我们提高了完全预训练VSR网络的性能，并产生了时间一致的视频帧。此外，我们提出了一种测试时知识提取技术，以较少的硬件资源加快了自适应速度。在我们的实验中，我们证明了我们的新学习算法可以微调最先进的VSR网络，并在大量的基准数据集上显著提高性能。
<details>	<summary>英文摘要</summary>	Recent single-image super-resolution (SISR) networks, which can adapt their network parameters to specific input images, have shown promising results by exploiting the information available within the input data as well as large external datasets. However, the extension of these self-supervised SISR approaches to video handling has yet to be studied. Thus, we present a new learning algorithm that allows conventional video super-resolution (VSR) networks to adapt their parameters to test video frames without using the ground-truth datasets. By utilizing many self-similar patches across space and time, we improve the performance of fully pre-trained VSR networks and produce temporally consistent video frames. Moreover, we present a test-time knowledge distillation technique that accelerates the adaptation speed with less hardware resources. In our experiments, we demonstrate that our novel learning algorithm can fine-tune state-of-the-art VSR networks and substantially elevate performance on numerous benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年03月19日</details>

# 20、结构化输出依赖建模的一般感知损失
- [ ] Generic Perceptual Loss for Modeling Structured Output Dependencies 
时间：2021年03月18日                         第一作者：Yifan Liu                       [链接](https://arxiv.org/abs/2103.10571).                     
## 摘要：感知损失作为一个有效的损失项被广泛应用于图像合成任务中，包括图像超分辨率、风格转换等。人们认为，成功的关键在于从经过大量图像预训练的CNNs中提取高层次的感知特征表示。这里我们揭示了，重要的是网络结构，而不是训练的权重。在没有任何学习的情况下，深层网络的结构就足以利用多层cnn捕获多个层次的变量统计之间的依赖关系。这种洞察消除了预先训练和特定网络结构（通常是VGG）的要求，这些都是先前假设的感知损失，从而实现了更广泛的应用。为此，我们证明了一个随机加权的深度CNN可以用来模拟输出的结构化依赖关系。在语义分割、深度估计和实例分割等稠密的逐像素预测任务中，与单独使用逐像素丢失的基线相比，扩展的随机感知丢失方法得到了更好的结果。我们希望这种简单的，扩展的知觉损失可以作为一种通用的结构化输出损失，适用于大多数结构化输出学习任务。
<details>	<summary>英文摘要</summary>	The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution, and style transfer. It was believed that the success lies in the high-level perceptual feature representations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the network structure instead of the trained weights. Without any learning, the structure of a deep network is sufficient to capture the dependencies between multiple levels of variable statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously assumed for the perceptual loss, thus enabling a significantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks. </details>
<details>	<summary>注释</summary>	Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 19、视频流预测辅助帧超分辨率
- [ ] Prediction-assistant Frame Super-Resolution for Video Streaming 
时间：2021年03月17日                         第一作者：Wang Shen                       [链接](https://arxiv.org/abs/2103.09455).                     
## 摘要：在在线视频游戏、现场直播等实时应用中，视频帧的传输延迟是至关重要的，新帧的接收截止时间必须赶上帧的渲染时间。否则，系统会缓冲一段时间，用户会遇到冻结屏幕，导致用户体验不尽如人意。一种有效的方法是在较差的带宽条件下传输较低质量的帧，例如使用可伸缩视频编码。在本文中，我们提出在两种情况下使用有损帧来提高视频质量。首先，当当前帧在渲染截止时间之前太晚而无法接收时（即丢失），我们建议使用先前接收到的高分辨率图像来预测未来的帧。第二，当当前接收到的帧的质量较低（即有损）时，我们建议使用先前接收到的高分辨率帧来增强低质量的当前帧。对于第一种情况，我们提出了一个小而有效的视频帧预测网络。对于第二种情况，我们将视频预测网络改进为视频增强网络，将当前帧和前一帧关联起来，以恢复高质量的图像。大量的实验结果表明，我们的方法在有损视频流环境中的性能优于现有的算法。
<details>	<summary>英文摘要</summary>	Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment. </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 18、ShipSRDet：一种基于超分辨特征表示的端到端遥感舰船探测器
- [ ] ShipSRDet: An End-to-End Remote Sensing Ship Detector Using Super-Resolved Feature Representation 
时间：2021年03月17日                         第一作者：Shitian He                       [链接](https://arxiv.org/abs/2103.09699).                     
## 摘要：高分辨率遥感图像可以为船舶检测提供丰富的外观信息。虽然已有的一些方法采用图像超分辨率（SR）来提高检测性能，但它们将图像超分辨率和船舶检测视为两个独立的过程，忽略了这两个相关任务之间的内在一致性。在本文中，我们探讨了图像SR对船舶检测的潜在好处，并提出了一种端到端网络ShipSRDet。在我们的方法中，我们不仅将超分辨图像提供给检测器，而且将SR网络的中间特征与检测网络的中间特征结合起来。这样，SR网络提取的信息性特征表示就可以充分用于船舶检测。在HRSC数据集上的实验结果验证了该方法的有效性。我们的ShipSRDet可以从输入图像中恢复丢失的细节，并取得了良好的船舶检测性能。
<details>	<summary>英文摘要</summary>	High-resolution remote sensing images can provide abundant appearance information for ship detection. Although several existing methods use image super-resolution (SR) approaches to improve the detection performance, they consider image SR and ship detection as two separate processes and overlook the internal coherence between these two correlated tasks. In this paper, we explore the potential benefits introduced by image SR to ship detection, and propose an end-to-end network named ShipSRDet. In our method, we not only feed the super-resolved images to the detector but also integrate the intermediate features of the SR network with those of the detection network. In this way, the informative feature representation extracted by the SR network can be fully used for ship detection. Experimental results on the HRSC dataset validate the effectiveness of our method. Our ShipSRDet can recover the missing details from the input image and achieves promising ship detection performance. </details>
<details>	<summary>注释</summary>	Accepted to IGARSS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 17、基于单镜头样本的超分辨跨域人脸模型
- [ ] Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar 
时间：2021年03月16日                         第一作者：Peike Li                       [链接](https://arxiv.org/abs/2103.08863).                     
## 摘要：传统的人脸超分辨率方法通常假设检测低分辨率（LR）图像与训练图像位于同一个域。由于光照条件和成像硬件的不同，在许多实际场景中，训练图像和测试图像之间不可避免地会出现域间隙。忽略这些域间隙将导致较低的人脸超分辨率（FSR）性能。然而，如何将训练好的FSR模型有效地转移到目标域中还没有被研究。为了解决这个问题，我们开发了一个基于域感知金字塔的人脸超分辨率网络，命名为DAP-FSR网络。我们的DAP-FSR是第一次尝试利用目标域中的一对高分辨率（HR）和LR样本从目标域超分辨LR人脸。具体来说，我们的DAP-FSR首先使用编码器来提取输入LR人脸的多尺度潜在表示。考虑到只有一个目标域的例子可用，我们建议通过混合目标域面和源域面的潜在表示来扩充目标域数据，然后将混合表示提供给我们的DAP-FSR解码器。解码器将生成与目标域图像样式相似的新人脸图像。生成的HR面依次用于优化我们的解码器以减少域间隙。通过迭代更新潜在的表示和我们的解码器，我们的DAP-FSR将适应目标域，从而实现真实和高质量的上采样HR人脸。在三个新构建的基准上的大量实验验证了我们的DAP-FSR的有效性和优越的性能。
<details>	<summary>英文摘要</summary>	Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art. </details>
<details>	<summary>邮件日期</summary>	2021年03月17日</details>

# 16、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年03月15日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
## 摘要：基于深度学习的方法，特别是卷积神经网络（CNNs）已经成功地应用于单幅图像超分辨率（SISR）领域。为了获得更好的逼真度和视觉质量，现有的网络大多采用繁重的设计和大量的计算。然而，现代移动设备的计算资源有限，难以承受昂贵的成本。为此，本文提出了一种基于离散余弦变换（DCT）域的频率感知动态网络，将输入信号按其系数分为多个部分。在实际应用中，高频部分采用昂贵的运算，低频部分采用廉价的运算，以减轻计算负担。由于像素或图像块属于低频区域，包含的纹理细节相对较少，因此这种动态网络不会影响生成的超分辨率图像的质量。此外，我们将预测器嵌入到所提出的动态网路中，以端到端微调手工制作的频率感知遮罩。在基准SISR模型和数据集上进行的大量实验表明，频率感知动态网络可以应用于各种SISR神经结构，在视觉质量和计算复杂度之间获得更好的折衷。例如，我们可以在保持最先进的SISR性能的同时，将EDSR模型的失败率降低大约50\%$。
<details>	<summary>英文摘要</summary>	Deep learning based methods, especially convolutional neural networks (CNNs) have been successfully applied in the field of single image super-resolution (SISR). To obtain better fidelity and visual quality, most of existing networks are of heavy design with massive computation. However, the computation resources of modern mobile devices are limited, which cannot easily support the expensive cost. To this end, this paper explores a novel frequency-aware dynamic network for dividing the input into multiple parts according to its coefficients in the discrete cosine transform (DCT) domain. In practice, the high-frequency part will be processed using expensive operations and the lower-frequency part is assigned with cheap operations to relieve the computation burden. Since pixels or image patches belong to low-frequency areas contain relatively few textural details, this dynamic network will not affect the quality of resulting super-resolution images. In addition, we embed predictors into the proposed dynamic network to end-to-end fine-tune the handcrafted frequency-aware masks. Extensive experiments conducted on benchmark SISR models and datasets show that the frequency-aware dynamic network can be employed for various SISR neural architectures to obtain the better tradeoff between visual quality and computational complexity. For instance, we can reduce the FLOPs of EDSR model by approximate $50\%$ while preserving state-of-the-art SISR performance. </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 15、低分辨率图像和视频中的三维人体姿势、形状和纹理
- [ ] 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos 
时间：2021年03月11日                         第一作者：Xiangyu Xu                       [链接](https://arxiv.org/abs/2103.06498).                     
## 摘要：基于单眼图像的三维人体姿态和形状估计一直是计算机视觉领域的一个研究热点。现有的深度学习方法依赖于高分辨率的输入，然而，在视频监控和体育广播等许多场景中并不总是可用的。处理低分辨率图像的两种常用方法是对输入应用超分辨率技术，这可能会导致不愉快的伪影，或者只是针对每个分辨率训练一个模型，这在许多实际应用中是不切实际的。针对上述问题，本文提出了一种新的算法RSC-Net，该算法由分辨率感知网络、自监督损失和对比学习机制组成。该方法能够在单个模型上学习不同分辨率下的三维人体姿态和形状。自我监督缺失加强了输出的尺度一致性，而对比学习方案加强了深层特征的尺度一致性。我们表明，这两个新的损失提供鲁棒性学习时，在弱监督的方式。此外，我们扩展了RSC网络来处理低分辨率的视频，并将其应用于从低分辨率输入中重建具有纹理的三维行人。大量的实验表明，RSC网络在处理低分辨率图像时，可以取得比现有方法更好的效果。
<details>	<summary>英文摘要</summary>	3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2007.13666 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 14、一种基于学习的轴向超分辨率视图外推方法
- [ ] A learning-based view extrapolation method for axial super-resolution 
时间：2021年03月11日                         第一作者：Zhaolin Xiao                       [链接](https://arxiv.org/abs/2103.06510).                     
## 摘要：轴向光场分辨率是指通过重新聚焦来区分不同深度特征的能力。轴向再聚焦精度相当于两个可分辨的再聚焦平面在轴向上的最小距离。高再聚焦精度对于一些光场应用（如显微镜）来说是必不可少的。在这篇论文中，我们提出了一种基于学习的方法来外推新的观点从轴向体积剪切极平面图像（EPIs）。与经典成像中的扩展数值孔径（NA）一样，外推光场可以获得具有较浅景深（DOF）的重聚焦图像，从而获得更精确的重聚焦结果。最重要的是，该方法不需要精确的深度估计。对合成光场和真实光场的实验结果表明，该方法不仅适用于全光相机（尤其是1.0型全光相机）拍摄的基线较小的光场，而且适用于基线较大的光场。
<details>	<summary>英文摘要</summary>	Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines. </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 13、使用真实退化图像的超分辨率卫星硬件
- [ ] Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images 
时间：2021年03月10日                         第一作者：Jack White                       [链接](https://arxiv.org/abs/2103.06270).                     
## 摘要：现代深超分辨率（SR）网络已成为图像重建和增强的重要技术。然而，这些网络通常是在缺乏真实图像中典型的图像降质噪声的基准图像数据上训练和测试的。在这篇论文中，我们通过评估SR在重建真实退化卫星图像中的性能，来测试在真实遥感有效载荷中使用深度SR的可行性。我们证明了一种称为增强深超分辨率网络（EDSR）的先进SR技术，在没有特定领域的预训练的情况下，只要地面分辨率足够远，就可以在地面采样距离较短的图像上恢复编码的像素数据。然而，这种恢复因所选地理类型而异。我们的结果表明，定制训练有可能进一步改善高架图像的重建，新的卫星硬件应优先考虑光学性能，而不是最小化像素大小，因为深SR可以克服后者的不足，但不能克服前者。
<details>	<summary>英文摘要</summary>	Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former. </details>
<details>	<summary>注释</summary>	6 pages, 6 figures, for supplementary results, see https://smpetrie.github.io/superres/ ACM-class: I.4.3 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 12、高光谱图像超分辨率空间光谱反馈网络
- [ ] Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral Imagery 
时间：2021年03月07日                         第一作者：Enhai Liu                       [链接](https://arxiv.org/abs/2103.04354).                     
## 摘要：近年来，基于深度学习的单灰度/RGB图像超分辨率（SR）方法取得了很大的成功。然而，单幅高光谱图像的超分辨率处理存在两大障碍，限制了技术的发展。一是高光谱图像中高维复杂的光谱模式，使得空间信息和波段间的光谱信息难以同时获取。另一方面，高光谱训练样本的数目非常小，在训练深度神经网络时容易导致过度拟合。为了解决这些问题，本文提出了一种新的空间谱反馈网络（SSFN），利用全局谱带的高阶信息来细化局部谱带间的低阶表示。它不仅可以缓解高光谱数据高维性给特征提取带来的困难，而且可以使训练过程更加稳定。具体地说，我们在具有有限展开的RNN中使用隐藏状态来实现这种反馈方式。为了充分利用空间和光谱先验知识，设计了空间光谱反馈块（SSFB）来处理反馈连接并生成强大的高层表示。提出的SSFN具有早期预测能力，可以逐步重建最终的高分辨率高光谱图像。在三个基准数据集上的大量实验结果表明，与现有的方法相比，所提出的SSFN具有更好的性能。源代码位于https://github.com/tangzhenjie/SSFN。
<details>	<summary>英文摘要</summary>	Recently, single gray/RGB image super-resolution (SR) methods based on deep learning have achieved great success. However, there are two obstacles to limit technical development in the single hyperspectral image super-resolution. One is the high-dimensional and complex spectral patterns in hyperspectral image, which make it difficult to explore spatial information and spectral information among bands simultaneously. The other is that the number of available hyperspectral training samples is extremely small, which can easily lead to overfitting when training a deep neural network. To address these issues, in this paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to refine low-level representations among local spectral bands with high-level information from global spectral bands. It will not only alleviate the difficulty in feature extraction due to high dimensional of hyperspectral data, but also make the training process more stable. Specifically, we use hidden states in an RNN with finite unfoldings to achieve such feedback manner. To exploit the spatial and spectral prior, a Spatial-Spectral Feedback Block (SSFB) is designed to handle the feedback connections and generate powerful high-level representations. The proposed SSFN comes with a early predictions and can reconstruct the final high-resolution hyperspectral image step by step. Extensive experimental results on three benchmark datasets demonstrate that the proposed SSFN achieves superior performance in comparison with the state-of-the-art methods. The source code is available at https://github.com/tangzhenjie/SSFN. </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 11、基于深度学习的小数据集超分辨荧光显微镜
- [ ] Deep learning-based super-resolution fluorescence microscopy on small datasets 
时间：2021年03月07日                         第一作者：Varun Mannam                       [链接](https://arxiv.org/abs/2103.04989).                     
## 摘要：荧光显微术以微米级的分辨率显示生物有机体，使现代生物学有了巨大的发展。然而，由于衍射极限的限制，亚微米/纳米级的特征很难分辨。虽然各种超分辨率技术是为了达到纳米级的分辨率而发展起来的，但它们往往需要昂贵的光学装置或专门的荧光团。近年来，深度学习显示出减少技术障碍和从衍射限制图像获得超分辨率的潜力。为了得到准确的结果，传统的深度学习技术需要成千上万的图像作为训练数据集。由于荧光团的光漂白、光毒性和生物体内发生的动态过程，从生物样品中获取大量数据通常是不可行的。因此，利用小数据集实现基于深度学习的超分辨率具有挑战性。我们用一种新的基于卷积神经网络的方法来解决这一限制，这种方法成功地用小数据集训练并获得超分辨率图像。我们从15个不同的视场共采集了750张图像作为训练数据集来演示该技术。在每个视场中，采用超分辨率径向起伏方法生成单个目标图像。正如预期的那样，这个小数据集无法使用传统的超分辨率体系结构生成可用的模型。然而，使用新的方法，可以训练一个网络来从这个小数据集获得超分辨率图像。这种深度学习模型可应用于其他生物医学成像模式，如MRI和X射线成像，在这些模式中获取大型训练数据集是一项挑战。
<details>	<summary>英文摘要</summary>	Fluorescence microscopy has enabled a dramatic development in modern biology by visualizing biological organisms with micrometer scale resolution. However, due to the diffraction limit, sub-micron/nanometer features are difficult to resolve. While various super-resolution techniques are developed to achieve nanometer-scale resolution, they often either require expensive optical setup or specialized fluorophores. In recent years, deep learning has shown the potentials to reduce the technical barrier and obtain super-resolution from diffraction-limited images. For accurate results, conventional deep learning techniques require thousands of images as a training dataset. Obtaining large datasets from biological samples is not often feasible due to the photobleaching of fluorophores, phototoxicity, and dynamic processes occurring within the organism. Therefore, achieving deep learning-based super-resolution using small datasets is challenging. We address this limitation with a new convolutional neural network-based approach that is successfully trained with small datasets and achieves super-resolution images. We captured 750 images in total from 15 different field-of-views as the training dataset to demonstrate the technique. In each FOV, a single target image is generated using the super-resolution radial fluctuation method. As expected, this small dataset failed to produce a usable model using traditional super-resolution architecture. However, using the new approach, a network can be trained to achieve super-resolution images from this small dataset. This deep learning model can be applied to other biomedical imaging modalities such as MRI and X-ray imaging, where obtaining large training datasets is challenging. </details>
<details>	<summary>注释</summary>	SPIE Proceedings Volume 11650, Single Molecule Spectroscopy and Superresolution Imaging XIV; 116500O (2021) DOI: 10.1117/12.2578519 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 10、ClassSR：一种利用数据特征加速超分辨率网络的通用框架
- [ ] ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic 
时间：2021年03月06日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2103.04039).                     
## 摘要：我们的目标是在大图像（2K-8K）上加速超分辨率（SR）网络。在实际应用中，大图像通常被分解成小的子图像。在此基础上，我们发现不同的图像区域具有不同的恢复难度，可以由不同容量的网络进行处理。直观地说，平滑区域比复杂纹理更容易超级求解。为了利用这一特性，我们可以采用适当的SR网络对分解后的不同子图像进行处理。在此基础上，我们提出了一种新的解决方案管道——ClassSR，它将分类和SR结合在一个统一的框架中。特别地，它首先使用一个类模块将子图像按恢复难度分为不同的类，然后应用一个SR模块对不同的类进行SR。类模块是一个传统的分类网络，而SR模块是一个由待加速SR网络及其简化版本组成的网络容器。我们进一步引入了一种新的两损失分类方法——类别损失和平均损失来产生分类结果。联合训练后，大部分子图像将通过较小的网络，从而大大降低了计算量。实验表明，我们的ClassSR可以帮助大多数现有方法（如FSRCNN、CARN、SRResNet、RCAN）在DIV8K数据集上节省高达50%的失败率。这个通用框架也可以应用于其他低层次的视觉任务。
<details>	<summary>英文摘要</summary>	We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks. </details>
<details>	<summary>注释</summary>	CVPR2021 paper + supplementary file </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 9、用稀疏表示生成图像
- [ ] Generating Images with Sparse Representations 
时间：2021年03月05日                         第一作者：Charlie Nash                       [链接](https://arxiv.org/abs/2103.03841).                     
## 摘要：图像的高维性对基于似然的生成模型的结构和采样效率提出了挑战。以前的方法，例如VQ-VAE，使用深度自动编码器来获得紧凑的表示，作为基于似然模型的输入更为实用。我们提出了另一种方法，受JPEG等常见图像压缩方法的启发，将图像转换为量化的离散余弦变换（DCT）块，这些块稀疏地表示为DCT通道、空间位置和DCT系数三元组的序列。我们提出了一种基于变换器的自回归结构，该结构被训练成序列预测下一个元素在这些序列中的条件分布，并有效地扩展到高分辨率图像。在一系列的图像数据集上，我们证明了我们的方法可以生成高质量、多样的图像，并且样本度量分数可以与最先进的方法相竞争。此外，我们还表明，简单的修改，我们的方法产生有效的图像着色和超分辨率模型。
<details>	<summary>英文摘要</summary>	The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 8、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月05日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	Accepted to CVPR 2021. The first two authors contributed equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 7、真实世界的单图像超分辨率：简要回顾
- [ ] Real-World Single Image Super-Resolution: A Brief Review 
时间：2021年03月03日                         第一作者：Honggang Chen                       [链接](https://arxiv.org/abs/2103.02368).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是近几十年来图像处理领域的一个研究热点，其目的是通过低分辨率（low-resolution，LR）观测重建高分辨率（high-resolution，HR）图像。特别是基于深度学习的超分辨率（SR）方法已经引起了人们的广泛关注，极大地提高了合成数据的重建性能。最近的研究表明，对合成数据的模拟结果通常高估了对真实世界图像的超分辨能力。在这样的背景下，越来越多的研究者致力于研究真实感图像的随机共振方法。本文对现实世界中的单幅图像超分辨率（RSISR）技术进行了综述。更具体地说，本综述涵盖了RSISR的关键公共可用数据集和评估指标，以及四大类RSISR方法，即基于退化建模的RSISR、基于图像对的RSISR、基于领域翻译的RSISR和基于自学习的RSISR。在基准数据集上比较了代表性RSISR方法的重建质量和计算效率。此外，我们还讨论了RSISR面临的挑战和未来的研究课题。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR), which aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) observation, has been an active research topic in the area of image processing in recent decades. Particularly, deep learning-based super-resolution (SR) approaches have drawn much attention and have greatly improved the reconstruction performance on synthetic data. Recent studies show that simulation results on synthetic data usually overestimate the capacity to super-resolve real-world images. In this context, more and more researchers devote themselves to develop SR approaches for realistic images. This article aims to make a comprehensive review on real-world single image super-resolution (RSISR). More specifically, this review covers the critical publically available datasets and assessment metrics for RSISR, and four major categories of RSISR methods, namely the degradation modeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR, and self-learning-based RSISR. Comparisons are also made among representative RSISR methods on benchmark datasets, in terms of both reconstruction quality and computational efficiency. Besides, we discuss challenges and promising research topics on RSISR. </details>
<details>	<summary>注释</summary>	18 pages, 12 figure, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 6、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年02月25日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
## 摘要：近年来，大量的研究活动致力于视频增强，同时提高时间帧速率和空间分辨率。然而，现有的方法要么不能揭示时空信息之间的内在联系，要么在最终的时空分辨率的选择上缺乏灵活性。在这项工作中，我们提出了一个无约束的时空视频超分辨率网络，它可以有效地利用时空相关性来提高性能。此外，通过使用光流技术和广义像素混洗操作，它在调整时间帧速率和空间分辨率方面具有完全的自由度。实验结果表明，该方法不仅优于现有的算法，而且所需参数少，运行时间短。
<details>	<summary>英文摘要</summary>	Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 5、ShuffleUNet：基于深度学习的磁共振弥散加权成像的超分辨率
- [ ] ShuffleUNet: Super resolution of diffusion-weighted MRIs using deep learning 
时间：2021年02月25日                         第一作者：Soumick Chatterjee                       [链接](https://arxiv.org/abs/2102.12898).                     
## 摘要：弥散加权磁共振成像（DW-MRI）可用于表征神经组织的微观结构，例如通过纤维追踪以非侵入性方式描绘脑白质连接。高空间分辨率的磁共振成像（MRI）将在以更好的方式显示这些纤维束方面发挥重要作用。然而，获得这种分辨率的图像是以较长的扫描时间为代价的。由于患者的心理和身体状况，较长的扫描时间可能与运动伪影的增加有关。单图像超分辨率（Single-Image Super-Resolution，SISR）是一种通过深度学习从单个低分辨率（low-Resolution，LR）输入图像中获取高分辨率细节的技术，是本研究的重点。与插值技术或稀疏编码算法相比，深度学习算法从大数据集中提取先验知识，并从低分辨率图像中产生更优的MRI图像。本研究提出一种基于深度学习的超分辨技术，并将其应用于DW-MRI。从IXI数据集得到的图像被用作地面真值，并被人工降采样以模拟低分辨率图像。所提出的方法在统计学上比基线有了显著的改进，实现了0.913美元-0.045美元的SSIM。
<details>	<summary>英文摘要</summary>	Diffusion-weighted magnetic resonance imaging (DW-MRI) can be used to characterise the microstructure of the nervous tissue, e.g. to delineate brain white matter connections in a non-invasive manner via fibre tracking. Magnetic Resonance Imaging (MRI) in high spatial resolution would play an important role in visualising such fibre tracts in a superior manner. However, obtaining an image of such resolution comes at the expense of longer scan time. Longer scan time can be associated with the increase of motion artefacts, due to the patient's psychological and physical conditions. Single Image Super-Resolution (SISR), a technique aimed to obtain high-resolution (HR) details from one single low-resolution (LR) input image, achieved with Deep Learning, is the focus of this study. Compared to interpolation techniques or sparse-coding algorithms, deep learning extracts prior knowledge from big datasets and produces superior MRI images from the low-resolution counterparts. In this research, a deep learning based super-resolution technique is proposed and has been applied for DW-MRI. Images from the IXI dataset have been used as the ground-truth and were artificially downsampled to simulate the low-resolution images. The proposed method has shown statistically significant improvement over the baselines and achieved an SSIM of $0.913\pm0.045$. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 4、用于视频超分辨率的深展开网络
- [ ] Deep Unrolled Network for Video Super-Resolution 
时间：2021年02月23日                         第一作者：Benjamin Naoto Chiche                       [链接](https://arxiv.org/abs/2102.11720).                     
## 摘要：视频超分辨率（VSR）的目的是从相应的低分辨率（LR）图像中重建一系列高分辨率（HR）图像。传统上，VSR问题的求解是基于迭代算法，该算法可以利用图像形成的先验知识和运动的假设。然而，这些经典的方法很难将自然图像中的复杂统计信息结合起来。此外，VSR最近受益于深度学习（DL）算法带来的改进。这些技术可以有效地从大量的图像集合中学习空间模式。然而，他们没有融入一些有关图像形成模型的知识，这限制了他们的灵活性。为解决反问题而开发的展开优化算法允许将先验信息纳入深度学习体系结构。它们主要用于单个图像恢复任务。采用展开的神经网络结构可以带来以下好处。首先，这可能会提高超分辨率任务的性能。这样，神经网络就具有更好的可解释性。最后，这允许灵活地学习单个模型，以非盲目地处理多个退化。本文提出了一种新的基于展开优化技术的VSR神经网络，并对其性能进行了讨论。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to reconstruct a sequence of high-resolution (HR) images from their corresponding low-resolution (LR) versions. Traditionally, solving a VSR problem has been based on iterative algorithms that can exploit prior knowledge on image formation and assumptions on the motion. However, these classical methods struggle at incorporating complex statistics from natural images. Furthermore, VSR has recently benefited from the improvement brought by deep learning (DL) algorithms. These techniques can efficiently learn spatial patterns from large collections of images. Yet, they fail to incorporate some knowledge about the image formation model, which limits their flexibility. Unrolled optimization algorithms, developed for inverse problems resolution, allow to include prior information into deep learning architectures. They have been used mainly for single image restoration tasks. Adapting an unrolled neural network structure can bring the following benefits. First, this may increase performance of the super-resolution task. Then, this gives neural networks better interpretability. Finally, this allows flexibility in learning a single model to nonblindly deal with multiple degradations. In this paper, we propose a new VSR neural network based on unrolled optimization techniques and discuss its performance. </details>
<details>	<summary>注释</summary>	6 pages. 3 figures. Published in: 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA) DOI: 10.1109/IPTA50016.2020.9286636 </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 3、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月23日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 2、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月21日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
## 摘要：最近COVID-19的爆发促使研究人员在利用人工智能和深度学习的医学成像领域做出贡献。超分辨率（SR）在过去的几年中，利用深度学习方法取得了显著的效果。深度学习方法学习从低分辨率（LR）图像到相应的高分辨率（HR）图像的非线性映射的能力导致了SR在不同研究领域的引人注目的结果。本文提出了一种基于深度学习的切比切夫变换域图像超分辨率结构。这是通过将转换层通过定制的Tchebichef卷积层（$TCL$）集成到提议的体系结构中来实现的。TCL的作用是利用Tchebichef基函数将LR图像从空间域转换到正交变换域。使用称为逆切比雪夫卷积层（ITCL）的另一层实现上述变换的反演，该层将LR图像从变换域转换回空间域。研究表明，利用Tchebichef变换域进行超分辨率重建，利用了图像的高低频特征，简化了超分辨率重建任务。我们进一步引入转移学习方法来提高基于Covid的医学图像的质量。结果表明，我们的结构提高了COVID-19的X线和CT图像质量，提供了更好的图像质量，有助于临床诊断。与使用较少可训练参数的大多数深度学习方法相比，使用所提出的切比雪夫变换域超分辨率（TTDSR）结构获得的实验结果提供了具有竞争力的结果。
<details>	<summary>英文摘要</summary>	The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters. </details>
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 1、用于原子分辨率图像的高精度原子分割、定位、去噪和超分辨率处理的TEMImageNet训练库和atomsenet深度学习模型
- [ ] TEMImageNet Training Library and AtomSegNet Deep-Learning Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-Resolution Processing of Atomic-Resolution Images 
时间：2021年02月20日                         第一作者：Ruoqian Lin                       [链接](https://arxiv.org/abs/2012.09093).                     
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

