# 80、立体图像超分辨率的对称视差注意
- [ ] Symmetric Parallax Attention for Stereo Image Super-Resolution 
时间：2021年04月20日                         第一作者：Yingqian Wang                       [链接](https://arxiv.org/abs/2011.03802).                     
<details>	<summary>注释</summary>	Accepted to NTIRE workshop at CVPR 2021. The first two authors contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 79、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月20日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 78、核不可知的真实图像超分辨率
- [ ] Kernel Agnostic Real-world Image Super-resolution 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2104.09008).                     
## 摘要：近年来，深度神经网络模型在各个研究领域取得了令人瞩目的成果。随着深度超分辨（SR）技术的发展，它引起了越来越多的关注。许多现有的方法都试图从直接下采样的低分辨率图像中恢复高分辨率图像，或者由于其简单性而假设高斯退化核具有加性噪声。然而，在真实场景中，即使失真图像在视觉上与清晰图像相似，也可能涉及高度复杂的核和非加性噪声。在这种情况下，现有的SR模型很难处理真实世界的图像。本文提出了一种新的核不可知SR框架来处理现实世界中的图像SR问题。这个框架可以无缝地挂接到多个主流模型上。在该框架中，退化核和噪声被自适应地建模而不是显式地指定。此外，我们还从正交的角度提出了一个迭代监督过程和频率参与目标，以进一步提高性能。实验验证了该框架在多个真实数据集上的有效性。
<details>	<summary>英文摘要</summary>	Recently, deep neural network models have achieved impressive results in various research fields. Come with it, an increasing number of attentions have been attracted by deep super-resolution (SR) approaches. Many existing methods attempt to restore high-resolution images from directly down-sampled low-resolution images or with the assumption of Gaussian degradation kernels with additive noises for their simplicities. However, in real-world scenarios, highly complex kernels and non-additive noises may be involved, even though the distorted images are visually similar to the clear ones. Existing SR models are facing difficulties to deal with real-world images under such circumstances. In this paper, we introduce a new kernel agnostic SR framework to deal with real-world image SR problem. The framework can be hanged seamlessly to multiple mainstream models. In the proposed framework, the degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and frequency-attended objective from orthogonal perspectives to further boost the performance. The experiments validate the effectiveness of the proposed framework on multiple real-world datasets. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 77、基于稠密搜索空间的神经网络超分辨率搜索：DeCoNAS
- [ ] Neural Architecture Search for Image Super-Resolution Using Densely Constructed Search Space: DeCoNAS 
时间：2021年04月19日                         第一作者：Joon Young Ahn                        [链接](https://arxiv.org/abs/2104.09048).                     
## 摘要：近年来，深度卷积神经网络在单幅图像超分辨率（SISR）和其他许多视觉任务中取得了巨大的成功。通过深化网络和开发更复杂的网络结构，它们的性能也在提高。然而，为给定的问题找到一个最优的结构是一项困难的任务，即使是对人类专家来说也是如此。为此，人们引入了神经结构搜索（NAS）方法，使结构的构建过程自动化。在本文中，我们将NAS扩展到超分辨率域，找到了一个轻量级的密集连接网络decoasnet。我们使用分层搜索策略来寻找与局部和全局特征的最佳连接。在这个过程中，我们定义了一个基于复杂度的惩罚来解决图像的超分辨率问题，这可以看作是一个多目标问题。实验结果表明，我们的decoasnet比现有的基于NAS的设计和手工设计的轻量级超分辨率网络具有更好的性能。
<details>	<summary>英文摘要</summary>	The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 76、基于无监督深度学习的三维荧光显微镜轴向-横向超分辨分析
- [ ] Axial-to-lateral super-resolution for 3D fluorescence microscopy using unsupervised deep learning 
时间：2021年04月19日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
## 摘要：与横向分辨率相比，荧光显微镜的体积成像通常受到轴向分辨率较低的各向异性空间分辨率的限制。为了解决这一问题，本文提出了一种基于深度学习的无监督超分辨技术来增强体荧光显微镜中的各向异性图像。与现有的需要匹配高分辨率目标体图像的深度学习方法相比，我们的方法大大减少了投入实践的工作量，因为网络的训练只需要一个3D图像堆栈，而不需要图像形成过程的先验知识、训练数据的配准或单独的训练获取目标数据。这是基于最优传输驱动循环一致生成对抗网络实现的，该网络从横向图像平面的高分辨率二维图像和其他平面的低分辨率二维图像之间的不成对匹配中学习。利用荧光共焦显微镜和光片显微镜，我们证明了训练的网络不仅提高了轴向分辨率超过衍射极限，而且增强了成像平面之间被抑制的视觉细节，消除了成像伪影。
<details>	<summary>英文摘要</summary>	Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution from inferior axial resolution compared to the lateral resolution. To address this problem, here we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target volume images, our method greatly reduces the effort to put into practice as the training of a network requires as little as a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in lateral image plane and low-resolution 2D images in the other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution beyond the diffraction limit, but also enhances suppressed visual details between the imaging planes and removes imaging artifacts. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 75、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年04月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
## 摘要：在过去的十年中，卷积神经网络在单幅图像超分辨率（SISR）方面取得了显著的进展。在SISR的最新进展中，注意机制是高性能SR模型的关键。然而，很少有作品真正讨论注意力为什么起作用以及它是如何起作用的。在这项工作中，我们试图量化和可视化的静态注意机制，并表明并非所有的注意模块都是同样有益的。然后，我们提出注意网络中的注意（A$^2$N）来获得高精度的图像SR。具体来说，我们的A$^2$N由一个非注意分支和一个耦合注意分支组成。提出了一种基于输入特征的动态注意权值提取模块，该模块能够有效地抑制不必要的注意调整。这允许注意模块专门化有益的例子，而无需其他惩罚，从而大大提高了注意网络的容量，而参数开销很小。实验表明，与现有的轻量级网络相比，该模型具有更好的折衷性能。在局部属性图上的实验也证明了注意力结构（attention-in-attention，A$^2$）可以从更广的范围内提取特征。
<details>	<summary>英文摘要</summary>	Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high performance SR models. However, few works really discuss why attention works and how it works. In this work, we attempt to quantify and visualize the static attention mechanisms and show that not all attention modules are equally beneficial. We then propose attention in attention network (A$^2$N) for highly accurate image SR. Specifically, our A$^2$N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. Experiments have demonstrated that our model could achieve superior trade-off performances comparing with state-of-the-art lightweight networks. Experiments on local attribution maps also prove attention in attention (A$^2$) structure can extract features from a wider range. </details>
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes will be available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 74、全量化图像超分辨率网络
- [ ] Fully Quantized Image Super-Resolution Networks 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2011.14265).                     
<details>	<summary>注释</summary>	Results updated </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 73、VSpSR：基于变分稀疏表示的可探索超分辨率
- [ ] VSpSR: Explorable Super-Resolution via Variational Sparse Representation 
时间：2021年04月17日                         第一作者：Hangqi Zhou                       [链接](https://arxiv.org/abs/2104.08575).                     
## 摘要：超分辨率（SR）是一个不适定问题，它意味着无限多幅高分辨率（HR）图像可以退化为同一幅低分辨率（LR）图像。为了研究一对多随机SR映射，我们隐式地表示了自然图像的非局部自相似性，并通过神经网络建立了一个变分稀疏的超分辨率框架（VSpSR）。由于HR图像的每一小块都可以很好地用原子在超完备字典中的稀疏表示来逼近，因此我们设计了一个双分支模块VSpM来探索SR空间。具体地说，VSpM的一个分支从LR输入中提取面片级基，另一个分支根据稀疏系数推断像素级的变分分布。通过重复采样系数，我们可以得到无限的稀疏表示，从而产生不同的HR图像。根据NTIRE 2021学习SR空间挑战赛的初步结果，我们团队（FUDANMIC21）的发布分数排名第7位。VSpSR的实现发布于https://zmiclab.github.io/。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 72、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月16日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
## 摘要：最新的视频编码标准，称为多功能视频编码（VVC），在编码链的不同层次上包含了几种新颖而精细的编码工具。与以前的高效视频编码（HEVC）标准相比，这些工具带来了显著的编码增益。然而，编码器仍然可以引入可见的编码伪影，主要由应用于将比特率调整到可用带宽的编码决策引起。因此，通常将预处理和后处理技术添加到编码管道以提高解码视频的质量。由于近年来在深度学习方面的进步，与传统方法相比，这些方法最近显示出了突出的效果。通常，多个神经网络被独立地训练来执行不同的任务，因此忽略了模型之间存在的冗余。在本文中，我们研究了一种基于学习的解决方案作为后处理步骤，以提高解码的VVC视频质量。我们的方法依赖于多任务学习来执行质量增强和超分辨率使用一个单一的共享网络优化多个退化水平。与传统的专用体系结构相比，该方案在减少编码伪影和超分辨率方面具有良好的性能，且网络参数较少。
<details>	<summary>英文摘要</summary>	The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 71、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年04月15日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年04月19日</details>

# 70、缩放SlowMo：一种高效的单级时空视频超分辨率框架
- [ ] Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution 
时间：2021年04月15日                         第一作者：Xiaoyu Xiang                       [链接](https://arxiv.org/abs/2104.07473).                     
## 摘要：本文提出了一种基于低分辨率（LR）和低帧速率（LFR）视频序列的高分辨率（HR）慢动作视频超分辨率算法。一种简单的方法是将其分解为两个子任务：视频帧插值（VFI）和视频超分辨率（VSR）。然而，在这个问题中，时间插值和空间上缩放是相互关联的。两阶段方法不能充分利用这一自然属性。另外，现有的VFI或VSR深度网络为了获得高质量的真实感视频帧，通常需要一个较大的帧重建模块，这使得两阶段的方法具有较大的模型，因而相对耗时。为了克服这一问题，我们提出了一种单级时空视频超分辨率框架，该框架可以直接从输入的LR和LFR视频中重建HR慢动作视频序列。我们不象VFI模型那样重建丢失的LR中间帧，而是通过特征时间插值模块对捕获局部时间上下文的丢失LR帧的LR帧特征进行时间插值。在广泛使用的基准测试上的大量实验表明，该框架不仅在干净和有噪声的LR帧上实现了更好的定性和定量性能，而且比最新的两级网络快数倍。源代码在中发布https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.
<details>	<summary>英文摘要</summary>	In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na\"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 . </details>
<details>	<summary>注释</summary>	Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 69、BAM：一种轻量级高效的单图像超分辨率均衡注意机制
- [ ] BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution 
时间：2021年04月15日                         第一作者：Fanyi Wang                       [链接](https://arxiv.org/abs/2104.07566).                     
## 摘要：单图像超分辨率（SISR）是计算机视觉领域中最具挑战性的问题之一。在基于深度卷积神经网络的方法中，注意机制显示出巨大的潜力。然而，由于网络结构的多样性，SISR任务缺乏一种通用的注意机制。本文提出了一种轻量级、高效的平衡注意机制（BAM），该机制可广泛适用于不同的SISR网络。它由Avgpool通道注意模块（ACAM）和Maxpool空间注意模块（MSAM）组成。这两个模块是并联的，以尽量减少误差积累和串扰。为了减少冗余信息对注意力产生的不良影响，我们仅将Avgpool应用于通道注意，因为Maxpool可以在空间维度上提取特征图中的虚幻极值点，我们只将Maxpool应用于空间注意，因为通道维度上的有用特征通常以最大值的形式存在于SISR任务中。为了验证BAM的有效性和鲁棒性，我们将其应用于12个最先进的SISR网络，其中8个没有注意，因此我们插入了BAM，4个有注意，因此我们用BAM替换了原有的注意模块。我们在Set5、Set14和BSD100基准数据集上进行了实验，其标度因子为x2、x3和x4。实验结果表明，BAM可以普遍提高网络性能。此外，我们还进行了烧蚀实验来证明BAM的极简性。结果表明，BAM的并行结构能够更好地平衡信道和空间注意，从而优于传统卷积块注意模块（CBAM）的串行结构。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) is one of the most challenging problems in the field of computer vision. Among the deep convolutional neural network based methods, attention mechanism has shown the enormous potential. However, due to the diverse network architectures, there is a lack of a universal attention mechanism for the SISR task. In this paper, we propose a lightweight and efficient Balanced Attention Mechanism (BAM), which can be generally applicable for different SISR networks. It consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM). These two modules are connected in parallel to minimize the error accumulation and the crosstalk. To reduce the undesirable effect of redundant information on the attention generation, we only apply Avgpool for channel attention because Maxpool could pick up the illusive extreme points in the feature map across the spatial dimensions, and we only apply Maxpool for spatial attention because the useful features along the channel dimension usually exist in the form of maximum values for SISR task. To verify the efficiency and robustness of BAM, we apply it to 12 state-of-the-art SISR networks, among which eight were without attention thus we plug BAM in and four were with attention thus we replace its original attention module with BAM. We experiment on Set5, Set14 and BSD100 benchmark datasets with the scale factor of x2 , x3 and x4 . The results demonstrate that BAM can generally improve the network performance. Moreover, we conduct the ablation experiments to prove the minimalism of BAM. Our results show that the parallel structure of BAM can better balance channel and spatial attentions, thus outperforming the series structure of prior Convolutional Block Attention Module (CBAM). </details>
<details>	<summary>注释</summary>	13 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 68、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年04月15日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
## 摘要：我们提出了SR3，一种通过重复细化实现图像超分辨率的方法。SR3采用去噪扩散概率模型生成条件图像，并通过随机去噪过程进行超分辨率处理。推理从纯高斯噪声开始，并使用在不同噪声水平下进行去噪训练的U网络模型迭代地细化噪声输出。SR3在不同放大倍数的超分辨率任务、人脸和自然图像上表现出很强的性能。我们在CelebA HQ上对标准8X人脸超分辨率任务进行了人体评估，并与SOTA-GAN方法进行了比较。SR3实现了接近50%的傻瓜率，这表明照片逼真的输出，而GANs不超过34%的傻瓜率。我们进一步证明了SR3在级联图像生成中的有效性，其中生成模型与超分辨率模型相链接，在ImageNet上产生了11.3的竞争性FID分数。
<details>	<summary>英文摘要</summary>	We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 67、用于制导深度图超分辨率的离散余弦变换网络
- [ ] Discrete Cosine Transform Network for Guided Depth Map Super-Resolution 
时间：2021年04月14日                         第一作者：Zixiang Zhao                       [链接](https://arxiv.org/abs/2104.06977).                     
## 摘要：制导深度超分辨率（GDSR）是多模图像处理中的一个热点问题。目标是使用高分辨率（HR）RGB图像提供关于边缘和对象轮廓的额外信息，以便低分辨率深度贴图可以向上采样到HR贴图。针对现有方法中存在的RGB纹理过度传输、跨模态特征提取困难、模块工作机制不明确等问题，提出了一种改进的离散余弦变换网络（DCTNet）。首先，将成对的RGB/深度图像输入到半耦合特征提取模块。共享卷积核分别提取跨模态的公共特征，私有核分别提取各自的独特特征。然后将RGB特征输入到边缘注意机制中，以突出显示对上采样有用的边缘。随后，在离散余弦变换（DCT）模块中，采用DCT来解决图像域GDSR的优化问题。将该方法推广到多通道RGB/depth特征上采样，提高了DCTNet的合理性，比传统方法更灵活有效。最终的深度预测由重建模块输出。大量的定性和定量实验证明了该方法的有效性，它可以生成准确的HR深度图，超过了现有的方法。同时，通过烧蚀实验验证了模块设计的合理性。
<details>	<summary>英文摘要</summary>	Guided depth super-resolution (GDSR) is a hot topic in multi-modal image processing. The goal is to use high-resolution (HR) RGB images to provide extra information on edges and object contours, so that low-resolution depth maps can be upsampled to HR ones. To solve the issues of RGB texture over-transferred, cross-modal feature extraction difficulty and unclear working mechanism of modules in existing methods, we propose an advanced Discrete Cosine Transform Network (DCTNet), which is composed of four components. Firstly, the paired RGB/depth images are input into the semi-coupled feature extraction module. The shared convolution kernels extract the cross-modal common features, and the private kernels extract their unique features, respectively. Then the RGB features are input into the edge attention mechanism to highlight the edges useful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT) module, where DCT is employed to solve the optimization problem designed for image domain GDSR. The solution is then extended to implement the multi-channel RGB/depth features upsampling, which increases the rationality of DCTNet, and is more flexible and effective than conventional methods. The final depth prediction is output by the reconstruction module. Numerous qualitative and quantitative experiments demonstrate the effectiveness of our method, which can generate accurate and HR depth maps, surpassing state-of-the-art methods. Meanwhile, the rationality of modules is also proved by ablation experiments. </details>
<details>	<summary>邮件日期</summary>	2021年04月15日</details>

# 66、SRR-Net：一种高分辨率MR图像的超分辨率重建方法
- [ ] SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging 
时间：2021年04月13日                         第一作者：Wenqi Huang                       [链接](https://arxiv.org/abs/2104.05901).                     
## 摘要：提高磁共振成像（MRI）的图像分辨率和采集速度是一个具有挑战性的问题。主要有两种策略来处理速度-分辨率的折衷：（1）$k$空间欠采样和高分辨率采集；（2）低分辨率图像重建和图像超分辨率流水线。然而，这些方法要么在某些高加速因子下性能有限，要么存在两级结构的误差积累。本文将MR重建和图像超分辨率的思想结合起来，直接从低分辨率的$k$空间采样数据中恢复HR图像。特别地，将SR重建问题描述为一个变分问题，并提出了一种从求解算法中展开的可学习网络。为了提高细节细化性能，引入了鉴别器。在体HR多线圈脑数据的实验结果表明，该SRR网络能够恢复高分辨率的脑图像，具有良好的视觉质量和感知质量。
<details>	<summary>英文摘要</summary>	Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 65、走向快速准确的真实世界深度超分辨率：基准数据集和基线
- [ ] Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline 
时间：2021年04月13日                         第一作者：Lingzhi He                       [链接](https://arxiv.org/abs/2104.06174).                     
## 摘要：商业深度传感器获取的深度图分辨率较低，难以用于各种计算机视觉任务。因此，深度图超分辨率（SR）是一项实用而有价值的工作，它可以将深度图提升到高分辨率（HR）空间。然而，由于缺乏真实世界的成对低分辨率（LR）和HR深度图，现有的方法大多采用降采样来获得成对的训练样本。为此，我们首先构建了一个名为RGB-D-D的大规模数据集，它可以极大地促进深度图SR的研究，甚至可以促进更多与深度相关的实际任务。数据集中的“D-D”表示从手机和Lucid Helios分别捕获的成对LR和HR深度图，范围从室内场景到具有挑战性的室外场景。此外，我们提供了一个快速的深度图超分辨率（FDSR）基线，其中高频分量从RGB图像自适应分解来指导深度图超分辨率。在现有公共数据集上的大量实验证明了我们的网络与现有方法相比的有效性和效率。此外，对于真实的LR深度图，我们的算法可以生成更精确的HR深度图，边界更清晰，并且在一定程度上修正了深度值误差。
<details>	<summary>英文摘要</summary>	Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which upscales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use downsampling to obtain paired training samples. To this end, we first construct a large-scale dataset named "RGB-D-D", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The "D-D" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 64、混叠是你的盟友：从原始图像突发端到端的超分辨率
- [ ] Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年04月13日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
## 摘要：本演示解决了从空间和时间上稍微不同的视点捕获的多个低分辨率快照重建高分辨率图像的问题。解决这个问题的关键挑战包括（i）以亚像素精度对齐输入图片，（ii）处理原始（噪声）图像以最大程度地忠实于本地相机数据，以及（iii）设计/学习非常适合该任务的图像先验（正则化器）。基于Wronski等人的见解，我们采用一种混合算法来解决这三个难题。在这种情况下，混叠是一个盟友，参数可以端到端地学习，同时保留了反问题经典方法的可解释性。我们的方法在合成和真实图像突发上的有效性得到了证明，在几个基准上建立了一个新的技术状态，并在智能手机和prosumer相机捕获的真实原始突发上提供了极好的定性结果。
<details>	<summary>英文摘要</summary>	This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 63、利用低分辨率流和掩模上采样实现高效的时空视频超分辨率
- [ ] Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling 
时间：2021年04月12日                         第一作者：Saikat Dutta                       [链接](https://arxiv.org/abs/2104.05778).                     
## 摘要：针对低分辨率、低帧速率的慢动作视频，提出了一种高效的时空超分辨率解决方案。一个简单的解决方案是连续运行视频超分辨率和视频帧插值模型。然而，这类解的内存效率低，推理时间长，不能充分利用时空关系的性质。为此，我们首先利用二次模型在LR空间中进行插值。使用最先进的视频超分辨率方法对输入LR帧进行超分辨率。利用双线性上采样技术，在HR空间重用用于合成LR插值帧的流程图和混合模板。这导致HR中间帧的粗略估计，该中间帧通常包含沿运动边界的伪影。通过残差学习，利用细化网络提高HR中间帧的质量。我们的模型是轻量级的，在REDS-STSR验证集中的性能比目前最先进的模型要好。
<details>	<summary>英文摘要</summary>	This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set. </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Project page: https://github.com/saikatdutta/FMU_STSR </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 62、基于深度学习的超分辨率网络边缘感知图像压缩
- [ ] Edge-Aware Image Compression using Deep Learning-based Super-resolution Network 
时间：2021年04月11日                         第一作者：Dipti Mishra                       [链接](https://arxiv.org/abs/2104.04926).                     
## 摘要：我们提出了一种基于学习的压缩方案，在预处理和后处理的深度cnn之间封装一个标准的编解码器。具体地说，我们通过引入：（a）一个边缘感知损失函数来防止在以前的工作中经常出现的模糊&（b）一个用于后处理的超分辨率卷积神经网络（CNN）以及一个相应的预处理网络，来展示对使用压缩-解压缩网络的先前方法的改进在低速率下提高率失真性能。该算法在从低分辨率到高分辨率的各种数据集上进行评估，即Set 5、Set 7、Classic 5、Set 14、Live 1、Kodak、General 100、CLIC 2019。与JPEG、JPEG2000、BPG和最近的CNN方法相比，该算法在低码率和高码率下的峰值信噪比分别提高了20.75%、8.47%、3.22%、3.23%和24.59%、14.46%、10.14%和8.57%。同样，在低比特率和高比特率下，MS-SSIM的这种改进分别约为71.43%、50%、36.36%、23.08%、64.70%和64.47%、61.29%、47.06%、51.52%和16.28%。使用CLIC 2019数据集，在低比特率和高比特率下，PSNR分别约为16.67%、10.53%、6.78%和24.62%、17.39%和14.08%，优于JPEG2000、BPG和最近的CNN方法。同样，与相同的方法相比，MS-SSIM在低比特率和高比特率下的性能分别约为72%、45.45%、39.13%、18.52%和71.43%、50%、41.18%和17.07%。其他数据集也实现了类似的改进。
<details>	<summary>英文摘要</summary>	We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%, 61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%, 10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%, and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively, compared to the same approaches. A similar type of improvement is achieved with other datasets also. </details>
<details>	<summary>注释</summary>	13 pages, 9 figures, 16 tables </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 61、CoPE：使用多项式展开的条件图像生成
- [ ] CoPE: Conditional image generation using Polynomial Expansions 
时间：2021年04月11日                         第一作者：Grigorios G Chrysos                       [链接](https://arxiv.org/abs/2104.05077).                     
## 摘要：生成建模已经发展成为机器学习的一个重要领域。深度多项式神经网络（PNNs）在无监督图像生成中取得了令人印象深刻的结果，其任务是将输入向量（即噪声）映射到合成图像。然而，PNNs的成功还没有在超分辨率等条件生成任务中得到推广。现有的pnn主要集中在单变量多项式展开上，对于两个变量的输入，即噪声变量和条件变量，pnn表现不好。在这项工作中，我们引入了一个通用的框架，称为CoPE，它可以对两个输入变量进行多项式展开，并捕捉它们的自相关和互相关。我们展示了CoPE如何被简单地扩充以接受任意数量的输入变量。CoPE分为五个任务（类条件生成、反问题、边缘到图像的转换、图像到图像的转换、属性引导生成），涉及八个数据集。全面评估表明，CoPE可用于处理各种条件生成任务。
<details>	<summary>英文摘要</summary>	Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 60、作物类型语义切分的语境自对比预训练
- [ ] Context-self contrastive pretraining for crop type semantic segmentation 
时间：2021年04月09日                         第一作者：Michail Tarasiou                       [链接](https://arxiv.org/abs/2104.04310).                     
## 摘要：本文提出了一种基于对比学习的全监督预训练方案，特别适合于密集分类任务。提出的上下文自对比丢失（CSCL）算法利用训练样本中每个位置与其局部上下文之间的相似性度量，学习一个使语义边界弹出的嵌入空间。对于卫星图像中作物类型的语义分割，我们发现包裹边界的性能是一个关键的瓶颈，并解释了CSCL如何解决该问题的根本原因，从而提高该任务的最新性能。此外，利用Sentinel-2（S2）卫星任务的图像，我们编制了据我们所知最大的卫星图像时间序列数据集，这些数据集由作物类型和包裹标识密集标注，我们与数据生成管道一起公开。利用这些数据，我们发现CSCL，即使在最小的预训练下，也可以改善所有的基线，并提出了一个超分辨率的语义分割过程，以获得更细粒度的作物类。该方法在二维和三维立体图像的语义分割任务中得到了进一步的验证，结果表明，该方法在竞争性基线下的性能得到了一致的提高。
<details>	<summary>英文摘要</summary>	In this paper we propose a fully-supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in an training sample and its local context. For crop type semantic segmentation from satellite images we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, dataset of satellite image timeseries densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The proposed method is further validated on the task of semantic segmentation on 2D and 3D volumetric images showing consistent performance improvements upon competitive baselines. </details>
<details>	<summary>注释</summary>	11 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 59、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月09日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 58、加法器：迈向节能图像超分辨率
- [ ] AdderSR: Towards Energy Efficient Image Super-Resolution 
时间：2021年04月09日                         第一作者：Dehua Song                       [链接](https://arxiv.org/abs/2009.08891).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 57、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月08日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
## 摘要：虽然单图像超分辨率（single-image super-resolution，SISR）方法在单次退化方面取得了很大的成功，但在实际应用中仍存在性能下降和多重退化的问题。近年来，人们对多重降解的盲模型和非盲模型进行了研究。然而，由于训练数据和测试数据之间的分布变化，这些方法通常会显著降低性能。为此，我们首次提出了一个条件元网络框架（CMDSR），帮助SR框架学习如何适应输入分布的变化。我们利用所提出的条件网在任务级提取退化先验，以适应基本SR网络（BaseNet）的参数。具体地说，我们框架的ConditionNet首先从一个支持集学习退化先验知识，该支持集由来自同一任务的一系列退化图像块组成。然后自适应基网根据条件特征快速地改变其参数。此外，为了更好地提取退化先验信息，我们提出了一种任务对比损失的方法来减小任务内部的距离，增加任务级特征之间的跨任务距离。在不预先定义退化映射的情况下，我们的盲框架可以进行单参数更新，从而产生可观的SR结果。大量的实验证明了CMDSR在各种盲甚至非盲方法中的有效性。灵活的基本网结构也表明CMDSR可以作为一个大系列SISR模型的通用框架。
<details>	<summary>英文摘要</summary>	Although single-image super-resolution (SISR) methods have achieved great success on single degradation, they still suffer performance drop with multiple degrading effects in real scenarios. Recently, some blind and non-blind models for multiple degradations have been explored. However, those methods usually degrade significantly for distribution shifts between the training and test data. Towards this end, we propose a conditional meta-network framework (named CMDSR) for the first time, which helps SR framework learn how to adapt to changes in input distribution. We extract degradation prior at task-level with the proposed ConditionNet, which will be used to adapt the parameters of the basic SR network (BaseNet). Specifically, the ConditionNet of our framework first learns the degradation prior from a support set, which is composed of a series of degraded image patches from the same task. Then the adaptive BaseNet rapidly shifts its parameters according to the conditional features. Moreover, in order to better extract degradation prior, we propose a task contrastive loss to decrease the inner-task distance and increase the cross-task distance between task-level features. Without predefining degradation maps, our blind framework can conduct one single parameter update to yield considerable SR results. Extensive experiments demonstrate the effectiveness of CMDSR over various blind, even non-blind methods. The flexible BaseNet structure also reveals that CMDSR can be a general framework for large series of SISR models. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 56、太阳电池检测的联合超分辨与校正
- [ ] Joint Super-Resolution and Rectification for Solar Cell Inspection 
时间：2021年04月07日                         第一作者：Mathis Hoffmann                       [链接](https://arxiv.org/abs/2011.05003).                     
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 55、BasicVSR：寻找视频超分辨率及更高分辨率的关键组件
- [ ] BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond 
时间：2021年04月07日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2012.02181).                     
<details>	<summary>注释</summary>	CVPR 2021 camera-ready </details>
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 54、基于内容自适应超分辨率的高效视频压缩
- [ ] Efficient Video Compression via Content-Adaptive Super-Resolution 
时间：2021年04月06日                         第一作者：Mehrdad Khani                       [链接](https://arxiv.org/abs/2104.02322).                     
## 摘要：视频压缩是互联网视频传输的重要组成部分。最近的研究表明，深度学习技术可以与人类设计的算法相媲美或优于人类设计的算法，但这些方法的计算效率和功耗明显低于现有的编解码器。本文提出了一种新的方法，通过一个小的、内容自适应的超分辨率模型来增强现有的编解码器，从而显著提高视频质量。我们的方法，SRVC，将视频编码成两个比特流：（i）内容流，通过使用现有的编解码器压缩下采样的低分辨率视频产生，（ii）模型流，对为视频的短片段定制的轻量级超分辨率神经网络的周期性更新进行编码。SRVC通过将解压缩后的低分辨率视频帧通过（时变）超分辨率模型来重构高分辨率视频帧，从而对视频进行解码。我们的结果表明，为了获得相同的PSNR，SRVC在慢模式下需要H.265每像素16%的比特，而在DVC（一种新的基于深度学习的视频压缩方案）中需要2%的每像素比特。SRVC在nvidiav100gpu上以每秒90帧的速度运行。
<details>	<summary>英文摘要</summary>	Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 53、超分辨率的测试时间调整：你只需要在更多的图像上过度调整
- [ ] Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a Few More Images 
时间：2021年04月06日                         第一作者：Mohammad Saeed Rad                       [链接](https://arxiv.org/abs/2104.02663).                     
## 摘要：现有的基于参考（RF）的超分辨率（SR）模型试图在高分辨率RF图像与低分辨率（LR）输入匹配的假设下提高SR的感知质量。由于射频图像在内容、颜色、对比度等方面应与测试图像相似，这妨碍了其在实际场景中的适用性。其他提高图像感知质量的方法，包括感知损失和对抗损失，往往通过显著降低PSNR/SSIM来显著降低对地面真实感的保真度。针对这两个问题，我们提出了一种简单而通用的方法，通过进一步微调训练数据集中具有与初始HR预测相似激活模式的图像子集上的SR网络，来提高预先训练的SR网络对给定LR输入的HR预测的感知质量，关于特征提取器的过滤器。特别地，我们从感知质量和PSNR/SSIM值方面展示了微调对这些图像的影响。与感知驱动的方法相反，我们证明了微调网络产生的HR预测具有更高的感知质量和相对于初始HR预测的PSNR/SSIM的最小变化。此外，我们还提出了有关随机共振网络滤波器的新的数值实验，通过滤波器相关性，我们表明，与基线网络或随机图像上微调的网络相比，我们方法中微调网络的滤波器更接近“理想”滤波器。
<details>	<summary>英文摘要</summary>	Existing reference (RF)-based super-resolution (SR) models try to improve perceptual quality in SR under the assumption of the availability of high-resolution RF images paired with low-resolution (LR) inputs at testing. As the RF images should be similar in terms of content, colors, contrast, etc. to the test image, this hinders the applicability in a real scenario. Other approaches to increase the perceptual quality of images, including perceptual loss and adversarial losses, tend to dramatically decrease fidelity to the ground-truth through significant decreases in PSNR/SSIM. Addressing both issues, we propose a simple yet universal approach to improve the perceptual quality of the HR prediction from a pre-trained SR network on a given LR input by further fine-tuning the SR network on a subset of images from the training dataset with similar patterns of activation as the initial HR prediction, with respect to the filters of a feature extractor. In particular, we show the effects of fine-tuning on these images in terms of the perceptual quality and PSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate that the fine-tuned network produces a HR prediction with both greater perceptual quality and minimal changes to the PSNR/SSIM with respect to the initial HR prediction. Further, we present novel numerical experiments concerning the filters of SR networks, where we show through filter correlation, that the filters of the fine-tuned network from our method are closer to "ideal" filters, than those of the baseline network or a network fine-tuned on random images. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 52、深脉冲超分辨率
- [ ] Deep Burst Super-Resolution 
时间：2021年04月06日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2101.10997).                     
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 51、基于注意的层次多模态融合高分辨率深度图成像
- [ ] High-resolution Depth Maps Imaging via Attention-based Hierarchical Multi-modal Fusion 
时间：2021年04月04日                         第一作者：Zhiwei Zhong                       [链接](https://arxiv.org/abs/2104.01530).                     
## 摘要：深度图记录了场景中视点和物体之间的距离，在许多实际应用中起着至关重要的作用。然而，消费者级RGB-D相机拍摄的深度图空间分辨率较低。引导深度图超分辨率（DSR）是解决这一问题的一种常用方法，它试图从输入的低分辨率（LR）深度及其作为引导的耦合HR-RGB图像恢复高分辨率（HR）深度图。如何正确地选择和传播一致性结构，正确地处理不一致性结构是指导DSR最具挑战性的问题。本文提出了一种新的基于注意的分层多模态融合（AHMF）网络。具体来说，为了有效地从LR深度和HR引导中提取和组合相关信息，我们提出了一种分层卷积层的多模式基于注意的融合（MMAF）策略，包括一个特征增强块，用于选择有价值的特征；一个特征重新校准块，用于统一具有不同外观特征的模式的相似性度量。在此基础上，提出了一种双向分层特征协作（BHFC）模型，充分利用多尺度特征间的低层空间信息和高层结构信息。实验结果表明，该方法在重建精度、运行速度和存储效率等方面均优于现有方法。
<details>	<summary>英文摘要</summary>	Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance. The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 50、具有光谱混合和异构数据集的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution with Spectral Mixup and Heterogeneous Datasets 
时间：2021年04月03日                         第一作者：Ke Li                       [链接](https://arxiv.org/abs/2101.07589).                     
<details>	<summary>注释</summary>	16 pages, 14 tables, 5 figures; Code available at https://github.com/kli8996/HSISR </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 49、用于学习失调光学变焦的平方变形对准网络
- [ ] SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom 
时间：2021年04月02日                         第一作者：Kangfu Mei                       [链接](https://arxiv.org/abs/2104.00848).                     
## 摘要：基于深度神经网络（DNN）的超分辨率算法大大提高了生成图像的质量。然而，由于学习失调光学变焦的困难，这些算法在处理真实世界的超分辨率问题时往往会产生明显的伪影。为了解决这一问题，本文提出了一种平方变形对准网络（SDAN）。我们的网络学习卷积核的每点平方偏移量，然后根据偏移量在校正的卷积窗口中对齐特征。因此，通过提取对齐的特征，可以最大限度地减少不对齐。与普通可变形卷积网络（DCN）中的逐点偏移不同，本文提出的平方偏移不仅加快了偏移学习，而且在参数较少的情况下提高了生成质量。此外，我们进一步提出一个有效的交叉堆积注意层来提高学习偏移量的准确性。它利用打包和解包操作来扩大偏移量学习的接受域，增强低分辨率图像与参考图像之间空间联系的提取能力。综合实验表明，该方法在计算效率和真实感细节方面均优于其他先进方法。
<details>	<summary>英文摘要</summary>	Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details. </details>
<details>	<summary>注释</summary>	ICME21. Code is available at https://github.com/MKFMIKU/SDAN </details>
<details>	<summary>邮件日期</summary>	2021年04月05日</details>

# 48、盲超分辨的无监督退化表示学习
- [ ] Unsupervised Degradation Representation Learning for Blind Super-Resolution 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2104.00416).                     
## 摘要：大多数现有的基于CNN的超分辨率（SR）方法都是在假设退化是固定的和已知的（例如双三次下采样）的基础上发展起来的。然而，当实际性能下降与假设不同时，这些方法的性能会严重下降。在实际应用中，为了处理各种未知的退化，以往的方法都是依靠退化估计来重建SR图像。然而，退化估计方法通常是耗时的，并且可能由于较大的估计误差而导致SR失效。本文提出了一种无监督退化表示学习方案，用于盲随机共振，无需显式退化估计。具体来说，我们学习抽象表示来区分表示空间中的各种退化，而不是像素空间中的显式估计。此外，我们还提出了一种基于学习表示的退化感知SR（DASR）网络，该网络能够灵活地适应各种退化。实验结果表明，我们的退化表征学习方法可以提取有区别的表征来获得精确的退化信息。在合成图像和真实图像上的实验表明，我们的网络对于盲SR任务达到了最先进的性能。代码位于：https://github.com/LongguangWang/DASR。
<details>	<summary>英文摘要</summary>	Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To handle various unknown degradations in real-world applications, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation estimation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this paper, we propose an unsupervised degradation representation learning scheme for blind SR without explicit degradation estimation. Specifically, we learn abstract representations to distinguish various degradations in the representation space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 47、探索图像超分辨率中的稀疏性实现高效推理
- [ ] Exploring Sparsity in Image Super-Resolution for Efficient Inference 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2006.09603).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 46、通过学习匹配内部斑块分布来估计MR切片轮廓
- [ ] MR Slice Profile Estimation by Learning to Match Internal Patch Distributions 
时间：2021年03月31日                         第一作者：Shuo Han                       [链接](https://arxiv.org/abs/2104.00100).                     
## 摘要：为了超分辨多层面二维磁共振（MR）图像的通平面方向，在训练监督算法时，可以将其切片选择剖面作为高分辨率（HR）到低分辨率（LR）的退化模型来生成成对数据。现有的超分辨率算法对切片选择轮廓进行了假设，因为给定的图像不容易知道它。在这项工作中，我们通过学习匹配其内部补丁分布来估计给定特定图像的切片选择剖面。具体来说，我们假设在应用正确的切片选择轮廓之后，沿着HR平面内方向的图像面片分布应该与沿着LR平面内方向的分布相匹配。因此，我们将切片选择轮廓的估计作为生成对抗网络（GAN）中学习生成器的一部分。这样，就可以在没有任何外部数据的情况下学习切片选择轮廓。我们的算法通过各向同性MR图像的模拟进行了测试，并将其与一种通过平面的超分辨率算法结合起来，以证明其优越性，同时也被用作测量图像分辨率的工具。我们的密码是https://github.com/shuohan/espreso2。
<details>	<summary>英文摘要</summary>	To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, accepted by Information Processing in Medical Imaging (IPMI) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 45、视频探索通过视频特定的自动编码器
- [ ] Video Exploration via Video-Specific Autoencoders 
时间：2021年03月31日                         第一作者：Kevin Wang                        [链接](https://arxiv.org/abs/2103.17261).                     
## 摘要：我们提出了简单的视频特定的自动编码器，使人类可控的视频探索。这包括各种各样的分析任务，例如（但不限于）空间和时间超分辨率、空间和时间编辑、对象移除、视频纹理、平均视频探索以及视频内部和跨视频的对应估计。以前的工作已经独立地研究了这些问题，并提出了不同的公式。在这项工作中，我们观察到一个简单的自动编码器训练（从头开始）对多个帧的特定视频，使一个人能够执行各种各样的视频处理和编辑任务。我们的任务是通过两个关键的观察来实现的：（1）由自动编码器学习的潜在代码捕获视频的空间和时间特性；（2）自动编码器可以将样本输入投射到视频特定的流形上。例如：（1）内插潜在代码实现了时间超分辨率和用户可控的视频纹理；（2）流形重投影实现了空间超分辨率、对象移除和去噪，而无需对任何任务进行训练。重要的是，通过主成分分析的潜在代码的二维可视化可以作为用户可视化和直观控制视频编辑的工具。最后，我们将我们的方法与现有技术进行定量对比，发现在没有任何监督和任务特定知识的情况下，我们的方法可以与专门为任务训练的监督方法进行比较。
<details>	<summary>英文摘要</summary>	We present simple video-specific autoencoders that enables human-controllable video exploration. This includes a wide variety of analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, object removal, video textures, average video exploration, and correspondence estimation within and across videos. Prior work has independently looked at each of these problems and proposed different formulations. In this work, we observe that a simple autoencoder trained (from scratch) on multiple frames of a specific video enables one to perform a large variety of video processing and editing tasks. Our tasks are enabled by two key observations: (1) latent codes learned by the autoencoder capture spatial and temporal properties of that video and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. For e.g. (1) interpolating latent codes enables temporal super-resolution and user-controllable video textures; (2) manifold reprojection enables spatial super-resolution, object removal, and denoising without training for any of the tasks. Importantly, a two-dimensional visualization of latent codes via principal component analysis acts as a tool for users to both visualize and intuitively control video edits. Finally, we quantitatively contrast our approach with the prior art and found that without any supervision and task-specific knowledge, our approach can perform comparably to supervised approaches specifically trained for a task. </details>
<details>	<summary>注释</summary>	Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/ </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 44、用于人脸超分辨率的边缘和身份保持网络
- [ ] Edge and Identity Preserving Network for Face Super-Resolution 
时间：2021年03月31日                         第一作者：Jonghyun Kim                       [链接](https://arxiv.org/abs/2008.11977).                     
<details>	<summary>注释</summary>	Neurocomputing'2021 DOI: 10.1016/j.neucom.2021.03.048 </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 43、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月31日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	The first two authors contributed equally to this work. Accepted to CVPR 2021 (camera-ready version) </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 42、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月30日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 41、传递学习：探索盲超分辨退化的传递性
- [ ] Transitive Learning: Exploring the Transitivity of Degradations for Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2103.15290).                     
## 摘要：现有的盲超分辨率（SR）方法由于对数据或模型的迭代估计和校正的依赖性，通常耗时且效率较低。针对这一问题，本文提出了一种基于端到端网络的盲随机共振传递学习方法。首先，我们分析并论证了退化的传递性，包括广泛使用的加法退化和卷积退化。在此基础上，提出了一种新的传递学习方法，通过自适应地推导传递变换函数来求解未知退化问题，而不需要任何迭代操作。具体而言，端到端TLSR网络由传递度（DoT）估计网络、齐次特征提取网络和传递学习模块组成。对盲SR任务的定量和定性评价表明，与现有的盲SR方法相比，本文提出的TLSR具有更好的性能和更少的时间消耗。代码可在https://github.com/YuanfeiHuang/TLSR。
<details>	<summary>英文摘要</summary>	Being extremely dependent on the iterative estimation and correction of data or models, the existing blind super-resolution (SR) methods are generally time-consuming and less effective. To address it, this paper proposes a transitive learning method for blind SR using an end-to-end network without any additional iterations in inference. To begin with, we analyze and demonstrate the transitivity of degradations, including the widely used additive and convolutive degradations. We then propose a novel Transitive Learning method for blind Super-Resolution on transitive degradations (TLSR), by adaptively inferring a transitive transformation function to solve the unknown degradations without any iterative operations in inference. Specifically, the end-to-end TLSR network consists of a degree of transitivity (DoT) estimation network, a homogeneous feature extraction network, and a transitive learning module. Quantitative and qualitative evaluations on blind SR tasks demonstrate that the proposed TLSR achieves superior performance and consumes less time against the state-of-the-art blind SR methods. The code is available at https://github.com/YuanfeiHuang/TLSR. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 40、高细节图像超分辨率的最佳伙伴
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年03月29日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
## 摘要：我们考虑了单图像超分辨率（SISR）问题，即基于低分辨率（LR）输入生成高分辨率（HR）图像。近年来，生成性对抗网络（generativediscountarialnetworks，GANs）开始流行于制造幻觉。沿着这条路线的大多数方法依赖于预定义的单个LR-single-HR映射，这对于SISR任务来说不够灵活。而且，甘生成的虚假细节往往会破坏整个图像的真实感。我们通过为丰富的细节SISR提出bestbuddygan（bebygan）来解决这些问题。放松了不可变的一对一约束，使得估计出的补丁在训练过程中动态地寻求最佳监督，有利于产生更合理的细节。此外，我们提出了一种区域感知的对抗式学习策略，使我们的模型能够自适应地生成纹理区域的细节。大量的实验证明了我们方法的有效性。同时还构建了一个超高分辨率4K数据集，为今后的超分辨率研究提供了便利。
<details>	<summary>英文摘要</summary>	We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the immutable one-to-one constraint, we allow the estimated patches to dynamically seek the best supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 39、全知视频超分辨率
- [ ] Omniscient Video Super-Resolution 
时间：2021年03月29日                         第一作者：Peng Yi                        [链接](https://arxiv.org/abs/2103.15683).                     
## 摘要：最新的视频超分辨率（SR）方法要么采用迭代的方式来处理来自时间滑动窗口的低分辨率（LR）帧，要么利用先前估计的SR输出来帮助循环地重构当前帧。一些研究试图将这两种结构结合起来形成一个混合框架，但未能充分发挥其作用。在本文中，我们提出了一个全知的框架，不仅可以利用前面的SR输出，还可以利用现在和将来的SR输出。全知框架更具通用性，因为迭代框架、循环框架和混合框架可以看作它的特例。提出的全知框架使得生成器比其他框架下的生成器表现得更好。在公共数据集上的大量实验表明，该方法在客观度量、主观视觉效果和复杂度方面均优于现有的方法。我们的代码将会公开。
<details>	<summary>英文摘要</summary>	Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity. Our code will be made public. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 38、交叉MPI：利用多平面图像实现图像超分辨率的交叉尺度立体成像
- [ ] Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images 
时间：2021年03月29日                         第一作者：Yuemei Zhou                       [链接](https://arxiv.org/abs/2011.14631).                     
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 37、基于隐式场景表示的现场场景标注与理解
- [ ] In-Place Scene Labelling and Understanding with Implicit Scene Representation 
时间：2021年03月29日                         第一作者：Shuaifeng Zhi                       [链接](https://arxiv.org/abs/2103.15875).                     
## 摘要：语义标注与几何和辐射重建高度相关，因为形状和外观相似的场景实体更可能来自相似的类。最近的隐式神经重建技术是有吸引力的，因为他们不需要事先训练数据，但同样的完全自我监督的方法是不可能的语义，因为标签是人类定义的属性。我们扩展了神经辐射场（NeRF）技术，将语义与外观和几何信息进行联合编码，从而使用少量的特定场景的就地标注就可以得到完整、准确的二维语义标注。NeRF内在的多视图一致性和平滑性使得稀疏标签能够有效地传播，从而有利于语义。我们展示了这种方法的好处，当标签是稀疏或非常嘈杂的房间规模的场景。在视觉语义映射系统中，我们展示了它在各种有趣的应用中的优势，如高效的场景标注工具、新颖的语义视图合成、标签去噪、超分辨率、标签插值和多视图语义标签融合。
<details>	<summary>英文摘要</summary>	Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems. </details>
<details>	<summary>注释</summary>	Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/ </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 36、基于流的核先验算法及其在盲超分辨中的应用
- [ ] Flow-based Kernel Prior with Application to Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2103.15977).                     
## 摘要：核估计通常是盲图像超分辨率（SR）的关键问题之一。最近，Double-DIP提出通过网络结构对核进行建模，KernelGAN则采用深度线性网络和一些正则化损失来约束核空间。然而，他们没有充分利用一般的SR核假设，即各向异性高斯核对图像SR是足够的。为了解决这个问题，本文提出了一种基于归一化流的核先验（FKP）核建模方法。通过学习各向异性高斯核分布和可处理的潜在分布之间的可逆映射，FKP可以很容易地代替双倾角和KernelGAN的核建模模块。具体地说，FKP在隐空间而不是网络参数空间中对核进行优化，从而生成合理的核初始化，遍历学习到的核流形，提高优化的稳定性。在合成图像和真实图像上的大量实验表明，所提出的FKP算法可以在较少的参数、运行时间和内存使用的情况下显著提高核估计精度，从而得到最新的盲SR结果。
<details>	<summary>英文摘要</summary>	Kernel estimation is generally one of the key problems for blind image super-resolution (SR). Recently, Double-DIP proposes to model the kernel via a network architecture prior, while KernelGAN employs the deep linear network and several regularization losses to constrain the kernel space. However, they fail to fully exploit the general SR kernel assumption that anisotropic Gaussian kernels are sufficient for image SR. To address this issue, this paper proposes a normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an invertible mapping between the anisotropic Gaussian kernel distribution and a tractable latent distribution, FKP can be easily used to replace the kernel modeling modules of Double-DIP and KernelGAN. Specifically, FKP optimizes the kernel in the latent space rather than the network parameter space, which allows it to generate reasonable kernel initialization, traverse the learned kernel manifold and improve the optimization stability. Extensive experiments on synthetic and real-world images demonstrate that the proposed FKP can significantly improve the kernel estimation accuracy with less parameters, runtime and memory usage, leading to state-of-the-art blind SR results. </details>
<details>	<summary>注释</summary>	Accepted by CVPR2021. Code: https://github.com/JingyunLiang/FKP </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 35、去噪去噪超分辨率流水线的再思考
- [ ] Rethinking the Pipeline of Demosaicing, Denoising and Super-Resolution 
时间：2021年03月29日                         第一作者：Guocheng Qian                        [链接](https://arxiv.org/abs/1905.02538).                     
<details>	<summary>注释</summary>	Code is available at: https://github.com/guochengqian/TENet </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 34、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年03月28日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 33、D2C-SR：一种从散度到收敛的图像超分辨率方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Image Super-Resolution 
时间：2021年03月26日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
## 摘要：本文提出了一种新的图像超分辨率（SR）框架D2C-SR。作为一个不适定问题，超分辨率相关任务的关键挑战是对于给定的低分辨率输入可以有多个预测。大多数经典的方法和早期的基于深度学习的方法忽略了这一基本事实，将这个问题建模为一个确定性的过程，这往往导致不满意的结果。受最近SRFlow等工作的启发，我们采用半概率的方法解决了这一问题，并提出了一种两阶段流水线：一个发散阶段用于学习离散形式的高分辨率输出的分布，然后一个收敛阶段用于将学习到的预测融合成最终的输出。更具体地说，我们提出了一种基于树结构的深度网络，其中每个分支被设计来学习可能的高分辨率预测。在发散阶段，对每个分支分别进行训练以拟合地面真值，并用三重损失来增强发散分支的输出。随后，我们添加一个保险丝模块来组合多个预测，因为第一阶段的输出可能是次优的。可以训练引信模块以端到端的方式将w.r.t收敛到最终的高分辨率图像。我们对几个基准进行了评估，包括一个新提出的具有8倍放大因子的数据集。我们的实验表明，D2C-SR可以在峰值信噪比和SSIM上实现最先进的性能，并且计算量显著减少。
<details>	<summary>英文摘要</summary>	In this paper, we present D2C-SR, a novel framework for the task of image super-resolution(SR). As an ill-posed problem, the key challenge for super-resolution related tasks is there can be multiple predictions for a given low-resolution input. Most classical methods and early deep learning based approaches ignored this fundamental fact and modeled this problem as a deterministic processing which often lead to unsatisfactory results. Inspired by recent works like SRFlow, we tackle this problem in a semi-probabilistic manner and propose a two-stage pipeline: a divergence stage is used to learn the distribution of underlying high-resolution outputs in a discrete form, and a convergence stage is followed to fuse the learned predictions into a final output. More specifically, we propose a tree-based structure deep network, where each branch is designed to learn a possible high-resolution prediction. At the divergence stage, each branch is trained separately to fit ground truth, and a triple loss is used to enforce the outputs from different branches divergent. Subsequently, we add a fuse module to combine the multiple predictions as the outputs from the first stage can be sub-optimal. The fuse module can be trained to converge w.r.t the final high-resolution image in an end-to-end manner. We conduct evaluations on several benchmarks, including a new proposed dataset with 8x upscaling factor. Our experiments demonstrate that D2C-SR can achieve state-of-the-art performance on PSNR and SSIM, with a significantly less computational cost. </details>
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 32、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月25日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
## 摘要：近五年来，深度卷积神经网络（CNNs）被广泛应用于低层视觉。根据不同应用的特点，设计合适的CNN结构。然而，定制的体系结构通过对所有像素点的等价处理来聚集不同的特征，从而提高了应用的性能，忽略了局部功率像素点的影响，导致训练效率低下。在本文中，我们提出了一个非对称CNN（ACNet）包括一个非对称块（AB），一个mem？图像超分辨率增强块（MEB）和高频特征增强块（HFFEB）。该算法利用一维非对称卷积，在水平方向和垂直方向上增强平方卷积核，以增强局部显著特征对SISR的影响。MEB通过残差学习（RL）技术融合AB的所有分层低频特征，以解决长期依赖问题，并转换得到的低频fea？转换成高频特性。HFFEB利用低频和高频特征来获得更健壮的超分辨率特征，并解决过多的特征增强问题。广告？另外，它还负责重建高分辨率（HR）图像。大量实验表明，该网络能有效地解决单图像超分辨率（SISR）、盲SISR和盲噪声的盲SISR问题。ACNet的代码如所示https://github.com/hellloxiaotian/ACNet。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks (CNNs) have been widely applied for low-level vision over the past five years. According to nature of different applications, designing appropriate CNN architectures is developed. However, customized architectures gather different features via treating all pixel points as equal to improve the performance of given application, which ignores the effects of local power pixel points and results in low training efficiency. In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric block (AB), a mem?ory enhancement block (MEB) and a high-frequency feature enhancement block (HFFEB) for image super-resolution. The AB utilizes one-dimensional asymmetric convolutions to intensify the square convolution kernels in horizontal and vertical directions for promoting the influences of local salient features for SISR. The MEB fuses all hierarchical low-frequency features from the AB via residual learning (RL) technique to resolve the long-term dependency problem and transforms obtained low-frequency fea?tures into high-frequency features. The HFFEB exploits low- and high-frequency features to obtain more robust super-resolution features and address excessive feature enhancement problem. Ad?ditionally, it also takes charge of reconstructing a high-resolution (HR) image. Extensive experiments show that our ACNet can effectively address single image super-resolution (SISR), blind SISR and blind SISR of blind noise problems. The code of the ACNet is shown at https://github.com/hellloxiaotian/ACNet. </details>
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 31、JDSR-GAN：构建联合协作的蒙面超分辨率学习网络
- [ ] JDSR-GAN: Constructing A Joint and Collaborative Learning Network for Masked Face Super-Resolution 
时间：2021年03月25日                         第一作者：Guangwei Gao                       [链接](https://arxiv.org/abs/2103.13676).                     
## 摘要：随着预防COVID-19病毒的重要性日益增强，在大多数视频监控场景中获得的人脸图像都是低分辨率的。然而，以往的人脸超分辨率算法大多不能在一个模型中同时处理两个任务。本文将遮罩遮挡视为图像噪声，构建了一个联合协作学习网络JDSR-GAN，用于遮罩人脸的超分辨率识别。给定一幅以掩模为输入的低质量人脸图像，由去噪模块和超分辨率模块组成的发生器的作用是获取高质量的高分辨率人脸图像。鉴别器利用了一些精心设计的损失函数来保证恢复的人脸图像的质量。此外，我们将身份信息和注意机制融入到我们的网络中，以实现可行的相关特征表达和信息性特征学习。通过联合进行去噪和人脸超分辨率处理，这两个任务可以相互补充，获得良好的性能。大量的定性和定量结果表明，我们提出的JDSR-GAN方法优于一些分别执行前两个任务的可比较方法。
<details>	<summary>英文摘要</summary>	With the growing importance of preventing the COVID-19 virus, face images obtained in most video surveillance scenarios are low resolution with mask simultaneously. However, most of the previous face super-resolution solutions can not handle both tasks in one model. In this work, we treat the mask occlusion as image noise and construct a joint and collaborative learning network, called JDSR-GAN, for the masked face super-resolution task. Given a low-quality face image with the mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some comparable methods which perform the previous two tasks separately. </details>
<details>	<summary>注释</summary>	24 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 30、噪声数据的多帧超分辨率分析
- [ ] Multi-frame Super-resolution from Noisy Data 
时间：2021年03月25日                         第一作者：Kireeti Bodduna                        [链接](https://arxiv.org/abs/2103.13778).                     
## 摘要：由于问题的病态性，从低分辨率数据中获得高分辨率图像在算法上具有挑战性。到目前为止，这类问题几乎没有得到解决，现有的一些方法使用了简单的正则化方法。我们证明了两种基于各向异性扩散思想的自适应正则化方法的有效性：除了评估经典的边缘增强各向异性扩散正则化方法外，我们还引入了一种新的非局部正则化方法。这被称为部门扩散。我们将其与经典超分辨率观测模型的所有六种变体结合起来，这些变体是由其三种扭曲、模糊和下采样算子的排列产生的。令人惊讶的是，在实际相关的噪声场景中进行的评估产生的排名与我们之前工作（SSVM 2017）中在无噪声环境中的排名不同。
<details>	<summary>英文摘要</summary>	Obtaining high resolution images from low resolution data with clipped noise is algorithmically challenging due to the ill-posed nature of the problem. So far such problems have hardly been tackled, and the few existing approaches use simplistic regularisers. We show the usefulness of two adaptive regularisers based on anisotropic diffusion ideas: Apart from evaluating the classical edge-enhancing anisotropic diffusion regulariser, we introduce a novel non-local one with one-sided differences and superior performance. It is termed sector diffusion. We combine it with all six variants of the classical super-resolution observational model that arise from permutations of its three operators for warping, blurring, and downsampling. Surprisingly, the evaluation in a practically relevant noisy scenario produces a different ranking than the one in the noise-free setting in our previous work (SSVM 2017). </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 29、一种实用的深盲图像超分辨率退化模型的设计
- [ ] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution 
时间：2021年03月25日                         第一作者：Kai Zhang                       [链接](https://arxiv.org/abs/2103.14006).                     
## 摘要：人们普遍认为，如果假设的退化模型与真实图像中的退化模型相背离，单图像超分辨率（SISR）方法将无法取得很好的效果。虽然有几种退化模型考虑了模糊等其他因素，但它们仍然不能有效地覆盖真实图像的各种退化。针对这一问题，本文提出了一种更为复杂但实用的退化模型，该模型由随机混洗模糊、下采样和噪声退化三部分组成。具体地说，模糊由两个具有各向同性和各向异性高斯核的卷积来逼近；下采样从最近点、双线性和双三次插值中随机选择；噪声由不同噪声级的高斯噪声叠加而成，采用不同质量因子的JPEG压缩，通过逆前向摄像机图像信号处理（ISP）流水线模型和原始图像噪声模型生成处理后的摄像机传感器噪声。为了验证新退化模型的有效性，我们训练了一个深度盲ESRGAN超分解器，并将其应用于不同退化程度的合成图像和真实图像的超分辨。实验结果表明，新的退化模型有助于提高深超旋转变压器的实用性，为实际SISR应用提供了一种强有力的替代方案。
<details>	<summary>英文摘要</summary>	It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ESRGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications. </details>
<details>	<summary>注释</summary>	Code: https://github.com/cszn/BSRGAN </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 28、基于物理激励下采样核的内窥镜零炮超分辨
- [ ] Zero-shot super-resolution with a physically-motivated downsampling kernel for endomicroscopy 
时间：2021年03月25日                         第一作者：Agnieszka Barbara Szczotka                       [链接](https://arxiv.org/abs/2103.14015).                     
## 摘要：随着卷积神经网络（CNNs）的发展，超分辨率（SR）方法得到了长足的发展。CNNs已被成功应用于提高内镜成像质量。然而，内窥镜下SR研究的固有局限性仍然是缺乏地面真实高分辨率（HR）图像，通常用于监督训练和基于参考的图像质量评估（IQA）。因此，替代方法，如无监督SR正在探索中。为了解决非参考图像质量改善的需要，我们设计了一种新的零炮超分辨率（ZSSR）方法，该方法仅依赖于内窥镜数据，不需要地面真实图像，而是以自我监督的方式进行处理。我们根据内窥镜的特殊性定制了建议的管道，引入了两种方法：一种物理激励的Voronoi降尺度核，用于解释内窥镜基于不规则纤维的采样模式和真实的噪声模式。我们还利用视频序列来开发一个图像序列，以提高自监督零拍图像的质量。我们进行了烧蚀研究，以评估我们在缩小核尺度和噪声模拟方面的贡献。我们在合成数据和原始数据上验证了我们的方法。合成实验用基于参考的IQA进行评估，而我们对原始图像的结果则在由专家和非专家观察者进行的用户研究中进行评估。结果表明，ZSSR重建的图像质量优于基线方法。与有监督的单幅图像SR相比，ZSSR具有很强的竞争力，尤其是作为专家们首选的重建技术。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) methods have seen significant advances thanks to the development of convolutional neural networks (CNNs). CNNs have been successfully employed to improve the quality of endomicroscopy imaging. Yet, the inherent limitation of research on SR in endomicroscopy remains the lack of ground truth high-resolution (HR) images, commonly used for both supervised training and reference-based image quality assessment (IQA). Therefore, alternative methods, such as unsupervised SR are being explored. To address the need for non-reference image quality improvement, we designed a novel zero-shot super-resolution (ZSSR) approach that relies only on the endomicroscopy data to be processed in a self-supervised manner without the need for ground-truth HR images. We tailored the proposed pipeline to the idiosyncrasies of endomicroscopy by introducing both: a physically-motivated Voronoi downscaling kernel accounting for the endomicroscope's irregular fibre-based sampling pattern, and realistic noise patterns. We also took advantage of video sequences to exploit a sequence of images for self-supervised zero-shot image quality improvement. We run ablation studies to assess our contribution in regards to the downscaling kernel and noise simulation. We validate our methodology on both synthetic and original data. Synthetic experiments were assessed with reference-based IQA, while our results for original images were evaluated in a user study conducted with both expert and non-expert observers. The results demonstrated superior performance in image quality of ZSSR reconstructions in comparison to the baseline method. The ZSSR is also competitive when compared to supervised single-image SR, especially being the preferred reconstruction technique by experts. </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 27、基于跨任务知识转移的单深度超分辨率场景结构引导学习
- [ ] Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for Single Depth Super-Resolution 
时间：2021年03月24日                         第一作者：Baoli Sun                       [链接](https://arxiv.org/abs/2103.12955).                     
## 摘要：现有的颜色引导深度超分辨率（DSR）方法需要成对的RGB-D数据作为训练样本，利用RGB图像的几何相似性作为结构引导来恢复退化的深度图。然而，在实际测试环境中，成对数据的收集可能有限或昂贵。因此，我们第一次探索在训练阶段学习跨模态知识，在训练阶段RGB和深度模态都可用，但在目标数据集上测试，只有单一的深度模态存在。我们的核心思想是在不改变网络结构的前提下，将场景结构制导知识从RGB模态提取到单个DSR任务。具体地说，我们构造了一个以RGB图像为输入的辅助深度估计（DE）任务来估计深度图，并协同训练DSR任务和DE任务来提高DSR的性能。在此基础上，提出了一个跨任务交互模块来实现双边跨任务知识转移。首先，我们设计了一个跨任务的提炼方案，鼓励DSR和DE网络以师生角色交换的方式相互学习。然后，我们提出了一个结构预测（SP）任务，该任务提供额外的结构正则化，以帮助DSR和DE网络学习更多信息的结构表示，以便进行深度恢复。大量实验表明，与其他DSR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 26、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年03月24日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
## 摘要：近年来，采用深度复杂卷积神经网络结构的单图像超分辨率（SISR）方法取得了良好的效果。然而，这些方法以较高的内存消耗为代价来提高性能，难以应用于存储和计算资源有限的移动设备。为了解决这个问题，我们提出了一个轻量级的多尺度特征交互网络（MSFIN）。对于轻量级SISR，MSFIN扩展了接收域，充分利用了低分辨率观测图像的信息特征，这些特征来自不同的尺度和交互连接。此外，我们还设计了一个轻量级的循环剩余信道注意块（RRCAB），使得网络能够在充分轻量级的同时受益于信道注意机制。在一些基准上的大量实验已经证实，我们提出的MSFIN可以通过更轻量化的模型实现与现有技术相当的性能。
<details>	<summary>英文摘要</summary>	Recently, the single image super-resolution (SISR) approaches with deep and complex convolutional neural network structures have achieved promising performance. However, those methods improve the performance at the cost of higher memory consumption, which is difficult to be applied for some mobile devices with limited storage and computing resources. To solve this problem, we present a lightweight multi-scale feature interaction network (MSFIN). For lightweight SISR, MSFIN expands the receptive field and adequately exploits the informative features of the low-resolution observed images from various scales and interactive connections. In addition, we design a lightweight recurrent residual channel attention block (RRCAB) so that the network can benefit from the channel attention mechanism while being sufficiently lightweight. Extensive experiments on some benchmarks have confirmed that our proposed MSFIN can achieve comparable performance against the state-of-the-arts with a more lightweight model. </details>
<details>	<summary>注释</summary>	Accepted by ICME2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 25、UltraSR：空间编码是基于隐式图像函数的任意尺度超分辨率的关键
- [ ] UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution 
时间：2021年03月23日                         第一作者：Xingqian Xu                       [链接](https://arxiv.org/abs/2103.12716).                     
## 摘要：NeRF和其他相关隐式神经表示方法的成功为连续图像表示开辟了一条新的途径，即不再需要从存储的离散二维阵列中查找像素值，而是可以从连续空间域上的神经网络模型中推断像素值。尽管最近的研究表明，这种新的方法在任意尺度的超分辨率任务中都能取得很好的效果，但由于高频纹理的错误预测，放大后的图像往往会出现结构性失真。在这项工作中，我们提出了一种简单而有效的基于隐式图像函数的新网络设计方法UltraSR，其中空间坐标和周期编码与隐式神经表示深度结合。我们通过大量的实验和研究表明，空间编码确实是下一阶段高精度隐式图像函数的关键。与以前最先进的方法相比，我们的UltraSR在DIV2K基准上设置了所有超分辨率标度下的最新性能。UltraSR在其他标准基准数据集上也取得了优异的性能，在几乎所有的实验中都优于以前的工作。我们的代码将在https://github.com/SHI-Labs/UltraSR-arbitral-Scale-Super-Resolution。
<details>	<summary>英文摘要</summary>	The recent success of NeRF and other related implicit neural representation methods has opened a new path for continuous image representation, where pixel values no longer need to be looked up from stored discrete 2D arrays but can be inferred from neural network models on a continuous spatial domain. Although the recent work LIIF has demonstrated that such novel approach can achieve good performance on the arbitrary-scale super-resolution task, their upscaled images frequently show structural distortion due to the faulty prediction on high-frequency textures. In this work, we propose UltraSR, a simple yet effective new network design based on implicit image functions in which spatial coordinates and periodic encoding are deeply integrated with the implicit neural representation. We show that spatial encoding is indeed a missing key towards the next-stage high-accuracy implicit image function through extensive experiments and ablation studies. Our UltraSR sets new state-of-the-art performance on the DIV2K benchmark under all super-resolution scales comparing to previous state-of-the-art methods. UltraSR also achieves superior performance on other standard benchmark datasets in which it outperforms prior works in almost all experiments. Our code will be released at https://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution. </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 24、双子网多级通信上采样大运动视频超分辨率
- [ ] Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling 
时间：2021年03月22日                         第一作者：Hongying Liu                       [链接](https://arxiv.org/abs/2103.11744).                     
## 摘要：视频超分辨率（VSR）的目标是恢复低分辨率（LR）的视频，并将其提高到更高的分辨率（HR）。由于视频任务的特点，在VSR算法中，对帧间运动信息的关注、总结和利用是非常重要的。特别是当视频包含大运动时，传统的方法容易产生非相干的结果或伪影。提出了一种新的双子网多级通信上采样深度神经网络（DSMC），用于大运动视频的超分辨率处理。设计了一个新的三维卷积U形残差密集网络（U3D-RDN）模块，用于精细隐式运动估计和运动补偿（MEMC）以及粗空间特征提取。提出了一种新的多级通信上采样（MSCU）模块，充分利用上采样的中间结果指导VSR。此外，本文还设计了一种新的双子网来辅助DSMC的训练，它的双子网损失有助于减少解空间和提高泛化能力。实验结果表明，与现有的方法相比，该方法在大运动视频上具有更好的性能。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月23日</details>

# 23、一种新的单图像超分辨率公共Alsat-2B数据集
- [ ] A new public Alsat-2B dataset for single-image super-resolution 
时间：2021年03月21日                         第一作者：Achraf Djerida                       [链接](https://arxiv.org/abs/2103.12547).                     
## 摘要：目前，当有可靠的训练数据集可用时，深度学习方法是图像超分辨率的主要解决方案。然而，对于遥感基准来说，获取高空间分辨率的图像是非常昂贵的。大多数超分辨率方法都采用下采样技术来模拟低分辨率和高分辨率的空间对，并构造训练样本。为了解决这一问题，本文提出了一种新的低分辨率和高分辨率（分别为10m和2.5m）的公共遥感数据集（Alsat2B），用于单幅图像的超分辨率处理。通过平移锐化得到高分辨率图像。此外，基于通用准则对数据集上一些超分辨率方法的性能进行了评估。结果表明，该方法是有前途的，并突出了数据集的挑战，这表明需要先进的方法来掌握低分辨率和高分辨率斑块之间的关系。
<details>	<summary>英文摘要</summary>	Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches. </details>
<details>	<summary>注释</summary>	This paper has been Accepted for publication in the International Geoscience and Remote Sensing Symposium (IGARSS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 22、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年03月19日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
## 摘要：高光谱图像具有较窄的光谱波段，能够获取丰富的光谱信息，适合于许多计算机视觉任务。HSI的一个基本限制是它的低空间分辨率，最近一些关于超分辨率（SR）的工作被提出来解决这个问题。然而，由于HSI摄像机的多样性，不同的摄像机捕捉到的图像具有不同的光谱响应函数和总通道数。现有的HSI数据集通常很小，因此不足以建模。提出了一种基于元学习的超分辨率（MLSR）模型，该模型可以在任意多个输入波段的峰值波长处获取HSI图像，并生成任意多个输出波段的峰值波长的超分辨率HSI。我们通过对NTIRE2020和ICVL数据集的波段进行采样，人工创建子数据集，模拟交叉数据集设置，并对其进行谱内插和外推的HSI SR。我们为所有子数据集训练单个MLSR模型，并为每个子数据集训练专用的基线模型。结果表明，与现有的HSI-SR方法相比，该模型具有相同的水平或更好的性能。
<details>	<summary>英文摘要</summary>	Hyperspectral images (HSIs) with narrow spectral bands can capture rich spectral information, making them suitable for many computer vision tasks. One of the fundamental limitations of HSI is its low spatial resolution, and several recent works on super-resolution(SR) have been proposed to tackle this challenge. However, due to HSI cameras' diversity, different cameras capture images with different spectral response functions and the number of total channels. The existing HSI datasets are usually small and consequently insufficient for modeling. We propose a Meta-Learning-Based Super-Resolution(MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate super-resolved HSIs with an arbitrary number of output bands' peak wavelengths. We artificially create sub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate the cross-dataset settings and perform HSI SR with spectral interpolation and extrapolation on them. We train a single MLSR model for all sub-datasets and train dedicated baseline models for each sub-dataset. The results show the proposed model has the same level or better performance compared to the-state-of-the-art HSI SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 21、视频超分辨率的自监督自适应算法
- [ ] Self-Supervised Adaptation for Video Super-Resolution 
时间：2021年03月18日                         第一作者：Jinsu Yoo                        [链接](https://arxiv.org/abs/2103.10081).                     
## 摘要：近年来，单图像超分辨率（single-image super-resolution，SISR）网络通过利用输入数据中的信息和大量外部数据集，使网络参数适应特定的输入图像，取得了良好的效果。然而，这些自监督SISR方法在视频处理中的扩展还有待研究。因此，我们提出了一种新的学习算法，使得传统的视频超分辨率（VSR）网络能够在不使用地面真实数据集的情况下调整参数来测试视频帧。通过利用空间和时间上的许多自相似块，我们提高了完全预训练VSR网络的性能，并产生了时间一致的视频帧。此外，我们提出了一种测试时知识提取技术，以较少的硬件资源加快了自适应速度。在我们的实验中，我们证明了我们的新学习算法可以微调最先进的VSR网络，并在大量的基准数据集上显著提高性能。
<details>	<summary>英文摘要</summary>	Recent single-image super-resolution (SISR) networks, which can adapt their network parameters to specific input images, have shown promising results by exploiting the information available within the input data as well as large external datasets. However, the extension of these self-supervised SISR approaches to video handling has yet to be studied. Thus, we present a new learning algorithm that allows conventional video super-resolution (VSR) networks to adapt their parameters to test video frames without using the ground-truth datasets. By utilizing many self-similar patches across space and time, we improve the performance of fully pre-trained VSR networks and produce temporally consistent video frames. Moreover, we present a test-time knowledge distillation technique that accelerates the adaptation speed with less hardware resources. In our experiments, we demonstrate that our novel learning algorithm can fine-tune state-of-the-art VSR networks and substantially elevate performance on numerous benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年03月19日</details>

# 20、结构化输出依赖建模的一般感知损失
- [ ] Generic Perceptual Loss for Modeling Structured Output Dependencies 
时间：2021年03月18日                         第一作者：Yifan Liu                       [链接](https://arxiv.org/abs/2103.10571).                     
## 摘要：感知损失作为一个有效的损失项被广泛应用于图像合成任务中，包括图像超分辨率、风格转换等。人们认为，成功的关键在于从经过大量图像预训练的CNNs中提取高层次的感知特征表示。这里我们揭示了，重要的是网络结构，而不是训练的权重。在没有任何学习的情况下，深层网络的结构就足以利用多层cnn捕获多个层次的变量统计之间的依赖关系。这种洞察消除了预先训练和特定网络结构（通常是VGG）的要求，这些都是先前假设的感知损失，从而实现了更广泛的应用。为此，我们证明了一个随机加权的深度CNN可以用来模拟输出的结构化依赖关系。在语义分割、深度估计和实例分割等稠密的逐像素预测任务中，与单独使用逐像素丢失的基线相比，扩展的随机感知丢失方法得到了更好的结果。我们希望这种简单的，扩展的知觉损失可以作为一种通用的结构化输出损失，适用于大多数结构化输出学习任务。
<details>	<summary>英文摘要</summary>	The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution, and style transfer. It was believed that the success lies in the high-level perceptual feature representations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the network structure instead of the trained weights. Without any learning, the structure of a deep network is sufficient to capture the dependencies between multiple levels of variable statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously assumed for the perceptual loss, thus enabling a significantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks. </details>
<details>	<summary>注释</summary>	Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 19、视频流预测辅助帧超分辨率
- [ ] Prediction-assistant Frame Super-Resolution for Video Streaming 
时间：2021年03月17日                         第一作者：Wang Shen                       [链接](https://arxiv.org/abs/2103.09455).                     
## 摘要：在在线视频游戏、现场直播等实时应用中，视频帧的传输延迟是至关重要的，新帧的接收截止时间必须赶上帧的渲染时间。否则，系统会缓冲一段时间，用户会遇到冻结屏幕，导致用户体验不尽如人意。一种有效的方法是在较差的带宽条件下传输较低质量的帧，例如使用可伸缩视频编码。在本文中，我们提出在两种情况下使用有损帧来提高视频质量。首先，当当前帧在渲染截止时间之前太晚而无法接收时（即丢失），我们建议使用先前接收到的高分辨率图像来预测未来的帧。第二，当当前接收到的帧的质量较低（即有损）时，我们建议使用先前接收到的高分辨率帧来增强低质量的当前帧。对于第一种情况，我们提出了一个小而有效的视频帧预测网络。对于第二种情况，我们将视频预测网络改进为视频增强网络，将当前帧和前一帧关联起来，以恢复高质量的图像。大量的实验结果表明，我们的方法在有损视频流环境中的性能优于现有的算法。
<details>	<summary>英文摘要</summary>	Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment. </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 18、ShipSRDet：一种基于超分辨特征表示的端到端遥感舰船探测器
- [ ] ShipSRDet: An End-to-End Remote Sensing Ship Detector Using Super-Resolved Feature Representation 
时间：2021年03月17日                         第一作者：Shitian He                       [链接](https://arxiv.org/abs/2103.09699).                     
## 摘要：高分辨率遥感图像可以为船舶检测提供丰富的外观信息。虽然已有的一些方法采用图像超分辨率（SR）来提高检测性能，但它们将图像超分辨率和船舶检测视为两个独立的过程，忽略了这两个相关任务之间的内在一致性。在本文中，我们探讨了图像SR对船舶检测的潜在好处，并提出了一种端到端网络ShipSRDet。在我们的方法中，我们不仅将超分辨图像提供给检测器，而且将SR网络的中间特征与检测网络的中间特征结合起来。这样，SR网络提取的信息性特征表示就可以充分用于船舶检测。在HRSC数据集上的实验结果验证了该方法的有效性。我们的ShipSRDet可以从输入图像中恢复丢失的细节，并取得了良好的船舶检测性能。
<details>	<summary>英文摘要</summary>	High-resolution remote sensing images can provide abundant appearance information for ship detection. Although several existing methods use image super-resolution (SR) approaches to improve the detection performance, they consider image SR and ship detection as two separate processes and overlook the internal coherence between these two correlated tasks. In this paper, we explore the potential benefits introduced by image SR to ship detection, and propose an end-to-end network named ShipSRDet. In our method, we not only feed the super-resolved images to the detector but also integrate the intermediate features of the SR network with those of the detection network. In this way, the informative feature representation extracted by the SR network can be fully used for ship detection. Experimental results on the HRSC dataset validate the effectiveness of our method. Our ShipSRDet can recover the missing details from the input image and achieves promising ship detection performance. </details>
<details>	<summary>注释</summary>	Accepted to IGARSS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 17、基于单镜头样本的超分辨跨域人脸模型
- [ ] Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar 
时间：2021年03月16日                         第一作者：Peike Li                       [链接](https://arxiv.org/abs/2103.08863).                     
## 摘要：传统的人脸超分辨率方法通常假设检测低分辨率（LR）图像与训练图像位于同一个域。由于光照条件和成像硬件的不同，在许多实际场景中，训练图像和测试图像之间不可避免地会出现域间隙。忽略这些域间隙将导致较低的人脸超分辨率（FSR）性能。然而，如何将训练好的FSR模型有效地转移到目标域中还没有被研究。为了解决这个问题，我们开发了一个基于域感知金字塔的人脸超分辨率网络，命名为DAP-FSR网络。我们的DAP-FSR是第一次尝试利用目标域中的一对高分辨率（HR）和LR样本从目标域超分辨LR人脸。具体来说，我们的DAP-FSR首先使用编码器来提取输入LR人脸的多尺度潜在表示。考虑到只有一个目标域的例子可用，我们建议通过混合目标域面和源域面的潜在表示来扩充目标域数据，然后将混合表示提供给我们的DAP-FSR解码器。解码器将生成与目标域图像样式相似的新人脸图像。生成的HR面依次用于优化我们的解码器以减少域间隙。通过迭代更新潜在的表示和我们的解码器，我们的DAP-FSR将适应目标域，从而实现真实和高质量的上采样HR人脸。在三个新构建的基准上的大量实验验证了我们的DAP-FSR的有效性和优越的性能。
<details>	<summary>英文摘要</summary>	Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art. </details>
<details>	<summary>邮件日期</summary>	2021年03月17日</details>

# 16、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年03月15日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
## 摘要：基于深度学习的方法，特别是卷积神经网络（CNNs）已经成功地应用于单幅图像超分辨率（SISR）领域。为了获得更好的逼真度和视觉质量，现有的网络大多采用繁重的设计和大量的计算。然而，现代移动设备的计算资源有限，难以承受昂贵的成本。为此，本文提出了一种基于离散余弦变换（DCT）域的频率感知动态网络，将输入信号按其系数分为多个部分。在实际应用中，高频部分采用昂贵的运算，低频部分采用廉价的运算，以减轻计算负担。由于像素或图像块属于低频区域，包含的纹理细节相对较少，因此这种动态网络不会影响生成的超分辨率图像的质量。此外，我们将预测器嵌入到所提出的动态网路中，以端到端微调手工制作的频率感知遮罩。在基准SISR模型和数据集上进行的大量实验表明，频率感知动态网络可以应用于各种SISR神经结构，在视觉质量和计算复杂度之间获得更好的折衷。例如，我们可以在保持最先进的SISR性能的同时，将EDSR模型的失败率降低大约50\%$。
<details>	<summary>英文摘要</summary>	Deep learning based methods, especially convolutional neural networks (CNNs) have been successfully applied in the field of single image super-resolution (SISR). To obtain better fidelity and visual quality, most of existing networks are of heavy design with massive computation. However, the computation resources of modern mobile devices are limited, which cannot easily support the expensive cost. To this end, this paper explores a novel frequency-aware dynamic network for dividing the input into multiple parts according to its coefficients in the discrete cosine transform (DCT) domain. In practice, the high-frequency part will be processed using expensive operations and the lower-frequency part is assigned with cheap operations to relieve the computation burden. Since pixels or image patches belong to low-frequency areas contain relatively few textural details, this dynamic network will not affect the quality of resulting super-resolution images. In addition, we embed predictors into the proposed dynamic network to end-to-end fine-tune the handcrafted frequency-aware masks. Extensive experiments conducted on benchmark SISR models and datasets show that the frequency-aware dynamic network can be employed for various SISR neural architectures to obtain the better tradeoff between visual quality and computational complexity. For instance, we can reduce the FLOPs of EDSR model by approximate $50\%$ while preserving state-of-the-art SISR performance. </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 15、低分辨率图像和视频中的三维人体姿势、形状和纹理
- [ ] 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos 
时间：2021年03月11日                         第一作者：Xiangyu Xu                       [链接](https://arxiv.org/abs/2103.06498).                     
## 摘要：基于单眼图像的三维人体姿态和形状估计一直是计算机视觉领域的一个研究热点。现有的深度学习方法依赖于高分辨率的输入，然而，在视频监控和体育广播等许多场景中并不总是可用的。处理低分辨率图像的两种常用方法是对输入应用超分辨率技术，这可能会导致不愉快的伪影，或者只是针对每个分辨率训练一个模型，这在许多实际应用中是不切实际的。针对上述问题，本文提出了一种新的算法RSC-Net，该算法由分辨率感知网络、自监督损失和对比学习机制组成。该方法能够在单个模型上学习不同分辨率下的三维人体姿态和形状。自我监督缺失加强了输出的尺度一致性，而对比学习方案加强了深层特征的尺度一致性。我们表明，这两个新的损失提供鲁棒性学习时，在弱监督的方式。此外，我们扩展了RSC网络来处理低分辨率的视频，并将其应用于从低分辨率输入中重建具有纹理的三维行人。大量的实验表明，RSC网络在处理低分辨率图像时，可以取得比现有方法更好的效果。
<details>	<summary>英文摘要</summary>	3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2007.13666 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 14、一种基于学习的轴向超分辨率视图外推方法
- [ ] A learning-based view extrapolation method for axial super-resolution 
时间：2021年03月11日                         第一作者：Zhaolin Xiao                       [链接](https://arxiv.org/abs/2103.06510).                     
## 摘要：轴向光场分辨率是指通过重新聚焦来区分不同深度特征的能力。轴向再聚焦精度相当于两个可分辨的再聚焦平面在轴向上的最小距离。高再聚焦精度对于一些光场应用（如显微镜）来说是必不可少的。在这篇论文中，我们提出了一种基于学习的方法来外推新的观点从轴向体积剪切极平面图像（EPIs）。与经典成像中的扩展数值孔径（NA）一样，外推光场可以获得具有较浅景深（DOF）的重聚焦图像，从而获得更精确的重聚焦结果。最重要的是，该方法不需要精确的深度估计。对合成光场和真实光场的实验结果表明，该方法不仅适用于全光相机（尤其是1.0型全光相机）拍摄的基线较小的光场，而且适用于基线较大的光场。
<details>	<summary>英文摘要</summary>	Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines. </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 13、使用真实退化图像的超分辨率卫星硬件
- [ ] Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images 
时间：2021年03月10日                         第一作者：Jack White                       [链接](https://arxiv.org/abs/2103.06270).                     
## 摘要：现代深超分辨率（SR）网络已成为图像重建和增强的重要技术。然而，这些网络通常是在缺乏真实图像中典型的图像降质噪声的基准图像数据上训练和测试的。在这篇论文中，我们通过评估SR在重建真实退化卫星图像中的性能，来测试在真实遥感有效载荷中使用深度SR的可行性。我们证明了一种称为增强深超分辨率网络（EDSR）的先进SR技术，在没有特定领域的预训练的情况下，只要地面分辨率足够远，就可以在地面采样距离较短的图像上恢复编码的像素数据。然而，这种恢复因所选地理类型而异。我们的结果表明，定制训练有可能进一步改善高架图像的重建，新的卫星硬件应优先考虑光学性能，而不是最小化像素大小，因为深SR可以克服后者的不足，但不能克服前者。
<details>	<summary>英文摘要</summary>	Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former. </details>
<details>	<summary>注释</summary>	6 pages, 6 figures, for supplementary results, see https://smpetrie.github.io/superres/ ACM-class: I.4.3 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 12、高光谱图像超分辨率空间光谱反馈网络
- [ ] Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral Imagery 
时间：2021年03月07日                         第一作者：Enhai Liu                       [链接](https://arxiv.org/abs/2103.04354).                     
## 摘要：近年来，基于深度学习的单灰度/RGB图像超分辨率（SR）方法取得了很大的成功。然而，单幅高光谱图像的超分辨率处理存在两大障碍，限制了技术的发展。一是高光谱图像中高维复杂的光谱模式，使得空间信息和波段间的光谱信息难以同时获取。另一方面，高光谱训练样本的数目非常小，在训练深度神经网络时容易导致过度拟合。为了解决这些问题，本文提出了一种新的空间谱反馈网络（SSFN），利用全局谱带的高阶信息来细化局部谱带间的低阶表示。它不仅可以缓解高光谱数据高维性给特征提取带来的困难，而且可以使训练过程更加稳定。具体地说，我们在具有有限展开的RNN中使用隐藏状态来实现这种反馈方式。为了充分利用空间和光谱先验知识，设计了空间光谱反馈块（SSFB）来处理反馈连接并生成强大的高层表示。提出的SSFN具有早期预测能力，可以逐步重建最终的高分辨率高光谱图像。在三个基准数据集上的大量实验结果表明，与现有的方法相比，所提出的SSFN具有更好的性能。源代码位于https://github.com/tangzhenjie/SSFN。
<details>	<summary>英文摘要</summary>	Recently, single gray/RGB image super-resolution (SR) methods based on deep learning have achieved great success. However, there are two obstacles to limit technical development in the single hyperspectral image super-resolution. One is the high-dimensional and complex spectral patterns in hyperspectral image, which make it difficult to explore spatial information and spectral information among bands simultaneously. The other is that the number of available hyperspectral training samples is extremely small, which can easily lead to overfitting when training a deep neural network. To address these issues, in this paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to refine low-level representations among local spectral bands with high-level information from global spectral bands. It will not only alleviate the difficulty in feature extraction due to high dimensional of hyperspectral data, but also make the training process more stable. Specifically, we use hidden states in an RNN with finite unfoldings to achieve such feedback manner. To exploit the spatial and spectral prior, a Spatial-Spectral Feedback Block (SSFB) is designed to handle the feedback connections and generate powerful high-level representations. The proposed SSFN comes with a early predictions and can reconstruct the final high-resolution hyperspectral image step by step. Extensive experimental results on three benchmark datasets demonstrate that the proposed SSFN achieves superior performance in comparison with the state-of-the-art methods. The source code is available at https://github.com/tangzhenjie/SSFN. </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 11、基于深度学习的小数据集超分辨荧光显微镜
- [ ] Deep learning-based super-resolution fluorescence microscopy on small datasets 
时间：2021年03月07日                         第一作者：Varun Mannam                       [链接](https://arxiv.org/abs/2103.04989).                     
## 摘要：荧光显微术以微米级的分辨率显示生物有机体，使现代生物学有了巨大的发展。然而，由于衍射极限的限制，亚微米/纳米级的特征很难分辨。虽然各种超分辨率技术是为了达到纳米级的分辨率而发展起来的，但它们往往需要昂贵的光学装置或专门的荧光团。近年来，深度学习显示出减少技术障碍和从衍射限制图像获得超分辨率的潜力。为了得到准确的结果，传统的深度学习技术需要成千上万的图像作为训练数据集。由于荧光团的光漂白、光毒性和生物体内发生的动态过程，从生物样品中获取大量数据通常是不可行的。因此，利用小数据集实现基于深度学习的超分辨率具有挑战性。我们用一种新的基于卷积神经网络的方法来解决这一限制，这种方法成功地用小数据集训练并获得超分辨率图像。我们从15个不同的视场共采集了750张图像作为训练数据集来演示该技术。在每个视场中，采用超分辨率径向起伏方法生成单个目标图像。正如预期的那样，这个小数据集无法使用传统的超分辨率体系结构生成可用的模型。然而，使用新的方法，可以训练一个网络来从这个小数据集获得超分辨率图像。这种深度学习模型可应用于其他生物医学成像模式，如MRI和X射线成像，在这些模式中获取大型训练数据集是一项挑战。
<details>	<summary>英文摘要</summary>	Fluorescence microscopy has enabled a dramatic development in modern biology by visualizing biological organisms with micrometer scale resolution. However, due to the diffraction limit, sub-micron/nanometer features are difficult to resolve. While various super-resolution techniques are developed to achieve nanometer-scale resolution, they often either require expensive optical setup or specialized fluorophores. In recent years, deep learning has shown the potentials to reduce the technical barrier and obtain super-resolution from diffraction-limited images. For accurate results, conventional deep learning techniques require thousands of images as a training dataset. Obtaining large datasets from biological samples is not often feasible due to the photobleaching of fluorophores, phototoxicity, and dynamic processes occurring within the organism. Therefore, achieving deep learning-based super-resolution using small datasets is challenging. We address this limitation with a new convolutional neural network-based approach that is successfully trained with small datasets and achieves super-resolution images. We captured 750 images in total from 15 different field-of-views as the training dataset to demonstrate the technique. In each FOV, a single target image is generated using the super-resolution radial fluctuation method. As expected, this small dataset failed to produce a usable model using traditional super-resolution architecture. However, using the new approach, a network can be trained to achieve super-resolution images from this small dataset. This deep learning model can be applied to other biomedical imaging modalities such as MRI and X-ray imaging, where obtaining large training datasets is challenging. </details>
<details>	<summary>注释</summary>	SPIE Proceedings Volume 11650, Single Molecule Spectroscopy and Superresolution Imaging XIV; 116500O (2021) DOI: 10.1117/12.2578519 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 10、ClassSR：一种利用数据特征加速超分辨率网络的通用框架
- [ ] ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic 
时间：2021年03月06日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2103.04039).                     
## 摘要：我们的目标是在大图像（2K-8K）上加速超分辨率（SR）网络。在实际应用中，大图像通常被分解成小的子图像。在此基础上，我们发现不同的图像区域具有不同的恢复难度，可以由不同容量的网络进行处理。直观地说，平滑区域比复杂纹理更容易超级求解。为了利用这一特性，我们可以采用适当的SR网络对分解后的不同子图像进行处理。在此基础上，我们提出了一种新的解决方案管道——ClassSR，它将分类和SR结合在一个统一的框架中。特别地，它首先使用一个类模块将子图像按恢复难度分为不同的类，然后应用一个SR模块对不同的类进行SR。类模块是一个传统的分类网络，而SR模块是一个由待加速SR网络及其简化版本组成的网络容器。我们进一步引入了一种新的两损失分类方法——类别损失和平均损失来产生分类结果。联合训练后，大部分子图像将通过较小的网络，从而大大降低了计算量。实验表明，我们的ClassSR可以帮助大多数现有方法（如FSRCNN、CARN、SRResNet、RCAN）在DIV8K数据集上节省高达50%的失败率。这个通用框架也可以应用于其他低层次的视觉任务。
<details>	<summary>英文摘要</summary>	We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks. </details>
<details>	<summary>注释</summary>	CVPR2021 paper + supplementary file </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 9、用稀疏表示生成图像
- [ ] Generating Images with Sparse Representations 
时间：2021年03月05日                         第一作者：Charlie Nash                       [链接](https://arxiv.org/abs/2103.03841).                     
## 摘要：图像的高维性对基于似然的生成模型的结构和采样效率提出了挑战。以前的方法，例如VQ-VAE，使用深度自动编码器来获得紧凑的表示，作为基于似然模型的输入更为实用。我们提出了另一种方法，受JPEG等常见图像压缩方法的启发，将图像转换为量化的离散余弦变换（DCT）块，这些块稀疏地表示为DCT通道、空间位置和DCT系数三元组的序列。我们提出了一种基于变换器的自回归结构，该结构被训练成序列预测下一个元素在这些序列中的条件分布，并有效地扩展到高分辨率图像。在一系列的图像数据集上，我们证明了我们的方法可以生成高质量、多样的图像，并且样本度量分数可以与最先进的方法相竞争。此外，我们还表明，简单的修改，我们的方法产生有效的图像着色和超分辨率模型。
<details>	<summary>英文摘要</summary>	The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 8、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月05日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	Accepted to CVPR 2021. The first two authors contributed equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 7、真实世界的单图像超分辨率：简要回顾
- [ ] Real-World Single Image Super-Resolution: A Brief Review 
时间：2021年03月03日                         第一作者：Honggang Chen                       [链接](https://arxiv.org/abs/2103.02368).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是近几十年来图像处理领域的一个研究热点，其目的是通过低分辨率（low-resolution，LR）观测重建高分辨率（high-resolution，HR）图像。特别是基于深度学习的超分辨率（SR）方法已经引起了人们的广泛关注，极大地提高了合成数据的重建性能。最近的研究表明，对合成数据的模拟结果通常高估了对真实世界图像的超分辨能力。在这样的背景下，越来越多的研究者致力于研究真实感图像的随机共振方法。本文对现实世界中的单幅图像超分辨率（RSISR）技术进行了综述。更具体地说，本综述涵盖了RSISR的关键公共可用数据集和评估指标，以及四大类RSISR方法，即基于退化建模的RSISR、基于图像对的RSISR、基于领域翻译的RSISR和基于自学习的RSISR。在基准数据集上比较了代表性RSISR方法的重建质量和计算效率。此外，我们还讨论了RSISR面临的挑战和未来的研究课题。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR), which aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) observation, has been an active research topic in the area of image processing in recent decades. Particularly, deep learning-based super-resolution (SR) approaches have drawn much attention and have greatly improved the reconstruction performance on synthetic data. Recent studies show that simulation results on synthetic data usually overestimate the capacity to super-resolve real-world images. In this context, more and more researchers devote themselves to develop SR approaches for realistic images. This article aims to make a comprehensive review on real-world single image super-resolution (RSISR). More specifically, this review covers the critical publically available datasets and assessment metrics for RSISR, and four major categories of RSISR methods, namely the degradation modeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR, and self-learning-based RSISR. Comparisons are also made among representative RSISR methods on benchmark datasets, in terms of both reconstruction quality and computational efficiency. Besides, we discuss challenges and promising research topics on RSISR. </details>
<details>	<summary>注释</summary>	18 pages, 12 figure, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 6、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年02月25日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
## 摘要：近年来，大量的研究活动致力于视频增强，同时提高时间帧速率和空间分辨率。然而，现有的方法要么不能揭示时空信息之间的内在联系，要么在最终的时空分辨率的选择上缺乏灵活性。在这项工作中，我们提出了一个无约束的时空视频超分辨率网络，它可以有效地利用时空相关性来提高性能。此外，通过使用光流技术和广义像素混洗操作，它在调整时间帧速率和空间分辨率方面具有完全的自由度。实验结果表明，该方法不仅优于现有的算法，而且所需参数少，运行时间短。
<details>	<summary>英文摘要</summary>	Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 5、ShuffleUNet：基于深度学习的磁共振弥散加权成像的超分辨率
- [ ] ShuffleUNet: Super resolution of diffusion-weighted MRIs using deep learning 
时间：2021年02月25日                         第一作者：Soumick Chatterjee                       [链接](https://arxiv.org/abs/2102.12898).                     
## 摘要：弥散加权磁共振成像（DW-MRI）可用于表征神经组织的微观结构，例如通过纤维追踪以非侵入性方式描绘脑白质连接。高空间分辨率的磁共振成像（MRI）将在以更好的方式显示这些纤维束方面发挥重要作用。然而，获得这种分辨率的图像是以较长的扫描时间为代价的。由于患者的心理和身体状况，较长的扫描时间可能与运动伪影的增加有关。单图像超分辨率（Single-Image Super-Resolution，SISR）是一种通过深度学习从单个低分辨率（low-Resolution，LR）输入图像中获取高分辨率细节的技术，是本研究的重点。与插值技术或稀疏编码算法相比，深度学习算法从大数据集中提取先验知识，并从低分辨率图像中产生更优的MRI图像。本研究提出一种基于深度学习的超分辨技术，并将其应用于DW-MRI。从IXI数据集得到的图像被用作地面真值，并被人工降采样以模拟低分辨率图像。所提出的方法在统计学上比基线有了显著的改进，实现了0.913美元-0.045美元的SSIM。
<details>	<summary>英文摘要</summary>	Diffusion-weighted magnetic resonance imaging (DW-MRI) can be used to characterise the microstructure of the nervous tissue, e.g. to delineate brain white matter connections in a non-invasive manner via fibre tracking. Magnetic Resonance Imaging (MRI) in high spatial resolution would play an important role in visualising such fibre tracts in a superior manner. However, obtaining an image of such resolution comes at the expense of longer scan time. Longer scan time can be associated with the increase of motion artefacts, due to the patient's psychological and physical conditions. Single Image Super-Resolution (SISR), a technique aimed to obtain high-resolution (HR) details from one single low-resolution (LR) input image, achieved with Deep Learning, is the focus of this study. Compared to interpolation techniques or sparse-coding algorithms, deep learning extracts prior knowledge from big datasets and produces superior MRI images from the low-resolution counterparts. In this research, a deep learning based super-resolution technique is proposed and has been applied for DW-MRI. Images from the IXI dataset have been used as the ground-truth and were artificially downsampled to simulate the low-resolution images. The proposed method has shown statistically significant improvement over the baselines and achieved an SSIM of $0.913\pm0.045$. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 4、用于视频超分辨率的深展开网络
- [ ] Deep Unrolled Network for Video Super-Resolution 
时间：2021年02月23日                         第一作者：Benjamin Naoto Chiche                       [链接](https://arxiv.org/abs/2102.11720).                     
## 摘要：视频超分辨率（VSR）的目的是从相应的低分辨率（LR）图像中重建一系列高分辨率（HR）图像。传统上，VSR问题的求解是基于迭代算法，该算法可以利用图像形成的先验知识和运动的假设。然而，这些经典的方法很难将自然图像中的复杂统计信息结合起来。此外，VSR最近受益于深度学习（DL）算法带来的改进。这些技术可以有效地从大量的图像集合中学习空间模式。然而，他们没有融入一些有关图像形成模型的知识，这限制了他们的灵活性。为解决反问题而开发的展开优化算法允许将先验信息纳入深度学习体系结构。它们主要用于单个图像恢复任务。采用展开的神经网络结构可以带来以下好处。首先，这可能会提高超分辨率任务的性能。这样，神经网络就具有更好的可解释性。最后，这允许灵活地学习单个模型，以非盲目地处理多个退化。本文提出了一种新的基于展开优化技术的VSR神经网络，并对其性能进行了讨论。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to reconstruct a sequence of high-resolution (HR) images from their corresponding low-resolution (LR) versions. Traditionally, solving a VSR problem has been based on iterative algorithms that can exploit prior knowledge on image formation and assumptions on the motion. However, these classical methods struggle at incorporating complex statistics from natural images. Furthermore, VSR has recently benefited from the improvement brought by deep learning (DL) algorithms. These techniques can efficiently learn spatial patterns from large collections of images. Yet, they fail to incorporate some knowledge about the image formation model, which limits their flexibility. Unrolled optimization algorithms, developed for inverse problems resolution, allow to include prior information into deep learning architectures. They have been used mainly for single image restoration tasks. Adapting an unrolled neural network structure can bring the following benefits. First, this may increase performance of the super-resolution task. Then, this gives neural networks better interpretability. Finally, this allows flexibility in learning a single model to nonblindly deal with multiple degradations. In this paper, we propose a new VSR neural network based on unrolled optimization techniques and discuss its performance. </details>
<details>	<summary>注释</summary>	6 pages. 3 figures. Published in: 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA) DOI: 10.1109/IPTA50016.2020.9286636 </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 3、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月23日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 2、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月21日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
## 摘要：最近COVID-19的爆发促使研究人员在利用人工智能和深度学习的医学成像领域做出贡献。超分辨率（SR）在过去的几年中，利用深度学习方法取得了显著的效果。深度学习方法学习从低分辨率（LR）图像到相应的高分辨率（HR）图像的非线性映射的能力导致了SR在不同研究领域的引人注目的结果。本文提出了一种基于深度学习的切比切夫变换域图像超分辨率结构。这是通过将转换层通过定制的Tchebichef卷积层（$TCL$）集成到提议的体系结构中来实现的。TCL的作用是利用Tchebichef基函数将LR图像从空间域转换到正交变换域。使用称为逆切比雪夫卷积层（ITCL）的另一层实现上述变换的反演，该层将LR图像从变换域转换回空间域。研究表明，利用Tchebichef变换域进行超分辨率重建，利用了图像的高低频特征，简化了超分辨率重建任务。我们进一步引入转移学习方法来提高基于Covid的医学图像的质量。结果表明，我们的结构提高了COVID-19的X线和CT图像质量，提供了更好的图像质量，有助于临床诊断。与使用较少可训练参数的大多数深度学习方法相比，使用所提出的切比雪夫变换域超分辨率（TTDSR）结构获得的实验结果提供了具有竞争力的结果。
<details>	<summary>英文摘要</summary>	The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters. </details>
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 1、用于原子分辨率图像的高精度原子分割、定位、去噪和超分辨率处理的TEMImageNet训练库和atomsenet深度学习模型
- [ ] TEMImageNet Training Library and AtomSegNet Deep-Learning Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-Resolution Processing of Atomic-Resolution Images 
时间：2021年02月20日                         第一作者：Ruoqian Lin                       [链接](https://arxiv.org/abs/2012.09093).                     
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

