# 258、D2C-SR：一种用于实际图像超分辨率的发散收敛方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution 
时间：2021年09月15日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月16日</details>

# 257、D2C-SR：一种用于实际图像超分辨率的发散收敛方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution 
时间：2021年09月14日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月15日</details>

# 256、通过自适应下采样模型实现真实世界的超分辨率
- [ ] Toward Real-World Super-Resolution via Adaptive Downsampling Models 
时间：2021年09月08日                         第一作者：Sanghyun Son                        [链接](https://arxiv.org/abs/2109.03444).                     
## 摘要：大多数图像超分辨率（SR）方法是在合成低分辨率（LR）和高分辨率（HR）图像对上开发的，这些图像对是通过预定操作（例如双三次下采样）构建的。由于现有方法通常学习特定函数的逆映射，因此当应用于精确公式不同且未知的真实图像时，会产生模糊结果。因此，有几种方法试图合成更多样化的LR样本或学习真实的下采样模型。然而，由于对下采样过程的限制性假设，它们仍然存在偏见，不太具有普遍性。本研究提出了一种新的方法来模拟未知的下采样过程，而不施加限制性的先验知识。我们在对抗性训练框架中提出了一种广义低频损耗（LFL）方法，以模拟目标LR图像的分布，而无需使用任何成对的例子。此外，我们还为下采样器设计了一种自适应数据丢失（ADL），它可以在训练循环中自适应地从数据中学习和更新。大量的实验验证了我们的下采样模型可以促进现有的SR方法对各种合成和真实示例执行比传统方法更精确的重建。
<details>	<summary>英文摘要</summary>	Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. Therefore, several methods attempt to synthesize much more diverse LR samples or learn a realistic downsampling model. However, due to restrictive assumptions on the downsampling process, they are still biased and less generalizable. This study proposes a novel method to simulate an unknown downsampling process without imposing restrictive prior knowledge. We propose a generalizable low-frequency loss (LFL) in the adversarial training framework to imitate the distribution of target LR images without using any paired examples. Furthermore, we design an adaptive data loss (ADL) for the downsampler, which can be adaptively learned and updated from the data during the training loops. Extensive experiments validate that our downsampling model can facilitate existing SR methods to perform more accurate reconstructions on various synthetic and real-world examples than the conventional approaches. </details>
<details>	<summary>注释</summary>	Accepted at TPAMI DOI: 10.1109/TPAMI.2021.3106790 </details>
<details>	<summary>邮件日期</summary>	2021年09月09日</details>

# 255、用物理引导神经网络重建高分辨率湍流
- [ ] Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks 
时间：2021年09月06日                         第一作者：Shengyu Chen                       [链接](https://arxiv.org/abs/2109.03327).                     
## 摘要：湍流的直接数值模拟（DNS）在计算上非常昂贵，无法应用于雷诺数较大的流动。大涡模拟（LES）是一种计算要求较低的替代方法，但无法准确捕捉湍流输送的所有尺度。我们在这项工作中的目标是建立一种新的基于超分辨率技术的数据驱动方法，以从LES预测中重建DNS数据。我们利用潜在的物理关系来规范不同物理变量之间的关系。我们还引入了分层生成过程和反向降级过程，以充分探索DNS和LES数据之间的对应关系。我们通过一个单快照实验和一个跨时间实验证明了该方法的有效性。结果表明，在像素级重建误差和结构相似性方面，我们的方法能够更好地在空间和时间上重建高分辨率DNS数据。视觉比较表明，我们的方法在捕捉精细水平流动力学方面表现得更好。
<details>	<summary>英文摘要</summary>	Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Large eddy simulation (LES) is an alternative that is computationally less demanding, but is unable to capture all of the scales of turbulent transport accurately. Our goal in this work is to build a new data-driven methodology based on super-resolution techniques to reconstruct DNS data from LES predictions. We leverage the underlying physical relationships to regularize the relationships amongst different physical variables. We also introduce a hierarchical generative process and a reverse degradation process to fully explore the correspondence between DNS and LES data. We demonstrate the effectiveness of our method through a single-snapshot experiment and a cross-time experiment. The results confirm that our method can better reconstruct high-resolution DNS data over space and over time in terms of pixel-wise reconstruction error and structural similarity. Visual comparisons show that our method performs much better in capturing fine-level flow dynamics. </details>
<details>	<summary>邮件日期</summary>	2021年09月09日</details>

# 254、双耳声网络：用双耳声音预测语义、深度和运动
- [ ] Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds 
时间：2021年09月06日                         第一作者：Dengxin Dai                       [链接](https://arxiv.org/abs/2109.02763).                     
## 摘要：人类可以通过视觉和/或听觉线索来稳健地识别和定位物体。虽然机器已经能够对视觉数据进行同样的处理，但对声音的处理却很少。这项工作开发了一种完全基于双耳声音的场景理解方法。所考虑的任务包括预测发声对象的语义掩码、发声对象的运动以及场景的深度图。为此，我们提出了一种新的传感器设置，并使用八个专业的双耳麦克风和一个360度摄像机记录了一个新的街景视听数据集。视觉和听觉线索的共存被用于监督转移。特别是，我们采用了一个跨模式蒸馏框架，该框架由多个视觉教师方法和一个良好的学生方法组成——学生方法经过训练，可以产生与教师方法相同的结果。这样，听觉系统就可以在不使用人类注释的情况下进行训练。为了进一步提高性能，我们提出了另一个新的辅助任务，即创造空间声音超分辨率，以提高声音的方向分辨率。然后，我们将这四个任务组成一个端到端可训练的多任务网络，以提高整体性能。实验结果表明：1）我们的方法在所有四项任务中都取得了良好的效果；2）这四项任务是互利的——共同训练它们可以获得最佳性能；3）麦克风的数量和方向都很重要，4）从标准频谱图学习的特征和通过经典信号处理管道获得的特征对于听觉感知任务是互补的。数据和代码已发布。
<details>	<summary>英文摘要</summary>	Humans can robustly recognize and localize objects by using visual and/or auditory cues. While machines are able to do the same with visual data already, less work has been done with sounds. This work develops an approach for scene understanding purely based on binaural sounds. The considered tasks include predicting the semantic masks of sound-making objects, the motion of sound-making objects, and the depth map of the scene. To this aim, we propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360-degree camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of multiple vision teacher methods and a sound student method -- the student method is trained to generate the same results as the teacher methods do. This way, the auditory system can be trained without using human annotations. To further boost the performance, we propose another novel auxiliary task, coined Spatial Sound Super-Resolution, to increase the directional resolution of sounds. We then formulate the four tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results show that 1) our method achieves good results for all four tasks, 2) the four tasks are mutually beneficial -- training them together achieves the best performance, 3) the number and orientation of microphones are both important, and 4) features learned from the standard spectrogram and features obtained by the classic signal processing pipeline are complementary for auditory perception tasks. The data and code are released. </details>
<details>	<summary>注释</summary>	Journal extension of our ECCV'20 Paper -- 15 pages. arXiv admin note: substantial text overlap with arXiv:2003.04210 </details>
<details>	<summary>邮件日期</summary>	2021年09月08日</details>

# 253、双摄像头超分辨率，带对齐注意模块
- [ ] Dual-Camera Super-Resolution with Aligned Attention Modules 
时间：2021年09月06日                         第一作者：Tengfei Wang                       [链接](https://arxiv.org/abs/2109.01349).                     
<details>	<summary>注释</summary>	Accepted to ICCV 2021 (oral) </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 252、Fusformer：一种基于变换器的高光谱图像超分辨率融合方法
- [ ] Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution 
时间：2021年09月05日                         第一作者：Jin-Fan Hu                        [链接](https://arxiv.org/abs/2109.02079).                     
## 摘要：高光谱图像由于其丰富的光谱信息而变得越来越重要。然而，受现有成像机制的限制，其空间分辨率较差。目前，许多卷积神经网络被提出用于高光谱图像的超分辨率问题。然而卷积神经网络（美国有线电视新闻网）的方法只考虑局部信息而不是全局信息，而卷积运算中接收域的核大小有限。本文设计了一种基于变压器的网络，用于融合低分辨率高光谱图像和高分辨率多光谱图像，获得高分辨率高光谱图像。由于transformer的表示能力，我们的方法能够在全局范围内探索特性的内在关系。此外，考虑到LR HSI是主要的光谱结构，网络将重点放在空间细节估计上，以减轻重建整个数据的负担。它减少了网络的映射空间，提高了网络的最终性能。各种实验和质量指标表明，与其他先进方法相比，我们的方法具有优越性。
<details>	<summary>英文摘要</summary>	Hyperspectral image has become increasingly crucial due to its abundant spectral information. However, It has poor spatial resolution with the limitation of the current imaging mechanism. Nowadays, many convolutional neural networks have been proposed for the hyperspectral image super-resolution problem. However, convolutional neural network (CNN) based methods only consider the local information instead of the global one with the limited kernel size of receptive field in the convolution operation. In this paper, we design a network based on the transformer for fusing the low-resolution hyperspectral images and high-resolution multispectral images to obtain the high-resolution hyperspectral images. Thanks to the representing ability of the transformer, our approach is able to explore the intrinsic relationships of features globally. Furthermore, considering the LR-HSIs hold the main spectral structure, the network focuses on the spatial detail estimation releasing from the burden of reconstructing the whole data. It reduces the mapping space of the proposed network, which enhances the final performance. Various experiments and quality indexes show our approach's superiority compared with other state-of-the-art methods. </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 251、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年09月04日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 250、双摄像头超分辨率，带对齐注意模块
- [ ] Dual-Camera Super-Resolution with Aligned Attention Modules 
时间：2021年09月03日                         第一作者：Tengfei Wang                       [链接](https://arxiv.org/abs/2109.01349).                     
## 摘要：我们提出了一种新的基于参考的超分辨率（RefSR）方法，重点是双摄像机超分辨率（DCSR），它利用参考图像获得高质量和高保真的结果。我们提出的方法推广了标准的基于面片的特征匹配和空间对齐操作。我们进一步探索了RefSR的一个有前途的应用——双摄像机超分辨率，并构建了一个数据集，该数据集由智能手机中主摄像机和长焦摄像机的146个图像对组成。为了弥补真实世界图像和训练图像之间的领域差距，我们提出了一种针对真实世界图像的自监督领域自适应策略。在我们的数据集和一个公共基准上进行的大量实验表明，我们的方法在定量评估和视觉比较方面都比最先进的方法有明显的改进。
<details>	<summary>英文摘要</summary>	We present a novel approach to reference-based super-resolution (RefSR) with the focus on dual-camera super-resolution (DCSR), which utilizes reference images for high-quality and high-fidelity results. Our proposed method generalizes the standard patch-based feature matching with spatial alignment operations. We further explore the dual-camera super-resolution that is one promising application of RefSR, and build a dataset that consists of 146 image pairs from the main and telephoto cameras in a smartphone. To bridge the domain gaps between real-world images and the training images, we propose a self-supervised domain adaptation strategy for real-world images. Extensive experiments on our dataset and a public benchmark demonstrate clear improvement achieved by our method over state of the art in both quantitative evaluation and visual comparisons. </details>
<details>	<summary>注释</summary>	ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年09月06日</details>

# 249、高光谱图像去噪、光谱校正和高分辨率RGB重建的深度学习方法
- [ ] Deep Learning Approach for Hyperspectral Image Demosaicking, Spectral Correction and High-resolution RGB Reconstruction 
时间：2021年09月03日                         第一作者：Peichao Li                       [链接](https://arxiv.org/abs/2109.01403).                     
## 摘要：高光谱成像是术中组织特征化最有前途的技术之一。快照马赛克相机可以在一次曝光中捕获高光谱数据，有可能使手术决策的实时高光谱成像系统成为可能。然而，对捕获数据的优化利用需要解决不适定的解模糊问题，并应用额外的光谱校正来恢复图像的空间和光谱信息。在这项工作中，我们提出了一种基于深度学习的基于监督学习方法的快照高光谱图像去噪算法。由于缺乏使用快照马赛克相机获取的公共可用医学图像，因此提出了一种合成图像生成方法，以模拟由高分辨率但速度较慢的高光谱成像设备捕获的现有医学图像数据集中的快照图像。利用卷积神经网络实现高光谱图像的超分辨率重建，然后利用传感器特定的校准矩阵进行串扰和泄漏校正。所得到的去马赛克图像进行了定量和定性评估，与使用线性插值的基线去马赛克方法相比，显示出明显的图像质量改进。此外，我们的算法为最先进的快照马赛克相机获得每帧超分辨率RGB或氧饱和度图的快速处理时间约为45 \，ms，表明其无缝集成到实时外科高光谱成像应用中的潜力。
<details>	<summary>英文摘要</summary>	Hyperspectral imaging is one of the most promising techniques for intraoperative tissue characterisation. Snapshot mosaic cameras, which can capture hyperspectral data in a single exposure, have the potential to make a real-time hyperspectral imaging system for surgical decision-making possible. However, optimal exploitation of the captured data requires solving an ill-posed demosaicking problem and applying additional spectral corrections to recover spatial and spectral information of the image. In this work, we propose a deep learning-based image demosaicking algorithm for snapshot hyperspectral images using supervised learning methods. Due to the lack of publicly available medical images acquired with snapshot mosaic cameras, a synthetic image generation approach is proposed to simulate snapshot images from existing medical image datasets captured by high-resolution, but slow, hyperspectral imaging devices. Image reconstruction is achieved using convolutional neural networks for hyperspectral image super-resolution, followed by cross-talk and leakage correction using a sensor-specific calibration matrix. The resulting demosaicked images are evaluated both quantitatively and qualitatively, showing clear improvements in image quality compared to a baseline demosaicking method using linear interpolation. Moreover, the fast processing time of~45\,ms of our algorithm to obtain super-resolved RGB or oxygenation saturation maps per image frame for a state-of-the-art snapshot mosaic camera demonstrates the potential for its seamless integration into real-time surgical hyperspectral imaging applications. </details>
<details>	<summary>邮件日期</summary>	2021年09月06日</details>

# 248、基于非均匀卷积WGAN的红外图像超分辨率分析
- [ ] Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN 
时间：2021年09月02日                         第一作者：Yongsong Huang                       [链接](https://arxiv.org/abs/2109.00960).                     
## 摘要：图像超分辨率在监控、遥感等领域具有重要意义。然而，由于光学设备相对昂贵，红外（IR）图像通常具有低分辨率。近年来，深度学习方法在图像超分辨率方面占据主导地位，并在可见光图像上取得了显著的效果；然而，红外图像受到的关注较少。红外图像具有较少的模式，因此，深度神经网络（DNN）很难从红外图像中学习不同的特征。在本文中，我们提出了一个采用异构卷积和对抗性训练的红外图像超分辨率框架，即基于异构核的超分辨率Wasserstein-GAN（HetSRWGAN）。HetSRWGAN算法是一种轻量级的GAN体系结构，它应用了一种即插即用的异构内核剩余块。此外，采用了一种新的基于图像梯度的损耗函数，该函数可以应用于任意模型。拟议的HetSRWGAN在定性和定量评估方面都取得了一致的更好的绩效。实验结果表明，整个训练过程较为稳定。
<details>	<summary>英文摘要</summary>	Image super-resolution is important in many fields, such as surveillance and remote sensing. However, infrared (IR) images normally have low resolution since the optical equipment is relatively expensive. Recently, deep learning methods have dominated image super-resolution and achieved remarkable performance on visible images; however, IR images have received less attention. IR images have fewer patterns, and hence, it is difficult for deep neural networks (DNNs) to learn diverse features from IR images. In this paper, we present a framework that employs heterogeneous convolution and adversarial training, namely, heterogeneous kernel-based super-resolution Wasserstein GAN (HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a lightweight GAN architecture that applies a plug-and-play heterogeneous kernel-based residual block. Moreover, a novel loss function that employs image gradients is adopted, which can be applied to an arbitrary model. The proposed HetSRWGAN achieves consistently better performance in both qualitative and quantitative evaluations. According to the experimental results, the whole training process is more stable. </details>
<details>	<summary>注释</summary>	To be published in the 18th Pacific Rim International Conference on Artificial Intelligence (PRICAI-2021) </details>
<details>	<summary>邮件日期</summary>	2021年09月03日</details>

# 247、基于CNN的图像超分辨率双参考训练数据采集方法
- [ ] An Efficient Dual-reference Training Data Acquisition Method for CNN-Based Image Super-Resolution 
时间：2021年09月02日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
<details>	<summary>邮件日期</summary>	2021年09月03日</details>

# 246、基于深度学习的人脸超分辨率研究综述
- [ ] Deep Learning-based Face Super-Resolution: A Survey 
时间：2021年09月01日                         第一作者：Junjun Jiang                       [链接](https://arxiv.org/abs/2101.03749).                     
<details>	<summary>注释</summary>	Accepted to ACM Computing Surveys </details>
<details>	<summary>邮件日期</summary>	2021年09月02日</details>

# 245、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年08月31日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
<details>	<summary>邮件日期</summary>	2021年09月02日</details>

# 244、用于高光谱人脸超分辨率的光谱分裂和聚合网络
- [ ] Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution 
时间：2021年08月31日                         第一作者：Junjun Jiang                        [链接](https://arxiv.org/abs/2108.13584).                     
## 摘要：高分辨率（HR）高光谱人脸图像在非受控条件下（如弱光环境和欺骗攻击）与人脸相关的计算机视觉任务中起着重要作用。然而，高光谱人脸图像的密集光谱带是以有限数量的光子平均到达狭窄的光谱窗口为代价的，这大大降低了高光谱人脸图像的空间分辨率。在本文中，我们研究了如何将深度学习技术应用于高光谱人脸图像超分辨率（HFSR），特别是在训练样本非常有限的情况下。利用谱带的数量，每个谱带都可以看作一幅图像，我们提出了一个训练样本有限的HFSR谱分裂和聚合网络（SSANet）。在浅层中，我们将高光谱图像分成不同的光谱组，并将每个光谱组作为一个单独的训练样本（从某种意义上说，每个光谱组将被送入同一个网络）。然后，我们逐渐聚集更深层次的相邻波段，以利用光谱相关性。通过这种光谱分割和聚集策略（SSAS），我们可以将原始高光谱图像分割成多个样本，以支持网络的有效训练，并有效地利用光谱之间的光谱相关性。为了应对小训练样本量（S3）问题的挑战，我们建议通过自表示模型和对称诱导增强来扩展训练样本。实验表明，引入的SSANet能够很好地模拟空间和光谱信息的联合相关性。通过扩展训练样本，我们提出的方法可以有效地缓解S3问题。比较结果表明，我们提出的方法优于现有的方法。
<details>	<summary>英文摘要</summary>	High-resolution (HR) hyperspectral face image plays an important role in face related computer vision tasks under uncontrolled conditions, such as low-light environment and spoofing attacks. However, the dense spectral bands of hyperspectral face images come at the cost of limited amount of photons reached a narrow spectral window on average, which greatly reduces the spatial resolution of hyperspectral face images. In this paper, we investigate how to adapt the deep learning techniques to hyperspectral face image super-resolution (HFSR), especially when the training samples are very limited. Benefiting from the amount of spectral bands, in which each band can be seen as an image, we present a spectral splitting and aggregation network (SSANet) for HFSR with limited training samples. In the shallow layers, we split the hyperspectral image into different spectral groups and take each of them as an individual training sample (in the sense that each group will be fed into the same network). Then, we gradually aggregate the neighbor bands at the deeper layers to exploit the spectral correlations. By this spectral splitting and aggregation strategy (SSAS), we can divide the original hyperspectral image into multiple samples to support the efficient training of the network and effectively exploit the spectral correlations among spectrum. To cope with the challenge of small training sample size (S3) problem, we propose to expand the training samples by a self-representation model and symmetry-induced augmentation. Experiments show that the introduced SSANet can well model the joint correlations of spatial and spectral information. By expanding the training samples, our proposed method can effectively alleviate the S3 problem. The comparison results demonstrate that our proposed method can outperform the state-of-the-arts. </details>
<details>	<summary>注释</summary>	12 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 243、基于注意的图像超分辨率多参考学习
- [ ] Attention-based Multi-Reference Learning for Image Super-Resolution 
时间：2021年08月31日                         第一作者：Marco Pesavento                       [链接](https://arxiv.org/abs/2108.13697).                     
## 摘要：本文提出了一种新的基于注意的多参考超分辨率网络（AMRSR），该网络在给定低分辨率图像的情况下，学习将多参考图像中最相似的纹理自适应地传输到超分辨率输出，同时保持空间一致性。在多个基准数据集上，使用多个参考图像和基于注意的采样可以显著提高最先进的参考超分辨率方法的性能。参考超分辨率方法最近被提出，通过提供来自高分辨率参考图像的附加信息来克服图像超分辨率的不适定问题。多参考超分辨率通过提供更多样化的图像特征池来扩展此方法，以克服固有的信息不足，同时保持内存效率。提出了一种新的基于分层注意的抽样方法，用于基于感知损失的低分辨率图像特征与多幅参考图像之间的相似性学习。消融证明了多参考和基于分层注意的抽样对整体表现的贡献。即使参考图像明显偏离目标图像，感知和定量的地面真实度评估也显示出显著的性能改进。项目网站可在以下网址找到：https://marcopesavento.github.io/AMRSR/
<details>	<summary>英文摘要</summary>	This paper proposes a novel Attention-based Multi-Reference Super-resolution network (AMRSR) that, given a low-resolution image, learns to adaptively transfer the most similar texture from multiple reference images to the super-resolution output whilst maintaining spatial coherence. The use of multiple reference images together with attention-based sampling is demonstrated to achieve significantly improved performance over state-of-the-art reference super-resolution approaches on multiple benchmark datasets. Reference super-resolution approaches have recently been proposed to overcome the ill-posed problem of image super-resolution by providing additional information from a high-resolution reference image. Multi-reference super-resolution extends this approach by providing a more diverse pool of image features to overcome the inherent information deficit whilst maintaining memory efficiency. A novel hierarchical attention-based sampling approach is introduced to learn the similarity between low-resolution image features and multiple reference images based on a perceptual loss. Ablation demonstrates the contribution of both multi-reference and hierarchical attention-based sampling to overall performance. Perceptual and quantitative ground-truth evaluation demonstrates significant improvement in performance even when the reference images deviate significantly from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/ </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 242、4D人体表演的超分辨率外观转移
- [ ] Super-Resolution Appearance Transfer for 4D Human Performances 
时间：2021年08月31日                         第一作者：Marco Pesavento                       [链接](https://arxiv.org/abs/2108.13739).                     
## 摘要：从多视点视频中对人进行4D重建的一个常见问题是捕获的动态纹理外观的质量，这取决于相机分辨率和捕获体积。通常，要求对摄像机进行帧处理以捕获动态性能的体积（$>50m^3$）导致人员仅占据视野的一小部分$<$10%。即使使用超高清晰度4k视频采集，这也会导致以低于标准清晰度0.5k视频分辨率对人进行采样，从而导致低质量渲染。在本文中，我们提出了一种解决方案，通过使用数字静物照相机（$>8k$）从静态高分辨率外观捕捉装置进行超分辨率外观转移，以小体积（$<8m^3$）捕捉人物。提出了一种从高分辨率静态捕获到动态视频性能捕获的超分辨率外观转换管道，以生成超分辨率动态纹理。这解决了两个关键问题：不同摄像机系统之间的颜色映射；并利用学习到的模型进行动态纹理贴图超分辨率处理。对比评估表明，在呈现具有超分辨率动态纹理外观的4D性能捕获方面，在定性和定量方面都有显著改进。所提出的方法再现了静态捕获的高分辨率细节，同时保持捕获视频的外观动态。
<details>	<summary>英文摘要</summary>	A common problem in the 4D reconstruction of people from multi-view video is the quality of the captured dynamic texture appearance which depends on both the camera resolution and capture volume. Typically the requirement to frame cameras to capture the volume of a dynamic performance ($>50m^3$) results in the person occupying only a small proportion $<$ 10% of the field of view. Even with ultra high-definition 4k video acquisition this results in sampling the person at less-than standard definition 0.5k video resolution resulting in low-quality rendering. In this paper we propose a solution to this problem through super-resolution appearance transfer from a static high-resolution appearance capture rig using digital stills cameras ($> 8k$) to capture the person in a small volume ($<8m^3$). A pipeline is proposed for super-resolution appearance transfer from high-resolution static capture to dynamic video performance capture to produce super-resolution dynamic textures. This addresses two key problems: colour mapping between different camera systems; and dynamic texture map super-resolution using a learnt model. Comparative evaluation demonstrates a significant qualitative and quantitative improvement in rendering the 4D performance capture with super-resolution dynamic texture appearance. The proposed approach reproduces the high-resolution detail of the static capture whilst maintaining the appearance dynamics of the captured video. </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 241、复杂噪声下的无监督单幅图像超分辨率
- [ ] Unsupervised Single Image Super-resolution Under Complex Noise 
时间：2021年08月30日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 240、通过对抗鲁棒性实现广义真实世界超分辨率
- [ ] Generalized Real-World Super-Resolution through Adversarial Robustness 
时间：2021年08月25日                         第一作者：Angela Castillo                       [链接](https://arxiv.org/abs/2108.11505).                     
## 摘要：传统上，解决真实世界超分辨率（SR）问题的方法是首先学习一种特定的退化模型，该模型类似于低分辨率图像中的噪声和腐败伪影。因此，当前的方法缺乏泛化性，在对看不见的腐败类型进行测试时会失去准确性。与传统方案相比，我们提出了鲁棒超分辨率（RSR），这是一种利用对抗性攻击的泛化能力来处理真实世界SR的方法。我们的新框架为真实世界SR方法的发展带来了范式转变。我们没有学习特定于数据集的降级，而是使用对抗性攻击来创建针对模型弱点的困难示例。之后，我们在训练期间使用这些对抗性示例来提高模型处理噪声输入的能力。我们在合成图像和真实图像上进行了大量实验，并通过经验证明，我们的RSR方法在数据集上具有良好的通用性，无需对特定的噪声先验进行重新训练。通过使用单一的稳健模型，我们在现实世界的基准测试中的表现优于最先进的专门方法。
<details>	<summary>英文摘要</summary>	Real-world Super-Resolution (SR) has been traditionally tackled by first learning a specific degradation model that resembles the noise and corruption artifacts in low-resolution imagery. Thus, current methods lack generalization and lose their accuracy when tested on unseen types of corruption. In contrast to the traditional proposal, we present Robust Super-Resolution (RSR), a method that leverages the generalization capability of adversarial attacks to tackle real-world SR. Our novel framework poses a paradigm shift in the development of real-world SR methods. Instead of learning a dataset-specific degradation, we employ adversarial attacks to create difficult examples that target the model's weaknesses. Afterward, we use these adversarial examples during training to improve our model's capacity to process noisy inputs. We perform extensive experimentation on synthetic and real-world images and empirically demonstrate that our RSR method generalizes well across datasets without re-training for specific noise priors. By using a single robust model, we outperform state-of-the-art specialized methods on real-world benchmarks. </details>
<details>	<summary>注释</summary>	ICCV Workshops, 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月27日</details>

# 239、基于记忆增强的视频超分辨率非局部注意
- [ ] Memory-Augmented Non-Local Attention for Video Super-Resolution 
时间：2021年08月25日                         第一作者：Jiyang Yu                       [链接](https://arxiv.org/abs/2108.11048).                     
## 摘要：在本文中，我们提出了一种新的视频超分辨率方法，旨在从低分辨率（LR）视频生成高保真高分辨率（HR）视频。以前的方法主要利用时间相邻帧来辅助当前帧的超分辨率。这些方法的性能有限，因为它们在空间帧对齐方面面临挑战，并且缺乏来自相似LR相邻帧的有用信息。相比之下，我们设计了一种跨帧非局部注意机制，允许视频超分辨率而无需帧对齐，从而对视频中的大运动更具鲁棒性。此外，为了获取相邻帧以外的信息，我们设计了一种新的记忆增强注意模块，用于在超分辨率训练中记忆一般的视频细节。实验结果表明，与现有的无帧对齐方法相比，我们的方法在大运动视频上可以获得更好的性能。我们的源代码将被发布。
<details>	<summary>英文摘要</summary>	In this paper, we propose a novel video super-resolution method that aims at generating high-fidelity high-resolution (HR) videos from low-resolution (LR) ones. Previous methods predominantly leverage temporal neighbor frames to assist the super-resolution of the current frame. Those methods achieve limited performance as they suffer from the challenge in spatial frame alignment and the lack of useful information from similar LR neighbor frames. In contrast, we devise a cross-frame non-local attention mechanism that allows video super-resolution without frame alignment, leading to be more robust to large motions in the video. In addition, to acquire the information beyond neighbor frames, we design a novel memory-augmented attention module to memorize general video details during the super-resolution training. Experimental results indicate that our method can achieve superior performance on large motion videos comparing to the state-of-the-art methods without aligning frames. Our source code will be released. </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 238、用于单图像超分辨率的高效变换器
- [ ] Efficient Transformer for Single Image Super-Resolution 
时间：2021年08月25日                         第一作者：Zhisheng Lu                       [链接](https://arxiv.org/abs/2108.11084).                     
## 摘要：随着深度学习的发展，单幅图像的超分辨率任务有了长足的发展。然而，现有的大多数研究都集中在构建一个具有大量层的更复杂的神经网络，这会带来沉重的计算成本和内存存储。近年来，随着Transformer在NLP任务中取得了辉煌的成果，越来越多的研究者开始探索Transformer在计算机视觉任务中的应用。但由于视觉转换器的计算量大、GPU内存占用率高，网络设计不能太深。为了解决这个问题，我们提出了一种新的高效超分辨率转换器（ESRT），用于快速准确的图像超分辨率。ESRT是一种混合变压器，首先在前端设计基于CNN的SR网络以提取深层特征。具体来说，有两个主干用于格式化ESRT：轻型CNN主干（LCB）和轻型转换器主干（LTB）。其中，LCB是一种轻量级SR网络，通过动态调整特征映射的大小，以较低的计算成本提取深层SR特征。LTB由一个高效转换器（ET）组成，具有较小的GPU内存占用，这得益于新型高效多头注意（EMHA）。在EMHA中，提出了一种特征分割模块（FSM）将长序列分割成子段，然后通过注意操作应用这些子段。该模块可以显著减少GPU内存占用。大量实验表明，我们的ESRT取得了有竞争力的结果。与原变压器占用16057M GPU内存相比，该变压器仅占用4191M GPU内存，性能更好。
<details>	<summary>英文摘要</summary>	Single image super-resolution task has witnessed great strides with the development of deep learning. However, most existing studies focus on building a more complex neural network with a massive number of layers, bringing heavy computational cost and memory storage. Recently, as Transformer yields brilliant results in NLP tasks, more and more researchers start to explore the application of Transformer in computer vision tasks. But with the heavy computational cost and high GPU memory occupation of the vision Transformer, the network can not be designed too deep. To address this problem, we propose a novel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image super-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is first designed in the front to extract deep features. Specifically, there are two backbones for formatting the ESRT: lightweight CNN backbone (LCB) and lightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR network to extract deep SR features at a low computational cost by dynamically adjusting the size of the feature map. LTB is made up of an efficient Transformer (ET) with a small GPU memory occupation, which benefited from the novel efficient multi-head attention (EMHA). In EMHA, a feature split module (FSM) is proposed to split the long sequence into sub-segments and then these sub-segments are applied by attention operation. This module can significantly decrease the GPU memory occupation. Extensive experiments show that our ESRT achieves competitive results. Compared with the original Transformer which occupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with better performance. </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 237、多属性结构化文本到人脸合成
- [ ] Multi-Attributed and Structured Text-to-Face Synthesis 
时间：2021年08月25日                         第一作者：Rohan Wadhawan                       [链接](https://arxiv.org/abs/2108.11100).                     
## 摘要：生成性对抗网络（GAN）通过人脸生成、照片编辑和图像超分辨率等许多应用，彻底改变了图像合成。使用GANs的图像合成主要是单峰的，很少有方法可以从文本或其他数据模式合成图像。文本到图像的合成，特别是文本到人脸的合成，在从目击者的叙述中生成健壮的人脸和用视觉线索增强阅读体验方面有着很好的应用前景。然而，只有几个数据集为文本到面合成提供了整合的面数据和文本描述。此外，这些文本注释不太广泛和描述性，这减少了从中生成的面的多样性。本文实证证明，增加每个文本描述中人脸属性的数量有助于生成更加多样化和真实的人脸。为了证明这一点，我们提出了一种新的方法，重点是使用结构化的文本描述。我们还整合了一个多属性和结构化文本到人脸（MAST）数据集，该数据集由具有结构化文本注释的高质量图像组成，并可供研究人员进行实验和构建。最后，我们报告了MAST数据集的基准Frechet起始距离（FID）、面部语义相似性（FSS）和面部语义距离（FSD）分数。
<details>	<summary>英文摘要</summary>	Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Frechet's Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset. </details>
<details>	<summary>注释</summary>	Accepted by IEEE TEMSMET 2020, Camera Ready Version </details>
<details>	<summary>邮件日期</summary>	2021年08月26日</details>

# 236、一种高效的CNN图像超分辨率双参考训练数据采集方法
- [ ] An Efficient Dual-reference Training Data Acquisition Method for CNN Image Super-Resolution 
时间：2021年08月24日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 235、边缘SR：质量的超分辨率
- [ ] edge-SR: Super-Resolution For The Masses 
时间：2021年08月23日                         第一作者：Pablo Navarrete Michelini                       [链接](https://arxiv.org/abs/2108.10335).                     
## 摘要：经典的图像缩放（如双三次）可以看作是一个卷积层和一个放大滤波器。它的实现在所有显示设备和图像处理软件中无处不在。在过去的十年中，深度学习系统已经被引入到图像超分辨率（SR）的任务中，使用了几个卷积层和许多滤波器。这些方法已经取代了放大任务的图像质量基准。是否有可能用显示面板、平板电脑、笔记本电脑等边缘设备上的深度学习体系结构取代经典的升级。？一方面，随着能够高效运行深度学习任务的硬件的快速发展，Edge AI芯片的当前趋势显示了这一方向的良好前景。另一方面，在image-SR中，只有少数体系结构将极限推到了可以在边缘设备上实时运行的极小尺寸。我们探索这个问题的可能解决方案，旨在填补经典升级和小型深度学习配置之间的差距。作为从经典到深度学习放大的过渡，我们提出了edge SR（eSR），这是一组使用可解释机制来放大图像的单层体系结构。当然，单层体系结构无法达到深度学习系统的质量。然而，我们发现，对于高速需求，eSR在权衡图像质量和运行时性能方面变得更好。填补经典体系结构和深度学习体系结构之间的差距，实现图像的放大，对于大规模采用这项技术至关重要。同样重要的是，有一个可解释的系统，可以揭示解决这个问题的内在策略，并指导我们未来的改进和更好地理解更大的网络。
<details>	<summary>英文摘要</summary>	Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks. </details>
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 234、SwinIR：使用Swin变压器进行图像恢复
- [ ] SwinIR: Image Restoration Using Swin Transformer 
时间：2021年08月23日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.10257).                     
## 摘要：图像恢复是一个长期存在的低级视觉问题，其目的是从低质量图像（例如缩小的、有噪声的和压缩的图像）恢复高质量图像。虽然最先进的图像恢复方法是基于卷积神经网络的，但很少有人尝试使用变压器，因为变压器在高级视觉任务中表现出令人印象深刻的性能。在本文中，我们提出了一种基于Swin变换的强基线图像恢复模型SwinIR。SwinIR由三部分组成：浅层特征提取、深层特征提取和高质量图像重建。具体地说，深度特征提取模块由几个剩余的Swin变压器块（RSTB）组成，每个块都有几个Swin变压器层和一个剩余连接。我们在三个具有代表性的任务上进行了实验：图像超分辨率（包括经典、轻量级和真实世界的图像超分辨率）、图像去噪（包括灰度和彩色图像去噪）和JPEG压缩伪影减少。实验结果表明，在不同的任务上，SwinIR的性能优于最先进的方法$\textbf{高达0.14$\sim$0.45dB}$，而参数总数可以减少$\textbf{高达67%}$。
<details>	<summary>英文摘要</summary>	Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\textbf{up to 0.14$\sim$0.45dB}$, while the total number of parameters can be reduced by $\textbf{up to 67%}$. </details>
<details>	<summary>注释</summary>	Sota results on classical/lightweight/real-world image SR, image denoising and JPEG compression artifact reduction. Code: https://github.com/JingyunLiang/SwinIR </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 233、Lucas Kanade重新加载：原始图像突发的端到端超分辨率
- [ ] Lucas-Kanade Reloaded: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年08月23日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 232、野外非配对深度超分辨率
- [ ] Unpaired Depth Super-Resolution in the Wild 
时间：2021年08月23日                         第一作者：Aleks                       [链接](https://arxiv.org/abs/2105.12038).                     
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 231、用局部属性图解释超分辨率网络
- [ ] Interpreting Super-Resolution Networks with Local Attribution Maps 
时间：2021年08月22日                         第一作者：Jinjin Gu                       [链接](https://arxiv.org/abs/2011.11036).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 230、用神经网络结构和剪枝搜索实现移动实时超分辨率
- [ ] Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search 
时间：2021年08月18日                         第一作者：Zheng Zhan                       [链接](https://arxiv.org/abs/2108.08910).                     
## 摘要：尽管近年来随着深度神经网络（DNN）的蓬勃发展，单图像超分辨率（SISR）任务取得了显著进展，但深度学习方法在实际应用中面临着计算和内存消耗问题，特别是对于资源有限的平台，如移动设备。为了克服这一挑战并便于在移动设备上实时部署SISR任务，我们将神经结构搜索与剪枝搜索相结合，提出了一种自动搜索框架，该框架在满足实时推理要求的同时，导出具有高图像质量的稀疏超分辨率（SR）模型。为了降低搜索成本，我们通过引入超网来利用权重共享策略，并将搜索问题分解为三个阶段，包括超网构造、编译器感知体系结构和剪枝搜索以及编译器感知剪枝比率搜索。利用所提出的框架，我们是第一个实现实时SR推断（每帧仅数十毫秒）的公司，用于在移动平台（三星Galaxy S20）上实现720p分辨率和具有竞争力的图像质量（PSNR和SSIM）。
<details>	<summary>英文摘要</summary>	Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20). </details>
<details>	<summary>邮件日期</summary>	2021年08月23日</details>

# 229、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年08月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes are available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 228、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年08月19日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 227、盲视频超分辨率的时间核一致性
- [ ] Temporal Kernel Consistency for Blind Video Super-Resolution 
时间：2021年08月18日                         第一作者：Lichuan Xiang                       [链接](https://arxiv.org/abs/2108.08305).                     
## 摘要：基于深度学习的盲超分辨率（SR）方法最近在未知退化的放大帧中取得了前所未有的性能。这些模型能够从给定的低分辨率（LR）图像中准确估计未知的降尺度核，以便在恢复过程中利用核。尽管这些方法在很大程度上取得了成功，但它们主要基于图像，因此不利用多个视频帧中内核的时间特性。在本文中，我们研究了核的时间特性，并强调了它在盲视频超分辨率任务中的重要性。具体地说，我们测量了真实世界视频的内核时间一致性，并说明了在场景及其对象的动态性不同的视频中，估计的内核在每帧中是如何变化的。有了这一新的见解，我们回顾了以前流行的视频SR方法，并表明以前在整个恢复过程中使用固定内核的假设在放大真实世界的视频时会导致视觉伪影。为了解决这个问题，我们定制了现有的单图像和视频SR技术，以在内核估计和视频放大过程中利用内核一致性。对合成视频和真实视频的大量实验表明，从数量和质量上都有很大的恢复收益，实现了盲视频SR的最新技术，并强调了利用内核时间一致性的潜力。
<details>	<summary>英文摘要</summary>	Deep learning-based blind super-resolution (SR) methods have recently achieved unprecedented performance in upscaling frames with unknown degradation. These models are able to accurately estimate the unknown downscaling kernel from a given low-resolution (LR) image in order to leverage the kernel during restoration. Although these approaches have largely been successful, they are predominantly image-based and therefore do not exploit the temporal properties of the kernels across multiple video frames. In this paper, we investigated the temporal properties of the kernels and highlighted its importance in the task of blind video super-resolution. Specifically, we measured the kernel temporal consistency of real-world videos and illustrated how the estimated kernels might change per frame in videos of varying dynamicity of the scene and its objects. With this new insight, we revisited previous popular video SR approaches, and showed that previous assumptions of using a fixed kernel throughout the restoration process can lead to visual artifacts when upscaling real-world videos. In order to counteract this, we tailored existing single-image and video SR techniques to leverage kernel consistency during both kernel estimation and video upscaling processes. Extensive experiments on synthetic and real-world videos show substantial restoration gains quantitatively and qualitatively, achieving the new state-of-the-art in blind video SR and underlining the potential of exploiting kernel temporal consistency. </details>
<details>	<summary>邮件日期</summary>	2021年08月20日</details>

# 226、基于物理启发的深度网络的热图像处理
- [ ] Thermal Image Processing via Physics-Inspired Deep Networks 
时间：2021年08月18日                         第一作者：Vishwanath Saragadam                       [链接](https://arxiv.org/abs/2108.07973).                     
## 摘要：我们介绍了DeepIR，一种新的热图像处理框架，它将物理精确的传感器建模与基于深度网络的图像表示相结合。我们的关键使能观测是，热传感器捕获的图像可以分解为缓慢变化、场景无关的传感器不均匀性（可以使用物理精确建模）和场景特定的辐射通量（使用基于深度网络的正则化器很好地表示）。DeepIR既不需要训练数据，也不需要定期对已知黑体目标进行地面真实性校准，因此非常适合实际的计算机视觉任务。我们通过开发新的去噪和超分辨率算法，利用相机抖动拍摄的多幅场景图像，展示了深入红外的威力。仿真和实际数据实验表明，DeepIR可以用三幅图像进行高质量的非均匀性校正，与其他方法相比，峰值信噪比（PSNR）提高了10分贝。
<details>	<summary>英文摘要</summary>	We introduce DeepIR, a new thermal image processing framework that combines physically accurate sensor modeling with deep network-based image representation. Our key enabling observations are that the images captured by thermal sensors can be factored into slowly changing, scene-independent sensor non-uniformities (that can be accurately modeled using physics) and a scene-specific radiance flux (that is well-represented using a deep network-based regularizer). DeepIR requires neither training data nor periodic ground-truth calibration with a known black body target--making it well suited for practical computer vision tasks. We demonstrate the power of going DeepIR by developing new denoising and super-resolution algorithms that exploit multiple images of the scene captured with camera jitter. Simulated and real data experiments demonstrate that DeepIR can perform high-quality non-uniformity correction with as few as three images, achieving a 10dB PSNR improvement over competing approaches. </details>
<details>	<summary>注释</summary>	Accepted to 2nd ICCV workshop on Learning for Computational Imaging (LCI) </details>
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 225、多帧超分辨率深度重参数化与去噪
- [ ] Deep Reparametrization of Multi-Frame Super-Resolution and Denoising 
时间：2021年08月18日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2108.08286).                     
## 摘要：我们对多帧图像恢复任务中常用的最大后验公式提出了一种深度再参数化方法。我们的方法是通过引入学习的误差度量和目标图像的潜在表示，将地图目标转换为深层特征空间。深度再参数化允许我们直接在潜在空间中建模图像形成过程，并将学习到的图像先验知识集成到预测中。因此，我们的方法利用了深度学习的优势，同时也受益于经典MAP公式提供的原则性多帧融合。我们通过对突发去噪和突发超分辨率数据集的综合实验来验证我们的方法。我们的方法为这两项任务设定了一个新的最先进水平，证明了所提议的公式的通用性和有效性。
<details>	<summary>英文摘要</summary>	We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation. </details>
<details>	<summary>注释</summary>	ICCV 2021 Oral </details>
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 224、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年08月18日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年08月19日</details>

# 223、带变压器的光场图像超分辨率
- [ ] Light Field Image Super-Resolution with Transformers 
时间：2021年08月17日                         第一作者：Zhengyu Liang                       [链接](https://arxiv.org/abs/2108.07597).                     
## 摘要：光场（LF）图像超分辨率（SR）旨在从低分辨率图像重建高分辨率LF图像。虽然基于CNN的方法在LF图像SR中取得了显著的效果，但这些方法不能完全模拟4D LF数据的非局部特性。在本文中，我们提出了一种简单但有效的基于变换器的LF图像SR方法。在我们的方法中，设计了一个角度变换器来整合不同视图之间的互补信息，并开发了一个空间变换器来捕获每个子孔径图像中的局部和长程相关性。利用所提出的角度和空间变换器，可以充分利用LF中的有益信息，提高SR性能。我们通过广泛的烧蚀研究验证了我们的角度和空间变换器的有效性，并在五个公共LF数据集上将我们的方法与最新的方法进行了比较。我们的方法以较小的模型尺寸和较低的计算成本实现了优越的SR性能。
<details>	<summary>英文摘要</summary>	Light field (LF) image super-resolution (SR) aims at reconstructing high-resolution LF images from their low-resolution counterparts. Although CNN-based methods have achieved remarkable performance in LF image SR, these methods cannot fully model the non-local properties of the 4D LF data. In this paper, we propose a simple but effective Transformer-based method for LF image SR. In our method, an angular Transformer is designed to incorporate complementary information among different views, and a spatial Transformer is developed to capture both local and long-range dependencies within each sub-aperture image. With the proposed angular and spatial Transformers, the beneficial information in an LF can be fully exploited and the SR performance is boosted. We validate the effectiveness of our angular and spatial Transformers through extensive ablation studies, and compare our method to recent state-of-the-art methods on five public LF datasets. Our method achieves superior SR performance with a small model size and low computational cost. </details>
<details>	<summary>注释</summary>	The first two authors contribute equally to this work. Code is available at https://github.com/ZhengyuLiang24/LFT </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 222、spectrai：光谱数据的深度学习框架
- [ ] spectrai: A deep learning framework for spectral data 
时间：2021年08月17日                         第一作者：Conor C. Horgan                        [链接](https://arxiv.org/abs/2108.07595).                     
## 摘要：近年来，深入学习计算机视觉技术在许多成像领域取得了许多成功。然而，由于需要增强例程、光谱数据的特定体系结构和显著的内存需求，对光谱数据的深度学习应用仍然是一项复杂的任务。在这里，我们介绍了spectrai，这是一个开源的深度学习框架，旨在促进对光谱数据的神经网络训练，并使不同方法之间能够进行比较。Spectrai提供了许多内置的光谱数据预处理和增强方法、光谱数据的神经网络，包括光谱（图像）去噪、光谱（图像）分类、光谱图像分割和光谱图像超分辨率。Spectrai包括命令行和图形用户界面（GUI），旨在指导用户通过各种应用程序的模型和超参数决策。
<details>	<summary>英文摘要</summary>	Deep learning computer vision techniques have achieved many successes in recent years across numerous imaging domains. However, the application of deep learning to spectral data remains a complex task due to the need for augmentation routines, specific architectures for spectral data, and significant memory requirements. Here we present spectrai, an open-source deep learning framework designed to facilitate the training of neural networks on spectral data and enable comparison between different methods. Spectrai provides numerous built-in spectral data pre-processing and augmentation methods, neural networks for spectral data including spectral (image) denoising, spectral (image) classification, spectral image segmentation, and spectral image super-resolution. Spectrai includes both command line and graphical user interfaces (GUI) designed to guide users through model and hyperparameter decisions for a wide range of applications. </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 221、真实ESRGAN：用纯合成数据训练真实世界的盲超分辨率
- [ ] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data 
时间：2021年08月17日                         第一作者：Xintao Wang                       [链接](https://arxiv.org/abs/2107.10833).                     
<details>	<summary>注释</summary>	Tech Report. Training/testing codes and executable files are in https://github.com/xinntao/Real-ESRGAN </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 220、端到端自适应蒙特卡罗去噪和超分辨率
- [ ] End-to-End Adaptive Monte Carlo Denoising and Super-Resolution 
时间：2021年08月16日                         第一作者：Xinyue Wei                       [链接](https://arxiv.org/abs/2108.06915).                     
## 摘要：经典的蒙特卡罗路径跟踪可以在大量计算的代价下实现高质量的渲染。最近的工作利用深度神经网络来加速这一过程，通过在后处理中使用超分辨率或去噪神经网络来改善低分辨率或更少的样本渲染。然而，在以前的工作中，去噪和超分辨率仅被单独考虑。我们在这项工作中表明，在后处理中，联合超分辨率和去噪（SRD）可以进一步加速蒙特卡罗路径跟踪。这种新型的联合滤波仅允许通过路径跟踪渲染低分辨率和较少样本（因此有噪声）的图像，然后将路径跟踪馈入深度神经网络以生成高分辨率和干净的图像。这项工作的主要贡献是一种新的端到端网络体系结构，专门为SRD任务设计。它包含两个具有共享组件的级联级。我们发现，去噪和超分辨率需要非常不同的感受野，这是导致在网络设计中引入可变形卷积的关键洞察。大量的实验表明，该方法的性能优于以前的SRD任务所采用的方法及其变体。
<details>	<summary>英文摘要</summary>	The classic Monte Carlo path tracing can achieve high quality rendering at the cost of heavy computation. Recent works make use of deep neural networks to accelerate this process, by improving either low-resolution or fewer-sample rendering with super-resolution or denoising neural networks in post-processing. However, denoising and super-resolution have only been considered separately in previous work. We show in this work that Monte Carlo path tracing can be further accelerated by joint super-resolution and denoising (SRD) in post-processing. This new type of joint filtering allows only a low-resolution and fewer-sample (thus noisy) image to be rendered by path tracing, which is then fed into a deep neural network to produce a high-resolution and clean image. The main contribution of this work is a new end-to-end network architecture, specifically designed for the SRD task. It contains two cascaded stages with shared components. We discover that denoising and super-resolution require very different receptive fields, a key insight that leads to the introduction of deformable convolution into the network design. Extensive experiments show that the proposed method outperforms previous methods and their variants adopted for the SRD task. </details>
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 219、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年08月16日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 218、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年08月15日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
<details>	<summary>邮件日期</summary>	2021年08月17日</details>

# 217、分层条件流：图像超分辨率和图像重缩放的统一框架
- [ ] Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling 
时间：2021年08月11日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.05301).                     
## 摘要：规范化流程最近在低级别视觉任务中显示了有希望的结果。对于图像超分辨率（SR），它学习从低分辨率（LR）图像预测不同的照片真实高分辨率（HR）图像，而不是学习确定性映射。对于图像重缩放，它通过联合建模降尺度和升尺度过程来实现高精度。虽然现有的方法对这两项任务使用专门的技术，但我们开始将它们统一到一个公式中。在本文中，我们提出了分层条件流（HCFlow）作为图像SR和图像重缩放的统一框架。更具体地说，HCFlow通过同时建模LR图像和其余高频分量的分布来学习HR和LR图像对之间的双射映射。具体地，高频分量以分层方式取决于LR图像。为了进一步提高性能，将感知损失和GAN损失等其他损失与训练中常用的负对数似然损失相结合。对普通图像SR、人脸图像SR和图像重缩放的大量实验表明，所提出的HCFlow在定量度量和视觉质量方面都达到了最先进的性能。
<details>	<summary>英文摘要</summary>	Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality. </details>
<details>	<summary>注释</summary>	Accepted by ICCV2021. Code: https://github.com/JingyunLiang/HCFlow </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 216、互仿射网络在盲图像超分辨率空间变异核估计中的应用
- [ ] Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution 
时间：2021年08月11日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2108.05302).                     
## 摘要：现有的盲图像超分辨率（SR）方法大多假设模糊核在整个图像中具有空间不变性。然而，这样的假设很少适用于真实图像，其模糊核通常由于对象运动和失焦等因素而在空间上变化。因此，现有的盲SR方法在实际应用中不可避免地会导致性能下降。为了解决这个问题，本文提出了一种用于空间变异核估计的互仿射网络（MANet）。具体来说，MANet有两个显著的特点。首先，它有一个适度的感受野，以保持退化的位置。其次，它涉及一个新的互仿射卷积（MAConv）层，该层在不增加感受野、模型大小和计算负担的情况下增强了特征表达能力。这是通过利用通道相互依赖性实现的，它使用仿射变换模块应用每个通道分割，仿射变换模块的输入是剩余通道分割。在合成图像和真实图像上的大量实验表明，所提出的MANet不仅具有良好的空间变异和不变核估计性能，而且在与非盲SR方法相结合时还具有最先进的盲SR性能。
<details>	<summary>英文摘要</summary>	Existing blind image super-resolution (SR) methods mostly assume blur kernels are spatially invariant across the whole image. However, such an assumption is rarely applicable for real images whose blur kernels are usually spatially variant due to factors such as object motion and out-of-focus. Hence, existing blind SR methods would inevitably give rise to poor performance in real applications. To address this issue, this paper proposes a mutual affine network (MANet) for spatially variant kernel estimation. Specifically, MANet has two distinctive features. First, it has a moderate receptive field so as to keep the locality of degradation. Second, it involves a new mutual affine convolution (MAConv) layer that enhances feature expressiveness without increasing receptive field, model size and computation burden. This is made possible through exploiting channel interdependence, which applies each channel split with an affine transformation module whose input are the rest channel splits. Extensive experiments on synthetic and real images show that the proposed MANet not only performs favorably for both spatially variant and invariant kernel estimation, but also leads to state-of-the-art blind SR performance when combined with non-blind SR methods. </details>
<details>	<summary>注释</summary>	Accepted by ICCV2021. Code: https://github.com/JingyunLiang/MANet </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 215、FA-GAN：用于MRI图像超分辨率的融合注意生成对抗网络
- [ ] FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution 
时间：2021年08月09日                         第一作者：Mingfeng Jiang                       [链接](https://arxiv.org/abs/2108.03920).                     
## 摘要：高分辨率磁共振图像可以提供精细的解剖信息，但获取这些数据需要很长的扫描时间。本文提出了一种称为融合注意生成对抗网络（FA-GAN）的框架，用于从低分辨率磁共振图像生成超分辨率磁共振图像，该框架可有效缩短扫描时间，但具有高分辨率磁共振图像。在FA-GAN框架下，提出了一种局部融合特征块，该特征块由不同的卷积核组成的三通网络组成，用于提取不同尺度下的图像特征。设计了全局特征融合模块，包括通道注意模块、自我注意模块和融合操作，以增强MR图像的重要特征。此外，为了使鉴别器网络稳定，还引入了频谱归一化过程。使用40组3D磁共振图像（每组图像包含256个切片）对网络进行训练，并使用10组图像对所提出的方法进行测试。实验结果表明，所提出的FA-GAN方法生成的超分辨率磁共振图像的PSNR和SSIM值高于现有的重建方法。
<details>	<summary>英文摘要</summary>	High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super-resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods. </details>
<details>	<summary>注释</summary>	27 pages, 12 figures, accepted by CMIG </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 214、基于空间角密集网络的高效光场重建
- [ ] Efficient Light Field Reconstruction via Spatio-Angular Dense Network 
时间：2021年08月08日                         第一作者：Zexi Hu                       [链接](https://arxiv.org/abs/2108.03635).                     
## 摘要：作为一种图像传感仪器，光场图像可以提供比单目图像更多的角度信息，并促进了广泛的测量应用。光场图像捕获设备通常在角度分辨率和空间分辨率之间存在固有的权衡。为了解决这一问题，人们提出了几种方法，如光场重建和光场超分辨率，但没有解决两个问题，即域不对称和有效信息流。在本文中，我们提出了一种用于光场重建的端到端空间角度密集网络（SADenseNet），该网络具有两个新的组件，即相关块和空间角度密集跳过连接来解决它们。前者以符合域不对称性的方式对相关信息进行有效建模。后者包括三种连接，增强了两个领域内的信息流。在真实世界和合成数据集上进行了大量实验，以证明所提出的SADenseNet以显著降低内存和计算成本的方式实现了最先进的性能。定性结果表明，重建的光场图像清晰，细节正确，可以作为预处理，提高相关测量应用的精度。
<details>	<summary>英文摘要</summary>	As an image sensing instrument, light field images can supply extra angular information compared with monocular images and have facilitated a wide range of measurement applications. Light field image capturing devices usually suffer from the inherent trade-off between the angular and spatial resolutions. To tackle this problem, several methods, such as light field reconstruction and light field super-resolution, have been proposed but leaving two problems unaddressed, namely domain asymmetry and efficient information flow. In this paper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for light field reconstruction with two novel components, namely correlation blocks and spatio-angular dense skip connections to address them. The former performs effective modeling of the correlation information in a way that conforms with the domain asymmetry. And the latter consists of three kinds of connections enhancing the information flow within two domains. Extensive experiments on both real-world and synthetic datasets have been conducted to demonstrate that the proposed SADenseNet's state-of-the-art performance at significantly reduced costs in memory and computation. The qualitative results show that the reconstructed light field images are sharp with correct details and can serve as pre-processing to improve the accuracy of related measurement applications. </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 213、Ada-VSR：具有元学习的自适应视频超分辨率
- [ ] Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning 
时间：2021年08月05日                         第一作者：Akash Gupta                       [链接](https://arxiv.org/abs/2108.02832).                     
## 摘要：现有的有监督时空视频超分辨率（STVSR）研究大多依赖于由成对的低分辨率低帧速率（LR-LFR）和高分辨率高帧速率（HR-HFR）视频组成的大规模外部数据集。尽管这些方法具有显著的性能，但它们预先假设低分辨率视频是通过使用已知的降级内核（在实际设置中不适用）对高分辨率视频进行降级来获得的。这些方法的另一个问题是，它们无法在测试时利用特定于实例的视频内部信息。最近，深层内部学习方法因其利用视频实例特定统计数据的能力而受到关注。然而，这些方法需要大量的推理时间，因为它们需要数千次梯度更新来学习数据的内在结构。在这项工作中，我们提出了AdaptiveVideoSuper Resolution（Ada VSR），它分别通过元迁移学习和内部学习利用外部和内部信息。具体而言，元学习用于使用大规模外部数据集获得自适应参数，该参数可在内部学习任务期间快速适应给定测试视频的新条件（退化模型），从而利用视频的外部和内部信息实现超分辨率。使用我们的方法训练的模型可以快速适应特定的视频条件，只需少量的梯度更新，从而显著减少了推理时间。在标准数据集上的大量实验表明，我们的方法与各种最先进的方法相比具有良好的性能。
<details>	<summary>英文摘要</summary>	Most of the existing works in supervised spatio-temporal video super-resolution (STVSR) heavily rely on a large-scale external dataset consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these methods make a prior assumption that the low-resolution video is obtained by down-scaling the high-resolution video using a known degradation kernel, which does not hold in practical settings. Another problem with these methods is that they cannot exploit instance-specific internal information of video at testing time. Recently, deep internal learning approaches have gained attention due to their ability to utilize the instance-specific statistics of a video. However, these methods have a large inference time as they require thousands of gradient updates to learn the intrinsic structure of the data. In this work, we presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as well as internal, information through meta-transfer learning and internal learning, respectively. Specifically, meta-learning is employed to obtain adaptive parameters, using a large-scale external dataset, that can adapt quickly to the novel condition (degradation model) of the given test video during the internal learning task, thereby exploiting external and internal information of a video for super-resolution. The model trained using our approach can quickly adapt to a specific video condition with only a few gradient updates, which reduces the inference time significantly. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches. </details>
<details>	<summary>邮件日期</summary>	2021年08月09日</details>

# 212、用于图像超分辨率的双参考训练数据采集和CNN构建
- [ ] Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution 
时间：2021年08月05日                         第一作者：Yanhui Guo                       [链接](https://arxiv.org/abs/2108.02348).                     
## 摘要：对于图像超分辨率的深度学习方法，最关键的问题是用于训练的成对低分辨率和高分辨率图像是否准确反映真实相机的采样过程。现有退化模型（如双三次下采样）合成的低分辨率和高分辨率（LR$\sim$HR）图像对与现实中的图像对不同；因此，由这些合成的LR$\sim$HR图像对训练的超分辨率CNN在应用于真实图像时表现不佳。在本文中，我们提出了一种新的方法，使用真实的摄像机捕获大量真实的LR$\sim$HR图像对。数据采集是在可控的实验室条件下以最少的人为干预和高吞吐量（大约每小时500个图像对）进行的。高度自动化使得为每台摄像机生成一组真实的LR$\sim$HR训练图像对变得容易。我们的创新在于以不同的分辨率拍摄显示在超高质量屏幕上的图像。我们的方法有三个独特的优点，允许我们收集高质量的训练数据集，以实现图像超分辨率。首先，由于LR和HR图像是从3D平面（屏幕）上拍摄的，因此配准问题完全符合单应模型。第二，我们可以在图像边缘显示特殊标记，以进一步提高配准精度。第三，可以利用显示的数字图像文件作为参考，优化恢复图像的高频内容。实验结果表明，在推理阶段，使用LR$\sim$HR数据集训练超分辨率CNN比使用现有数据集训练CNN具有更好的恢复性能。
<details>	<summary>英文摘要</summary>	For deep learning methods of image super-resolution, the most critical issue is whether the paired low and high resolution images for training accurately reflect the sampling process of real cameras. Low and high resolution (LR$\sim$HR) image pairs synthesized by existing degradation models (\eg, bicubic downsampling) deviate from those in reality; thus the super-resolution CNN trained by these synthesized LR$\sim$HR image pairs does not perform well when being applied to real images. In this paper, we propose a novel method to capture a large set of realistic LR$\sim$HR image pairs using real cameras.The data acquisition is carried out under controllable lab conditions with minimum human intervention and at high throughput (about 500 image pairs per hour). The high level of automation makes it easy to produce a set of real LR$\sim$HR training image pairs for each camera. Our innovation is to shoot images displayed on an ultra-high quality screen at different resolutions.There are three distinctive advantages with our method that allow us to collect high-quality training datasets for image super-resolution. First, as the LR and HR images are taken of a 3D planar surface (the screen) the registration problem fits exactly to a homography model. Second, we can display special markers on the image margin to further improve the registration precision.Third, the displayed digital image file can be exploited as a reference to optimize the high frequency content of the restored image. Experimental results show that training a super-resolution CNN by our LR$\sim$HR dataset has superior restoration performance than training it by existing datasets on real world images at the inference stage. </details>
<details>	<summary>邮件日期</summary>	2021年08月06日</details>

# 211、在盲超分辨中寻找特定退化的鉴别滤波器
- [ ] Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution 
时间：2021年08月02日                         第一作者：Liangbin Xie                       [链接](https://arxiv.org/abs/2108.01070).                     
## 摘要：最近的盲超分辨率（SR）方法通常由两个分支组成，一个用于退化预测，另一个用于条件恢复。然而，我们的实验表明，单分支网络可以实现与双分支方案相当的性能。然后我们想知道：单分支网络如何自动学会区分退化？为了找到答案，我们提出了一种新的诊断工具——基于积分梯度的滤波归因方法（FAIG）。与以前的积分梯度方法不同，我们的FAIG旨在寻找最具辨别力的滤波器，而不是用于盲SR网络中去除退化的输入像素/特征。利用所发现的滤波器，我们进一步发展了一种简单而有效的方法来预测输入图像的退化。基于FAIG，我们证明，在单分支盲SR网络中，1）我们能够为每个特定退化找到非常少的（1%）鉴别滤波器；2） 发现的过滤器的权重、位置和连接对于确定特定网络功能都很重要。3） 退化预测的任务可以通过这些判别滤波器隐式地实现，而无需显式的监督学习。我们的发现不仅可以帮助我们更好地理解单分支盲SR网络中的网络行为，而且可以为设计更有效的结构和诊断盲SR网络提供指导。
<details>	<summary>英文摘要</summary>	Recent blind super-resolution (SR) methods typically consist of two branches, one for degradation prediction and the other for conditional restoration. However, our experiments show that a one-branch network can achieve comparable performance to the two-branch scheme. Then we wonder: how can one-branch networks automatically learn to distinguish degradations? To find the answer, we propose a new diagnostic tool -- Filter Attribution method based on Integral Gradient (FAIG). Unlike previous integral gradient methods, our FAIG aims at finding the most discriminative filters instead of input pixels/features for degradation removal in blind SR networks. With the discovered filters, we further develop a simple yet effective method to predict the degradation of an input image. Based on FAIG, we show that, in one-branch blind SR networks, 1) we are able to find a very small number of (1%) discriminative filters for each specific degradation; 2) The weights, locations and connections of the discovered filters are all important to determine the specific network function. 3) The task of degradation prediction can be implicitly realized by these discriminative filters without explicit supervised learning. Our findings can not only help us better understand network behaviors inside one-branch blind SR networks, but also provide guidance on designing more efficient architectures and diagnosing networks for blind SR. </details>
<details>	<summary>注释</summary>	Tech report </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 210、基于互Dirichlet网的无监督非注册高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年08月02日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	IEEE Transactions on Remote Sensing and Geoscience DOI: 10.1109/TGRS.2021.3079518 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 209、超分辨网络中的语义发现
- [ ] Discovering "Semantics" in Super-Resolution Networks 
时间：2021年08月01日                         第一作者：Yihao Liu                       [链接](https://arxiv.org/abs/2108.00406).                     
## 摘要：超分辨率（SR）是低水平视觉领域的一项基础性和代表性任务。一般认为，从SR网络中提取的特征没有特定的语义信息，网络只是简单地学习从输入到输出的复杂非线性映射。我们能在SR网络中找到任何“语义”吗？在本文中，我们对这个问题给出了肯定的回答。通过对特征表示进行降维和可视化分析，我们成功地发现了与图像退化类型和程度相关的SR网络中的深层语义表示、深层退化表示（DDR）。我们还揭示了分类和SR网络在表示语义上的差异。通过大量的实验和分析，我们得出了一系列的观察结果和结论，这些观察结果和结论对今后的工作具有重要意义，例如解释低层CNN网络的内在机制，开发新的盲SR评估方法。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is a fundamental and representative task of low-level vision area. It is generally thought that the features extracted from the SR network have no specific semantic information, and the network simply learns complex non-linear mappings from input to output. Can we find any "semantics" in SR networks? In this paper, we give affirmative answers to this question. By analyzing the feature representations with dimensionality reduction and visualization, we successfully discover the deep semantic representations in SR networks, \textit{i.e.}, deep degradation representations (DDR), which relate to the image degradation types and degrees. We also reveal the differences in representation semantics between classification and SR networks. Through extensive experiments and analysis, we draw a series of observations and conclusions, which are of great significance for future work, such as interpreting the intrinsic mechanisms of low-level CNN networks and developing new evaluation approaches for blind SR. </details>
<details>	<summary>注释</summary>	discovering and interpreting deep degradation representations (DDR) in super-resolution networks </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 208、利用可变感受野二阶通道注意的热图像超分辨率
- [ ] Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields 
时间：2021年07月30日                         第一作者：Nolan B. Gutierrez                       [链接](https://arxiv.org/abs/2108.00094).                     
## 摘要：热图像模拟了电磁光谱的长红外范围，即使在没有可见光照明的情况下也能提供有意义的信息。然而，与代表可见光连续体辐射的图像不同，由于硬件限制，红外图像固有的低分辨率。热图像的恢复对于涉及安全、搜索和救援以及军事行动的应用至关重要。在本文中，我们介绍了一个系统，以有效地重建热图像。具体来说，我们探讨了如何有效地处理对比感受野（RFs），在这种情况下，增加网络的RFs在计算上可能非常昂贵。为此，我们对可变感受野网络（AVRFN）进行了深入的研究。我们提供了一个门控卷积层，其中包含从不同的RFs中提取的高阶信息，其中RF由膨胀率参数化。通过这种方式，可以调整扩张率以使用较少的参数，从而提高AVRFN的疗效。我们的实验结果表明，与竞争对手的热图像超分辨率方法相比，我们的技术水平有所提高。
<details>	<summary>英文摘要</summary>	Thermal images model the long-infrared range of the electromagnetic spectrum and provide meaningful information even when there is no visible illumination. Yet, unlike imagery that represents radiation from the visible continuum, infrared images are inherently low-resolution due to hardware constraints. The restoration of thermal images is critical for applications that involve safety, search and rescue, and military operations. In this paper, we introduce a system to efficiently reconstruct thermal images. Specifically, we explore how to effectively attend to contrasting receptive fields (RFs) where increasing the RFs of a network can be computationally expensive. For this purpose, we introduce a deep attention to varying receptive fields network (AVRFN). We supply a gated convolutional layer with higher-order information extracted from disparate RFs, whereby an RF is parameterized by a dilation rate. In this way, the dilation rate can be tuned to use fewer parameters thus increasing the efficacy of AVRFN. Our experimental results show an improvement over the state of the art when compared against competing thermal image super-resolution methods. </details>
<details>	<summary>注释</summary>	To be published in the 2021 13th International Conference on Computer Vision Systems (ICVS) </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 207、低分辨率扫描病理图像的多尺度超分辨率生成
- [ ] Multi-scale super-resolution generation of low-resolution scanned pathological images 
时间：2021年07月30日                         第一作者：Kai Sun (1)                       [链接](https://arxiv.org/abs/2105.07200).                     
<details>	<summary>注释</summary>	27 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 206、基于傅里叶级数展开的等变卷积滤波器参数化
- [ ] Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions 
时间：2021年07月30日                         第一作者：Qi Xie                        [链接](https://arxiv.org/abs/2107.14519).                     
## 摘要：已经证明，等变卷积对于许多类型的计算机视觉任务都非常有用。近年来，二维滤波器参数化技术在设计等变卷积时发挥了重要作用。然而，目前的滤波器参数化方法仍存在明显的缺陷，其中最关键的是滤波器表示的精度问题。针对这个问题，本文修改了二维滤波器的经典傅里叶级数展开，提出了一组新的原子基函数用于滤波器参数化。所提出的滤波器参数化方法不仅能很好地表示滤波器未旋转时误差为零的二维滤波器，而且能显著地减轻旋转滤波器时由于栅栏效应引起的质量下降。因此，我们提出了一种新的基于共变函数的离散化方法，称之为共变函数。在共变函数的基础上，我们提出了一种新的基于共变函数的离散化方法。大量实验表明了该方法的优越性。特别地，我们将旋转等变卷积方法应用于图像超分辨率任务中，F-Conv在该任务中的性能明显优于以前基于滤波器参数化的方法，反映了其在局部图像特征中忠实保持旋转对称性的内在能力。
<details>	<summary>英文摘要</summary>	It has been shown that equivariant convolution is very helpful for many types of computer vision tasks. Recently, the 2D filter parametrization technique plays an important role when designing equivariant convolutions. However, the current filter parametrization method still has its evident drawbacks, where the most critical one lies in the accuracy problem of filter representation. Against this issue, in this paper we modify the classical Fourier series expansion for 2D filters, and propose a new set of atomic basis functions for filter parametrization. The proposed filter parametrization method not only finely represents 2D filters with zero error when the filter is not rotated, but also substantially alleviates the fence-effect-caused quality degradation when the filter is rotated. Accordingly, we construct a new equivariant convolution method based on the proposed filter parametrization method, named F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the continuous domain, which becomes approximate only after discretization. Extensive experiments show the superiority of the proposed method. Particularly, we adopt rotation equivariant convolution methods to image super-resolution task, and F-Conv evidently outperforms previous filter parametrization based method in this task, reflecting its intrinsic capability of faithfully preserving rotation symmetries in local image features. </details>
<details>	<summary>注释</summary>	27 pages, 19 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月02日</details>

# 205、基于像素自适应核注意的内容感知定向传播网络
- [ ] Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention 
时间：2021年07月28日                         第一作者：Min-Cheol Sagong                       [链接](https://arxiv.org/abs/2107.13144).                     
## 摘要：卷积神经网络（CNN）不仅得到了广泛的应用，而且在图像分类、恢复和生成等众多应用领域也取得了显著的成果。虽然卷积的权重共享特性使其广泛应用于各种任务中，但其内容不可知的特性也可以被认为是一个主要缺点。为了解决这个问题，本文提出了一种新的操作，称为像素自适应核注意（PAKA）。PAKA通过将来自可学习特征的空间变化注意力相乘，为过滤器权重提供方向性。该方法分别沿通道和空间方向推断像素自适应注意图，以较少的参数处理分解模型。我们的方法可以以端到端的方式进行培训，并且适用于任何基于CNN的模型。此外，我们还提出了一种改进的带有PAKA的信息聚合模块，称为层次PAKA模块（HPM）。与传统的信息聚合模块相比，我们展示了HPM在语义分割方面的最新性能，从而证明了HPM的优越性。我们通过额外的消融研究和可视化PAKA提供卷积权重方向性的效果来验证所提出的方法。我们还通过将该方法应用于多模态任务，特别是彩色引导深度图超分辨率任务，证明了该方法的通用性。
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) have been not only widespread but also achieved noticeable results on numerous applications including image classification, restoration, and generation. Although the weight-sharing property of convolutions makes them widely adopted in various tasks, its content-agnostic characteristic can also be considered a major drawback. To solve this problem, in this paper, we propose a novel operation, called pixel adaptive kernel attention (PAKA). PAKA provides directivity to the filter weights by multiplying spatially varying attention from learnable features. The proposed method infers pixel-adaptive attention maps along the channel and spatial directions separately to address the decomposed model with fewer parameters. Our method is trainable in an end-to-end manner and applicable to any CNN-based models. In addition, we propose an improved information aggregation module with PAKA, called the hierarchical PAKA module (HPM). We demonstrate the superiority of our HPM by presenting state-of-the-art performance on semantic segmentation compared to the conventional information aggregation modules. We validate the proposed method through additional ablation studies and visualizing the effect of PAKA providing directivity to the weights of convolutions. We also show the generalizability of the proposed method by applying it to multi-modal tasks especially color-guided depth map super-resolution. </details>
<details>	<summary>注释</summary>	submitted to IEEE transactions on Neural Networks and Learning System </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 204、通过超分辨率提高多视点立体感
- [ ] Improving Multi-View Stereo via Super-Resolution 
时间：2021年07月28日                         第一作者：Eugenio Lomurno                       [链接](https://arxiv.org/abs/2107.13261).                     
## 摘要：如今，多视图立体技术能够重建健壮而详细的三维模型，特别是从高分辨率图像开始时。然而，在某些情况下，输入图像的分辨率相对较低，例如，在处理旧照片时，或者在硬件限制可以获取的数据量时。在本文中，我们研究了通过超分辨率技术提高此类输入图像的分辨率是否、如何以及在多大程度上反映了重建三维模型的质量改进，尽管有时可能会产生伪影。我们表明，在大多数情况下，在恢复深度贴图之前应用超分辨率步骤可以在基于补丁匹配和基于深度学习的算法中获得更好的三维模型。超分辨率的使用特别提高了重建模型的完整性，并且在纹理场景的情况下特别有效。
<details>	<summary>英文摘要</summary>	Today, Multi-View Stereo techniques are able to reconstruct robust and detailed 3D models, especially when starting from high-resolution images. However, there are cases in which the resolution of input images is relatively low, for instance, when dealing with old photos, or when hardware constrains the amount of data that can be acquired. In this paper, we investigate if, how, and how much increasing the resolution of such input images through Super-Resolution techniques reflects in quality improvements of the reconstructed 3D models, despite the artifacts that sometimes this may generate. We show that applying a Super-Resolution step before recovering the depth maps in most cases leads to a better 3D model both in the case of PatchMatch-based and deep-learning-based algorithms. The use of Super-Resolution improves especially the completeness of reconstructed models and turns out to be particularly effective in the case of textured scenes. </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 203、BridgeNet：深度图超分辨率和单目深度估计的联合学习网络
- [ ] BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation 
时间：2021年07月27日                         第一作者：Qi Tang                       [链接](https://arxiv.org/abs/2107.12541).                     
## 摘要：深度图超分辨率是一项实际应用要求很高的任务。现有的彩色引导深度图超分辨率重建方法通常需要一个额外的分支从RGB图像中提取高频细节信息来指导低分辨率深度图的重建。然而，由于两种模式之间仍然存在一些差异，在特征维度或边缘贴图维度中的直接信息传输无法达到令人满意的结果，甚至可能在RGB-D对的结构不一致的区域触发纹理复制。受多任务学习的启发，我们提出了一种深度图超分辨率（DSR）和单目深度估计（MDE）的联合学习网络，无需引入额外的监督标签。对于两个子网之间的相互作用，我们采用了差异化的引导策略，并相应地设计了两个网桥。一种是为特征编码过程设计的高频注意桥（HABdg），它学习MDE任务的高频信息来指导DSR任务。另一个是为深度图重建过程设计的内容指导桥（CGBdg），它为MDE任务提供从DSR任务中学习到的内容指导。整个网络体系结构具有高度的可移植性，可以为DSR和MDE任务的关联提供范例。在基准数据集上的大量实验表明，我们的方法达到了有竞争力的性能。我们的型号和代码都有https://rmcong.github.io/proj_BridgeNet.html.
<details>	<summary>英文摘要</summary>	Depth map super-resolution is a task with high practical application requirements in the industry. Existing color-guided depth map super-resolution methods usually necessitate an extra branch to extract high-frequency detail information from RGB image to guide the low-resolution depth map reconstruction. However, because there are still some differences between the two modalities, direct information transmission in the feature dimension or edge map dimension cannot achieve satisfactory result, and may even trigger texture copying in areas where the structures of the RGB-D pair are inconsistent. Inspired by the multi-task learning, we propose a joint learning network of depth map super-resolution (DSR) and monocular depth estimation (MDE) without introducing additional supervision labels. For the interaction of two subnetworks, we adopt a differentiated guidance strategy and design two bridges correspondingly. One is the high-frequency attention bridge (HABdg) designed for the feature encoding process, which learns the high-frequency information of the MDE task to guide the DSR task. The other is the content guidance bridge (CGBdg) designed for the depth map reconstruction process, which provides the content guidance learned from DSR task for MDE task. The entire network architecture is highly portable and can provide a paradigm for associating the DSR and MDE tasks. Extensive experiments on benchmark datasets demonstrate that our method achieves competitive performance. Our code and models are available at https://rmcong.github.io/proj_BridgeNet.html. </details>
<details>	<summary>注释</summary>	10 pages, 7 figures, Accepted by ACM MM 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 202、局部自适应卷积用于图像融合
- [ ] LAConv: Local Adaptive Convolution for Image Fusion 
时间：2021年07月24日                         第一作者：Zi-Rong Jin                        [链接](https://arxiv.org/abs/2107.11617).                     
## 摘要：卷积运算是一种强有力的特征提取工具，在计算机视觉领域占有重要地位。然而，在针对图像融合等像素级任务时，如果在不同的面片上使用均匀卷积核，则无法充分感知图像中每个像素的特殊性。在本文中，我们提出了一种局部自适应卷积（LAConv），它是动态调整到不同的空间位置。LAConv使网络能够关注学习过程中的每一个特定的局部区域。此外，引入动态偏差（DYB），为特征描述提供了更多的可能性，使网络更加灵活。我们进一步设计了一个包含LAConv和DYB模块的残差结构网络，并将其应用于两个图像融合任务中。通过对泛锐化和高光谱图像超分辨率（HISR）的实验，证明了该方法的优越性。值得一提的是，LAConv还可以以较少的计算量胜任其他超分辨率任务。
<details>	<summary>英文摘要</summary>	The convolution operation is a powerful tool for feature extraction and plays a prominent role in the field of computer vision. However, when targeting the pixel-wise tasks like image fusion, it would not fully perceive the particularity of each pixel in the image if the uniform convolution kernel is used on different patches. In this paper, we propose a local adaptive convolution (LAConv), which is dynamically adjusted to different spatial locations. LAConv enables the network to pay attention to every specific local area in the learning process. Besides, the dynamic bias (DYB) is introduced to provide more possibilities for the depiction of features and make the network more flexible. We further design a residual structure network equipped with the proposed LAConv and DYB modules, and apply it to two image fusion tasks. Experiments for pansharpening and hyperspectral image super-resolution (HISR) demonstrate the superiority of our method over other state-of-the-art methods. It is worth mentioning that LAConv can also be competent for other super-resolution tasks with less computation effort. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 201、任意尺度超分辨率的单网络学习
- [ ] Learning A Single Network for Scale-Arbitrary Super-Resolution 
时间：2021年07月23日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2004.03791).                     
<details>	<summary>注释</summary>	Accepted by ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月26日</details>

# 200、联合隐式图像函数在制导深度超分辨中的应用
- [ ] Joint Implicit Image Function for Guided Depth Super-Resolution 
时间：2021年07月23日                         第一作者：Jiaxiang Tang                       [链接](https://arxiv.org/abs/2107.08717).                     
<details>	<summary>注释</summary>	Accepted by ACM MM 2021 DOI: 10.1145/3474085.3475584 </details>
<details>	<summary>邮件日期</summary>	2021年07月26日</details>

# 199、真实ESRGAN：用纯合成数据训练真实世界的盲超分辨率
- [ ] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data 
时间：2021年07月22日                         第一作者：Xintao Wang                       [链接](https://arxiv.org/abs/2107.10833).                     
## 摘要：尽管人们在盲超分辨率技术中已经进行了许多尝试，以恢复具有未知和复杂退化的低分辨率图像，但它们仍然远远不能解决一般的真实退化图像。在这项工作中，我们将强大的ESRGAN扩展到一个实际的恢复应用程序（即真实的ESRGAN），它是用纯合成数据训练的。特别地，引入了高阶退化建模过程来更好地模拟复杂的真实退化。我们还考虑了合成过程中常见的振铃和过冲伪影。此外，我们还采用了一个具有谱归一化的U-Net鉴别器，以提高鉴别器的性能并稳定训练动态。广泛的比较表明，它的视觉性能优于以往的工作对各种真实数据集。我们还提供了有效的实现来动态合成训练对。
<details>	<summary>英文摘要</summary>	Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. </details>
<details>	<summary>注释</summary>	Tech Report. Training codes, testing codes, and executable files are in https://github.com/xinntao/Real-ESRGAN </details>
<details>	<summary>邮件日期</summary>	2021年07月23日</details>

# 198、基于注意和循环丢失的生成对抗网络的高分辨率骨盆MRI重建
- [ ] High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss 
时间：2021年07月21日                         第一作者：Guangyuan Li                       [链接](https://arxiv.org/abs/2107.09989).                     
## 摘要：磁共振成像（MRI）是一种重要的医学成像手段，但由于生理限制，其采集速度较慢。近年来，超分辨率方法在磁共振成像加速方面表现出了优异的性能。在某些情况下，即使扫描时间延长，也很难获得高分辨率图像。因此，我们提出了一种新的超分辨率方法，它使用具有循环丢失和注意机制的生成对抗网络（GAN）从低分辨率的MR图像中生成高分辨率的MR图像，因子为2。我们将我们的模型应用于健康受试者的骨盆图像作为训练和验证数据，而这些来自患者的数据被用于检测。使用不同的成像序列获得MR数据集，包括T2、T2W-SPAIR和mDIXON-W。采用双三次、SRCNN、SRGAN和EDSR四种方法进行比较。以结构相似性、峰值信噪比、均方根误差和方差膨胀因子为计算指标，评价了该方法的性能。实验结果表明，与其他方法相比，该方法能更好地恢复高分辨率MR图像的细节。此外，重建的高分辨率MR图像可以为肿瘤患者提供更好的病变纹理，在临床诊断中具有广阔的应用前景。
<details>	<summary>英文摘要</summary>	Magnetic resonance imaging (MRI) is an important medical imaging modality, but its acquisition speed is quite slow due to the physiological limitations. Recently, super-resolution methods have shown excellent performance in accelerating MRI. In some circumstances, it is difficult to obtain high-resolution images even with prolonged scan time. Therefore, we proposed a novel super-resolution method that uses a generative adversarial network (GAN) with cyclic loss and attention mechanism to generate high-resolution MR images from low-resolution MR images by a factor of 2. We implemented our model on pelvic images from healthy subjects as training and validation data, while those data from patients were used for testing. The MR dataset was obtained using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison. Structural similarity, peak signal to noise ratio, root mean square error, and variance inflation factor were used as calculation indicators to evaluate the performances of the proposed method. Various experimental results showed that our method can better restore the details of the high-resolution MR image as compared to the other methods. In addition, the reconstructed high-resolution MR image can provide better lesion textures in the tumor patients, which is promising to be used in clinical diagnosis. </details>
<details>	<summary>注释</summary>	21 pages, 7 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年07月22日</details>

# 197、RankSRGAN：学习排名的超分辨率生成对抗网络
- [ ] RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank 
时间：2021年07月20日                         第一作者：Wenlong Zhang                       [链接](https://arxiv.org/abs/2107.09427).                     
## 摘要：生成对抗网络（GAN）已经证明了在单图像超分辨率（SISR）中恢复真实细节的潜力。为了进一步提高超分辨结果的视觉质量，PIRM2018-SR Challenge采用了感知度量来评估感知质量，如PI、NIQE和Ma。然而，现有的方法无法直接优化这些与人类评分高度相关的无差别感知指标。为了解决这个问题，我们提出了一种基于Ranker（RankSRGAN）的超分辨率生成对抗网络，在不同感知度量的方向上对生成元进行优化。具体地说，我们首先训练一个能够学习感知度量行为的等级者，然后引入一种新的等级内容损失来优化感知质量。最吸引人的部分是，该方法可以结合不同SR方法的优点，产生更好的结果。此外，我们将我们的方法扩展到多个ranker，为生成器提供多维约束。大量的实验表明，RankSRGAN在视觉上达到了令人满意的效果，并且在感知度量和质量方面达到了最先进的水平。项目页面：https://wenlongzhang0517.github.io/Projects/RankSRGAN
<details>	<summary>英文摘要</summary>	Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of different perceptual metrics. Specifically, we first train a Ranker which can learn the behaviour of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Furthermore, we extend our method to multiple Rankers to provide multi-dimension constraints for the generator. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics and quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN </details>
<details>	<summary>注释</summary>	IEEE PAMI accepted. arXiv admin note: substantial text overlap with arXiv:1908.06382 </details>
<details>	<summary>邮件日期</summary>	2021年07月21日</details>

# 196、联合隐式图像函数在制导深度超分辨中的应用
- [ ] Joint Implicit Image Function for Guided Depth Super-Resolution 
时间：2021年07月19日                         第一作者：Jiaxiang Tang                       [链接](https://arxiv.org/abs/2107.08717).                     
## 摘要：引导深度超分辨率是一个实际的任务，低分辨率和噪声的输入深度地图恢复到一个高分辨率版本的帮助下，一个高分辨率的RGB引导图像。现有的方法通常把这一任务看作是一个依赖于设计显式滤波器和目标函数的广义引导滤波问题，或是一个通过深度神经网络直接预测目标图像的稠密回归问题。这些方法要么有模型能力，要么有解释能力。受隐式神经表示的启发，我们提出将引导超分辨率问题描述为一个神经隐式图像插值问题，其中我们采用了一般图像插值的形式，但使用了一种新的联合隐式图像函数（JIIF）表示来学习插值权重和插值值。JIIF用从输入图像和引导图像中提取的空间分布的局部潜码来表示目标图像域，并利用图形注意机制在一个统一的深隐函数中同时学习插值权值。我们证明了我们的JIIF表示在引导深度超分辨率任务上的有效性，在三个公共基准上显著优于最先进的方法。可以在\url中找到代码{https://git.io/JC2sU}.
<details>	<summary>英文摘要</summary>	Guided depth super-resolution is a practical task where a low-resolution and noisy input depth map is restored to a high-resolution version, with the help of a high-resolution RGB guide image. Existing methods usually view this task as a generalized guided filtering problem that relies on designing explicit filters and objective functions, or a dense regression problem that directly predicts the target image via deep neural networks. These methods suffer from either model capability or interpretability. Inspired by the recent progress in implicit neural representation, we propose to formulate the guided super-resolution as a neural implicit image interpolation problem, where we take the form of a general image interpolation but use a novel Joint Implicit Image Function (JIIF) representation to learn both the interpolation weights and values. JIIF represents the target image domain with spatially distributed local latent codes extracted from the input image and the guide image, and uses a graph attention mechanism to learn the interpolation weights at the same time in one unified deep implicit function. We demonstrate the effectiveness of our JIIF representation on guided depth super-resolution task, significantly outperforming state-of-the-art methods on three public benchmarks. Code can be found at \url{https://git.io/JC2sU}. </details>
<details>	<summary>注释</summary>	Accepted by ACM MM 2021 DOI: 10.1145/3474085.3475584 </details>
<details>	<summary>邮件日期</summary>	2021年07月20日</details>

# 195、混合对抗性高斯域自适应联合半监督三维超分辨率分割
- [ ] Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation 
时间：2021年07月16日                         第一作者：Nicolo Savioli                       [链接](https://arxiv.org/abs/2107.07975).                     
## 摘要：优化心脏结构和功能的分析需要精确的形状和运动的三维表示。然而，心脏磁共振成像等技术通常局限于获取连续的横截面切片，这些切片具有低的穿透平面分辨率和潜在的层间空间错位。医学成像中的超分辨率是为了提高图像的分辨率，但传统的超分辨率训练方法是基于低分辨率数据集的特征，而不是对相应的分割进行超分辨率。本文提出了一种半监督的多任务生成对抗网络（Gemini-GAN），该网络利用高分辨率3D电影和分割的地面真值，对图像及其标签进行联合超分辨率处理，而非监督的变分对抗混合自动编码器（V-AMA）用于连续域自适应。我们提出的方法被广泛评价在两个跨国多民族人口的1331和205分别成年人，提供了一个改进的国家的最先进的方法方面的骰子指数，峰值信噪比和结构相似性指数的措施。这个框架在外部验证方面也超过了最新的生成域适应模型的性能（左心室的Dice指数为0.81 vs 0.74）。这说明了如何联合超分辨率和分割，训练三维地面真相数据与跨领域推广，使稳健的精度表型在不同的人口。
<details>	<summary>英文摘要</summary>	Optimising the analysis of cardiac structure and function requires accurate 3D representations of shape and motion. However, techniques such as cardiac magnetic resonance imaging are conventionally limited to acquiring contiguous cross-sectional slices with low through-plane resolution and potential inter-slice spatial misalignment. Super-resolution in medical imaging aims to increase the resolution of images but is conventionally trained on features from low resolution datasets and does not super-resolve corresponding segmentations. Here we propose a semi-supervised multi-task generative adversarial network (Gemini-GAN) that performs joint super-resolution of the images and their labels using a ground truth of high resolution 3D cines and segmentations, while an unsupervised variational adversarial mixture autoencoder (V-AMA) is used for continuous domain adaptation. Our proposed approach is extensively evaluated on two transnational multi-ethnic populations of 1,331 and 205 adults respectively, delivering an improvement on state of the art methods in terms of Dice index, peak signal to noise ratio, and structural similarity index measure. This framework also exceeds the performance of state of the art generative domain adaptation models on external validation (Dice index 0.81 vs 0.74 for the left ventricle). This demonstrates how joint super-resolution and segmentation, trained on 3D ground-truth data with cross-domain generalization, enables robust precision phenotyping in diverse populations. </details>
<details>	<summary>邮件日期</summary>	2021年07月19日</details>

# 194、关卡生成与风格提升——游戏开发深度学习概述
- [ ] Level generation and style enhancement -- deep learning for game development overview 
时间：2021年07月15日                         第一作者：Piotr Migda{\l}                       [链接](https://arxiv.org/abs/2107.07397).                     
## 摘要：我们提出了使用深度学习来创建和增强视频游戏（桌面、移动和网络）的水平贴图和纹理的实用方法。我们的目标是为游戏开发者和水平艺术家提供新的可能性。设计关卡并用细节填充关卡的任务很有挑战性。这既费时又费劲，使水平丰富，复杂，并有一种自然的感觉。幸运的是，最近在深度学习方面取得的进展为高级设计师和视觉艺术家提供了新的工具。此外，他们还提供了一种方法来生成无限的游戏可重复性世界，并根据玩家的需要调整教育游戏。我们提出了七种创建层次图的方法，每种方法都使用统计方法、机器学习或深度学习。特别是，我们包括：-从现有示例（如ProGAN）创建新图像的生成性对抗网络在保留清晰细节的同时放大图像的超分辨率技术（如ESRGAN）改变视觉主题的神经风格转换图像翻译-将语义地图转换为图像（如GauGAN）用于将图像转换为语义掩码的语义分割（例如U-Net）用于提取语义特征（例如Tile2Vec）的无监督语义分割-纹理合成-创建一个较小的样本（如英根）为基础的大模式。
<details>	<summary>英文摘要</summary>	We present practical approaches of using deep learning to create and enhance level maps and textures for video games -- desktop, mobile, and web. We aim to present new possibilities for game developers and level artists. The task of designing levels and filling them with details is challenging. It is both time-consuming and takes effort to make levels rich, complex, and with a feeling of being natural. Fortunately, recent progress in deep learning provides new tools to accompany level designers and visual artists. Moreover, they offer a way to generate infinite worlds for game replayability and adjust educational games to players' needs. We present seven approaches to create level maps, each using statistical methods, machine learning, or deep learning. In particular, we include: - Generative Adversarial Networks for creating new images from existing examples (e.g. ProGAN). - Super-resolution techniques for upscaling images while preserving crisp detail (e.g. ESRGAN). - Neural style transfer for changing visual themes. - Image translation - turning semantic maps into images (e.g. GauGAN). - Semantic segmentation for turning images into semantic masks (e.g. U-Net). - Unsupervised semantic segmentation for extracting semantic features (e.g. Tile2Vec). - Texture synthesis - creating large patterns based on a smaller sample (e.g. InGAN). </details>
<details>	<summary>注释</summary>	16 pages, 10 figures, submitted to the 52nd International Simulation and Gaming Association (ISAGA) Conference 2021 ACM-class: I.2.10; I.4.3; J.5 </details>
<details>	<summary>邮件日期</summary>	2021年07月16日</details>

# 193、遥感图像超分辨率的多注意生成对抗网络
- [ ] Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution 
时间：2021年07月14日                         第一作者：Meng Xu                       [链接](https://arxiv.org/abs/2107.06536).                     
## 摘要：图像超分辨率（SR）方法可以在不增加成本的前提下生成高空间分辨率的遥感图像，从而为获取高分辨率遥感图像提供了一种可行的方法，而高分辨率遥感图像由于采集设备成本高、天气复杂而难以获取。显然，图像超分辨率是一个严重的不适定问题。幸运的是，随着深度学习的发展，深度神经网络强大的拟合能力在一定程度上解决了这一问题。本文提出了一种基于生成对抗网络（GAN）的高分辨率遥感图像生成网络，称为多注意生成对抗网络（MA-GAN）。我们首先设计了一个基于GAN的图像SR任务框架。完成SR任务的核心是我们设计的带后上采样的图像发生器。发电机主体包括两个模块；一种是剩余密集块中的金字塔卷积（PCRDB），另一种是基于注意的上采样（AUP）块。PCRDB块中的注意金字塔卷积（AttPConv）是一个将多尺度卷积和信道注意相结合的模块，用于自动学习和调整残差的比例以获得更好的结果。AUP块是一个模块，它结合像素注意（PA）来执行任意倍数的上采样。这两个块一起工作，以帮助生成更好的图像质量。对于损失函数，我们设计了一个基于像素损失的损失函数，并引入对抗损失和特征损失来指导生成器的学习。在一个遥感场景图像数据集上，我们将该方法与现有的几种方法进行了比较，实验结果一致地证明了该方法的有效性。
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost, thereby providing a feasible way to acquire high-resolution remote sensing images, which are difficult to obtain due to the high cost of acquisition equipment and complex weather. Clearly, image super-resolution is a severe ill-posed problem. Fortunately, with the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. In this paper, we propose a network based on the generative adversarial network (GAN) to generate high resolution remote sensing images, named the multi-attention generative adversarial network (MA-GAN). We first designed a GAN-based framework for the image SR task. The core to accomplishing the SR task is the image generator with post-upsampling that we designed. The main body of the generator contains two blocks; one is the pyramidal convolution in the residual-dense block (PCRDB), and the other is the attention-based upsample (AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB block is a module that combines multi-scale convolution and channel attention to automatically learn and adjust the scaling of the residuals for better results. The AUP block is a module that combines pixel attention (PA) to perform arbitrary multiples of upsampling. These two blocks work together to help generate better quality images. For the loss function, we design a loss function based on pixel loss and introduce both adversarial loss and feature loss to guide the generator learning. We have compared our method with several state-of-the-art methods on a remote sensing scene image dataset, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 192、基于深度学习的4K视频实时超分辨率系统
- [ ] Real-Time Super-Resolution System of 4K-Video Based on Deep Learning 
时间：2021年07月14日                         第一作者：Yanpeng Cao                       [链接](https://arxiv.org/abs/2107.05307).                     
<details>	<summary>注释</summary>	8 pages, 7 figures, ASAP </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 191、基于深度学习的单幅图像超分辨率研究综述
- [ ] A Comprehensive Review of Deep Learning-based Single Image Super-resolution 
时间：2021年07月13日                         第一作者：Syed Muhammad Arsalan Bashir                       [链接](https://arxiv.org/abs/2102.09351).                     
<details>	<summary>注释</summary>	56 Pages, 11 Figures, 5 Tables Journal-ref: PeerJ Computer Science 7:e621, 2021 DOI: 10.7717/peerj-cs.621 </details>
<details>	<summary>邮件日期</summary>	2021年07月14日</details>

# 190、学习超分辨率超声改善乳腺病变特征
- [ ] Learned super resolution ultrasound for improved breast lesion characterization 
时间：2021年07月12日                         第一作者：Or Bar-Shira                       [链接](https://arxiv.org/abs/2107.05270).                     
## 摘要：乳腺癌是女性最常见的恶性肿瘤。微钙化和肿块等乳房X线表现以及超声扫描中肿块的形态特征是肿瘤检测的主要诊断目标。然而，需要提高这些成像方式的特异性。一个主要的替代靶点是新生血管生成。病理学上，它有助于许多类型的肿瘤的发展和转移的形成。因此，通过微血管的可视化显示新生血管可能是非常重要的。超分辨超声定位显微术可以在毛细血管水平成像微血管。然而，将超分辨率超声转化为临床需要解决的挑战包括重建时间长、依赖于系统点扩散函数（PSF）的先验知识以及超声造影剂（uca）的可分性。在这项工作中，我们使用了一个深层的神经网络架构，有效地利用信号结构来应对这些挑战。我们介绍了三种不同乳腺病变的活体人体结果。通过利用我们训练的网络，微血管结构可以在短时间内恢复，而无需事先了解PSF，也不需要uca的可分性。每种回收物都显示出与已知组织结构相对应的不同结构。这项研究证明了基于临床扫描仪的活体人体超分辨技术的可行性，以提高超声对不同乳腺病变的特异性，并促进超声在乳腺疾病诊断中的应用。
<details>	<summary>英文摘要</summary>	Breast cancer is the most common malignancy in women. Mammographic findings such as microcalcifications and masses, as well as morphologic features of masses in sonographic scans, are the main diagnostic targets for tumor detection. However, improved specificity of these imaging modalities is required. A leading alternative target is neoangiogenesis. When pathological, it contributes to the development of numerous types of tumors, and the formation of metastases. Hence, demonstrating neoangiogenesis by visualization of the microvasculature may be of great importance. Super resolution ultrasound localization microscopy enables imaging of the microvasculature at the capillary level. Yet, challenges such as long reconstruction time, dependency on prior knowledge of the system Point Spread Function (PSF), and separability of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation of super-resolution US into the clinic. In this work we use a deep neural network architecture that makes effective use of signal structure to address these challenges. We present in vivo human results of three different breast lesions acquired with a clinical US scanner. By leveraging our trained network, the microvasculature structure is recovered in a short time, without prior PSF knowledge, and without requiring separability of the UCAs. Each of the recoveries exhibits a different structure that corresponds with the known histological structure. This study demonstrates the feasibility of in vivo human super resolution, based on a clinical scanner, to increase US specificity for different breast lesions and promotes the use of US in the diagnosis of breast pathologies. </details>
<details>	<summary>注释</summary>	to be published in MICCAI 2021 proceedings </details>
<details>	<summary>邮件日期</summary>	2021年07月13日</details>

# 189、基于深度学习的4K视频实时超分辨率系统
- [ ] Real-Time Super-Resolution System of 4K-Video Based on Deep Learning 
时间：2021年07月12日                         第一作者：Yanpeng Cao                       [链接](https://arxiv.org/abs/2107.05307).                     
## 摘要：视频超分辨率（VSR）技术在重建低质量视频方面有着突出的优势，避免了基于插值的算法带来的令人不快的模糊效果。然而，在实际应用中，特别是在大规模VSR任务中，庞大的计算复杂度和内存占用严重影响了系统的可靠性和运行时推理。本文探讨了实时VSR系统的可能性，并设计了一种高效通用的VSR网络EGVSR。提出了一种基于时空对抗学习的EGVSR算法。为了在4K分辨率下追求更快的VSR处理能力，本文尝试在保证高视觉质量的前提下，选择轻量级的网络结构和高效的上采样方法来减少EGVSR网络所需的计算量。此外，在实际硬件平台上实现了批量归一化计算融合、卷积加速算法等神经网络加速技术，优化了EGVSR网络的推理过程。最后，我们的EGVSR达到了实时处理的能力4K@29.61FPS. 与目前最先进的VSR网络TecoGAN相比，计算密度降低了85.04%，性能提高了7.92倍。在视觉质量方面，所提出的EGVSR在公共测试数据集Vid4的大多数度量（如LPIPS、tOF、tLP等）中名列前茅，在总体性能得分上超过了其他最先进的方法。这个项目的源代码可以在https://github.com/Thmen/EGVSR.
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92x performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on https://github.com/Thmen/EGVSR. </details>
<details>	<summary>注释</summary>	8 pages, 7 figures, ASAP </details>
<details>	<summary>邮件日期</summary>	2021年07月13日</details>

# 188、区域差分信息熵在超分辨率图像质量评价中的应用
- [ ] Regional Differential Information Entropy for Super-Resolution Image Quality Assessment 
时间：2021年07月08日                         第一作者：Ningyuan Xu                       [链接](https://arxiv.org/abs/2107.03642).                     
## 摘要：PSNR和SSIM是超分辨率问题中应用最广泛的指标，因为它们易于使用，并且可以评估生成图像和参考图像之间的相似性。然而，单幅图像的超分辨率是一个不适定问题，同一幅低分辨率图像对应多幅高分辨率图像。相似性不能完全反映修复效果。生成的图像的感知质量也很重要，但是PSNR和SSIM不能很好地反映感知质量。为了解决这个问题，我们提出了一种区域差分信息熵的方法来度量相似度和感知质量。针对传统的图像信息熵不能反映图像的结构信息的问题，提出了用滑动窗口来度量各区域的信息熵。考虑到人类视觉系统对低亮度下的亮度差异更为敏感，我们采用$\gamma$量化而不是线性量化。为了加快计算速度，我们用神经网络重新组织了信息熵的计算过程。通过在我们的IQA数据集和PIPAL上的实验，证明了RDIE能够更好地量化图像尤其是基于GAN的图像的感知质量。
<details>	<summary>英文摘要</summary>	PSNR and SSIM are the most widely used metrics in super-resolution problems, because they are easy to use and can evaluate the similarities between generated images and reference images. However, single image super-resolution is an ill-posed problem, there are multiple corresponding high-resolution images for the same low-resolution image. The similarities can't totally reflect the restoration effect. The perceptual quality of generated images is also important, but PSNR and SSIM do not reflect perceptual quality well. To solve the problem, we proposed a method called regional differential information entropy to measure both of the similarities and perceptual quality. To overcome the problem that traditional image information entropy can't reflect the structure information, we proposed to measure every region's information entropy with sliding window. Considering that the human visual system is more sensitive to the brightness difference at low brightness, we take $\gamma$ quantization rather than linear quantization. To accelerate the method, we reorganized the calculation procedure of information entropy with a neural network. Through experiments on our IQA dataset and PIPAL, this paper proves that RDIE can better quantify perceptual quality of images especially GAN-based images. </details>
<details>	<summary>注释</summary>	8 pages, 9 figures, 4 tables ACM-class: I.4.3; I.4.4 </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 187、基于潜在优化的联合运动校正和超分辨率心脏分割
- [ ] Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation 
时间：2021年07月08日                         第一作者：Shuo Wang                       [链接](https://arxiv.org/abs/2107.03887).                     
## 摘要：在心脏磁共振成像（CMR）中，心脏的三维高分辨率分割对于详细描述其解剖结构至关重要。然而，由于采集时间和呼吸/心脏运动的限制，临床上通常需要采集多层二维图像。这些图像的分割提供了心脏解剖结构的低分辨率表示，其中可能包含由运动引起的人工制品。在这里，我们提出了一个新的潜在优化框架，联合执行运动校正和超分辨率心脏图像分割。在给定低分辨率分割作为输入的情况下，该框架考虑了心脏MR成像中的层间运动，并将输入超分辨为与输入一致的高分辨率分割。一个多视图的损失是结合利用信息从短轴和长轴视图的心脏成像。为了解决反问题，在一个潜在空间中进行迭代优化，以确保解剖学上的合理性。这减轻了有监督学习对低分辨率和高分辨率图像的需要。在两个心脏MR数据集上的实验表明，该框架具有很高的性能，与最新的超分辨率方法相当，并且具有更好的跨域通用性和解剖学合理性。
<details>	<summary>英文摘要</summary>	In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution segmentation of the heart is essential for detailed description of its anatomical structures. However, due to the limit of acquisition duration and respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in clinical routine. The segmentation of these images provides a low-resolution representation of cardiac anatomy, which may contain artefacts caused by motion. Here we propose a novel latent optimisation framework that jointly performs motion correction and super resolution for cardiac image segmentations. Given a low-resolution segmentation as input, the framework accounts for inter-slice motion in cardiac MR imaging and super-resolves the input into a high-resolution segmentation consistent with input. A multi-view loss is incorporated to leverage information from both short-axis view and long-axis view of cardiac imaging. To solve the inverse problem, iterative optimisation is performed in a latent space, which ensures the anatomical plausibility. This alleviates the need of paired low-resolution and high-resolution images for supervised learning. Experiments on two cardiac MR datasets show that the proposed framework achieves high performance, comparable to state-of-the-art super-resolution approaches and with better cross-domain generalisability and anatomical plausibility. </details>
<details>	<summary>注释</summary>	The paper is early accepted to MICCAI 2021. The codes are available at https://github.com/shuowang26/SRHeart </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 186、盲图像超分辨率研究综述及展望
- [ ] Blind Image Super-Resolution: A Survey and Beyond 
时间：2021年07月07日                         第一作者：Anran Liu                       [链接](https://arxiv.org/abs/2107.03055).                     
## 摘要：盲图像超分辨率（SR）技术是一种针对低分辨率图像的超分辨率技术，具有重要的现实意义。近年来，人们提出了许多新颖有效的解决方案，特别是借助于强大的深度学习技术。尽管经过多年的努力，它仍然是一个具有挑战性的研究问题。本文系统地回顾了近年来盲图像SR的研究进展，提出了一种分类方法，根据退化建模的方法和求解SR模型所用的数据，将现有的方法分为三类。这种分类法有助于总结和区分现有的方法。我们希望能对当前的研究状况提供一些见解，同时也能揭示出值得探索的新的研究方向。此外，本文还对常用的数据集和以往与盲图像相关的比赛进行了总结。最后，对各种方法进行了比较，并用合成图像和真实测试图像详细分析了它们的优缺点。
<details>	<summary>英文摘要</summary>	Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with the powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used for solving the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as to reveal novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images. </details>
<details>	<summary>邮件日期</summary>	2021年07月08日</details>

# 185、一种用于多域图像超分辨率的深剩余恒星生成对抗网络
- [ ] A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution 
时间：2021年07月07日                         第一作者：Rao Muhammad Umer                       [链接](https://arxiv.org/abs/2107.03145).                     
## 摘要：近年来，利用深度卷积神经网络（DCNNs）实现的最新单图像超分辨率（SISR）方法取得了令人瞩目的性能。现有的SR方法由于固定的退化设置（即通常是低分辨率（LR）图像的双三次缩小）而性能有限。然而，在现实环境中，LR退化过程是未知的，可以是双三次LR、双线性LR、最近邻LR或真实LR。因此，大多数SR方法在处理单个网络中的多个降级设置时是无效和低效的。为了解决多重退化问题，即多域图像的超分辨率问题，我们提出了一种深度超分辨率残差StarGAN（SR2*GAN）算法，该算法只需一个模型就可以对多个LR域的LR图像进行超分辨率处理。该方案是在一个类似StarGAN的网络拓扑结构中训练的，该拓扑结构由一个生成器和一个鉴别器网络组成。与其他先进的方法相比，我们在定量和定性实验中证明了我们提出的方法的有效性。
<details>	<summary>英文摘要</summary>	Recently, most of state-of-the-art single image super-resolution (SISR) methods have attained impressive performance by using deep convolutional neural networks (DCNNs). The existing SR methods have limited performance due to a fixed degradation settings, i.e. usually a bicubic downscaling of low-resolution (LR) image. However, in real-world settings, the LR degradation process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR, or real LR. Therefore, most SR methods are ineffective and inefficient in handling more than one degradation settings within a single network. To handle the multiple degradation, i.e. refers to multi-domain image super-resolution, we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and scalable approach that super-resolves the LR images for the multiple LR domains using only a single model. The proposed scheme is trained in a StarGAN like network topology with a single generator and discriminator networks. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments compared to other state-of-the-art methods. </details>
<details>	<summary>注释</summary>	5 pages, 6th International Conference on Smart and Sustainable Technologies 2021. arXiv admin note: text overlap with arXiv:2009.03693, arXiv:2005.00953 </details>
<details>	<summary>邮件日期</summary>	2021年07月08日</details>

# 184、从一般到特殊：盲超分辨率的在线更新
- [ ] From General to Specific: Online Updating for Blind Super-Resolution 
时间：2021年07月06日                         第一作者：Shang Li                       [链接](https://arxiv.org/abs/2107.02398).                     
## 摘要：大多数基于深度学习的超分辨率（SR）方法都不是图像特定的：1）它们在由预定义模糊核（如双三次）合成的数据集上进行穷尽训练，而不考虑与测试图像的域间距。2） 它们的模型权值在测试过程中是固定的，这意味着具有不同退化的测试图像被同一组权值所超分辨。然而，真实图像的退化是多种多样且未知的（\ie blind SR）。单个模型很难在所有情况下都表现良好。为了解决这些问题，我们提出了一种在线超分辨率（ONSR）方法。它不依赖于预定义的模糊核，并且允许根据测试图像的退化更新模型权重。具体来说，ONSR由两个分支组成，即内部分支（IB）和外部分支（EB）。IB可以学习给定测试LR图像的特定退化，EB可以学习由所学习的退化所退化的图像的超分辨率。通过这种方式，ONSR可以为每个测试图像定制一个特定的模型，从而在实际应用中对各种退化具有更高的容忍度。在合成图像和真实图像上的大量实验表明，ONSR能产生更为直观的随机共振结果，并在盲随机共振中获得最先进的性能。
<details>	<summary>英文摘要</summary>	Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are exhaustively trained on datasets synthesized by predefined blur kernels (\eg bicubic), regardless of the domain gap with test images. 2) Their model weights are fixed during testing, which means that test images with various degradations are super-resolved by the same set of weights. However, degradations of real images are various and unknown (\ie blind SR). It is hard for a single model to perform well in all cases. To address these issues, we propose an online super-resolution (ONSR) method. It does not rely on predefined blur kernels and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus could be more tolerant with various degradations in real applications. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR. </details>
<details>	<summary>注释</summary>	Submitted to Pattern Recognition </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 183、基于深度学习的图像超分辨率对二值信号检测的影响
- [ ] Impact of deep learning-based image super-resolution on binary signal detection 
时间：2021年07月06日                         第一作者：Xiaohui Zhang                       [链接](https://arxiv.org/abs/2107.02338).                     
## 摘要：基于深度学习的图像超分辨率（DL-SR）在医学成像领域有着广阔的应用前景。迄今为止，大多数提出的DL-SR方法仅通过使用计算机视觉领域中常用的传统图像质量（IQ）度量进行评估。然而，这些方法对与医学成像任务相关的图像质量的客观度量的影响仍然很大程度上未被探索。在这项研究中，我们研究了DL-SR方法对二进制信号检测性能的影响。利用模拟医学图像数据训练了两种常用的DL-SR方法：超分辨率卷积神经网络（SRCNN）和超分辨率生成对抗网络（SRGAN）。提出了背景统计已知二值信号（SKE/BKS）和背景统计已知信号（SKS/BKS）检测任务。数值观察器包括神经网络逼近理想观察器和普通线性数值观察器，用于评估DL-SR对任务绩效的影响。量化了DL-SR网络结构的复杂性对任务性能的影响。此外，还研究了DL-SR在改善次优观测器任务性能方面的作用。我们的数值实验证实，DL-SR可以改善传统的智商测量方法。然而，对于所考虑的许多研究设计，DL-SR方法对任务绩效的改善很少或没有改善，甚至可能降低任务绩效。结果表明，在一定条件下，DL-SR可以提高次优观测器的任务性能。本研究强调了客观评估DL-SR方法的迫切需要，并提出了提高其在医学成像应用中的有效性的途径。
<details>	<summary>英文摘要</summary>	Deep learning-based image super-resolution (DL-SR) has shown great promise in medical imaging applications. To date, most of the proposed methods for DL-SR have only been assessed by use of traditional measures of image quality (IQ) that are commonly employed in the field of computer vision. However, the impact of these methods on objective measures of image quality that are relevant to medical imaging tasks remains largely unexplored. In this study, we investigate the impact of DL-SR methods on binary signal detection performance. Two popular DL-SR methods, the super-resolution convolutional neural network (SRCNN) and the super-resolution generative adversarial network (SRGAN), were trained by use of simulated medical image data. Binary signal-known-exactly with background-known-statistically (SKE/BKS) and signal-known-statistically with background-known-statistically (SKS/BKS) detection tasks were formulated. Numerical observers, which included a neural network-approximated ideal observer and common linear numerical observers, were employed to assess the impact of DL-SR on task performance. The impact of the complexity of the DL-SR network architectures on task-performance was quantified. In addition, the utility of DL-SR for improving the task-performance of sub-optimal observers was investigated. Our numerical experiments confirmed that, as expected, DL-SR could improve traditional measures of IQ. However, for many of the study designs considered, the DL-SR methods provided little or no improvement in task performance and could even degrade it. It was observed that DL-SR could improve the task-performance of sub-optimal observers under certain conditions. The presented study highlights the urgent need for the objective assessment of DL-SR methods and suggests avenues for improving their efficacy in medical imaging applications. </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 182、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年07月05日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
<details>	<summary>注释</summary>	10 pages, 3 figures Journal-ref: International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2021) </details>
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 181、用于关节MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年07月05日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
<details>	<summary>邮件日期</summary>	2021年07月07日</details>

# 180、复杂噪声下的无监督单幅图像超分辨率
- [ ] Unsupervised Single Image Super-resolution Under Complex Noise 
时间：2021年07月02日                         第一作者：Zongsheng Yue                       [链接](https://arxiv.org/abs/2107.00986).                     
## 摘要：近年来，单图像超分辨率（SISR）的研究，特别是基于深度神经网络（DNNs）的单图像超分辨率（SISR）的研究取得了巨大的成就，但仍存在两大局限性。首先，真实的图像退化通常是未知的，并且在不同的情况下变化很大，因此很难训练单一的模型来处理一般的SISR任务。其次，现有的方法大多集中在退化的降采样过程，而忽略或低估了不可避免的噪声污染。例如，通常使用的独立同分布（i.i.d.）高斯噪声分布总是很大程度上偏离真实图像噪声（例如相机传感器噪声），这限制了它们在真实场景中的性能。针对这些问题，本文提出了一种基于模型的无监督SISR方法来处理具有未知退化的一般SISR任务。提出了一种新的基于面片的非i.i.d.噪声建模方法，代替了传统的i.i.d.高斯噪声假设。此外，利用DNN参数化的深度发生器将潜变量映射到高分辨率图像中，并将传统的超拉普拉斯先验知识嵌入到深度发生器中，进一步约束图像梯度。最后，设计了一个montecarlo-EM算法来求解我们的模型，它提供了一个通用的推理框架来更新图像发生器的w.r.t.潜变量和网络参数。综合实验表明，该方法不仅模型较细（0.34M比2.40M），而且速度较快，明显优于现有的SotA方法（峰值信噪比1dB左右）。
<details>	<summary>英文摘要</summary>	While the researches on single image super-resolution (SISR), especially equipped with deep neural networks (DNNs), have achieved tremendous successes recently, they still suffer from two major limitations. Firstly, the real image degradation is usually unknown and highly variant from one to another, making it extremely hard to train a single model to handle the general SISR task. Secondly, most of current methods mainly focus on the downsampling process of the degradation, but ignore or underestimate the inevitable noise contamination. For example, the commonly-used independent and identically distributed (i.i.d.) Gaussian noise distribution always largely deviates from the real image noise (e.g., camera sensor noise), which limits their performance in real scenarios. To address these issues, this paper proposes a model-based unsupervised SISR method to deal with the general SISR task with unknown degradations. Instead of the traditional i.i.d. Gaussian noise assumption, a novel patch-based non-i.i.d. noise modeling method is proposed to fit the complex real noise. Besides, a deep generator parameterized by a DNN is used to map the latent variable to the high-resolution image, and the conventional hyper-Laplacian prior is also elaborately embedded into such generator to further constrain the image gradients. Finally, a Monte Carlo EM algorithm is designed to solve our model, which provides a general inference framework to update the image generator both w.r.t. the latent variable and the network parameters. Comprehensive experiments demonstrate that the proposed method can evidently surpass the current state of the art (SotA) method (about 1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster speed. </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 179、深部图像先验光谱偏差的测量与控制
- [ ] On Measuring and Controlling the Spectral Bias of the Deep Image Prior 
时间：2021年07月02日                         第一作者：Zenglin Shi                       [链接](https://arxiv.org/abs/2107.01125).                     
## 摘要：深度图像先验证明了未经训练的网络能够通过对单个退化图像进行优化来解决逆成像问题，如去噪、修复和超分辨率。尽管它有承诺，但它有两个局限性。首先，除了网络架构的选择之外，人们如何控制先验知识还不清楚。第二，当性能达到峰值后下降时，需要oracle确定何时停止优化。为了解决这些问题，本文从光谱偏移的角度研究了深部图像的先验知识。通过引入频带对应度量，我们观察到用于逆成像的深度图像先验在优化过程中表现出光谱偏差，其中低频图像信号比高频噪声信号学习得更快更好。这就明确了当优化在适当的时间停止时，为什么退化的图像可以去噪或修复。基于我们的观察，我们建议在深度图像中控制光谱偏差，以防止性能下降和加速优化收敛。我们在两种核心层类型的逆成像网络中实现：卷积层和上采样层。我们提出了卷积的Lipschitz控制方法和上采样层的高斯控制方法。为了避免多余的计算，我们进一步引入了停止准则。去噪、修复和超分辨率实验表明，该方法在优化过程中不再出现性能下降的问题，从而避免了oracle准则的提前终止。我们进一步概述了停止标准，以避免多余的计算。最后，我们证明了与现有方法相比，我们的方法在所有任务中都取得了良好的恢复效果。
<details>	<summary>英文摘要</summary>	The deep image prior has demonstrated the remarkable ability that untrained networks can address inverse imaging problems, such as denoising, inpainting and super-resolution, by optimizing on just a single degraded image. Despite its promise, it suffers from two limitations. First, it remains unclear how one can control the prior beyond the choice of the network architecture. Second, it requires an oracle to determine when to stop the optimization as the performance degrades after reaching a peak. In this paper, we study the deep image prior from a spectral bias perspective to address these problems. By introducing a frequency-band correspondence measure, we observe that deep image priors for inverse imaging exhibit a spectral bias during optimization, where low-frequency image signals are learned faster and better than high-frequency noise signals. This pinpoints why degraded images can be denoised or inpainted when the optimization is stopped at the right time. Based on our observations, we propose to control the spectral bias in the deep image prior to prevent performance degradation and to speed up optimization convergence. We do so in the two core layer types of inverse imaging networks: the convolution layer and the upsampling layer. We present a Lipschitz-controlled approach for the convolution and a Gaussian-controlled approach for the upsampling layer. We further introduce a stopping criterion to avoid superfluous computation. The experiments on denoising, inpainting and super-resolution show that our method no longer suffers from performance degradation during optimization, relieving us from the need for an oracle criterion to stop early. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable restoration results compared to current approaches, across all tasks. </details>
<details>	<summary>注释</summary>	Spectral bias; Deep image prior; 18 pages </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 178、基于对比表征学习的盲图像超分辨
- [ ] Blind Image Super-Resolution via Contrastive Representation Learning 
时间：2021年07月01日                         第一作者：Jiahui Zhang                       [链接](https://arxiv.org/abs/2107.00708).                     
## 摘要：近年来，由于卷积神经网络（CNNs）的发展，图像超分辨率（SR）的研究取得了令人瞩目的进展。然而，大多数现有的SR方法是非盲的，并且假设退化具有单一的固定的和已知的分布（例如，双三次分布），在处理通常遵循多模态、空间变化和未知分布的真实世界数据中的退化时，这些分布很困难。最近的盲SR研究通过退化估计来解决这一问题，但它们不能很好地推广到多源退化，也不能处理空间变异退化。我们设计了CRL-SR，这是一个对比表征学习网络，主要研究具有多模态和空间变异分布的图像的盲SR。CRL-SR从两个角度解决了盲SR挑战。第一种是对比解耦编码，在双向对比丢失的指导下，引入对比学习来提取分辨率不变的嵌入和丢弃分辨率可变的嵌入。第二种是对比特征提取，在条件对比丢失的指导下产生丢失或损坏的高频细节。在合成数据集和真实图像上的大量实验表明，该方法能有效地处理盲环境下的多模态和空间变异退化问题，并且在定性和定量上都优于现有的随机共振方法。
<details>	<summary>英文摘要</summary>	Image super-resolution (SR) research has witnessed impressive progress thanks to the advance of convolutional neural networks (CNNs) in recent years. However, most existing SR methods are non-blind and assume that degradation has a single fixed and known distribution (e.g., bicubic) which struggle while handling degradation in real-world data that usually follows a multi-modal, spatially variant, and unknown distribution. The recent blind SR studies address this issue via degradation estimation, but they do not generalize well to multi-source degradation and cannot handle spatially variant degradation. We design CRL-SR, a contrastive representation learning network that focuses on blind SR of images with multi-modal and spatially variant distributions. CRL-SR addresses the blind SR challenges from two perspectives. The first is contrastive decoupling encoding which introduces contrastive learning to extract resolution-invariant embedding and discard resolution-variant embedding under the guidance of a bidirectional contrastive loss. The second is contrastive feature refinement which generates lost or corrupted high-frequency details under the guidance of a conditional contrastive loss. Extensive experiments on synthetic datasets and real images show that the proposed CRL-SR can handle multi-modal and spatially variant degradation effectively under blind settings and it also outperforms state-of-the-art SR methods qualitatively and quantitatively. </details>
<details>	<summary>邮件日期</summary>	2021年07月05日</details>

# 177、文本优先引导场景文本图像超分辨率
- [ ] Text Prior Guided Scene Text Image Super-resolution 
时间：2021年06月30日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2106.15368).                     
<details>	<summary>注释</summary>	Code has been released on https://github.com/mjq11302010044/TPGSR </details>
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 176、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年06月30日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 175、应用自监督学习实现动态胎儿MRI的超分辨率
- [ ] STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning 
时间：2021年06月30日                         第一作者：Junshen Xu                       [链接](https://arxiv.org/abs/2106.12407).                     
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 174、文本优先引导场景文本图像超分辨率
- [ ] Text Prior Guided Scene Text Image Super-resolution 
时间：2021年06月29日                         第一作者：Jianqi Ma                       [链接](https://arxiv.org/abs/2106.15368).                     
## 摘要：场景文本图像超分辨率（STISR）旨在提高低分辨率（LR）场景文本图像的分辨率和视觉质量，从而提高文本识别的性能。然而，现有的STISR方法大多将文本图像视为自然场景图像，忽略了文本的分类信息。在本文中，我们做了一个鼓舞人心的尝试，将分类文本嵌入到STISR模型训练中。具体地说，我们采用字符概率序列作为文本先验，这可以方便地从文本识别模型中获得。文本优先提供分类指导，以恢复高分辨率（HR）文本图像。另一方面，重构后的HR图像可以对文本进行细化。最后，我们提出了一个多阶段的文本优先引导超分辨率（TPGSR）框架。在基准TextZoom数据集上的实验表明，TPGSR不仅能有效地提高场景文本图像的视觉质量，而且比现有的STISR方法显著提高了文本识别的准确率。在TextZoom上训练的模型对其它数据集中的LR图像也具有一定的泛化能力。
<details>	<summary>英文摘要</summary>	Scene text image super-resolution (STISR) aims to improve the resolution and visual quality of low-resolution (LR) scene text images, and consequently boost the performance of text recognition. However, most of existing STISR methods regard text images as natural scene images, ignoring the categorical information of text. In this paper, we make an inspiring attempt to embed categorical text prior into STISR model training. Specifically, we adopt the character probability sequence as the text prior, which can be obtained conveniently from a text recognition model. The text prior provides categorical guidance to recover high-resolution (HR) text images. On the other hand, the reconstructed HR image can refine the text prior in return. Finally, we present a multi-stage text prior guided super-resolution (TPGSR) framework for STISR. Our experiments on the benchmark TextZoom dataset show that TPGSR can not only effectively improve the visual quality of scene text images, but also significantly improve the text recognition accuracy over existing STISR methods. Our model trained on TextZoom also demonstrates certain generalization capability to the LR images in other datasets. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 173、IREM：通过隐式神经表示的高分辨率磁共振（MR）图像重建
- [ ] IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation 
时间：2021年06月29日                         第一作者：Qing Wu                       [链接](https://arxiv.org/abs/2106.15097).                     
## 摘要：为了采集高质量的高分辨率（HR）MR图像，我们提出了一种新的图像重建网络IREM，该网络对多幅低分辨率（LR）MR图像进行训练，实现了任意上采样率的HR图像重建。在这项工作中，我们假设期望的HR图像是3D图像空间坐标的隐式连续函数，而厚切片LR图像是该函数的几个稀疏离散采样。然后，超分辨率（SR）任务是使用完全连接的神经网络结合Fourier特征位置编码从有限的观测值中学习连续体积函数。通过简单地最小化网络预测和每个成像平面上获得的LR图像强度之间的误差，IREM被训练成表示观察到的组织解剖结构的连续模型。实验结果表明，IREM成功地表达了高频图像特征，在真实场景数据采集中，IREM缩短了扫描时间，在信噪比和局部图像细节方面实现了高质量的高分辨率MR成像。
<details>	<summary>英文摘要</summary>	For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 172、一种混合监督多级GAN图像质量增强框架
- [ ] A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement 
时间：2021年06月29日                         第一作者：Uddeshya Upadhyay                       [链接](https://arxiv.org/abs/2106.15575).                     
## 摘要：用于图像质量增强的深度神经网络通常需要大量由一对低质量图像及其相应的高质量图像组成的高度精确的训练数据。虽然高质量图像采集通常昂贵且耗时，但中等质量图像的采集速度更快，设备成本更低，并且可以大量获取。因此，我们提出了一种新的生成性对抗网络（GAN），它可以在多个质量级别（例如，高质量和中等质量）上利用训练数据来提高性能，同时限制数据管理的成本。我们将我们的混合监督GAN应用于（i）超分辨率组织病理学图像和（ii）结合超分辨率和外科消烟来增强腹腔镜图像。在大量临床和临床前数据集上的结果表明，我们的混合监督机制优于现有技术。
<details>	<summary>英文摘要</summary>	Deep neural networks for image quality enhancement typically need large quantities of highly-curated training data comprising pairs of low-quality images and their corresponding high-quality images. While high-quality image acquisition is typically expensive and time-consuming, medium-quality images are faster to acquire, at lower equipment costs, and available in larger quantities. Thus, we propose a novel generative adversarial network (GAN) that can leverage training data at multiple levels of quality (e.g., high and medium quality) to improve performance while limiting costs of data curation. We apply our mixed-supervision GAN to (i) super-resolve histopathology images and (ii) enhance laparoscopy images by combining super-resolution and surgical smoke removal. Results on large clinical and pre-clinical datasets show the benefits of our mixed-supervision GAN over the state of the art. </details>
<details>	<summary>注释</summary>	MICCAI 2019 </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 171、“零炮”点云上采样
- [ ] "Zero Shot" Point Cloud Upsampling 
时间：2021年06月25日                         第一作者：Kaiyue Zhou                       [链接](https://arxiv.org/abs/2106.13765).                     
## 摘要：在过去的几年中，利用深度学习进行点云上采样已经付出了各种努力。最近的有监督深度学习方法局限于训练数据的大小，并且局限于覆盖所有形状的点云。此外，获取如此数量的数据是不现实的，而且网络在看不见的记录上的性能通常不如预期。在本文中，我们提出了一种无监督的点云上采样方法，内部称为“零炮”点云上采样（ZSPU）在整体水平。我们的方法完全基于特定点云提供的内部信息，而不是在自我训练和测试阶段进行修补。这种单流设计通过学习低分辨率（LR）点云和高分辨率（HR）点云之间的关系，显著减少了上采样任务的训练时间。当原始点云作为输入加载时，此关联将提供超分辨率（SR）输出。与其他上采样方法相比，我们在基准点云数据集上展示了具有竞争力的性能。此外，ZSPU在局部细节复杂或曲率较大的形状上取得了较好的定性结果。
<details>	<summary>英文摘要</summary>	Point cloud upsampling using deep learning has been paid various efforts in the past few years. Recent supervised deep learning methods are restricted to the size of training data and is limited in terms of covering all shapes of point clouds. Besides, the acquisition of such amount of data is unrealistic, and the network generally performs less powerful than expected on unseen records. In this paper, we present an unsupervised approach to upsample point clouds internally referred as "Zero Shot" Point Cloud Upsampling (ZSPU) at holistic level. Our approach is solely based on the internal information provided by a particular point cloud without patching in both self-training and testing phases. This single-stream design significantly reduces the training time of the upsampling task, by learning the relation between low-resolution (LR) point clouds and their high (original) resolution (HR) counterparts. This association will provide super-resolution (SR) outputs when original point clouds are loaded as input. We demonstrate competitive performance on benchmark point cloud datasets when compared to other upsampling methods. Furthermore, ZSPU achieves superior qualitative results on shapes with complex local details or high curvatures. </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 170、基于长时自样本的视频超分辨率
- [ ] Video Super-Resolution with Long-Term Self-Exemplars 
时间：2021年06月24日                         第一作者：Guotao Meng                       [链接](https://arxiv.org/abs/2106.12778).                     
## 摘要：现有的视频超分辨率方法通常利用几个相邻帧为每一帧生成更高分辨率的图像。然而，这些方法并没有充分利用帧间的冗余信息：同一实例对应的补丁在不同尺度的帧间出现。基于这一观察，我们提出了一种长期跨尺度聚集的视频超分辨率方法，该方法利用了跨远帧的相似块（自样本）。我们的模型还包括一个多参考对齐模块，用于融合来自相似面片的特征：我们融合来自远处参考的特征来执行高质量的超分辨率。提出了一种新颖实用的基于参考的超分辨率训练策略。为了评估我们提出的方法的性能，我们在收集的CarCam数据集和Waymo开放数据集上进行了大量的实验，结果表明我们的方法优于现有的方法。我们的源代码将公开。
<details>	<summary>英文摘要</summary>	Existing video super-resolution methods often utilize a few neighboring frames to generate a higher-resolution image for each frame. However, the redundant information between distant frames has not been fully exploited in these methods: corresponding patches of the same instance appear across distant frames at different scales. Based on this observation, we propose a video super-resolution method with long-term cross-scale aggregation that leverages similar patches (self-exemplars) across distant frames. Our model also consists of a multi-reference alignment module to fuse the features derived from similar patches: we fuse the features of distant references to perform high-quality super-resolution. We also propose a novel and practical training strategy for referenced-based super-resolution. To evaluate the performance of our proposed method, we conduct extensive experiments on our collected CarCam dataset and the Waymo Open dataset, and the results demonstrate our method outperforms state-of-the-art methods. Our source code will be publicly available. </details>
<details>	<summary>邮件日期</summary>	2021年06月25日</details>

# 169、通过深入学习提高生物超分辨显微镜：一个简要回顾
- [ ] Advancing biological super-resolution microscopy through deep learning: a brief review 
时间：2021年06月24日                         第一作者：Tianjie Yang                       [链接](https://arxiv.org/abs/2106.13064).                     
## 摘要：超分辨显微镜克服了传统光学显微镜在空间分辨率上的衍射限制。通过提供具有分子特异性的纳米尺度生物过程的时空信息，它在生命科学中发挥着越来越重要的作用。然而，它的技术局限性要求权衡其空间分辨率、时间分辨率和样品的光照。近年来，深度学习在许多图像处理和计算机视觉任务中取得了突破性的进展。它在推动超分辨显微镜的性能包络方面也显示出巨大的前景。在这篇简短的综述中，我们综述了利用深度学习提高超分辨显微镜性能的最新进展。我们主要关注深度学习如何促进超分辨率图像的重建。讨论了相关的关键技术挑战。尽管面临挑战，深度学习将在超分辨显微镜的发展中发挥不可或缺的变革性作用。最后，我们展望了深度学习如何影响新一代光学显微镜技术的未来。
<details>	<summary>英文摘要</summary>	Super-resolution microscopy overcomes the diffraction limit of conventional light microscopy in spatial resolution. By providing novel spatial or spatio-temporal information on biological processes at nanometer resolution with molecular specificity, it plays an increasingly important role in life sciences. However, its technical limitations require trade-offs to balance its spatial resolution, temporal resolution, and light exposure of samples. Recently, deep learning has achieved breakthrough performance in many image processing and computer vision tasks. It has also shown great promise in pushing the performance envelope of super-resolution microscopy. In this brief Review, we survey recent advances in using deep learning to enhance performance of super-resolution microscopy. We focus primarily on how deep learning ad-vances reconstruction of super-resolution images. Related key technical challenges are discussed. Despite the challenges, deep learning is set to play an indispensable and transformative role in the development of super-resolution microscopy. We conclude with an outlook on how deep learning could shape the future of this new generation of light microscopy technology. </details>
<details>	<summary>邮件日期</summary>	2021年06月25日</details>

# 168、应用自监督学习实现动态胎儿MRI的超分辨率
- [ ] STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning 
时间：2021年06月23日                         第一作者：Junshen Xu                       [链接](https://arxiv.org/abs/2106.12407).                     
## 摘要：在常规MR扫描时间范围内，胎动是不可预测和快速的。因此，动态胎儿磁共振成像（dynamic fetal MRI）仅限于图像质量和分辨率较差的快速成像技术，其目的是捕捉胎儿运动和胎儿功能的动态变化。动态胎儿磁共振成像的超分辨率仍然是一个挑战，特别是当用于过采样的多方向图像切片堆栈不可用并且需要用于记录胎儿或胎盘动态的高时间分辨率时。此外，胎儿运动使得在有监督学习方法中很难获得高分辨率图像。为了解决这个问题，在这项工作中，我们提出了STRESS（时空分辨率增强与模拟扫描），一个自我监督的超分辨率框架，用于动态胎儿MRI的交错切片采集。我们提出的方法在原始采集的数据上模拟沿高分辨率轴的交错切片采集，以生成低分辨率和高分辨率图像对。然后，利用MR时间序列的时空相关性训练超分辨率网络，提高原始数据的分辨率。对模拟数据和子宫内数据的评价表明，该方法优于其他自监督超分辨率方法，提高了图像质量，有利于后续任务和评价。
<details>	<summary>英文摘要</summary>	Fetal motion is unpredictable and rapid on the scale of conventional MR scan times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and dynamics of fetal function, is limited to fast imaging techniques with compromises in image quality and resolution. Super-resolution for dynamic fetal MRI is still a challenge, especially when multi-oriented stacks of image slices for oversampling are not available and high temporal resolution for recording the dynamics of the fetus or placenta is desired. Further, fetal motion makes it difficult to acquire high-resolution images for supervised learning methods. To address this problem, in this work, we propose STRESS (Spatio-Temporal Resolution Enhancement with Simulated Scans), a self-supervised super-resolution framework for dynamic fetal MRI with interleaved slice acquisitions. Our proposed method simulates an interleaved slice acquisition along the high-resolution axis on the originally acquired data to generate pairs of low- and high-resolution images. Then, it trains a super-resolution network by exploiting both spatial and temporal correlations in the MR time series, which is used to enhance the resolution of the original data. Evaluations on both simulated and in utero data show that our proposed method outperforms other self-supervised super-resolution methods and improves image quality, which is beneficial to other downstream tasks and evaluations. </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 167、基于条件像元合成的卫星图像时空超分辨率分析
- [ ] Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis 
时间：2021年06月22日                         第一作者：Yutong He                       [链接](https://arxiv.org/abs/2106.11485).                     
## 摘要：高分辨率卫星图像已被证明对广泛的任务有用，包括测量全球人口、当地经济生计和生物多样性等。不幸的是，高分辨率图像收集的频率很低，购买成本也很高，因此很难在时间和空间上有效地扩展这些下游任务。我们提出了一种新的条件像素合成模型，该模型利用丰富、低成本、低分辨率的图像在不可用的时间和地点生成精确的高分辨率图像。我们的研究表明，我们的模型在一个关键的下游任务——物体计数——上达到了照片真实的样本质量，并且优于竞争基线，特别是在地面条件变化迅速的地理位置上。
<details>	<summary>英文摘要</summary>	High-resolution satellite imagery has proven useful for a broad range of tasks, including measurement of global human population, local economic livelihoods, and biodiversity, among many others. Unfortunately, high-resolution imagery is both infrequently collected and expensive to purchase, making it hard to efficiently and effectively scale these downstream tasks over both time and space. We propose a new conditional pixel synthesis model that uses abundant, low-cost, low-resolution imagery to generate accurate high-resolution imagery at locations and times in which it is unavailable. We show that our model attains photo-realistic sample quality and outperforms competing baselines on a key downstream task -- object counting -- particularly in geographic locations where conditions on the ground are changing rapidly. </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 166、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年06月22日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
<details>	<summary>注释</summary>	ICME2021, https://ieeexplore.ieee.org/abstract/document/9428136 DOI: 10.1109/ICME51207.2021.9428136 </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 165、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年06月22日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 164、科学数据降阶与可视化的深层超分辨率方法
- [ ] Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization 
时间：2021年05月30日                         第一作者：Skylar W. Wurster                       [链接](https://arxiv.org/abs/2107.00462).                     
## 摘要：提出了一种基于八叉树数据表示的神经网络层次超分辨方法。我们训练了一个神经网络层次结构，每个神经网络能够在两个细节层次之间的每个空间维度上进行2倍的放大，并将这些网络串联使用，以促进大尺度因子超分辨率，随着训练网络的数量进行缩放。我们利用这些网络在一个分层超分辨率算法，提高多分辨率数据到一个统一的高分辨率，而不引入接缝伪影八叉树节点的边界。我们通过将输入数据动态降尺度到基于八叉树的数据结构来表示多分辨率数据，然后压缩以减少额外的存储空间，来评估该算法在数据简化框架中的应用。我们证明了我们的方法避免了多分辨率数据格式中常见的seam伪影，并且说明了在相同的压缩比下，神经网络超分辨率辅助数据约简如何比单独使用压缩器更好地保留全局特征。
<details>	<summary>英文摘要</summary>	We present an approach for hierarchical super resolution (SR) using neural networks on an octree data representation. We train a hierarchy of neural networks, each capable of 2x upscaling in each spatial dimension between two levels of detail, and use these networks in tandem to facilitate large scale factor super resolution, scaling with the number of trained networks. We utilize these networks in a hierarchical super resolution algorithm that upscales multiresolution data to a uniform high resolution without introducing seam artifacts on octree node boundaries. We evaluate application of this algorithm in a data reduction framework by dynamically downscaling input data to an octree-based data structure to represent the multiresolution data before compressing for additional storage reduction. We demonstrate that our approach avoids seam artifacts common to multiresolution data formats, and show how neural network super resolution assisted data reduction can preserve global features better than compressors alone at the same compression ratios. </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 163、用于高保真图像生成的级联扩散模型
- [ ] Cascaded Diffusion Models for High Fidelity Image Generation 
时间：2021年05月30日                         第一作者：Jonathan Ho                       [链接](https://arxiv.org/abs/2106.15282).                     
## 摘要：我们证明了级联扩散模型能够在类条件ImageNet生成挑战上生成高保真图像，而不需要任何辅助图像分类器的帮助来提高样本质量。级联扩散模型包括多个扩散模型的管道，这些扩散模型生成分辨率不断提高的图像，首先是最低分辨率的标准扩散模型，然后是一个或多个超分辨率扩散模型，这些模型依次对图像进行上采样并添加更高分辨率的细节。我们发现级联管道的样本质量主要依赖于条件增强，我们提出的方法是将低分辨率条件输入数据增强到超分辨率模型中。我们的实验表明，条件增强可以防止级联模型采样过程中的复合误差，帮助我们训练级联管道，在64x64、128x128和256x256分辨率下的FID分数分别达到1.48、3.52和4.88，优于BigGAN-deep。
<details>	<summary>英文摘要</summary>	We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 162、在聚焦二十面体网格上应用VertexShuffle实现360度视频超分辨率
- [ ] Applying VertexShuffle Toward 360-Degree Video Super-Resolution on Focused-Icosahedral-Mesh 
时间：2021年06月21日                         第一作者：Na Li                        [链接](https://arxiv.org/abs/2106.11253).                     
## 摘要：随着360度图像/视频、增强现实（AR）和虚拟现实（VR）的出现，人们对球形信号的分析和处理的需求越来越大。然而，大量的研究工作都集中在从球面信号投影出来的平面信号上，导致了像素浪费、失真等问题。球形CNN的最新进展为直接分析球形信号提供了可能。然而，他们关注的是全网格，这使得在实际应用中由于对带宽的要求非常大而难以处理。为了解决360度视频流的带宽浪费问题和节省计算量，我们利用聚焦二十面体网格来表示一个小区域，并构造矩阵将球形内容旋转到聚焦网格区域。我们还提出了一种新的顶点洗牌操作，与UGSCNN中引入的MeshConv转置操作相比，该操作可以显著提高性能和效率。我们进一步将所提出的方法应用于超分辨率模型，这是第一个提出一个直接操作360度数据的球形像素网格表示的球形超分辨率模型。为了评估我们的模型，我们还收集了一组高分辨率的360度视频来生成一个球形图像数据集。我们的实验表明，与使用简单MeshConv转置操作的基线球面超分辨率模型相比，我们提出的球面超分辨率模型在性能和推理时间方面都取得了显著的优势。综上所述，我们的模型在360度输入上取得了很好的超分辨率性能，在对网格上的16x顶点进行超分辨率处理时，平均达到32.79db的PSNR。
<details>	<summary>英文摘要</summary>	With the emerging of 360-degree image/video, augmented reality (AR) and virtual reality (VR), the demand for analysing and processing spherical signals get tremendous increase. However, plenty of effort paid on planar signals that projected from spherical signals, which leading to some problems, e.g. waste of pixels, distortion. Recent advances in spherical CNN have opened up the possibility of directly analysing spherical signals. However, they pay attention to the full mesh which makes it infeasible to deal with situations in real-world application due to the extremely large bandwidth requirement. To address the bandwidth waste problem associated with 360-degree video streaming and save computation, we exploit Focused Icosahedral Mesh to represent a small area and construct matrices to rotate spherical content to the focused mesh area. We also proposed a novel VertexShuffle operation that can significantly improve both the performance and the efficiency compared to the original MeshConv Transpose operation introduced in UGSCNN. We further apply our proposed methods on super resolution model, which is the first to propose a spherical super-resolution model that directly operates on a mesh representation of spherical pixels of 360-degree data. To evaluate our model, we also collect a set of high-resolution 360-degree videos to generate a spherical image dataset. Our experiments indicate that our proposed spherical super-resolution model achieves significant benefits in terms of both performance and inference time compared to the baseline spherical super-resolution model that uses the simple MeshConv Transpose operation. In summary, our model achieves great super-resolution performance on 360-degree inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices on the mesh. </details>
<details>	<summary>注释</summary>	This paper introduce a new mesh representation and a new upsampling method on a mesh </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 161、基于深度度量学习的生成建模对手流形匹配
- [ ] Adversarial Manifold Matching via Deep Metric Learning for Generative Modeling 
时间：2021年06月20日                         第一作者：Mengyu Dai                        [链接](https://arxiv.org/abs/2106.10777).                     
## 摘要：我们提出了一种生成模型的流形匹配方法，它包括一个分布生成器（或数据生成器）和一个度量生成器。在我们的框架中，我们将真实的数据集看作嵌入高维欧氏空间的流形。分布生成器的目标是生成样本，这些样本遵循围绕真实数据流形压缩的某种分布。它是通过使用两组点的几何形状描述符（例如质心和直径）和学习的距离度量来匹配两组点来实现的；度量生成器利用真实数据和生成的样本来学习距离度量，该距离度量接近真实数据流形上的某个固有测地距离。生成的距离度量进一步用于流形匹配。在训练过程中，两个网络同时学习。我们将该方法应用于无监督学习和有监督学习任务中：在无条件图像生成任务中，与已有的生成模型相比，该方法取得了较好的效果；在超分辨率任务中，我们将该框架融入到基于感知的模型中，通过生成具有更自然纹理的样本来提高视觉质量。理论分析和实际数据实验都证明了该框架的可行性和有效性。
<details>	<summary>英文摘要</summary>	We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and $p$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Both theoretical analysis and real data experiments guarantee the feasibility and effectiveness of the proposed framework. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 160、提高超分辨率的一对多方法
- [ ] One-to-many Approach for Improving Super-Resolution 
时间：2021年06月19日                         第一作者：Sieun Park                       [链接](https://arxiv.org/abs/2106.10437).                     
## 摘要：超分辨率（SR）是一个一对多的任务，有多种可能的解决方案。然而，以往的研究并不关注这一特征。对于一对多管道，生成器应该能够生成重建的多个估计，并且不会因为生成相似且同样真实的图像而受到惩罚。为了实现这一点，我们建议在剩余密集块（RRDB）中的每一个残差之后加入加权像素噪声，以使生成器能够生成各种图像。我们修改了严格的内容丢失，只要内容一致，就不会惩罚重建图像中的随机变化。此外，我们观察到，在DIV2K和DIV8K数据集中，有一些没有焦点的区域提供了毫无帮助的指导方针。我们使用[10]的方法过滤训练数据中的模糊区域。最后，我们修改鉴别器以接收低解析度影像作为参考影像与目标影像，以提供更好的回馈给产生器。使用我们提出的方法，我们能够提高ESRGAN在x4知觉SR中的性能，并在x16知觉极限SR中获得最先进的LPIPS分数。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is a one-to-many task with multiple possible solutions. However, previous works were not concerned about this characteristic. For a one-to-many pipeline, the generator should be able to generate multiple estimates of the reconstruction, and not be penalized for generating similar and equally realistic images. To achieve this, we propose adding weighted pixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable the generator to generate various images. We modify the strict content loss to not penalize the stochastic variation in reconstructed images as long as it has consistent content. Additionally, we observe that there are out-of-focus regions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We filter blurry regions in the training data using the method of [10]. Finally, we modify the discriminator to receive the low-resolution image as a reference image along with the target image to provide better feedback to the generator. Using our proposed methods, we were able to improve the performance of ESRGAN in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual extreme SR. </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 159、基于主观评价的真实图像增强方法
- [ ] Debiased Subjective Assessment of Real-World Image Enhancement 
时间：2021年06月18日                         第一作者：Cao Peibei. Wang Zhangyang                       [链接](https://arxiv.org/abs/2106.10080).                     
## 摘要：在实际图像增强中，获取地面真实数据往往是一个挑战（如果不是不可能的话），这就妨碍了采用距离度量进行客观质量评估。因此，人们常常求助于主观质量评估，这是评估图像增强最直接和可靠的方法。传统的主观测试需要人工预选一小部分视觉样本，这可能会产生三种偏差：1）由于所选样本在图像空间的分布极为稀疏，导致采样偏差；2） 由于所选样本的潜在过拟合导致的算法偏差；3） 由于进一步潜在的樱桃采摘试验结果而产生的主观偏见。这最终使得现实世界中的图像增强领域更像是一门艺术，而不是一门科学。在这里，我们采取步骤，通过自动采样一组自适应和多样性的图像进行后续测试，来削弱传统的主观评估。这是通过将样本选择转化为增强子之间的差异和所选输入图像之间的多样性的联合最大化来实现的。仔细的视觉检查得到的增强图像提供了一个增强算法的排名。我们展示了我们的主观评估方法使用三个流行的和实际要求的图像增强任务：去杂波，超分辨率和弱光增强。
<details>	<summary>英文摘要</summary>	In real-world image enhancement, it is often challenging (if not impossible) to acquire ground-truth data, preventing the adoption of distance metrics for objective quality assessment. As a result, one often resorts to subjective quality assessment, the most straightforward and reliable means of evaluating image enhancement. Conventional subjective testing requires manually pre-selecting a small set of visual examples, which may suffer from three sources of biases: 1) sampling bias due to the extremely sparse distribution of the selected samples in the image space; 2) algorithmic bias due to potential overfitting the selected samples; 3) subjective bias due to further potential cherry-picking test results. This eventually makes the field of real-world image enhancement more of an art than a science. Here we take steps towards debiasing conventional subjective assessment by automatically sampling a set of adaptive and diverse images for subsequent testing. This is achieved by casting sample selection into a joint maximization of the discrepancy between the enhancers and the diversity among the selected input images. Careful visual inspection on the resulting enhanced images provides a debiased ranking of the enhancement algorithms. We demonstrate our subjective assessment method using three popular and practically demanding image enhancement tasks: dehazing, super-resolution, and low-light enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月21日</details>

# 158、从模糊中恢复形状：快速移动对象的纹理三维形状和运动
- [ ] Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects 
时间：2021年06月16日                         第一作者：Denys Rozumnyi                       [链接](https://arxiv.org/abs/2106.08762).                     
## 摘要：我们提出了一个新的任务，联合重建三维形状，纹理和运动的物体从一个单一的运动模糊图像。虽然以前的方法只在二维图像域解决去模糊问题，但我们提出的在三维域中对所有对象属性的严格建模能够正确描述任意对象的运动。这将导致更好的图像分解和更清晰的去模糊结果。我们将观察到的运动模糊物体的外观建模为背景和具有恒定平移和旋转的三维物体的组合。我们的方法通过使用合适的正则化器进行可微渲染来最小化重建输入图像的损失。这使得能够以高保真度估计模糊对象的纹理三维网格。在快速运动物体去模糊的几个基准上，我们的方法明显优于其他方法。定性结果表明，重建的三维网格生成了高质量的时间超分辨率和新颖的图像。
<details>	<summary>英文摘要</summary>	We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to significantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high fidelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object. </details>
<details>	<summary>注释</summary>	15 pages, 8 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年06月17日</details>

# 157、受感知启发的压缩视频超分辨率
- [ ] Perceptually-inspired super-resolution of compressed videos 
时间：2021年06月15日                         第一作者：Di Ma                       [链接](https://arxiv.org/abs/2106.08147).                     
## 摘要：空间分辨率自适应是视频压缩中常用的一种提高编码效率的技术。这种方法对输入视频的低分辨率版本进行编码，并在解码过程中重建原始分辨率。为了进一步提高重建质量，最近的工作采用了基于卷积神经网络（CNNs）的先进的超分辨率方法来代替传统的上采样滤波器。这些方法通常被训练来最小化基于像素的损失，例如均方误差（MSE），尽管这种类型的损失度量与主观观点没有很好的相关性。本文提出了一种基于感知启发的超分辨率方法（M-SRGAN），该方法利用一种改进的CNN模型对压缩视频进行空间上采样，该模型是在具有感知损失函数的压缩内容上用生成对抗网络（GAN）训练的。该方法与HEVC-hm16.20相结合，并在JVET通用测试条件（UHD测试序列）上使用随机访问配置进行了评估。结果表明，与原来的hm16.20相比，感知质量有了明显的改善，基于感知质量度量VMAF的平均比特率节省了35.6%（Bj{\o}ntegaard Delta度量）。
<details>	<summary>英文摘要</summary>	Spatial resolution adaptation is a technique which has often been employed in video compression to enhance coding efficiency. This approach encodes a lower resolution version of the input video and reconstructs the original resolution during decoding. Instead of using conventional up-sampling filters, recent work has employed advanced super-resolution methods based on convolutional neural networks (CNNs) to further improve reconstruction quality. These approaches are usually trained to minimise pixel-based losses such as Mean-Squared Error (MSE), despite the fact that this type of loss metric does not correlate well with subjective opinions. In this paper, a perceptually-inspired super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of compressed video using a modified CNN model, which has been trained using a generative adversarial network (GAN) on compressed content with perceptual loss functions. The proposed method was integrated with HEVC HM 16.20, and has been evaluated on the JVET Common Test Conditions (UHD test sequences) using the Random Access configuration. The results show evident perceptual quality improvement over the original HM 16.20, with an average bitrate saving of 35.6% (Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF. </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 156、SinIR：单图像重建的高效通用图像处理
- [ ] SinIR: Efficient General Image Manipulation with Single Image Reconstruction 
时间：2021年06月14日                         第一作者：Jihyeong Yoo                        [链接](https://arxiv.org/abs/2106.07140).                     
## 摘要：我们提出了一个基于SinIR的高效重建框架，它训练在单个自然图像上进行一般的图像处理，包括超分辨率、编辑、协调、绘制到图像、照片真实感风格转换和艺术风格转换。我们通过级联多尺度学习在单个图像上训练我们的模型，每个尺度上的每个网络负责图像重建。与GAN目标相比，该重建目标大大降低了训练的复杂度和运行时间。然而，重建目标也加剧了产出质量。因此，为了解决这个问题，我们进一步利用简单的随机像素洗牌，这也提供了控制操作，受去噪自动编码器的启发。通过定量评估，我们发现SinIR在各种图像处理任务上都有很强的竞争力。此外，使用更简单的训练目标（即重建），SinIR的训练速度是SinGAN（500 X 500图像）的33.5倍，后者可以解决类似的任务。我们的代码可在github.com/YooJiHyeong/SinIR上公开获取。
<details>	<summary>英文摘要</summary>	We propose SinIR, an efficient reconstruction-based framework trained on a single natural image for general image manipulation, including super-resolution, editing, harmonization, paint-to-image, photo-realistic style transfer, and artistic style transfer. We train our model on a single image with cascaded multi-scale learning, where each network at each scale is responsible for image reconstruction. This reconstruction objective greatly reduces the complexity and running time of training, compared to the GAN objective. However, the reconstruction objective also exacerbates the output quality. Therefore, to solve this problem, we further utilize simple random pixel shuffling, which also gives control over manipulation, inspired by the Denoising Autoencoder. With quantitative evaluation, we show that SinIR has competitive performance on various image manipulation tasks. Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our code is publicly available at github.com/YooJiHyeong/SinIR. </details>
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 155、基于群的双向递归小波神经网络在视频超分辨率中的应用
- [ ] Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video Super-Resolution 
时间：2021年06月14日                         第一作者：Young-Ju Choi                       [链接](https://arxiv.org/abs/2106.07190).                     
## 摘要：视频超分辨率（VSR）的目标是从低分辨率（LR）帧中估计出高分辨率（HR）帧。VSR的关键挑战在于有效地利用帧内空间相关性和连续帧间的时间相关性。然而，以往的方法大多对不同类型的空间特征进行统一处理，从分离的模块中提取空间和时间特征。这导致缺乏获得有意义的信息和加强细节。在VSR中，有三种时态建模框架：二维卷积神经网络（CNN）、三维CNN和递归神经网络（RNN）。其中，基于RNN的方法适用于序列数据。因此，利用相邻帧的隐藏状态可以大大提高SR性能。然而，在递归结构中的每一个时间步，基于RNN的以往工作都限制性地利用相邻特征。由于每个时间步的可访问运动范围很窄，因此对于动态或大运动，恢复丢失的细节仍然存在限制。本文提出了一种基于群的双向递归小波神经网络（GBR-WNN）来有效地挖掘VSR的序列数据和时空信息。提出了一种基于组的双向RNN（GBR）时序建模框架，该框架建立在具有图片组（GOP）的结构良好的过程之上。我们提出了一个时间小波注意（TWA）模块，其中注意被用于空间和时间特征。实验结果表明，与现有方法相比，该方法在定量和定性评价方面都取得了较好的效果。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame from a low-resolution (LR) frames. The key challenge for VSR lies in the effective exploitation of spatial correlation in an intra-frame and temporal dependency between consecutive frames. However, most of the previous methods treat different types of the spatial features identically and extract spatial and temporal features from the separated modules. It leads to lack of obtaining meaningful information and enhancing the fine details. In VSR, there are three types of temporal modeling frameworks: 2D convolutional neural networks (CNN), 3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach is suitable for sequential data. Thus the SR performance can be greatly improved by using the hidden states of adjacent frames. However, at each of time step in a recurrent structure, the RNN-based previous works utilize the neighboring features restrictively. Since the range of accessible motion per time step is narrow, there are still limitations to restore the missing details for dynamic or large motion. In this paper, we propose a group-based bi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the sequential data and spatio-temporal information effectively for VSR. The proposed group-based bi-directional RNN (GBR) temporal modeling framework is built on the well-structured process with the group of pictures (GOP). We propose a temporal wavelet attention (TWA) module, in which attention is adopted for both spatial and temporal features. Experimental results demonstrate that the proposed method achieves superior performance compared with state-of-the-art methods in both of quantitative and qualitative evaluations. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 154、单幅图像超分辨率的反馈金字塔注意网络
- [ ] Feedback Pyramid Attention Networks for Single Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06966).                     
## 摘要：近年来，基于卷积神经网络（CNN）的图像超分辨率（SR）方法取得了显著的性能改进。然而，大多数基于CNN的方法主要集中在前馈结构设计上，而忽略了人类视觉系统中普遍存在的反馈机制。在本文中，我们提出了反馈金字塔注意网络（FPAN）来充分利用特征之间的相互依赖性。具体地说，本文提出了一种新的反馈连接结构，利用高层信息增强低层特征表达。在我们的方法中，每一层在第一阶段的输出也被用作下一阶段对应层的输入，以重新更新先前的低层滤波器。此外，我们还引入了金字塔非局部结构来模拟不同尺度下的全局上下文信息，提高了网络的区分性。在各种数据集上的大量实验结果证明了我们的FPAN与最先进的SR方法相比的优越性。
<details>	<summary>英文摘要</summary>	Recently, convolutional neural network (CNN) based image super-resolution (SR) methods have achieved significant performance improvement. However, most CNN-based methods mainly focus on feed-forward architecture design and neglect to explore the feedback mechanism, which usually exists in the human visual system. In this paper, we propose feedback pyramid attention networks (FPAN) to fully exploit the mutual dependencies of features. Specifically, a novel feedback connection structure is developed to enhance low-level feature expression with high-level information. In our method, the output of each layer in the first stage is also used as the input of the corresponding layer in the next state to re-update the previous low-level filters. Moreover, we introduce a pyramid non-local structure to model global contextual information in different scales and improve the discriminative representation of the network. Extensive experimental results on various datasets demonstrate the superiority of our FPAN in comparison with the state-of-the-art SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 153、用于超分辨率轻量化图像的金字塔密集注意网络
- [ ] Pyramidal Dense Attention Networks for Lightweight Image Super-Resolution 
时间：2021年06月13日                         第一作者：Huapeng Wu                       [链接](https://arxiv.org/abs/2106.06996).                     
## 摘要：近年来，深度卷积神经网络方法在图像超分辨率（SR）方面取得了很好的效果，但由于存储开销大，不易应用于嵌入式设备。为了解决这一问题，本文提出了一种用于轻量化图像超分辨率的金字塔密集注意网络（PDAN）。在我们的方法中，所提出的金字塔密集学习可以逐渐增加金字塔密集块内部密集连接层的宽度，从而有效地提取深层特征。同时，引入了组数随卷积层数线性增长的自适应组卷积，以减少参数爆炸。此外，我们还提出了一种新的联合注意方法，以有效地捕捉空间维度和通道维度之间的跨维度交互，从而提供丰富的区分性特征表示。大量的实验结果表明，与目前最先进的轻量级SR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural network methods have achieved an excellent performance in image superresolution (SR), but they can not be easily applied to embedded devices due to large memory cost. To solve this problem, we propose a pyramidal dense attention network (PDAN) for lightweight image super-resolution in this paper. In our method, the proposed pyramidal dense learning can gradually increase the width of the densely connected layer inside a pyramidal dense block to extract deep features efficiently. Meanwhile, the adaptive group convolution that the number of groups grows linearly with dense convolutional layers is introduced to relieve the parameter explosion. Besides, we also present a novel joint attention to capture cross-dimension interaction between the spatial dimensions and channel dimension in an efficient way for providing rich discriminative feature representations. Extensive experimental results show that our method achieves superior performance in comparison with the state-of-the-art lightweight SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 152、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年06月13日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
<details>	<summary>注释</summary>	10 pages, 3 figures Journal-ref: International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2021) </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 151、用于关节MRI重建和超分辨率的任务变压器网络
- [ ] Task Transformer Network for Joint MRI Reconstruction and Super-Resolution 
时间：2021年06月12日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2106.06742).                     
## 摘要：磁共振成像（MRI）的核心问题是加速度和图像质量之间的权衡。图像重建和超分辨率是磁共振成像（MRI）的两项关键技术。当前的方法被设计为分别执行这些任务，而忽略了它们之间的相关性。在这项工作中，我们提出了一个端到端的任务变压器网络（T$^2$Net）用于关节MRI重建和超分辨率，它允许在多个任务之间共享表示和特征传输，从而从高度欠采样和退化的MRI数据中获得更高质量、超分辨率和无运动伪影的图像。我们的框架结合了重建和超分辨率，分为两个子分支，其特征表示为查询和键。具体来说，我们鼓励两个任务之间的联合特征学习，从而传递准确的任务信息。我们首先使用两个独立的CNN分支来提取特定于任务的特征。然后，设计了一个任务转换器模块来嵌入和合成两个任务之间的相关性。实验结果表明，我们的多任务模型在定量和定性上都明显优于先进的序贯方法。
<details>	<summary>英文摘要</summary>	The core problem of Magnetic Resonance Imaging (MRI) is the trade off between acceleration and image quality. Image reconstruction and super-resolution are two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are designed to perform these tasks separately, ignoring the correlations between them. In this work, we propose an end-to-end task transformer network (T$^2$Net) for joint MRI reconstruction and super-resolution, which allows representations and feature transmission to be shared between multiple task to achieve higher-quality, super-resolved and motion-artifacts-free images from highly undersampled and degenerated MRI data. Our framework combines both reconstruction and super-resolution, divided into two sub-branches, whose features are expressed as queries and keys. Specifically, we encourage joint feature learning between the two tasks, thereby transferring accurate task information. We first use two separate CNN branches to extract task-specific features. Then, a task transformer module is designed to embed and synthesize the relevance between the two tasks. Experimental results show that our multi-task model significantly outperforms advanced sequential methods, both quantitatively and qualitatively. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 150、视频超分辨率转换器
- [ ] Video Super-Resolution Transformer 
时间：2021年06月12日                         第一作者：Jiezhang Cao                       [链接](https://arxiv.org/abs/2106.06847).                     
## 摘要：视频超分辨率（VSR）是一个时空序列预测问题，其目的是从相应的低分辨率视频中恢复出高分辨率的视频。近年来，Transformer以其对序列间建模的并行计算能力得到了广泛的应用。因此，应用视觉变换器来求解VSR似乎是很简单的。然而，由于以下两个原因，具有全连接自关注层和令牌前馈层的变压器的典型块设计不适合VSR。首先，完全连通的自注意层由于依赖于线性层来计算注意图而忽略了对数据局部性的利用。其次，令牌前馈层缺乏特征对齐，这对于VSR很重要，因为该层独立地处理每个输入令牌嵌入，它们之间没有任何交互。本文对VSR用变压器进行了首次尝试。具体来说，为了解决第一个问题，我们提出了一个时空卷积自我注意层的理论理解，以利用局部信息。对于第二个问题，我们设计了一个基于双向光流的前馈层来发现不同视频帧之间的相关性并对齐特征。在多个基准数据集上的大量实验证明了该方法的有效性。代码将在https://github.com/caojiezhang/VSR-Transformer.
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 149、用于GAN自动设计的自适应超分辨结构框架
- [ ] A self-adapting super-resolution structures framework for automatic design of GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.06011).                     
## 摘要：随着深度学习的发展，单一的超分辨率图像重建网络模型变得越来越复杂。模型超参数的微小变化对模型性能的影响较大。在现有的工作中，专家们已经逐渐探索出一套基于经验值或进行暴力搜索的最优模型参数。本文介绍了一种新的超分辨率图像重建生成对抗性网络框架，并用贝叶斯优化方法对发生器和鉴别器的超参数进行了优化。发生器采用自校正卷积法，鉴别器采用卷积法。定义了网络层数、神经元数等超参数。该方法采用贝叶斯优化作为GAN的优化策略。不仅可以自动找到最优超参数解，而且可以构造超分辨率图像重建网络，减少人工工作量。实验表明，贝叶斯优化算法比其他两种优化算法能更早地搜索到最优解。
<details>	<summary>英文摘要</summary>	With the development of deep learning, the single super-resolution image reconstruction network models are becoming more and more complex. Small changes in hyperparameters of the models have a greater impact on model performance. In the existing works, experts have gradually explored a set of optimal model parameters based on empirical values or performing brute-force search. In this paper, we introduce a new super-resolution image reconstruction generative adversarial network framework, and a Bayesian optimization method used to optimizing the hyperparameters of the generator and discriminator. The generator is made by self-calibrated convolution, and discriminator is made by convolution lays. We have defined the hyperparameters such as the number of network layers and the number of neurons. Our method adopts Bayesian optimization as a optimization policy of GAN in our model. Not only can find the optimal hyperparameter solution automatically, but also can construct a super-resolution image reconstruction network, reducing the manual workload. Experiments show that Bayesian optimization can search the optimal solution earlier than the other two optimization algorithms. </details>
<details>	<summary>注释</summary>	9 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月14日</details>

# 148、基于自标定卷积GAN的超分辨率图像重建
- [ ] Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN 
时间：2021年06月10日                         第一作者：Yibo Guo                       [链接](https://arxiv.org/abs/2106.05545).                     
## 摘要：随着深度学习在计算机视觉中的有效应用，超分辨率图像重建的研究取得了突破性进展。然而，许多研究指出，神经网络对图像特征提取的不足可能会导致新重建图像的恶化。另一方面，由于过度平滑，生成的图片有时过于人工。为了解决上述问题，我们提出了一种新的自校正卷积生成对抗网络。该发生器由特征提取和图像重建两部分组成。特征提取使用自校正卷积，卷积包含四个部分，每个部分都有特定的功能。它不仅可以扩大感受野的范围，而且可以获得长距离的空间依赖性和通道间依赖性。然后进行图像重建，最后重建超分辨率图像。在SSIM评估方法下，我们对set5、set14和BSD100等不同的数据集进行了深入的实验。实验结果证明了该网络的有效性。
<details>	<summary>英文摘要</summary>	With the effective application of deep learning in computer vision, breakthroughs have been made in the research of super-resolution images reconstruction. However, many researches have pointed out that the insufficiency of the neural network extraction on image features may bring the deteriorating of newly reconstructed image. On the other hand, the generated pictures are sometimes too artificial because of over-smoothing. In order to solve the above problems, we propose a novel self-calibrated convolutional generative adversarial networks. The generator consists of feature extraction and image reconstruction. Feature extraction uses self-calibrated convolutions, which contains four portions, and each portion has specific functions. It can not only expand the range of receptive fields, but also obtain long-range spatial and inter-channel dependencies. Then image reconstruction is performed, and finally a super-resolution image is reconstructed. We have conducted thorough experiments on different datasets including set5, set14 and BSD100 under the SSIM evaluation method. The experimental results prove the effectiveness of the proposed network. </details>
<details>	<summary>注释</summary>	8 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月11日</details>

# 147、基于参考图像超分辨率的变分自动编码器
- [ ] Variational AutoEncoder for Reference based Image Super-Resolution 
时间：2021年06月08日                         第一作者：Zhi-Song Liu                        [链接](https://arxiv.org/abs/2106.04090).                     
## 摘要：本文提出了一种新的基于参考的变分自编码器（RefVAE）图像超分辨率方法。现有的超分辨率方法主要集中在单幅图像的超分辨率上，对于较大的上采样因子（如8$\times$）效果不佳。提出了一种基于参考的图像超分辨率方法，任意一幅图像都可以作为参考进行超分辨率处理。即使使用随机映射或低分辨率图像本身，该方法也能将参考图像中的知识转化为超分辨率图像。根据不同的参考文献，该方法可以从隐藏的超分辨率空间生成不同版本的超分辨率图像。除了使用不同的数据集对PSNR和SSIM进行一些标准评估外，我们还参加了NTIR2021 SR空间挑战赛，并提供了我们方法的随机性评估结果。与其他最先进的方法相比，我们的方法获得了更高的多样性分数。
<details>	<summary>英文摘要</summary>	In this paper, we propose a novel reference based image super-resolution approach via Variational AutoEncoder (RefVAE). Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8$\times$. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space. Besides using different datasets for some standard evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space challenge and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores. </details>
<details>	<summary>注释</summary>	10 pages, 6 figures Journal-ref: 2021 IEEE Conference on Computer Vision and Pattern Recognition Workshop </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 146、NTIRE 2021突发超分辨率挑战：方法和结果
- [ ] NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results 
时间：2021年06月07日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2106.03839).                     
## 摘要：本文回顾了NTIRE2021对突发超分辨率的挑战。给定一个原始噪声脉冲作为输入，挑战中的任务是生成一个分辨率高出4倍的干净RGB图像。挑战包含两个轨道；轨道1评估综合生成的数据，轨道2使用移动摄像机的真实世界爆发。在最后的测试阶段，6个团队使用不同的解决方案提交了结果。性能最好的方法为突发超分辨率任务设置了一个新的最新技术。
<details>	<summary>英文摘要</summary>	This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a RAW noisy burst as input, the task in the challenge was to generate a clean RGB image with 4 times higher resolution. The challenge contained two tracks; Track 1 evaluating on synthetically generated data, and Track 2 using real-world bursts from mobile camera. In the final testing phase, 6 teams submitted results using a diverse set of solutions. The top-performing methods set a new state-of-the-art for the burst super-resolution task. </details>
<details>	<summary>注释</summary>	NTIRE 2021 Burst Super-Resolution challenge report </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 145、基于深度神经网络的图像和视频流增强技术：综述和未来发展方向
- [ ] Deep Neural Network-based Enhancement for Image and Video Streaming Systems: A Survey and Future Directions 
时间：2021年06月07日                         第一作者：Royson Lee                       [链接](https://arxiv.org/abs/2106.03727).                     
## 摘要：支持互联网的智能手机和超宽显示器正在将各种视频应用程序从点播电影和360度视频转变为视频会议和流媒体直播。然而，在不稳定的网络条件下，在具有不同功能的设备上可靠地交付可视内容仍然是一个开放的问题。近年来，在诸如超分辨率和图像增强等任务的深度学习领域取得的进展使得从低质量图像生成高质量图像的性能达到了前所未有的水平，我们称之为神经增强。在本文中，我们调查了最先进的内容交付系统，这些系统将神经增强作为实现快速响应时间和高视觉质量的关键组件。我们首先介绍现有内容交付系统的组件和体系结构，强调它们面临的挑战，并鼓励使用神经增强模型作为对策。然后，我们将讨论这些模型的部署挑战，并分析现有系统及其设计决策，以有效地克服这些技术挑战。此外，我们强调了针对不同用例的跨系统的关键趋势和通用方法。最后，基于深度学习研究的最新见解，我们提出了未来有希望的方向，以进一步提高内容交付系统的体验质量。
<details>	<summary>英文摘要</summary>	Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360{\deg} videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the components and architecture of existing content delivery systems, highlighting their challenges and motivating the use of neural enhancement models as a countermeasure. We then cover the deployment challenges of these models and analyze existing systems and their design decisions in efficiently overcoming these technical challenges. Additionally, we underline the key trends and common approaches across systems that target diverse use-cases. Finally, we present promising future directions based on the latest insights from deep learning research to further boost the quality of experience of content delivery systems. </details>
<details>	<summary>注释</summary>	Accepted for publication at the ACM Computing Surveys (CSUR) journal, 2021. arXiv admin note: text overlap with arXiv:2010.05838 </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 144、深度学习使体积荧光显微镜无参考各向同性超分辨率成为可能
- [ ] Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy 
时间：2021年06月07日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 143、学习超分辨空间的噪声条件流模型
- [ ] Noise Conditional Flow Model for Learning the Super-Resolution Space 
时间：2021年06月06日                         第一作者：Younggeun Kim                       [链接](https://arxiv.org/abs/2106.04428).                     
## 摘要：从根本上说，超分辨率是一个病态的问题，因为低分辨率图像可以从许多高分辨率图像。最近对超分辨率的研究不能产生多样化的超分辨率图像。尽管SRFlow试图通过预测给定低分辨率图像的多幅高分辨率图像来解释超分辨率的不适定性，但仍有提高多样性和视觉质量的空间。本文提出了噪声条件流超分辨率模型NCSR，通过噪声条件层提高图像的视觉质量和多样性。为了了解更多不同的数据分布，我们在训练数据中加入噪声。然而，低质量的图像是由于添加噪声造成的。我们提出了噪声条件层来克服这一现象。噪声条件层使得我们的模型生成的图像更加多样化，视觉质量也更高。此外，我们还证明了该层可以克服数据分布不匹配的问题，这是在规范化流模型时出现的一个问题。基于这些优点，NCSR在多样性和视觉质量方面优于基线，并且比基于GAN的传统模型获得更好的视觉质量。在NTIRE 2021挑战赛中，我们也获得了优异的成绩。
<details>	<summary>英文摘要</summary>	Fundamentally, super-resolution is ill-posed problem because a low-resolution image can be obtained from many high-resolution images. Recent studies for super-resolution cannot create diverse super-resolution images. Although SRFlow tried to account for ill-posed nature of the super-resolution by predicting multiple high-resolution images given a low-resolution image, there is room to improve the diversity and visual quality. In this paper, we propose Noise Conditional flow model for Super-Resolution, NCSR, which increases the visual quality and diversity of images through noise conditional layer. To learn more diverse data distribution, we add noise to training data. However, low-quality images are resulted from adding noise. We propose the noise conditional layer to overcome this phenomenon. The noise conditional layer makes our model generate more diverse images with higher visual quality than other works. Furthermore, we show that this layer can overcome data distribution mismatch, a problem that arises in normalizing flow models. With these benefits, NCSR outperforms baseline in diversity and visual quality and achieves better visual quality than traditional GAN-based models. We also get outperformed scores at NTIRE 2021 challenge. </details>
<details>	<summary>注释</summary>	Final CVPR2021 workshop version </details>
<details>	<summary>邮件日期</summary>	2021年06月09日</details>

# 142、基于参考的超分辨率图像匹配加速与空间自适应
- [ ] MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution 
时间：2021年06月04日                         第一作者：Liying Lu                       [链接](https://arxiv.org/abs/2106.02299).                     
## 摘要：基于参考的图像超分辨率（RefSR）在利用外部参考图像（Ref）恢复高频细节方面取得了成功。在这个任务中，纹理细节从参考图像传输到低分辨率（LR）图像根据其点或面片的对应关系。因此，高质量的匹配是至关重要的。它还要求计算效率高。此外，现有的RefSR方法往往忽略了LR图像和Ref图像在分布上可能存在的较大差异，影响了信息利用的有效性。在本文中，我们提出了RefSR的MASA网络，其中设计了两个新的模块来解决这些问题。该匹配与提取模块通过粗到细的匹配方案显著降低了计算量。空间自适应模块学习LR和Ref图像之间的分布差异，并以空间自适应的方式将Ref特征的分布重新映射到LR特征的分布。该方案使得网络对不同的参考图像具有很强的鲁棒性。大量的定量和定性实验验证了该模型的有效性。
<details>	<summary>英文摘要</summary>	Reference-based image super-resolution (RefSR) has shown promising success in recovering high-frequency details by utilizing an external reference image (Ref). In this task, texture details are transferred from the Ref image to the low-resolution (LR) image according to their point- or patch-wise correspondence. Therefore, high-quality correspondence matching is critical. It is also desired to be computationally efficient. Besides, existing RefSR methods tend to ignore the potential large disparity in distributions between the LR and Ref images, which hurts the effectiveness of the information utilization. In this paper, we propose the MASA network for RefSR, where two novel modules are designed to address these problems. The proposed Match & Extraction Module significantly reduces the computational cost by a coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module learns the difference of distribution between the LR and Ref images, and remaps the distribution of Ref features to that of LR features in a spatially adaptive way. This scheme makes the network robust to handle different reference images. Extensive quantitative and qualitative experiments validate the effectiveness of our proposed model. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 141、SOUP-GAN：基于生成对抗网络的超分辨率MRI
- [ ] SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks 
时间：2021年06月04日                         第一作者：Kuan Zhang                       [链接](https://arxiv.org/abs/2106.02599).                     
## 摘要：在临床和科研应用中，对高分辨率医学图像的需求越来越大。为了获得更好的患者舒适度、更低的检查成本、更低的剂量和更少的运动诱发伪影，图像质量不可避免地要与采集时间进行权衡。对于许多基于图像的任务，通常使用增加垂直平面上的视分辨率来生成多平面重新格式化或三维图像。单图像超分辨率（SR）是一种很有前途的基于无监督学习的HR图像提供技术，可以提高2D图像的分辨率，但是关于3D SR的报道很少。此外，文献中提出的感知损失比使用像素损失函数更好地捕捉文本细节和边缘，通过比较预先训练的二维网络（如VGG）在高维特征空间中的语义距离。然而，目前尚不清楚如何将其推广到三维医学图像中，随之而来的影响仍不清楚。在本文中，我们提出了一个称为SOUP-GAN的框架：使用感知调谐生成对抗网络（GAN）优化的超分辨率，以产生具有抗混叠和去模糊功能的更薄切片（例如，Z平面上的高分辨率）医学图像。通过定性和定量的比较，该方法优于其它常规的分辨率增强方法和以往的SR方法。具体地说，我们检验了该模型在各种SR比值和成像模式下的泛化。通过解决这些限制，我们的模型显示了作为一种新的三维SR插值技术的前景，在临床和研究环境中提供了潜在的应用。
<details>	<summary>英文摘要</summary>	There is a growing demand for high-resolution (HR) medical images in both the clinical and research applications. Image quality is inevitably traded off with the acquisition time for better patient comfort, lower examination costs, dose, and fewer motion-induced artifacts. For many image-based tasks, increasing the apparent resolution in the perpendicular plane to produce multi-planar reformats or 3D images is commonly used. Single image super-resolution (SR) is a promising technique to provide HR images based on unsupervised learning to increase resolution of a 2D image, but there are few reports on 3D SR. Further, perceptual loss is proposed in the literature to better capture the textual details and edges than using pixel-wise loss functions, by comparing the semantic distances in the high-dimensional feature space of a pre-trained 2D network (e.g., VGG). However, it is not clear how one should generalize it to 3D medical images, and the attendant implications are still unclear. In this paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using Perceptual-tuned Generative Adversarial Network (GAN), in order to produce thinner slice (e.g., high resolution in the 'Z' plane) medical images with anti-aliasing and deblurring. The proposed method outperforms other conventional resolution-enhancement methods and previous SR work on medical images upon both qualitative and quantitative comparisons. Specifically, we examine the model in terms of its generalization for various SR ratios and imaging modalities. By addressing those limitations, our model shows promise as a novel 3D SR interpolation technique, providing potential applications in both clinical and research settings. </details>
<details>	<summary>注释</summary>	10 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 140、基于C2匹配的鲁棒参考超分辨方法
- [ ] Robust Reference-based Super-Resolution via C2-Matching 
时间：2021年06月03日                         第一作者：Yuming Jiang                       [链接](https://arxiv.org/abs/2106.01863).                     
## 摘要：基于参考的超分辨率（Ref-SR）通过引入额外的高分辨率（HR）参考图像来增强低分辨率（LR）输入图像是一种很有前途的方法。现有的Ref-SR方法大多依靠隐式对应匹配从参考图像中借用HR纹理来补偿输入图像中的信息丢失。然而，由于输入图像和参考图像之间存在两个间隙：变换间隙（例如缩放和旋转）和分辨率间隙（例如HR和LR），因此执行局部传输是困难的。为了应对这些挑战，我们在这项工作中提出了C2匹配，它产生了显式的鲁棒匹配交叉变换和分辨率。1） 对于变换间隙，我们提出了一种对比对应网络，该网络利用输入图像的增广视图学习变换鲁棒对应。2） 对于分辨率差距，我们采用了师生相关提取的方法，从比较容易的HR-HR匹配中提取知识，指导比较模糊的LR-HR匹配。3） 最后，我们设计了一个动态聚合模块来解决潜在的失调问题。此外，为了真实地评估Ref-SR在现实环境下的性能，我们模拟实际使用场景，提供了web引用SR（WR-SR）数据集。大量的实验表明，我们提出的C2匹配明显优于国家的艺术超过1dB的标准CUFED5基准。值得注意的是，它在WR-SR数据集上具有很强的通用性，并且在大规模变换和旋转变换中具有很强的鲁棒性。
<details>	<summary>英文摘要</summary>	Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g. scale and rotation) and the resolution gap (e.g. HR and LR). To tackle these challenges, we propose C2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C2-Matching significantly outperforms state of the arts by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation transformations. </details>
<details>	<summary>注释</summary>	To appear in CVPR2021. The source code is available at https://github.com/yumingj/C2-Matching </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 139、互增强立体图像超分辨率和视差估计的反馈网络
- [ ] Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation 
时间：2021年06月02日                         第一作者：Qinyan Dai                       [链接](https://arxiv.org/abs/2106.00985).                     
## 摘要：在立体背景下，图像超分辨率（SR）和视差估计问题是相互关联的，每个问题的结果可以帮助解决另一个问题。有效地利用不同视图之间的对应关系有利于SR性能的提高，而细节更丰富的高分辨率特征有利于对应关系的估计。基于这一动机，我们提出了一种立体图像超分辨率和视差估计反馈网络（ssrdefnet），它在统一的框架下同时处理立体图像的超分辨率和视差估计，并将它们相互作用以进一步提高它们的性能。具体而言，SSRDE-FNet由两个用于左视图和右视图的双重递归子网络组成。除了在低分辨率（LR）空间中利用交叉视图信息外，还利用SR过程产生的HR表示来进行更高精度的HR视差估计，通过该方法可以聚集HR特征以产生更精细的SR结果。然后，提出的HR视差信息反馈（HRDIF）机制将HR视差所携带的信息反馈给前一层，进一步细化SR图像重建。大量实验证明了SSRDE-FNet的有效性和先进性。
<details>	<summary>英文摘要</summary>	Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 138、有效感知图像超分辨率的傅里叶空间损失
- [ ] Fourier Space Losses for Efficient Perceptual Image Super-Resolution 
时间：2021年06月01日                         第一作者：Dario Fuoli                       [链接](https://arxiv.org/abs/2106.00783).                     
## 摘要：许多超分辨率（SR）模型只针对高性能进行优化，因此由于模型复杂度大而缺乏效率。由于大模型在实际应用中往往不实用，我们研究并提出了新的损失函数，以使SR从更有效的模型中获得更高的感知质量。一个给定的低复杂度发电网络的代表功率只有通过对最优参数集的有力指导才能得到充分利用。我们证明了仅应用我们提出的损失函数就可以提高最近引入的高效发电机结构的性能。特别地，我们使用傅立叶空间监督损失来改进从地面真实图像中恢复丢失的高频（HF）内容，并设计了一种直接在傅立叶域工作的鉴别器结构，以更好地匹配目标HF分布。我们发现，我们的损失直接强调在傅立叶空间的频率显着提高了感知图像质量，同时保持了较高的恢复质量相比，以前提出的损失函数的任务。由于两种表示在训练期间提供互补信息，因此通过利用空间域和频域损失的组合来进一步改进性能。除此之外，经过训练的生成器与最新的感知SR方法RankSRGAN和SRFlow相比，分别快2.4倍和48倍。
<details>	<summary>英文摘要</summary>	Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4x and 48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 137、CTSpine1K：一个用于ct脊柱分割的大规模数据集
- [ ] CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography 
时间：2021年05月31日                         第一作者：Yang Deng                       [链接](https://arxiv.org/abs/2105.14711).                     
## 摘要：脊柱相关疾病发病率高，造成巨大的社会成本负担。脊柱成像是无创可视化和评估脊柱病理学的重要工具。CT图像中椎体的分割是脊柱疾病临床诊断和手术计划的定量医学图像分析的基础。目前公开获得的脊柱注释数据集规模较小。由于缺乏大规模的带注释脊柱图像数据集，目前主流的基于深度学习的数据驱动分割方法受到很大的限制。在本文中，我们介绍了一个大型脊柱CT数据集CTSpine1K，该数据集来自多个来源，用于椎体分割，包含1005个CT体积，超过11100个标记的椎体属于不同的脊柱状况。基于此数据集，我们进行了多个脊椎分割实验来建立第一个基准。我们相信，这个大规模的数据集将有助于进一步研究许多与脊柱相关的图像分析任务，包括但不限于椎骨分割、标记、双平面x线片的三维脊柱重建、图像超分辨率和增强。
<details>	<summary>英文摘要</summary>	Spine-related diseases have high morbidity and cause a huge burden of social cost. Spine imaging is an essential tool for noninvasively visualizing and assessing spinal pathology. Segmenting vertebrae in computed tomography (CT) images is the basis of quantitative medical image analysis for clinical diagnosis and surgery planning of spine diseases. Current publicly available annotated datasets on spinal vertebrae are small in size. Due to the lack of a large-scale annotated spine image dataset, the mainstream deep learning-based segmentation methods, which are data-driven, are heavily restricted. In this paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated from multiple sources for vertebra segmentation, which contains 1,005 CT volumes with over 11,100 labeled vertebrae belonging to different spinal conditions. Based on this dataset, we conduct several spinal vertebrae segmentation experiments to set the first benchmark. We believe that this large-scale dataset will facilitate further research in many spine-related image analysis tasks, including but not limited to vertebrae segmentation, labeling, 3D spine reconstruction from biplanar radiographs, image super-resolution, and enhancement. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 136、SNIPS：随机求解含噪反问题
- [ ] SNIPS: Solving Noisy Inverse Problems Stochastically 
时间：2021年05月31日                         第一作者：Bahjat Kawar                       [链接](https://arxiv.org/abs/2105.14951).                     
## 摘要：在这项工作中，我们介绍了一种新的随机算法称为SNIPS，它从任何线性逆问题的后验分布中提取样本，假设观测值被加性高斯白噪声污染。我们的解决方案结合了朗之万动力学和牛顿法的思想，并利用预先训练的最小均方误差（MMSE）高斯去噪器。所提出的方法依赖于后验分数函数的复杂推导，该后验分数函数包括退化算子的奇异值分解（SVD），以便获得所需采样的易于处理的迭代算法。由于其随机性，该算法可以为同一噪声观测产生多个高感知质量的样本。我们证明了所提出的模式的能力，图像去模糊，超分辨率，压缩传感。我们发现，所产生的样本是尖锐的，详细的，并与给定的测量一致，其多样性暴露了所解决的反问题固有的不确定性。
<details>	<summary>英文摘要</summary>	In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 135、光谱之外：通过再合成检测假货
- [ ] Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis 
时间：2021年05月29日                         第一作者：Yang He                        [链接](https://arxiv.org/abs/2105.14376).                     
## 摘要：在过去的几年里，深度生成模型的快速发展已经导致了高度真实的媒体，称为深度赝品，通常从真实的眼睛到人类的眼睛是无法区分的。这些进步使得评估视觉数据的真实性变得越来越困难，并对视觉内容的可信性构成了错误信息的威胁。尽管最近的工作已经显示出这种深度假货的强检测精度，但成功在很大程度上依赖于识别生成图像中的频率伪影，这将不会产生一种可持续的检测方法，因为生成模型继续发展并缩小与真实图像的差距。为了克服这一问题，我们提出了一种新的伪检测方法，该方法通过对检测图像的重新合成和提取视觉线索进行检测。再合成过程是灵活的，允许我们纳入一系列的视觉任务-我们采用超分辨率，去噪和彩色作为再合成。我们在CelebA HQ、FFHQ和LSUN数据集上的多个生成器的各种检测场景中展示了改进的有效性、交叉通用性和抗干扰的鲁棒性。源代码位于https://github.com/SSAW14/BeyondtheSpectrum.
<details>	<summary>英文摘要</summary>	The rapid advances in deep generative models over the past years have led to highly {realistic media, known as deepfakes,} that are commonly indistinguishable from real to human eyes. These advances make assessing the authenticity of visual data increasingly difficult and pose a misinformation threat to the trustworthiness of visual content in general. Although recent work has shown strong detection accuracy of such deepfakes, the success largely relies on identifying frequency artifacts in the generated images, which will not yield a sustainable detection approach as generative models continue evolving and closing the gap to real images. In order to overcome this issue, we propose a novel fake detection that is designed to re-synthesize testing images and extract visual cues for detection. The re-synthesis procedure is flexible, allowing us to incorporate a series of visual tasks - we adopt super-resolution, denoising and colorization as the re-synthesis. We demonstrate the improved effectiveness, cross-GAN generalization, and robustness against perturbations of our approach in a variety of detection scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN datasets. Source code is available at https://github.com/SSAW14/BeyondtheSpectrum. </details>
<details>	<summary>注释</summary>	To appear in IJCAI2021. Source code at https://github.com/SSAW14/BeyondtheSpectrum </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 134、动态时空学习满足静态图像理解的盲运动去模糊超分辨率方法
- [ ] Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding 
时间：2021年05月27日                         第一作者：Wenjia Niu                       [链接](https://arxiv.org/abs/2105.13077).                     
## 摘要：单帧超分辨率（SR）和多帧超分辨率（SR）是超分辨率低分辨率图像的两种方法。单个图像SR通常独立地处理每个图像，但忽略连续帧中隐含的时间信息。多帧SR能够通过捕获运动信息来建模时间相关性。然而，它依赖于在现实世界中并不总是可用的相邻帧。同时，轻微的相机抖动容易导致长距离拍摄低分辨率图像的运动模糊。针对这些问题，提出了一种盲运动去模糊超分辨率网络BMDSRNet，用于从单个静态运动模糊图像中学习动态时空信息。运动模糊图像是相机曝光过程中随时间的积累，而BMDSRNet学习反向过程，并基于精心设计的重建损失函数，使用三个流来学习双向时空信息，以恢复干净的高分辨率图像。大量实验表明，该算法的性能优于现有的图像去模糊和随机共振处理方法。
<details>	<summary>英文摘要</summary>	Single-image super-resolution (SR) and multi-frame SR are two ways to super resolve low-resolution images. Single-Image SR generally handles each image independently, but ignores the temporal information implied in continuing frames. Multi-frame SR is able to model the temporal dependency via capturing motion information. However, it relies on neighbouring frames which are not always available in the real world. Meanwhile, slight camera shake easily causes heavy motion blur on long-distance-shot low-resolution images. To address these problems, a Blind Motion Deblurring Super-Reslution Networks, BMDSRNet, is proposed to learn dynamic spatio-temporal information from single static motion-blurred images. Motion-blurred images are the accumulation over time during the exposure of cameras, while the proposed BMDSRNet learns the reverse process and uses three-streams to learn Bidirectional spatio-temporal information based on well designed reconstruction loss functions to recover clean high-resolution images. Extensive experiments demonstrate that the proposed BMDSRNet outperforms recent state-of-the-art methods, and has the ability to simultaneously deal with image deblurring and SR. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 133、CogView：通过变压器控制文本到图像的生成
- [ ] CogView: Mastering Text-to-Image Generation via Transformers 
时间：2021年05月26日                         第一作者：Ming Ding                       [链接](https://arxiv.org/abs/2105.13290).                     
## 摘要：文本到图像的生成在一般领域一直是一个开放的问题，它需要生成模型和跨模态理解。我们提出了CogView，一个带有VQ-VAE标记器的40亿参数变压器来解决这个问题。我们还展示了各种下游任务的微调策略，例如风格学习、超分辨率、文本图像排序和时装设计，以及稳定预训练的方法，例如消除NaN损失。CogView（zero-shot）在COCO上实现了一种新的最先进的FID，优于以前基于GAN的模型和最近的类似工作DALL-E。
<details>	<summary>英文摘要</summary>	Text-to-Image generation in the general domain has long been an open problem, which requires both generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 132、低分辨率信息也很重要：学习多分辨率表示以重新识别人
- [ ] Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification 
时间：2021年05月26日                         第一作者：Guoqing Zhang                       [链接](https://arxiv.org/abs/2105.12684).                     
## 摘要：人员再识别（re-ID）是视频监控和取证领域的一项重要任务，其目的是对非重叠摄像机拍摄到的人员图像进行匹配。在不受约束的场景中，人物图像通常会遇到分辨率不匹配的问题，即\emph{Cross resolution person Re ID}。为了克服这一问题，现有的大多数方法都是通过超分辨率（SR）将低分辨率（LR）图像恢复到高分辨率（HR）。然而，它们只关注于HR特征的提取，而忽略了原始LR图像的有效信息。在这项工作中，我们探讨了分辨率对特征提取的影响，并提出了一种新的交叉分辨率人物识别方法，称为多分辨率表示法。该方法由分辨率重建网络（RRN）和双特征融合网络（DFFN）组成。RRN使用一个输入图像构造一个HR版本和一个LR版本，其中包含一个编码器和两个解码器，而DFFN采用双分支结构从多分辨率图像生成人物表示。在五个基准上的综合实验验证了所提出的MRJL方法相对于现有方法的优越性。
<details>	<summary>英文摘要</summary>	As a prevailing task in video surveillance and forensics field, person re-identification (re-ID) aims to match person images captured from non-overlapped cameras. In unconstrained scenarios, person images often suffer from the resolution mismatch problem, i.e., \emph{Cross-Resolution Person Re-ID}. To overcome this problem, most existing methods restore low resolution (LR) images to high resolution (HR) by super-resolution (SR). However, they only focus on the HR feature extraction and ignore the valid information from original LR images. In this work, we explore the influence of resolutions on feature extraction and develop a novel method for cross-resolution person re-ID called \emph{\textbf{M}ulti-Resolution \textbf{R}epresentations \textbf{J}oint \textbf{L}earning} (\textbf{MRJL}). Our method consists of a Resolution Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN uses an input image to construct a HR version and a LR version with an encoder and two decoders, while the DFFN adopts a dual-branch structure to generate person representations from multi-resolution images. Comprehensive experiments on five benchmarks verify the superiority of the proposed MRJL over the relevent state-of-the-art methods. </details>
<details>	<summary>注释</summary>	accepted by IJCAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 131、多时相图像超分辨率的排列不变性和不确定性
- [ ] Permutation invariance and uncertainty in multitemporal image super-resolution 
时间：2021年05月26日                         第一作者：Diego Valsesia                       [链接](https://arxiv.org/abs/2105.12409).                     
## 摘要：最近的进展表明，从低分辨率图像的多时相采集开始，深层神经网络在超分辨率遥感图像方面是多么的有效。然而，现有的模型忽略了时间排列的问题，即输入图像的时间顺序不携带任何与超分辨率任务相关的信息，并且导致这种模型对于可用于训练的、通常稀缺的地面真实数据是低效的。因此，模型不应该学习依赖于时间顺序的特征提取器。在本文中，我们展示了如何建立一个对时间排列完全不变的模型来显著地提高性能和数据效率。此外，我们还研究了如何量化超分辨率图像的不确定性，以便最终用户了解产品的局部质量。我们展示了不确定性如何与序列中的时间变化相关，以及量化它如何进一步提高模型性能。在Proba-V挑战数据集上的实验表明，在不需要自我感知的情况下，比现有技术有了显著的改进，并且提高了数据效率，仅用25%的训练数据就达到了挑战胜利者的表现。
<details>	<summary>英文摘要</summary>	Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of low-resolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data. </details>
<details>	<summary>邮件日期</summary>	2021年05月27日</details>

# 130、基于对比自蒸馏的紧凑型单幅图像超分辨率研究
- [ ] Towards Compact Single Image Super-Resolution via Contrastive Self-distillation 
时间：2021年05月25日                         第一作者：Yanbo Wang                       [链接](https://arxiv.org/abs/2105.11683).                     
## 摘要：卷积神经网络（CNNs）是一种非常成功的超分辨率（SR）网络，但其结构复杂，内存开销大，计算开销大，极大地限制了其在资源有限的设备上的实际应用。在本文中，我们提出了一个新的对比自蒸馏（CSD）框架来同时压缩和加速各种现成的SR模型。特别地，信道分裂超分辨率网络可以首先从目标教师网络构造为紧凑的学生网络。然后，我们提出了一种新的对比度损失方法，通过显式知识转移来提高SR图像和PSNR/SSIM的质量。大量实验表明，该方案有效地压缩和加速了EDSR、RCAN和CARN等标准SR模型。代码位于https://github.com/Booooooooooo/CSD.
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) are highly successful for super-resolution (SR) but often require sophisticated architectures with heavy memory cost and computational overhead, significantly restricts their practical deployments on resource-limited devices. In this paper, we proposed a novel contrastive self-distillation (CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models. In particular, a channel-splitting super-resolution network can first be constructed from a target teacher network as a compact student network. Then, we propose a novel contrastive loss to improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer. Extensive experiments demonstrate that the proposed CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN. Code is available at https://github.com/Booooooooooo/CSD. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI-21 </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 129、高频感知图像增强
- [ ] High-Frequency aware Perceptual Image Enhancement 
时间：2021年05月25日                         第一作者：Hyungmin Roh                        [链接](https://arxiv.org/abs/2105.11711).                     
## 摘要：本文介绍了一种适用于多尺度分析的深度神经网络，并提出了一种有效的模型不可知方法，帮助该网络从高频域提取信息，重建出更清晰的图像。该模型可应用于多尺度图像增强问题，包括去噪、去模糊和单图像超分辨率。在SIDD、Flickr2K、DIV2K和REDS数据集上的实验表明，我们的方法在每个任务上都达到了最先进的性能。此外，我们还证明了我们的模型可以克服现有面向PSNR的方法中普遍存在的过度平滑问题，并通过对抗性训练生成更自然的高分辨率图像。
<details>	<summary>英文摘要</summary>	In this paper, we introduce a novel deep neural network suitable for multi-scale analysis and propose efficient model-agnostic methods that help the network extract information from high-frequency domains to reconstruct clearer images. Our model can be applied to multi-scale image enhancement problems including denoising, deblurring and single image super-resolution. Experiments on SIDD, Flickr2K, DIV2K, and REDS datasets show that our method achieves state-of-the-art performance on each task. Furthermore, we show that our model can overcome the over-smoothing problem commonly observed in existing PSNR-oriented methods and generate more natural high-resolution images by applying adversarial training. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 128、基于快速RCNN检测模型的无人机RGB图像玉米密度估计：空间分辨率的影响
- [ ] Estimates of maize plant density from UAV RGB images using Faster-RCNN detection model: impact of the spatial resolution 
时间：2021年05月25日                         第一作者：Kaaviya Velumani                       [链接](https://arxiv.org/abs/2105.11857).                     
## 摘要：在给定的环境条件和管理措施下，早期植株密度是决定基因型命运的重要性状。使用从无人机上拍摄的RGB图像可以取代传统的现场视觉计数，从而提高吞吐量、精确度和工厂定位。然而，需要高分辨率（HR）图像来检测早期存在的小植物。研究了图像地面采样距离（GSD）对快速RCNN在3-5叶期玉米植株检测性能的影响。在HR（GSD=0.3cm）收集的6个对比部位的数据用于模型训练。另外两个具有高分辨率和低分辨率（GSD=0.6cm）图像的位置用于模型评估。结果表明，当本地HR图像同时用于训练和验证时，更快的RCNN可以获得非常好的植物检测和计数性能（rRMSE=0.08）。同样地，当模型在通过对本地训练HR图像下采样获得的合成低分辨率（LR）图像上训练并应用于合成LR验证图像时，观察到良好的性能（rRMSE=0.11）。相反，当模型在给定的空间分辨率上训练并应用到另一个空间分辨率上时，会获得较差的性能。混合HR和LR图像的训练允许在本地HR（rRMSE=0.06）和合成LR（rRMSE=0.10）图像上获得非常好的性能。然而，在本地LR图像（rRMSE=0.48）上仍然观察到非常低的性能，主要是由于本地LR图像的质量差。最后，提出了一种基于生成对抗网络（generativediscountarial network，GAN）的改进的超分辨率方法，该方法引入了来自本地HR图像的额外纹理信息，并应用于本地LR验证图像。结果表明，与双三次上采样方法相比，该方法有一些显著的改进（rRMSE=0.22）。
<details>	<summary>英文摘要</summary>	Early-stage plant density is an essential trait that determines the fate of a genotype under given environmental conditions and management practices. The use of RGB images taken from UAVs may replace traditional visual counting in fields with improved throughput, accuracy and access to plant localization. However, high-resolution (HR) images are required to detect small plants present at early stages. This study explores the impact of image ground sampling distance (GSD) on the performances of maize plant detection at 3-5 leaves stage using Faster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used for model training. Two additional sites with images acquired both at high and low (GSD=0.6cm) resolution were used for model evaluation. Results show that Faster-RCNN achieved very good plant detection and counting (rRMSE=0.08) performances when native HR images are used both for training and validation. Similarly, good performances were observed (rRMSE=0.11) when the model is trained over synthetic low-resolution (LR) images obtained by down-sampling the native training HR images, and applied to the synthetic LR validation images. Conversely, poor performances are obtained when the model is trained on a given spatial resolution and applied to another spatial resolution. Training on a mix of HR and LR images allows to get very good performances on the native HR (rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low performances are still observed over the native LR images (rRMSE=0.48), mainly due to the poor quality of the native LR images. Finally, an advanced super-resolution method based on GAN (generative adversarial network) that introduces additional textural information derived from the native HR images was applied to the native LR validation images. Results show some significant improvement (rRMSE=0.22) compared to bicubic up-sampling approach. </details>
<details>	<summary>注释</summary>	16 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 127、野外非配对深度增强与超分辨率研究进展
- [ ] Towards Unpaired Depth Enhancement and Super-Resolution in the Wild 
时间：2021年05月25日                         第一作者：Aleks                       [链接](https://arxiv.org/abs/2105.12038).                     
## 摘要：商品传感器获取的深度图通常质量和分辨率较低；这些地图需要增强才能在许多应用中使用。最先进的深度图超分辨率数据驱动方法依赖于同一场景的低分辨率和高分辨率深度图的注册对。获取真实世界的成对数据需要专门的设置。另一种替代方法是，通过子采样、添加噪声和其他人工退化方法从高分辨率地图生成低分辨率地图，这种方法不能完全捕捉现实世界中低分辨率图像的特征。因此，在这种人工配对数据上训练的有监督学习方法在现实世界的低分辨率输入上可能表现不好。我们考虑一种基于未配对数据学习的深度图增强方法。虽然已经提出了许多非配对图像到图像的转换技术，但大多数技术并不直接适用于深度图。我们提出了一种同时进行深度增强和超分辨率的非配对学习方法，该方法基于可学习退化模型和表面法线估计作为特征来生成更精确的深度图。我们证明了我们的方法优于现有的非配对方法，并在我们开发的新的非配对学习基准上与配对方法相当。
<details>	<summary>英文摘要</summary>	Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth map enhancement based on learning from unpaired data. While many techniques for unpaired image-to-image translation have been proposed, most are not directly applicable to depth maps. We propose an unpaired learning method for simultaneous depth enhancement and super-resolution, which is based on a learnable degradation model and surface normal estimates as features to produce more accurate depth maps. We demonstrate that our method outperforms existing unpaired methods and performs on par with paired methods on a new benchmark for unpaired learning that we developed. </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 126、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月23日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 125、MIASSR：一种医学图像任意尺度超分辨率处理方法
- [ ] MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution 
时间：2021年05月22日                         第一作者：Jin Zhu                       [链接](https://arxiv.org/abs/2105.10738).                     
## 摘要：单幅图像超分辨率（Single-image super-resolution，SISR）的目标是从一幅低分辨率图像中获得高分辨率的输出。目前，基于深度学习的SISR方法在医学图像处理中得到了广泛的讨论，因为它们可以在不需要额外扫描的情况下获得高质量、高空间分辨率的图像。然而，大多数现有的方法是针对特定规模的SR任务而设计的，无法在放大规模上推广。本文提出了一种医学图像任意尺度超分辨率（MIASSR）方法，该方法将元学习与生成对抗网络（GANs）相结合，实现了（1，4）中任意尺度的医学图像超分辨率。与单模态磁共振（MR）脑图像（OASIS-brains）和多模态MR脑图像（BraTS）上最先进的SISR算法相比，MIASSR以最小的模型尺寸获得了相当的保真度性能和最佳的感知质量。我们还利用转移学习使MIASSR能够处理新医学模式的SR任务，如心脏MR图像（ACDC）和胸部CT图像（COVID-CT）。我们工作的源代码也是公开的。因此，MIASSR有可能成为临床图像分析任务（如重建、图像质量增强和分割）中一个新的基础前/后处理步骤。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalise over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation. </details>
<details>	<summary>邮件日期</summary>	2021年05月25日</details>

# 124、用卷积鉴别器组合变压器发生器
- [ ] Combining Transformer Generators with Convolutional Discriminators 
时间：2021年05月21日                         第一作者：Ricard Durall                       [链接](https://arxiv.org/abs/2105.10189).                     
## 摘要：变压器模型最近引起了计算机视觉研究人员的极大兴趣，并已成功地应用于一些传统的卷积神经网络解决的问题。同时，在过去的几年中，使用生成性对抗网络（GANs）的图像合成有了很大的改进。最近提出的TransGAN是第一个只使用基于变压器的结构的GAN，并且与卷积GAN相比取得了竞争性的结果。然而，由于变压器是数据饥渴的架构，TransGAN需要数据扩充、训练期间的辅助超分辨率任务以及引导自我注意机制之前的掩蔽。在本文中，我们研究了基于变压器的发生器和卷积鉴别器的组合，成功地消除了上述设计选择的需要。我们通过对著名的CNN鉴别器进行基准测试来评估我们的方法，烧蚀基于变压器的发电机的大小，并表明将两种结构元素结合到一个混合模型中可以得到更好的结果。此外，我们研究了生成图像的频谱特性，并观察到我们的模型保留了基于注意的生成器的优点。
<details>	<summary>英文摘要</summary>	Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator. </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 123、线性组合像素自适应回归网络用于单图像超分辨率及更高分辨率
- [ ] LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond 
时间：2021年05月21日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2105.10422).                     
## 摘要：单图像超分辨率（SISR）是将低分辨率（LR）图像上采样为高分辨率（HR）图像的一个基本问题。在过去的几年里，在深度学习方法的推动下取得了令人瞩目的进步。然而，现有方法面临的一个关键挑战是如何找到一个深度模型复杂性和由此产生的SISR质量的最佳点。本文提出了一种线性组合像素自适应回归网络（LAPAR），将LR到HR的直接映射学习转化为多个预定义滤波器基字典上的线性系数回归任务。这样的参数表示使得我们的模型具有高度的轻量级和易于优化，同时在SISR基准上获得最先进的结果。此外，基于同样的思想，将LAPAR算法扩展到图像去噪和JPEG图像去块等恢复任务中，同样获得了很好的性能。代码可在https://github.com/dvlab-research/Simple-SR.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) deals with a fundamental problem of upsampling a low-resolution (LR) image to its high-resolution (HR) version. Last few years have witnessed impressive progress propelled by deep learning methods. However, one critical challenge faced by existing methods is to strike a sweet spot of deep model complexity and resulting SISR quality. This paper addresses this pain point by proposing a linearly-assembled pixel-adaptive regression network (LAPAR), which casts the direct LR to HR mapping learning into a linear coefficient regression task over a dictionary of multiple predefined filter bases. Such a parametric representation renders our model highly lightweight and easy to optimize while achieving state-of-the-art results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended to tackle other restoration tasks, e.g., image denoising and JPEG image deblocking, and again, yields strong performance. The code is available at https://github.com/dvlab-research/Simple-SR. </details>
<details>	<summary>注释</summary>	NeurIPS2020 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 122、用于图像去模糊和超分辨率的特征空间图卷积网络
- [ ] Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution 
时间：2021年05月21日                         第一作者：Boyan Xu                        [链接](https://arxiv.org/abs/2105.10465).                     
## 摘要：图卷积网络（GCNs）在处理非欧几里德结构的数据方面取得了巨大的成功。它们的成功直接归功于将图形结构有效地与社交媒体和知识数据库中的数据相匹配。对于图像处理应用，图形结构和gcn的使用还没有得到充分的探讨。本文提出了一种新的加图卷积的编解码网络，通过将特征映射转换为预生成图的顶点来综合构造图结构数据。通过这样做，我们莫名其妙地应用图拉普拉斯正则化的特征地图，使他们更结构化。实验结果表明，该算法能显著提高图像恢复的性能，包括去模糊和超分辨率。我们相信它为更多应用中基于GCN的方法提供了机会。
<details>	<summary>英文摘要</summary>	Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications. </details>
<details>	<summary>注释</summary>	Accepted by IJCNN 2021 (Oral) Journal-ref: International Joint Conference on Neural Networks (IJCNN) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月24日</details>

# 121、快速图像超分辨率的内容自适应表示学习
- [ ] Content-adaptive Representation Learning for Fast Image Super-resolution 
时间：2021年05月20日                         第一作者：Yukai Shi                       [链接](https://arxiv.org/abs/2105.09645).                     
## 摘要：深度卷积网络在图像恢复和增强中受到了广泛的关注。通常，通过构造更多的卷积块来提高恢复质量。然而，这些方法大多学习一个特定的模型来处理所有的图像，忽略了难度的多样性。换言之，图像中的高频区域在压缩过程中往往丢失更多信息，而低频区域往往丢失更少信息。本文针对图像检索中的效率问题，提出了一种基于分片滚动网络（PRN）的内容自适应恢复算法。与现有的忽略难度多样性的研究不同，本文采用不同阶段的神经网络进行图像恢复。此外，我们还提出了一种滚动策略，使得每一阶段的参数都更加灵活。大量的实验表明，我们的模型不仅显示了显著的加速，而且保持了最先进的性能。
<details>	<summary>英文摘要</summary>	Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance. </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 120、基于锚网的移动图像超分辨率平面网
- [ ] Anchor-based Plain Net for Mobile Image Super-Resolution 
时间：2021年05月20日                         第一作者：Zongcai Du                       [链接](https://arxiv.org/abs/2105.09750).                     
## 摘要：随着实际应用的快速发展，对图像超分辨率（SR）的精度和效率提出了更高的要求。现有的方法虽然取得了显著的成功，但大多需要大量的计算资源和大量的RAM，不能很好地应用于移动设备。本文旨在设计高效的8位量化体系结构，并将其部署到移动设备上。首先，我们通过分解轻量级SR架构来进行关于元节点延迟的实验，这决定了我们可以利用的可移植操作。在此基础上，深入探讨了什么样的体系结构有利于8位量化，并提出了基于锚的平面网（ABPN）。最后，我们采用量化感知训练策略来进一步提升绩效。该模型在满足实际需要的同时，PSNR性能比8位量化FSRCNN提高了近2dB。代码可在https://github.com/NJU- Jet/SR\移动\量化。
<details>	<summary>英文摘要</summary>	Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization. </details>
<details>	<summary>注释</summary>	accepted by CVPR2021 MAI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 119、XCycles反投影声学超分辨率
- [ ] XCycles Backprojection Acoustic Super-Resolution 
时间：2021年05月19日                         第一作者：Feras Almasri                       [链接](https://arxiv.org/abs/2105.09128).                     
## 摘要：利用深度神经网络（DNNs）进行可见光图像超分辨率（SR）的研究已引起计算机视觉界的广泛关注，并取得了令人瞩目的成果。声成像传感器等非可见光传感器的发展引起了人们的广泛关注，因为它们可以使人们直观地看到可见光谱以外的声波强度。然而，由于获取声学数据的局限性，需要新的方法来提高声学图像的分辨率。目前，还没有为SR问题设计的声学成像数据集。本文提出了一种新的用于声像超分辨率问题的反投影模型体系结构，并与声地图成像VUB-ULB数据集（AMIVU）相结合。数据集提供了不同分辨率的大型模拟和真实捕获图像。与前馈模型方法相比，本文提出的XCycles反投影模型（XCBP）充分利用了每个周期的迭代校正过程，在低分辨率和高分辨率空间中重建编码特征的残差校正。在数据集上对所提出的方法进行了评估，结果表明，与经典的插值算子和最新的前馈模型相比，该方法具有更高的性能。这也大大减少了数据采集过程中产生的次采样误差。
<details>	<summary>英文摘要</summary>	The computer vision community has paid much attention to the development of visible image super-resolution (SR) using deep neural networks (DNNs) and has achieved impressive results. The advancement of non-visible light sensors, such as acoustic imaging sensors, has attracted much attention, as they allow people to visualize the intensity of sound waves beyond the visible spectrum. However, because of the limitations imposed on acquiring acoustic data, new methods for improving the resolution of the acoustic images are necessary. At this time, there is no acoustic imaging dataset designed for the SR problem. This work proposed a novel backprojection model architecture for the acoustic image super-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset (AMIVU). The dataset provides large simulated and real captured images at different resolutions. The proposed XCycles BackProjection model (XCBP), in contrast to the feedforward model approach, fully uses the iterative correction procedure in each cycle to reconstruct the residual error correction for the encoded features in both low- and high-resolution space. The proposed approach was evaluated on the dataset and showed high outperformance compared to the classical interpolation operators and to the recent feedforward state-of-the-art models. It also contributed to a drastically reduced sub-sampling error produced during the data acquisition. </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 118、基于多级积分网络的多对比度MRI超分辨率成像
- [ ] Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network 
时间：2021年05月19日                         第一作者：Chun-Mei Feng                       [链接](https://arxiv.org/abs/2105.08949).                     
## 摘要：超分辨率（SR）对提高磁共振成像（MRI）的图像质量起着至关重要的作用。磁共振成像产生多对比度图像，可以提供软组织的清晰显示。然而，目前的超分辨率方法仅采用单一对比度，或者采用简单的多对比度融合机制，忽略了不同对比度之间丰富的关系，这对提高磁共振成像的分辨率是有价值的，它明确地建模了不同阶段的多对比度图像之间的依赖关系以指导图像SR。特别地，我们的MINet首先从多个卷积阶段学习不同对比度图像的分层特征表示。随后，我们引入了一个多级集成模块来挖掘多对比度图像表示之间的综合关系。具体来说，该模块将每个表示与所有其他特征相匹配，这些特征根据其相似性进行集成，以获得丰富的表示。在fastMRI和真实临床数据集上的大量实验表明：1）我们的MINet在各种指标方面优于最先进的多对比度SR方法；2）我们的多阶段集成模块能够挖掘不同阶段多对比度特征之间的复杂交互作用，从而提高目标图像质量。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) plays a crucial role in improving the image quality of magnetic resonance imaging (MRI). MRI produces multi-contrast images and can provide a clear display of soft tissues. However, current super-resolution methods only employ a single contrast, or use a simple multi-contrast fusion mechanism, ignoring the rich relations among different contrasts, which are valuable for improving SR. In this work, we propose a multi-stage integration network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the dependencies between multi-contrast images at different stages to guide image SR. In particular, our MINet first learns a hierarchical feature representation from multiple convolutional stages for each of different-contrast image. Subsequently, we introduce a multi-stage integration module to mine the comprehensive relations between the representations of the multi-contrast images. Specifically, the module matches each representation with all other features, which are integrated in terms of their similarities to obtain an enriched representation. Extensive experiments on fastMRI and real-world clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art multi-contrast SR methods in terms of various metrics and 2) our multi-stage integration module is able to excavate complex interactions among multi-contrast features at different stages, leading to improved target-image quality. </details>
<details>	<summary>注释</summary>	10 pages, 3 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 117、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年05月19日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 116、道路网序列中小目标检测的改进方法
- [ ] Improved detection of small objects in road network sequences 
时间：2021年05月18日                         第一作者：Iv\'an Garc\'ia                       [链接](https://arxiv.org/abs/2105.08416).                     
## 摘要：当前道路网络中现有的大量IP摄像机为利用捕获的数据、分析视频和检测任何重大事件提供了机会。为此，有必要检测移动的车辆，直到几年前，这项任务一直是使用经典的人工视觉技术来完成的。如今，深度学习网络已经取得了显著的进步。尽管如此，目标检测仍然被认为是计算机视觉领域的主要开放性问题之一。目前的情况是不断发展的，新的模型和技术正在出现，试图改善这一领域。特别地，在检测小目标方面出现了新的问题和缺点，这些小目标主要对应于道路场景中出现的车辆。所有这一切意味着，新的解决方案，试图提高低检出率的小元素是必不可少的。在众多新兴的研究领域中，本文主要研究小目标的检测。特别是，我们的建议旨在从视频监控摄像头捕获的图像中检测车辆。本文提出了一种基于卷积神经网络（CNN）检测的超分辨过程检测小尺度目标的新方法。将神经网络与提高图像分辨率的过程相结合，提高目标检测性能。通过对一组包含不同尺度元素的交通图像的测试，验证了该方法的有效性，表明该方法在多种情况下都取得了良好的效果。
<details>	<summary>英文摘要</summary>	The vast number of existing IP cameras in current road networks is an opportunity to take advantage of the captured data and analyze the video and detect any significant events. For this purpose, it is necessary to detect moving vehicles, a task that was carried out using classical artificial vision techniques until a few years ago. Nowadays, significant improvements have been obtained by deep learning networks. Still, object detection is considered one of the leading open issues within computer vision. The current scenario is constantly evolving, and new models and techniques are appearing trying to improve this field. In particular, new problems and drawbacks appear regarding detecting small objects, which correspond mainly to the vehicles that appear in the road scenes. All this means that new solutions that try to improve the low detection rate of small elements are essential. Among the different emerging research lines, this work focuses on the detection of small objects. In particular, our proposal aims to vehicle detection from images captured by video surveillance cameras. In this work, we propose a new procedure for detecting small-scale objects by applying super-resolution processes based on detections performed by convolutional neural networks \emph{(CNN)}. The neural network is integrated with processes that are in charge of increasing the resolution of the images to improve the object detection performance. This solution has been tested for a set of traffic images containing elements of different scales to test the efficiency according to the detections obtained by the model, thus demonstrating that our proposal achieves good results in a wide range of situations. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 115、超网络在固定触发器计数下的过参数化实现了快速的神经图像增强
- [ ] Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast Neural Image Enhancement 
时间：2021年05月18日                         第一作者：Lorenz K. Muller                       [链接](https://arxiv.org/abs/2105.08470).                     
## 摘要：深度卷积神经网络可以增强小型移动相机传感器拍摄的图像，擅长去噪、去噪和超分辨率等任务。然而，在移动设备上的实际应用中，这些网络往往需要太多的触发器，减少了卷积层的触发器，也减少了其参数计数。鉴于最近的研究发现，过度参数化的神经网络通常是泛化效果最好的网络，这是有问题的。在本文中，我们建议使用超网络来打破固定比例的触发器参数的标准卷积。这使我们能够在苏黎世原始到DSLR（ZRR）数据集上以低于10倍的浮点计数超过SSIM和MS-SSIM中以前最先进的体系结构。在ZRR上，我们进一步观察到在大图像极限下，在固定的FLOP计数下与“双下降”行为一致的泛化曲线。最后，我们证明了同样的技术可以应用于一个现有的网络（VDN），以降低其计算成本，同时保持对智能手机图像去噪数据集（SIDD）的保真度。附录中给出了关键功能的代码。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks can enhance images taken with small mobile camera sensors and excel at tasks like demoisaicing, denoising and super-resolution. However, for practical use on mobile devices these networks often require too many FLOPs and reducing the FLOPs of a convolution layer, also reduces its parameter count. This is problematic in view of the recent finding that heavily over-parameterized neural networks are often the ones that generalize best. In this paper we propose to use HyperNetworks to break the fixed ratio of FLOPs to parameters of standard convolutions. This allows us to exceed previous state-of-the-art architectures in SSIM and MS-SSIM on the Zurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we further observe generalization curves consistent with 'double-descent' behavior at fixed FLOP-count, in the large image limit. Finally we demonstrate the same technique can be applied to an existing network (VDN) to reduce its computational cost while maintaining fidelity on the Smartphone Image Denoising Dataset (SIDD). Code for key functions is given in the appendix. </details>
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 114、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年05月18日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
<details>	<summary>邮件日期</summary>	2021年05月19日</details>

# 113、深度学习智能手机上的实时视频超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Video Super-Resolution on Smartphones with Deep Learning, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.08826).                     
## 摘要：随着视频通信和流媒体服务的兴起，视频超分辨率已经成为移动领域的一个重要问题。虽然已经有许多解决方案被提出来完成这项任务，但是大多数解决方案的计算成本太高，无法在硬件资源有限的便携式设备上运行。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的视频超分辨率解决方案，可以在移动gpu上实现实时性能。向参与者提供了REDS数据集，并训练他们的模型进行有效的4X视频放大。所有模型的运行时间都在OPPO Find X2智能手机上进行了评估，Snapdragon 865 SoC能够加速Adreno GPU上的浮点网络。所提出的解决方案完全兼容任何移动GPU，可以将视频放大到高清分辨率，分辨率高达80帧/秒，同时显示高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial text overlap with arXiv:2105.07825. substantial text overlap with arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.08630 </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 112、移动NPU上的实时量化图像超分辨率，移动AI 2021挑战：报告
- [ ] Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI 2021 Challenge: Report 
时间：2021年05月17日                         第一作者：Andrey Ignatov                       [链接](https://arxiv.org/abs/2105.07825).                     
## 摘要：图像超分辨率是计算机视觉领域的一个热点问题，在移动设备中有着重要的应用。虽然已经有许多解决方案被提出用于这项任务，但它们通常甚至没有针对常见的智能手机AI硬件进行优化，更不用说通常只支持INT8推理的更受约束的智能电视平台了。为了解决这个问题，我们引入了第一个移动人工智能挑战，目标是开发一个基于端到端深度学习的图像超分辨率解决方案，可以在移动或边缘NPU上展示实时性能。为此，参与者被提供了DIV2K数据集和经过训练的量化模型来进行有效的3X图像放大。所有模型的运行时间均在Synaptics VS680智能家居板上进行评估，该智能家居板配有一个能够加速量化神经网络的专用NPU。所提出的解决方案与所有主要的移动人工智能加速器完全兼容，能够在40-60毫秒的时间内重建全高清图像，同时获得高保真效果。本文详细描述了在挑战中开发的所有模型。
<details>	<summary>英文摘要</summary>	Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. </details>
<details>	<summary>注释</summary>	Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/ </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 111、用于高保真材料标签传输的无监督超分辨率卫星图像
- [ ] Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer 
时间：2021年05月16日                         第一作者：Arthita Ghosh                       [链接](https://arxiv.org/abs/2105.07322).                     
## 摘要：遥感图像中的城市物质识别是一个高度相关但极具挑战性的问题，尤其是在低分辨率卫星图像上，由于人类注释的获取非常困难。为此，我们提出了一种基于对抗学习的无监督领域自适应方法。我们的目标是从少量的高分辨率数据（源域）中获取信息，并利用这些数据对低分辨率图像（目标域）进行超分辨率处理。这可能有助于语义以及材料标签从带丰富注释的源到目标域的转移。
<details>	<summary>英文摘要</summary>	Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain. </details>
<details>	<summary>注释</summary>	Published in the proceedings of the 2019 IEEE International Geoscience and Remote Sensing Symposium Journal-ref: IGARSS (2019), 5144-5147 DOI: 10.1109/IGARSS.2019.8900639 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 110、图像超分辨率质量评估：结构保真度与统计自然度
- [ ] Image Super-Resolution Quality Assessment: Structural Fidelity Versus Statistical Naturalness 
时间：2021年05月15日                         第一作者：Wei Zhou                       [链接](https://arxiv.org/abs/2105.07139).                     
## 摘要：单图像超分辨率（SISR）算法用低分辨率（LR）重建高分辨率（HR）图像。发展图像质量评价（IQA）方法不仅可以评价和比较SISR算法，而且可以指导其未来的发展。在本文中，我们评估了在结构保真度与统计自然度的二维（2D）空间中SISR生成图像的质量。这使得我们能够观察不同SISR算法在2D空间中的行为。具体地说，SISR方法传统上是为了获得高的结构保真度而设计的，但往往牺牲了统计的自然性，而最近基于生成性对抗网络（GAN）的算法倾向于创建更自然的结果，但在结构保真度上损失很大。此外，这样的2D评估可以容易地与标量质量预测融合。有趣的是，我们发现一个简单的线性组合一个简单的局部结构保真度和一个全球性的统计自然度措施产生令人惊讶的准确预测SISR图像质量时，测试使用公共科目评分SISR图像数据集。建议的SFSN模型的代码可在\url上公开获取{https://github.com/weizhou-geek/SFSN}.
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) algorithms reconstruct high-resolution (HR) images with their low-resolution (LR) counterparts. It is desirable to develop image quality assessment (IQA) methods that can not only evaluate and compare SISR algorithms, but also guide their future development. In this paper, we assess the quality of SISR generated images in a two-dimensional (2D) space of structural fidelity versus statistical naturalness. This allows us to observe the behaviors of different SISR algorithms as a tradeoff in the 2D space. Specifically, SISR methods are traditionally designed to achieve high structural fidelity but often sacrifice statistical naturalness, while recent generative adversarial network (GAN) based algorithms tend to create more natural-looking results but lose significantly on structural fidelity. Furthermore, such a 2D evaluation can be easily fused to a scalar quality prediction. Interestingly, we find that a simple linear combination of a straightforward local structural fidelity and a global statistical naturalness measures produce surprisingly accurate predictions of SISR image quality when tested using public subject-rated SISR image datasets. Code of the proposed SFSN model is publicly available at \url{https://github.com/weizhou-geek/SFSN}. </details>
<details>	<summary>注释</summary>	Accepted by QoMEX 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 109、低分辨率扫描病理图像的多尺度超分辨率生成
- [ ] Multi-scale super-resolution generation of low-resolution scanned pathological images 
时间：2021年05月15日                         第一作者：Yanhua Gao (1)                       [链接](https://arxiv.org/abs/2105.07200).                     
## 摘要：数字化病理切片易于存储和管理，便于浏览和传输。然而，由于数字化过程中的高分辨率扫描（例如40倍放大率），每张幻灯片图像的文件大小超过1G字节，最终导致巨大的存储容量和非常缓慢的网络传输。我们设计了一种低分辨率（5X）幻灯片扫描策略，并提出了一种超分辨率方法来恢复诊断时的图像细节。该方法基于多尺度生成对抗网络，依次生成10X、20X和40X三幅高分辨率图像。在三种图像分辨率下比较了生成图像和真实图像的感知损失、发生器损失，并用鉴别器来评价最高分辨率生成图像和真实图像的差异。一个由10万张人体组织病理图像组成的数据集被用来训练和测试网络。生成的图像具有较高的峰值信噪比（PSNR）和结构相似性指数（SSIM）。10X-40X图像的峰值信噪比分别为24.16、22.27和20.44，SSIM分别为0.845、0.680和0.512，优于DBPN、ESPCN、RDN、EDSR和MDSR等超分辨率网络。此外，视觉检测表明，我们的网络生成的高分辨率图像具有足够的诊断细节，色彩再现性好，接近真实图像，而其他五个网络严重模糊，局部变形或遗漏重要细节。而且，基于生成的图像和真实图像的病理诊断没有显著差异。提出的多尺度网络能够生成高分辨率的病理图像，为数字病理学提供了一种低成本的存储（约15MB/5X图像），更快的图像共享方法。
<details>	<summary>英文摘要</summary>	Digital pathology slide is easy to store and manage, convenient to browse and transmit. However, because of the high-resolution scan for example 40 times magnification(40X) during the digitization, the file size of each whole slide image exceeds 1Gigabyte, which eventually leads to huge storage capacity and very slow network transmission. We design a strategy to scan slides with low resolution (5X) and a super-resolution method is proposed to restore the image details when in diagnosis. The method is based on a multi-scale generative adversarial network, which sequentially generate three high-resolution images such as 10X, 20X and 40X. The perceived loss, generator loss of the generated images and real images are compared on three image resolutions, and a discriminator is used to evaluate the difference of highest-resolution generated image and real image. A dataset consisting of 100,000 pathological images from 10 types of human tissues is performed for training and testing the network. The generated images have high peak-signal-to-noise-ratio (PSNR) and structural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16, 22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than other super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR. Moreover, visual inspections show that the generated high-resolution images by our network have enough details for diagnosis, good color reproduction and close to real images, while other five networks are severely blurred, local deformation or miss important details. Moreover, no significant differences can be found on pathological diagnosis based on the generated and real images. The proposed multi-scale network can generate good high-resolution pathological images, and will provide a low-cost storage (about 15MB/image on 5X), faster image sharing method for digital pathology. </details>
<details>	<summary>注释</summary>	27 pages,12 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 108、盲超分辨的端到端交替优化
- [ ] End-to-end Alternating Optimization for Blind Super Resolution 
时间：2021年05月14日                         第一作者：Zhengxiong Luo                       [链接](https://arxiv.org/abs/2105.06878).                     
## 摘要：以前的方法将盲超分辨率（SR）问题分解为两个连续的步骤：\textit{i}从给定的低分辨率（LR）图像中估计模糊核和\textit{ii}基于估计核恢复SR图像。这两个步骤的解决方案涉及两个独立训练的模型，这两个模型可能不太兼容。第一步的微小估计误差可能导致第二步的性能严重下降。另一方面，第一步只能利用有限的LR图像信息，难以预测出高精度的模糊核。针对这两个问题，本文采用交替优化算法，在单一模型下估计模糊核，恢复SR图像，而不是单独考虑这两个步骤。具体来说，我们设计了两个卷积神经模块，即\textit{Restorer}和\textit{Estimator}\textit{Restorer}基于预测核对SR图像进行恢复，而\textit{Estimator}借助恢复的SR图像对模糊核进行估计。我们反复交替这两个模块，展开这个过程，形成一个端到端的培训网络。通过这种方式，\textit{Estimator}利用来自LR和SR图像的信息，这使得模糊核的估计更容易。更重要的是，用估计核来训练恢复核，而不是用真核来训练恢复核，因此恢复核对估计核的估计误差有更大的容忍度。在合成数据集和真实图像上的大量实验表明，我们的模型在很大程度上优于最先进的方法，并以更高的速度产生更直观的结果。源代码位于\url{https://github.com/greatlog/DAN.git}.
<details>	<summary>英文摘要</summary>	Previous methods decompose the blind super-resolution (SR) problem into two sequential steps: \textit{i}) estimating the blur kernel from given low-resolution (LR) image and \textit{ii}) restoring the SR image based on the estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. A small estimation error of the first step could cause a severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from the LR image, which makes it difficult to predict a highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate the blur kernel and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the predicted kernel, and \textit{Estimator} estimates the blur kernel with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of the blur kernel easier. More importantly, \textit{Restorer} is trained with the kernel estimated by \textit{Estimator}, instead of the ground-truth kernel, thus \textit{Restorer} could be more tolerant to the estimation error of \textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at a much higher speed. The source code is available at \url{https://github.com/greatlog/DAN.git}. </details>
<details>	<summary>注释</summary>	Submited to PAMI. arXiv admin note: substantial text overlap with arXiv:2010.02631 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 107、合成X射线图像超分辨率的频域约束
- [ ] A Frequency Domain Constraint for Synthetic X-ray Image Super Resolution 
时间：2021年05月14日                         第一作者：Qing Ma                       [链接](https://arxiv.org/abs/2105.06887).                     
## 摘要：合成的X射线图像有助于图像引导系统和虚拟现实仿真。然而，由于CT扫描分辨率有限、计算资源要求高或算法复杂等原因，很难实时生成高质量的任意视点合成X射线图像。我们的目标是通过对低分辨率图像进行上采样，实时生成高分辨率的合成X射线图像。基于参考的超分辨率（RefSR）近年来得到了广泛的研究，并被证明比传统的单图像超分辨率（SISR）更强大。RefSR可以利用参考图像产生精细的细节，但仍不可避免地产生一些伪影和噪声。本文提出了一种基于频域的纹理变换器超分辨率（TTSR-FD）。我们引入频域损耗作为约束条件，进一步提高了RefSR结果的质量，细节清晰，无明显伪影。这使得实时合成X射线图像引导程序VR仿真系统成为可能。据我们所知，这是第一篇利用频域作为超分辨率领域损失函数的论文。我们在合成X射线图像数据集上评估了TTSR-FD，并取得了最新的结果。
<details>	<summary>英文摘要</summary>	Synthetic X-ray images can be helpful for image guiding systems and VR simulations. However, it is difficult to produce high-quality arbitrary view synthetic X-ray images in real-time due to limited CT scanning resolution, high computation resource demand or algorithm complexity. Our goal is to generate high-resolution synthetic X-ray images in real-time by upsampling low-resolution im-ages. Reference-based Super Resolution (RefSR) has been well studied in recent years and has been proven to be more powerful than traditional Single Image Su-per-Resolution (SISR). RefSR can produce fine details by utilizing the reference image but it still inevitably generates some artifacts and noise. In this paper, we propose texture transformer super-resolution with frequency domain (TTSR-FD). We introduce frequency domain loss as a constraint to further improve the quality of the RefSR results with fine details and without obvious artifacts. This makes a real-time synthetic X-ray image-guided procedure VR simulation system possible. To the best of our knowledge, this is the first paper utilizing the frequency domain as part of the loss functions in the field of super-resolution. We evaluated TTSR-FD on our synthetic X-ray image dataset and achieved state-of-the-art results. </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 106、用于视频超分辨率的流引导可变形对准网络FDAN
- [ ] FDAN: Flow-guided Deformable Alignment Network for Video Super-Resolution 
时间：2021年05月12日                         第一作者：Jiayi Lin                       [链接](https://arxiv.org/abs/2105.05640).                     
## 摘要：大多数视频超分辨率（VSR）方法通过对齐相邻帧并挖掘这些帧上的信息来增强视频参考帧。近年来，可变形对齐技术以其能自适应地将相邻帧和参考帧对齐的优异性能，引起了VSR界的广泛关注。然而，我们在实验中发现，可变形对齐方法仍然会因局部损失驱动的偏移预测而受到快速运动的影响，并且缺乏明确的运动约束。因此，我们提出了一个基于匹配的流量估计（MFE）模块来进行全局语义特征匹配，并将光流估计为每个位置的粗偏移量。提出了一种流引导可变形模块（FDM），将光流集成到可变形卷积中。FDM首先利用光流对相邻帧进行扭曲。然后，利用扭曲的相邻帧和参考帧对每个粗偏移量预测一组精细偏移量。一般来说，我们提出了一种端到端的深度网络，称为流引导可变形对齐网络（FDAN），它在两个基准数据集上达到了最先进的性能，同时在计算和内存消耗方面仍然具有竞争力。
<details>	<summary>英文摘要</summary>	Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption. </details>
<details>	<summary>邮件日期</summary>	2021年05月13日</details>

# 105、增强的深金字塔模糊图像恢复网络EDPN
- [ ] EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration 
时间：2021年05月11日                         第一作者：Ruikang Xu                       [链接](https://arxiv.org/abs/2105.04872).                     
## 摘要：随着深度神经网络的发展，图像去模糊技术有了很大的发展。然而，在实际应用中，模糊图像常常会受到诸如缩小尺度和压缩之类的额外退化。为了解决这些问题，我们提出了一种改进的深度金字塔网络（EDPN），通过充分利用退化图像的自相似性和跨尺度相似性，从多个退化图像中恢复模糊图像。，金字塔递进转移（PPT）模块和金字塔自我注意（PSA）模块作为该网络的主要组成部分。PPT模块以多幅复制的模糊图像作为输入，以渐进的方式传输同一退化图像的自相似性和跨尺度相似性信息。然后，PSA模块利用自我和空间注意机制将上述特征融合起来进行后续恢复。实验结果表明，该方法在模糊图像超分辨率和模糊图像去块方面的性能明显优于现有的方法。在NTIRE 2021图像去模糊挑战中，EDPN在第1轨（低分辨率）中获得最佳PSNR/SSIM/LPIPS分数，在第2轨（JPEG伪影）中获得最佳SSIM/LPIPS分数。
<details>	<summary>英文摘要</summary>	Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image.Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts). </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Ruikang and Zeyu contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年05月12日</details>

# 104、超低分辨率印刷文本图像的端到端光学字符识别方法
- [ ] An end-to-end Optical Character Recognition approach for ultra-low-resolution printed text images 
时间：2021年05月10日                         第一作者：Julian D. Gilbey                       [链接](https://arxiv.org/abs/2105.04515).                     
## 摘要：一些历史和较新的打印文档以非常低的分辨率（如60 dpi）进行扫描或存储。尽管这样的扫描相对容易让人阅读，但对于光学字符识别（OCR）系统来说仍然是一个巨大的挑战。目前的技术水平是使用超分辨率重建原始高分辨率图像的近似值，并将其输入标准OCR系统。我们新的端到端方法绕过了超分辨率步骤，产生了更好的OCR结果。这种方法的灵感来自我们对人类视觉系统的理解，并建立在已建立的用于执行OCR的神经网络的基础上。我们的实验表明，在60 dpi扫描的英文文本图像上进行OCR是可能的，这是一个显著低于最新技术的分辨率，并且在一组1000页左右的60 dpi文本中，在广泛的字体范围内，我们实现了99.7%的平均字符级准确率（CLA）和98.9%的单词级准确率（WLA）。对于75dpi图像，同一文本样本的平均CLA为99.9%，平均WLA为99.4%。我们公开我们的代码和数据（包括一组低分辨率图像及其基本事实），作为该领域未来工作的基准。
<details>	<summary>英文摘要</summary>	Some historical and more recent printed documents have been scanned or stored at very low resolutions, such as 60 dpi. Though such scans are relatively easy for humans to read, they still present significant challenges for optical character recognition (OCR) systems. The current state-of-the art is to use super-resolution to reconstruct an approximation of the original high-resolution image and to feed this into a standard OCR system. Our novel end-to-end method bypasses the super-resolution step and produces better OCR results. This approach is inspired from our understanding of the human visual system, and builds on established neural networks for performing OCR. Our experiments have shown that it is possible to perform OCR on 60 dpi scanned images of English text, which is a significantly lower resolution than the state-of-the-art, and we achieved a mean character level accuracy (CLA) of 99.7% and word level accuracy (WLA) of 98.9% across a set of about 1000 pages of 60 dpi text in a wide range of fonts. For 75 dpi images, the mean CLA was 99.9% and the mean WLA was 99.4% on the same sample of texts. We make our code and data (including a set of low-resolution images with their ground truths) publicly available as a benchmark for future work in this field. </details>
<details>	<summary>注释</summary>	8 pages MSC-class: 68T10 ACM-class: I.7.5 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 103、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年05月10日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 102、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年05月10日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 101、基于分层可微神经搜索的轻量级图像超分辨率
- [ ] Lightweight Image Super-Resolution with Hierarchical and Differentiable Neural Architecture Search 
时间：2021年05月09日                         第一作者：Han Huang                       [链接](https://arxiv.org/abs/2105.03939).                     
## 摘要：单图像超分辨率（SISR）任务在深度神经网络中取得了显著的性能。然而，在基于CNN的SISR任务处理方法中，大量的参数需要大量的计算。虽然最近提出了几种有效的SISR模型，但大多数都是手工制作的，因此缺乏灵活性。在这项工作中，我们提出了一种新的可微神经结构搜索（NAS）方法，在细胞和网络两个层面上搜索轻量级SISR模型。具体来说，单元级搜索空间是基于信息提取机制设计的，关注轻量级操作的组合，旨在构建更轻量级、更精确的SR结构。网络级搜索空间的设计考虑了小区间的特征联系，目的是找出哪些信息流对小区最有利，从而提高小区的性能。与现有的基于强化学习（RL）或进化算法（EA）的SISR任务NAS方法不同，我们的搜索管道是完全可微的，并且轻量级SISR模型可以在单个GPU上同时在单元级和网络级进行有效搜索。实验表明，我们的方法在PSNR、SSIM和模型复杂度方面可以在基准数据集上达到最先进的性能，而$\乘以2$的任务只需要68G多次加法，而$\乘以4$的任务只需要18G多次加法。代码将在\url处提供{https://github.com/DawnHH/DLSR-PyTorch}.
<details>	<summary>英文摘要</summary>	Single Image Super-Resolution (SISR) tasks have achieved significant performance with deep neural networks. However, the large number of parameters in CNN-based methods for SISR tasks require heavy computations. Although several efficient SISR models have been recently proposed, most are handcrafted and thus lack flexibility. In this work, we propose a novel differentiable Neural Architecture Search (NAS) approach on both the cell-level and network-level to search for lightweight SISR models. Specifically, the cell-level search space is designed based on an information distillation mechanism, focusing on the combinations of lightweight operations and aiming to build a more lightweight and accurate SR structure. The network-level search space is designed to consider the feature connections among the cells and aims to find which information flow benefits the cell most to boost the performance. Unlike the existing Reinforcement Learning (RL) or Evolutionary Algorithm (EA) based NAS methods for SISR tasks, our search pipeline is fully differentiable, and the lightweight SISR models can be efficiently searched on both the cell-level and network-level jointly on a single GPU. Experiments show that our methods can achieve state-of-the-art performance on the benchmark datasets in terms of PSNR, SSIM, and model complexity with merely 68G Multi-Adds for $\times 2$ and 18G Multi-Adds for $\times 4$ SR tasks. Code will be available at \url{https://github.com/DawnHH/DLSR-PyTorch}. </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 100、基于偏移图像先验的无监督遥感超分辨率方法
- [ ] Unsupervised Remote Sensing Super-Resolution via Migration Image Prior 
时间：2021年05月08日                         第一作者：Jiaming Wang                       [链接](https://arxiv.org/abs/2105.03579).                     
## 摘要：近年来，高时间分辨率卫星在各种实际应用中引起了广泛的关注。然而，由于带宽和硬件成本的限制，这类卫星的空间分辨率相当低，很大程度上限制了它们在需要空间明确信息的情况下的潜力。为了提高图像分辨率，人们提出了许多基于训练低-高分辨率对的方法来解决超分辨率问题。然而，尽管它们取得了成功，但在具有高时间分辨率的卫星中通常很难获得低/高空间分辨率对，这使得在SR中使用这种方法不切实际。在本文中，我们提出了一种新的无监督学习框架，称为MIP，它可以在没有低/高分辨率图像对的情况下完成SR任务。首先，将随机噪声映射输入到设计的生成对抗网络（GAN）中进行重构。然后，该方法将参考图像转换为潜空间作为偏移图像的先验信息。最后，利用隐式方法对输入噪声进行更新，进一步传递参考图像的纹理和结构信息。在Draper数据集上的大量实验结果表明，MIP在数量和质量上都比最先进的方法有显著的改进。提议的MIP是开源的http://github.com/jiaming-wang/MIP.
<details>	<summary>英文摘要</summary>	Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. Despite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called "MIP", which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at http://github.com/jiaming-wang/MIP. </details>
<details>	<summary>注释</summary>	6 pages, 4 figures. IEEE International Conference on Multimedia and Expo (ICME) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 99、用消噪器中的先验隐式法求解线性反问题
- [ ] Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser 
时间：2021年05月07日                         第一作者：Zahra Kadkhodaie                        [链接](https://arxiv.org/abs/2007.13640).                     
<details>	<summary>注释</summary>	19 pages, 12 figures. Changes: more detailed description of relationships to previous literature, including empirical comparisons for super-resolution, debarring, and compressive sensing </details>
<details>	<summary>邮件日期</summary>	2021年05月10日</details>

# 98、基于局部推理和全局参数联合估计的实时视频超分辨率
- [ ] Real-Time Video Super-Resolution by Joint Local Inference and Global Parameter Estimation 
时间：2021年05月06日                         第一作者：Noam Elron                       [链接](https://arxiv.org/abs/2105.02794).                     
## 摘要：视频超分辨率（SR）是基于深度学习的技术，但在现实世界的视频中表现不佳（见图1）。原因是训练图像对通常是通过缩小高分辨率图像的比例来产生低分辨率的对应图像。因此，深度模型被训练为撤消缩小尺度，而不是推广到超分辨率的真实世界图像。最近的一些出版物提出了改进基于学习的随机共振泛化的技术，但都不适合实时应用。我们提出了一种新的方法来合成训练数据，通过模拟两个不同尺度的数码相机图像捕获过程。我们的方法产生图像对，其中两幅图像都具有自然图像的特性。使用这些数据训练SR模型可以更好地推广到真实世界的图像和视频。此外，深视频SR模型的特点是每像素运算量大，这使得其无法实时应用。我们提出了一种有效的CNN架构，使得视频SR能够在低功耗边缘设备上实时应用。我们将SR任务分为两个子任务：一个控制流，用于估计输入视频的全局属性，并调整执行实际处理的CNN的权重和偏差。由于CNN过程是根据输入的统计信息定制的，因此它的容量保持较低，同时保持了有效性。另外，由于视频统计发展缓慢，控制流以远低于视频帧速率的速率操作。这减少了多达两个数量级的总体计算负载。这种将算法的自适应性与像素处理解耦的框架，可以应用于一大类实时视频增强应用中，如视频去噪、局部色调映射、稳定等。
<details>	<summary>英文摘要</summary>	The state of the art in video super-resolution (SR) are techniques based on deep learning, but they perform poorly on real-world videos (see Figure 1). The reason is that training image-pairs are commonly created by downscaling a high-resolution image to produce a low-resolution counterpart. Deep models are therefore trained to undo downscaling and do not generalize to super-resolving real-world images. Several recent publications present techniques for improving the generalization of learning-based SR, but are all ill-suited for real-time application. We present a novel approach to synthesizing training data by simulating two digital-camera image-capture processes at different scales. Our method produces image-pairs in which both images have properties of natural images. Training an SR model using this data leads to far better generalization to real-world images and videos. In addition, deep video-SR models are characterized by a high operations-per-pixel count, which prohibits their application in real-time. We present an efficient CNN architecture, which enables real-time application of video SR on low-power edge-devices. We split the SR task into two sub-tasks: a control-flow which estimates global properties of the input video and adapts the weights and biases of a processing-CNN that performs the actual processing. Since the process-CNN is tailored to the statistics of the input, its capacity kept low, while retaining effectivity. Also, since video-statistics evolve slowly, the control-flow operates at a much lower rate than the video frame-rate. This reduces the overall computational load by as much as two orders of magnitude. This framework of decoupling the adaptivity of the algorithm from the pixel processing, can be applied in a large family of real-time video enhancement applications, e.g., video denoising, local tone-mapping, stabilization, etc. </details>
<details>	<summary>注释</summary>	Technical report; accompanying a poster appearing in ICCP 2021 Journal-ref: ICCP 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月07日</details>

# 97、FC$^2$N：用于单图像超分辨率的全通道连接网络
- [ ] FC$^2$N: Fully Channel-Concatenated Network for Single Image Super-Resolution 
时间：2021年05月05日                         第一作者：Xiaole Zhao                       [链接](https://arxiv.org/abs/1907.03221).                     
<details>	<summary>注释</summary>	17 pages, 8 figures and 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 96、COMISR：压缩信息视频超分辨率
- [ ] COMISR: Compression-Informed Video Super-Resolution 
时间：2021年05月04日                         第一作者：Yinxiao Li                       [链接](https://arxiv.org/abs/2105.01237).                     
## 摘要：大多数视频超分辨率方法的重点是从低分辨率视频中恢复高分辨率的视频帧，而不考虑压缩。然而，网络或移动设备上的大多数视频都是压缩的，当带宽有限时，压缩会很严重。本文提出了一种新的基于压缩信息的视频超分辨率模型，在不引入压缩伪影的情况下恢复高分辨率的视频内容。该模型由三个视频超分辨率模块组成：双向递归扭曲、细节保持流估计和拉普拉斯增强。所有这三个模块都用于处理压缩特性，例如输入帧中帧内的位置和输出帧中的平滑度。为了进行全面的性能评估，我们在标准数据集上进行了广泛的实验，包括许多真实的视频用例。结果表明，该方法不仅能从广泛使用的基准数据集中恢复未压缩帧上的高分辨率内容，而且在基于大量量化指标的超分辨率压缩视频中也取得了最新的性能。我们还通过模拟YouTube上的流媒体来评估该方法的有效性和鲁棒性。
<details>	<summary>英文摘要</summary>	Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compression-informed video super-resolution model to restore high-resolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. </details>
<details>	<summary>注释</summary>	14 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 95、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年05月03日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 94、基于对抗图神经网络的脑图超分辨及其在脑功能连接中的应用
- [ ] Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity 
时间：2021年05月02日                         第一作者：Megi Isallari                        [链接](https://arxiv.org/abs/2105.00425).                     
## 摘要：近年来，随着以不同分辨率获取的神经影像数据的激增，脑图像分析得到了长足的发展。近年来，脑图像超分辨率的研究得到了迅速的发展，但由于非欧氏图数据的复杂性，脑图像超分辨率的研究还很薄弱。在本文中，我们提出了有史以来第一个深度图超分辨率（GSR）框架，该框架试图从具有N个节点的低分辨率（LR）图（其中N<N'）自动生成具有N'节点的高分辨率（HR）脑图（即解剖感兴趣区域（roi））。首先，我们将GSR问题形式化为一个节点特征嵌入学习任务。一旦学习了HR节点的嵌入，就可以通过基于一种新的图形U-Net结构的聚合规则来获得大脑roi之间的成对连接强度。图U-Net是一种典型的以节点为中心的结构，其中图的嵌入主要依赖于节点的属性，我们提出了一种以节点为中心的结构，其中节点特征的嵌入基于图的拓扑结构。其次，受图谱理论的启发，我们打破了U-Net结构的对称性，用一个GSR层和两个图卷积网络层对低分辨率的脑图结构和节点内容进行了超分辨，进一步了解了HR图中的节点嵌入。第三，为了处理基本真值和预测的HR脑图之间的域转移，我们结合了对抗正则化来对齐它们各自的分布。我们提出的AGSR网络框架在从低分辨率脑功能图预测高分辨率脑功能图方面优于其变体。我们的AGSR网络代码可在GitHub上获得https://github.com/basiralab/AGSR-Net.
<details>	<summary>英文摘要</summary>	Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2009.11080 </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 93、基于无监督深度学习的磁共振弥散加权成像超分辨率和运动伪影同步去除
- [ ] Simultaneous super-resolution and motion artifact removal in diffusion-weighted MRI using unsupervised deep learning 
时间：2021年05月01日                         第一作者：Hyungjin Chung                       [链接](https://arxiv.org/abs/2105.00240).                     
## 摘要：磁共振弥散加权成像由于其预后的能力，现在常规进行，但扫描质量往往不令人满意，这可能会阻碍临床应用。为了克服这些局限性，本文提出了一种完全无监督的质量增强方案，该方案在提高分辨率的同时去除了运动伪影。这一过程是通过使用具有随机退化块的最优传输驱动cycleGAN训练网络来实现的，cycleGAN学习去除混叠伪影并提高分辨率，然后在测试阶段使用训练好的网络，利用bootstrap子采样和聚集来抑制运动伪影。进一步证明了在推理阶段通过控制bootstrap子采样率可以控制伪影校正量和分辨率之间的折衷。据我们所知，提出的方法是第一个解决超分辨率和运动伪影校正，同时在磁共振背景下使用无监督学习。我们通过将该方法应用于模拟研究的定量评价和活体扩散加权MR扫描，证明了该方法的有效性，这表明该方法优于目前最先进的方法。该方法具有灵活性，可以应用于其他类型MR扫描的各种质量增强方案，也可以直接应用于表观扩散系数图的质量增强。
<details>	<summary>英文摘要</summary>	Diffusion-weighted MRI is nowadays performed routinely due to its prognostic ability, yet the quality of the scans are often unsatisfactory which can subsequently hamper the clinical utility. To overcome the limitations, here we propose a fully unsupervised quality enhancement scheme, which boosts the resolution and removes the motion artifact simultaneously. This process is done by first training the network using optimal transport driven cycleGAN with stochastic degradation block which learns to remove aliasing artifacts and enhance the resolution, then using the trained network in the test stage by utilizing bootstrap subsampling and aggregation for motion artifact suppression. We further show that we can control the trade-off between the amount of artifact correction and resolution by controlling the bootstrap subsampling ratio at the inference stage. To the best of our knowledge, the proposed method is the first to tackle super-resolution and motion artifact correction simultaneously in the context of MRI using unsupervised learning. We demonstrate the efficiency of our method by applying it to both quantitative evaluation using simulation study, and to in vivo diffusion-weighted MR scans, which shows that our method is superior to the current state-of-the-art methods. The proposed method is flexible in that it can be applied to various quality enhancement schemes in other types of MR scans, and also directly to the quality enhancement of apparent diffusion coefficient maps. </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 92、NTIRE 2021视频超分辨率挑战赛
- [ ] NTIRE 2021 Challenge on Video Super-Resolution 
时间：2021年04月30日                         第一作者：Sanghyun Son                       [链接](https://arxiv.org/abs/2104.14852).                     
## 摘要：超分辨率（Super-Resolution，SR）是一项基本的计算机视觉任务，其目标是从给定的低分辨率图像中获得高分辨率的清晰图像。本文回顾了NTIRE 2021在视频超分辨率方面的挑战。我们给出了两条赛道的评估结果以及建议的解决方案。track1的目标是开发传统的视频SR方法，重点是恢复质量。轨道2假设一个具有较低帧速率的更具挑战性的环境，投射时空SR问题。在每项比赛中，分别有247名和223名参赛者报名。在最后的测试阶段，14个团队在每条赛道上进行比赛，以实现视频SR任务的最先进性能。
<details>	<summary>英文摘要</summary>	Super-Resolution (SR) is a fundamental computer vision task that aims to obtain a high-resolution clean image from the given low-resolution counterpart. This paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We present evaluation results from two competition tracks as well as the proposed solutions. Track 1 aims to develop conventional video SR methods focusing on the restoration quality. Track 2 assumes a more challenging environment with lower frame rates, casting spatio-temporal SR problem. In each competition, 247 and 223 participants have registered, respectively. During the final testing phase, 14 teams competed in each track to achieve state-of-the-art performance on video SR tasks. </details>
<details>	<summary>注释</summary>	An official report for NTIRE 2021 Video Super-Resolution Challenge, in conjunction with CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 91、SRDiff：基于扩散概率模型的单幅图像超分辨率
- [ ] SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models 
时间：2021年04月30日                         第一作者：Haoying Li                       [链接](https://arxiv.org/abs/2104.14951).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是从给定的低分辨率（low-resolution，LR）图像重建出高分辨率（high-resolution，HR）图像，由于一幅LR图像对应于多幅HR图像，因此这是一个病态问题。近年来，基于学习的SISR方法在性能上明显优于传统的SISR方法，而面向PSNR、GAN驱动和基于流的SISR方法存在过平滑、模式崩溃和模型占用大等问题。为了解决这些问题，我们提出了一种新的单图像超分辨率扩散概率模型（SRDiff），这是第一个基于扩散的SISR模型。SRDiff利用数据似然的变分界变量进行优化，通过马尔可夫链将高斯噪声逐步转化为LR输入条件下的超分辨率（SR）图像，可以提供多样化和真实的SR预测。另外，在整个框架中引入残差预测，加快了收敛速度。我们在面部和一般基准测试（CelebA和DIV2K数据集）上的大量实验表明：1）SRDiff只需一个LR输入，就可以生成丰富细节的不同SR结果，具有最先进的性能；2） SRDiff易于训练，占地面积小；SRDiff可以进行灵活的图像处理，包括潜在空间插值和内容融合。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion. </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 90、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月30日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
<details>	<summary>注释</summary>	This paper is accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 89、BasicVSR++：通过增强传播和对齐提高视频超分辨率
- [ ] BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment 
时间：2021年04月27日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2104.13371).                     
## 摘要：递归结构是视频超分辨率处理的常用框架选择。最先进的方法BasicVSR采用双向传播和特征对齐，有效地利用整个输入视频中的信息。在这项研究中，我们提出二阶网格传播和流动导向的变形对准来重新设计BasicVSR。我们表明，通过增强传播和对齐的循环框架，可以更有效地利用跨错位视频帧的时空信息。在类似的计算约束下，新的组件可以提高性能。特别是，我们的模型BasicVSR++在参数数目相近的情况下，PSNR比BasicVSR高出0.82db。除了视频超分辨率，BasicVSR++还可以很好地推广到其他视频恢复任务，如压缩视频增强。2021年，BasicVSR++在视频超分辨率和压缩视频增强挑战中获得三个冠军和一个亚军。代码和模型将发布给MMEditing。
<details>	<summary>英文摘要</summary>	A recurrent structure is a popular framework choice for the task of video super-resolution. The state-of-the-art method BasicVSR adopts bidirectional propagation with feature alignment to effectively exploit information from the entire input video. In this study, we redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment. We show that by empowering the recurrent framework with the enhanced propagation and alignment, one can exploit spatiotemporal information across misaligned video frames more effectively. The new components lead to an improved performance under a similar computational constraint. In particular, our model BasicVSR++ surpasses BasicVSR by 0.82 dB in PSNR with similar number of parameters. In addition to video super-resolution, BasicVSR++ generalizes well to other video restoration tasks such as compressed video enhancement. In NTIRE 2021, BasicVSR++ obtains three champions and one runner-up in the Video Super-Resolution and Compressed Video Enhancement Challenges. Codes and models will be released to MMEditing. </details>
<details>	<summary>注释</summary>	3 champions and 1 runner-up in NTIRE 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月28日</details>

# 88、好的艺术家复制，伟大的艺术家窃取：模型提取攻击图像翻译生成对抗网络
- [ ] Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks 
时间：2021年04月26日                         第一作者：Sebastian Szyller                       [链接](https://arxiv.org/abs/2104.12623).                     
## 摘要：机器学习模型通常通过推理api提供给潜在的客户机用户。当恶意客户端使用从查询中收集到的信息到受害者模型$F_V$的推理API来构建具有类似功能的代理模型$F_a$时，就会发生模型提取攻击。最近的研究表明，针对图像分类和NLP模型的模型提取攻击是成功的。在本文中，我们展示了第一个针对真实世界生成性对抗网络（GAN）图像翻译模型的模型提取攻击。我们提出了一个对图像翻译模型进行模型提取攻击的框架，并证明了对手可以成功地提取功能代理模型。敌方不需要知道$F\V$的体系结构或任何其他超出其预期图像翻译任务的信息，并且使用从与$F\V$的训练数据相同的域中提取的数据来查询$F\V$的推理接口。我们使用两种流行的图像翻译类型的三个不同实例来评估我们的攻击的有效性：（1）自拍到动画，（2）莫奈到照片（图像样式转换），和（3）超分辨率（超分辨率）。使用GANs的标准性能指标，我们证明了我们的攻击在这三种情况下都是有效的，$F_V$和$F_A$与目标之间的差异在以下范围内：自拍到动画：FID$13.36-68.66$，莫奈到照片：FID$3.57-4.40$，超分辨率：SSIM:$0.06-0.08$，PSNR:$1.43-4.46$。此外，我们进行了一项大规模（125名参与者）的用户研究，从自拍到动画，从莫奈到照片，以表明人类对受害者和代理模型产生的图像的感知是等效的，在科恩的$d=0.3$的等效范围内。
<details>	<summary>英文摘要</summary>	Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ that has comparable functionality. Recent research has shown successful model extraction attacks against image classification, and NLP models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting model extraction attacks against image translation models, and show that the adversary can successfully extract functional surrogate models. The adversary is not required to know $F_V$'s architecture or any other information about it beyond its intended image translation task, and queries $F_V$'s inference interface using data drawn from the same domain as the training data for $F_V$. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective in each of the three cases -- the differences between $F_V$ and $F_A$, compared to the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$, Monet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and PSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by the victim and surrogate models can be considered equivalent, within an equivalence bound of Cohen's $d=0.3$. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 87、意向性深度过度适应学习（IDOL）：一种新的适应性放射治疗深度学习策略
- [ ] Intentional Deep Overfit Learning (IDOL): A Novel Deep Learning Strategy for Adaptive Radiation Therapy 
时间：2021年04月23日                         第一作者：Jaehee Chun (3)                       [链接](https://arxiv.org/abs/2104.11401).                     
## 摘要：在这项研究中，我们提出了一个针对患者特定表现的定制DL框架，该框架利用了一个模型的行为，该模型故意过度拟合患者特定的训练数据集，该数据集是从ART工作流中可用的先验信息扩充而来的——我们称之为故意深度过度拟合学习（IDOL）。在放射治疗的任何任务中实施IDOL框架包括两个训练阶段：1）训练一个具有N个患者的不同训练数据集的广义模型，就像传统的DL方法一样，2）有意地将该一般模型过度拟合到特定于感兴趣的患者（N+1）的小训练数据集，该数据集是通过对可用的特定于任务和患者的先验信息的扰动和增强而生成的，以建立个性化的偶像模型。IDOL框架本身是任务无关的，因此广泛适用于ART工作流的许多组件，其中三个我们在这里用作概念证明：用于传统ART的重新规划CT的自动轮廓任务，用于MRI引导ART的MRI超分辨率（SR）任务，以及仅用于MRI ART的合成CT（sCT）重建任务。在重新规划CT自动轮廓任务中，采用Dice相似系数测量的精度由一般模型的0.847提高到采用IDOL模型的0.935。在MRI-SR的情况下，使用IDOL框架比传统模型的平均绝对误差（MAE）提高了40%。最后，在sCT重建任务中，利用IDOL框架将MAE从68降到22hu。
<details>	<summary>英文摘要</summary>	In this study, we propose a tailored DL framework for patient-specific performance that leverages the behavior of a model intentionally overfitted to a patient-specific training dataset augmented from the prior information available in an ART workflow - an approach we term Intentional Deep Overfit Learning (IDOL). Implementing the IDOL framework in any task in radiotherapy consists of two training stages: 1) training a generalized model with a diverse training dataset of N patients, just as in the conventional DL approach, and 2) intentionally overfitting this general model to a small training dataset-specific the patient of interest (N+1) generated through perturbations and augmentations of the available task- and patient-specific prior information to establish a personalized IDOL model. The IDOL framework itself is task-agnostic and is thus widely applicable to many components of the ART workflow, three of which we use as a proof of concept here: the auto-contouring task on re-planning CTs for traditional ART, the MRI super-resolution (SR) task for MRI-guided ART, and the synthetic CT (sCT) reconstruction task for MRI-only ART. In the re-planning CT auto-contouring task, the accuracy measured by the Dice similarity coefficient improves from 0.847 with the general model to 0.935 by adopting the IDOL model. In the case of MRI SR, the mean absolute error (MAE) is improved by 40% using the IDOL framework over the conventional model. Finally, in the sCT reconstruction task, the MAE is reduced from 68 to 22 HU by utilizing the IDOL framework. </details>
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 86、基于三层神经网络结构的高效单幅图像超分辨率搜索
- [ ] Trilevel Neural Architecture Search for Efficient Single Image Super-Resolution 
时间：2021年04月23日                         第一作者：Yan Wu                       [链接](https://arxiv.org/abs/2101.06658).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 85、利用先验知识微调深度学习模型参数提高动态MRI超分辨率
- [ ] Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge 
时间：2021年04月23日                         第一作者：Chompunuch Sarasaen                       [链接](https://arxiv.org/abs/2102.02711).                     
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 84、SRWarp：任意变换下的广义图像超分辨率
- [ ] SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation 
时间：2021年04月21日                         第一作者：Sanghyun Son                        [链接](https://arxiv.org/abs/2104.10325).                     
## 摘要：深部CNNs在图像处理和应用方面取得了巨大的成功，包括单图像超分辨率（SR）。然而，传统方法仍然求助于一些预定的整数比例因子，例如x2或x4。因此，当需要任意的目标分辨率时，它们很难被应用。最近的方法将范围扩展到实值上采样因子，甚至使用不同的纵横比来处理限制。在本文中，我们提出SRWarp框架来进一步将SR任务推广到任意图像变换。我们将传统的图像扭曲任务，特别是当输入被放大时，解释为一个空间变化的SR问题。我们还提出了一些新的公式，包括自适应翘曲层和多尺度混合，重建视觉上良好的结果在转换过程中。与以前的方法相比，我们没有将SR模型限制在规则的网格上，而是允许多种可能的变形，以实现灵活多样的图像编辑。大量的实验和烧蚀研究证明了该方法的必要性，并证明了该方法在各种变换下的优越性。
<details>	<summary>英文摘要</summary>	Deep CNNs have achieved significant successes in image processing and its applications, including single image super-resolution (SR). However, conventional methods still resort to some predetermined integer scaling factors, e.g., x2 or x4. Thus, they are difficult to be applied when arbitrary target resolutions are required. Recent approaches extend the scope to real-valued upsampling factors, even with varying aspect ratios to handle the limitation. In this paper, we propose the SRWarp framework to further generalize the SR tasks toward an arbitrary image transformation. We interpret the traditional image warping task, specifically when the input is enlarged, as a spatially-varying SR problem. We also propose several novel formulations, including the adaptive warping layer and multiscale blending, to reconstruct visually favorable results in the transformation process. Compared with previous methods, we do not constrain the SR model on a regular grid but allow numerous possible deformations for flexible and diverse image editing. Extensive experiments and ablation studies justify the necessity and demonstrate the advantage of the proposed SRWarp method under various transformations. </details>
<details>	<summary>注释</summary>	Accepted to CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 83、可控时空视频超分辨率的时间调制网络
- [ ] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution 
时间：2021年04月21日                         第一作者：Gang Xu                        [链接](https://arxiv.org/abs/2104.10642).                     
## 摘要：时空视频超分辨率（STVSR）旨在提高低分辨率、低帧速率视频的时空分辨率。近年来，基于可变形卷积的STVSR方法取得了很好的性能，但它们只能在训练阶段推断出预先定义的中间帧。此外，这些方法低估了相邻帧之间的短期运动线索。本文提出了一种时间调制网络（TMNet）来插值任意中间帧，实现精确的高分辨率重建。具体地说，我们提出了一种时间调制块（TMB）来调制可变形卷积核以实现可控的特征插值。为了更好地利用视频中的时间信息，我们提出了一种局部时间特征比较（LFC）模块和双向可变形ConvLSTM来提取视频中的短期和长期运动线索。在三个基准数据集上的实验表明，我们的TMNet算法优于以前的STVSR算法。代码可在https://github.com/CS-GangXu/TMNet.
<details>	<summary>英文摘要</summary>	Space-time video super-resolution (STVSR) aims to increase the spatial and temporal resolutions of low-resolution and low-frame-rate videos. Recently, deformable convolution based methods have achieved promising STVSR performance, but they could only infer the intermediate frame pre-defined in the training stage. Besides, these methods undervalued the short-term motion cues among adjacent frames. In this paper, we propose a Temporal Modulation Network (TMNet) to interpolate arbitrary intermediate frame(s) with accurate high-resolution reconstruction. Specifically, we propose a Temporal Modulation Block (TMB) to modulate deformable convolution kernels for controllable feature interpolation. To well exploit the temporal information, we propose a Locally-temporal Feature Comparison (LFC) module, along with the Bi-directional Deformable ConvLSTM, to extract short-term and long-term motion cues in videos. Experiments on three benchmark datasets demonstrate that our TMNet outperforms previous STVSR methods. The code is available at https://github.com/CS-GangXu/TMNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 82、单图像超分辨率的两级注意网络
- [ ] A Two-Stage Attentive Network for Single Image Super-Resolution 
时间：2021年04月21日                         第一作者：Jiqing Zhang                       [链接](https://arxiv.org/abs/2104.10488).                     
## 摘要：近年来，深度卷积神经网络（CNNs）在单幅图像超分辨率（SISR）中得到了广泛的应用，并取得了显著的进展。然而，现有的基于CNNs的SISR方法大多在特征提取阶段没有充分挖掘背景信息，对最终的高分辨率图像重建步骤关注较少，从而影响了理想的SR性能。为了解决上述两个问题，本文提出了一种从粗到精的两级注意网络（TSAN）方法。具体来说，我们设计了一个新的多上下文注意块（MCAB），使网络关注更多的信息上下文特征。此外，我们提出了一个必要的精细注意块（RAB），它可以在HR空间中寻找有用的线索来重建精细的HR图像。对四个基准数据集的广泛评估显示了我们提出的TSAN在定量指标和视觉效果方面的有效性。代码位于https://github.com/Jee-King/TSAN.
<details>	<summary>英文摘要</summary>	Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at https://github.com/Jee-King/TSAN. </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 81、TWIST-GAN：面向小波变换和传输GAN的时空单图像超分辨率研究
- [ ] TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatio-Temporal Single Image Super Resolution 
时间：2021年04月20日                         第一作者：Fayaz Ali Dharejo                       [链接](https://arxiv.org/abs/2104.10268).                     
## 摘要：单图像超分辨率（Single-Image Super-resolution，SISR）是从低空间分辨率的遥感图像中提取高分辨率、高空间分辨率的图像。近年来，针对具有挑战性的单图像超分辨率（SISR）问题，深入学习和生成对抗网络（GANs）取得了突破性进展。然而，生成的图像仍然存在一些不需要的伪影，例如缺少纹理特征表示和高频信息。提出了一种基于频域的时空遥感单幅图像超分辨率重建技术，结合不同频段的生成性对抗网络（GANs）对HR图像进行重建。介绍了一种结合小波变换（WT）特性和传递生成对抗网络的新方法。利用小波变换将LR图像分解为多个频段，而传递生成对抗网络则通过一种新的结构来预测高频分量。最后，通过小波变换的逆变换得到超分辨率的重建图像。该模型首先在一个外部DIV2 Kdataset上进行训练，并用UC-Merceed Landsat遥感数据集和Set14进行验证，每个图像大小为256x256。然后，为了减小计算量的差异，提高纹理信息的质量，利用传递的GANs对时空遥感图像进行处理。对研究结果进行了定性和定性的比较，并与现有的研究方法进行了比较。此外，我们在训练期间节省了大约43%的GPU内存，并通过消除批处理规范化层加快了简化版本的执行。
<details>	<summary>英文摘要</summary>	Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from aremotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks(GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, thegenerated image still suffers from undesirable artifacts such as, the absence of texture-feature representationand high-frequency information. We propose a frequency domain-based spatio-temporal remote sensingsingle image super-resolution technique to reconstruct the HR image combined with generative adversarialnetworks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporatingWavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image hasbeen split into various frequency bands by using the WT, whereas, the transfer generative adversarial networkpredicts high-frequency components via a proposed architecture. Finally, the inverse transfer of waveletsproduces a reconstructed image with super-resolution. The model is first trained on an external DIV2 Kdataset and validated with the UC Merceed Landsat remote sensing dataset and Set14 with each image sizeof 256x256. Following that, transferred GANs are used to process spatio-temporal remote sensing images inorder to minimize computation cost differences and improve texture information. The findings are comparedqualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of theGPU memory during training and accelerated the execution of our simplified version by eliminating batchnormalization layers. </details>
<details>	<summary>注释</summary>	Accepted: ACM TIST (10-03-2021) </details>
<details>	<summary>邮件日期</summary>	2021年04月22日</details>

# 80、立体图像超分辨率的对称视差注意
- [ ] Symmetric Parallax Attention for Stereo Image Super-Resolution 
时间：2021年04月20日                         第一作者：Yingqian Wang                       [链接](https://arxiv.org/abs/2011.03802).                     
<details>	<summary>注释</summary>	Accepted to NTIRE workshop at CVPR 2021. The first two authors contribute equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 79、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月20日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
<details>	<summary>注释</summary>	accepted as a conference paper to Picture Coding Symposium (PCS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 78、核不可知的真实图像超分辨率
- [ ] Kernel Agnostic Real-world Image Super-resolution 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2104.09008).                     
## 摘要：近年来，深度神经网络模型在各个研究领域取得了令人瞩目的成果。随着深度超分辨（SR）技术的发展，它引起了越来越多的关注。许多现有的方法都试图从直接下采样的低分辨率图像中恢复高分辨率图像，或者由于其简单性而假设高斯退化核具有加性噪声。然而，在真实场景中，即使失真图像在视觉上与清晰图像相似，也可能涉及高度复杂的核和非加性噪声。在这种情况下，现有的SR模型很难处理真实世界的图像。本文提出了一种新的核不可知SR框架来处理现实世界中的图像SR问题。这个框架可以无缝地挂接到多个主流模型上。在该框架中，退化核和噪声被自适应地建模而不是显式地指定。此外，我们还从正交的角度提出了一个迭代监督过程和频率参与目标，以进一步提高性能。实验验证了该框架在多个真实数据集上的有效性。
<details>	<summary>英文摘要</summary>	Recently, deep neural network models have achieved impressive results in various research fields. Come with it, an increasing number of attentions have been attracted by deep super-resolution (SR) approaches. Many existing methods attempt to restore high-resolution images from directly down-sampled low-resolution images or with the assumption of Gaussian degradation kernels with additive noises for their simplicities. However, in real-world scenarios, highly complex kernels and non-additive noises may be involved, even though the distorted images are visually similar to the clear ones. Existing SR models are facing difficulties to deal with real-world images under such circumstances. In this paper, we introduce a new kernel agnostic SR framework to deal with real-world image SR problem. The framework can be hanged seamlessly to multiple mainstream models. In the proposed framework, the degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and frequency-attended objective from orthogonal perspectives to further boost the performance. The experiments validate the effectiveness of the proposed framework on multiple real-world datasets. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 77、基于稠密搜索空间的神经网络超分辨率搜索：DeCoNAS
- [ ] Neural Architecture Search for Image Super-Resolution Using Densely Constructed Search Space: DeCoNAS 
时间：2021年04月19日                         第一作者：Joon Young Ahn                        [链接](https://arxiv.org/abs/2104.09048).                     
## 摘要：近年来，深度卷积神经网络在单幅图像超分辨率（SISR）和其他许多视觉任务中取得了巨大的成功。通过深化网络和开发更复杂的网络结构，它们的性能也在提高。然而，为给定的问题找到一个最优的结构是一项困难的任务，即使是对人类专家来说也是如此。为此，人们引入了神经结构搜索（NAS）方法，使结构的构建过程自动化。在本文中，我们将NAS扩展到超分辨率域，找到了一个轻量级的密集连接网络decoasnet。我们使用分层搜索策略来寻找与局部和全局特征的最佳连接。在这个过程中，我们定义了一个基于复杂度的惩罚来解决图像的超分辨率问题，这可以看作是一个多目标问题。实验结果表明，我们的decoasnet比现有的基于NAS的设计和手工设计的轻量级超分辨率网络具有更好的性能。
<details>	<summary>英文摘要</summary>	The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 76、基于无监督深度学习的三维荧光显微镜轴向-横向超分辨分析
- [ ] Axial-to-lateral super-resolution for 3D fluorescence microscopy using unsupervised deep learning 
时间：2021年04月19日                         第一作者：Hyoungjun Park                       [链接](https://arxiv.org/abs/2104.09435).                     
## 摘要：与横向分辨率相比，荧光显微镜的体积成像通常受到轴向分辨率较低的各向异性空间分辨率的限制。为了解决这一问题，本文提出了一种基于深度学习的无监督超分辨技术来增强体荧光显微镜中的各向异性图像。与现有的需要匹配高分辨率目标体图像的深度学习方法相比，我们的方法大大减少了投入实践的工作量，因为网络的训练只需要一个3D图像堆栈，而不需要图像形成过程的先验知识、训练数据的配准或单独的训练获取目标数据。这是基于最优传输驱动循环一致生成对抗网络实现的，该网络从横向图像平面的高分辨率二维图像和其他平面的低分辨率二维图像之间的不成对匹配中学习。利用荧光共焦显微镜和光片显微镜，我们证明了训练的网络不仅提高了轴向分辨率超过衍射极限，而且增强了成像平面之间被抑制的视觉细节，消除了成像伪影。
<details>	<summary>英文摘要</summary>	Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution from inferior axial resolution compared to the lateral resolution. To address this problem, here we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target volume images, our method greatly reduces the effort to put into practice as the training of a network requires as little as a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in lateral image plane and low-resolution 2D images in the other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution beyond the diffraction limit, but also enhances suppressed visual details between the imaging planes and removes imaging artifacts. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 75、图像超分辨率注意网络中的注意
- [ ] Attention in Attention Network for Image Super-Resolution 
时间：2021年04月19日                         第一作者：Haoyu Chen                       [链接](https://arxiv.org/abs/2104.09497).                     
## 摘要：在过去的十年中，卷积神经网络在单幅图像超分辨率（SISR）方面取得了显著的进展。在SISR的最新进展中，注意机制是高性能SR模型的关键。然而，很少有作品真正讨论注意力为什么起作用以及它是如何起作用的。在这项工作中，我们试图量化和可视化的静态注意机制，并表明并非所有的注意模块都是同样有益的。然后，我们提出注意网络中的注意（A$^2$N）来获得高精度的图像SR。具体来说，我们的A$^2$N由一个非注意分支和一个耦合注意分支组成。提出了一种基于输入特征的动态注意权值提取模块，该模块能够有效地抑制不必要的注意调整。这允许注意模块专门化有益的例子，而无需其他惩罚，从而大大提高了注意网络的容量，而参数开销很小。实验表明，与现有的轻量级网络相比，该模型具有更好的折衷性能。在局部属性图上的实验也证明了注意力结构（attention-in-attention，A$^2$）可以从更广的范围内提取特征。
<details>	<summary>英文摘要</summary>	Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high performance SR models. However, few works really discuss why attention works and how it works. In this work, we attempt to quantify and visualize the static attention mechanisms and show that not all attention modules are equally beneficial. We then propose attention in attention network (A$^2$N) for highly accurate image SR. Specifically, our A$^2$N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. Experiments have demonstrated that our model could achieve superior trade-off performances comparing with state-of-the-art lightweight networks. Experiments on local attribution maps also prove attention in attention (A$^2$) structure can extract features from a wider range. </details>
<details>	<summary>注释</summary>	10 pages, 8 figures. Codes will be available at $\href{https://github.com/haoyuc/A2N}{\text{this https URL}}$ </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 74、全量化图像超分辨率网络
- [ ] Fully Quantized Image Super-Resolution Networks 
时间：2021年04月19日                         第一作者：Hu Wang                       [链接](https://arxiv.org/abs/2011.14265).                     
<details>	<summary>注释</summary>	Results updated </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 73、VSpSR：基于变分稀疏表示的可探索超分辨率
- [ ] VSpSR: Explorable Super-Resolution via Variational Sparse Representation 
时间：2021年04月17日                         第一作者：Hangqi Zhou                       [链接](https://arxiv.org/abs/2104.08575).                     
## 摘要：超分辨率（SR）是一个不适定问题，它意味着无限多幅高分辨率（HR）图像可以退化为同一幅低分辨率（LR）图像。为了研究一对多随机SR映射，我们隐式地表示了自然图像的非局部自相似性，并通过神经网络建立了一个变分稀疏的超分辨率框架（VSpSR）。由于HR图像的每一小块都可以很好地用原子在超完备字典中的稀疏表示来逼近，因此我们设计了一个双分支模块VSpM来探索SR空间。具体地说，VSpM的一个分支从LR输入中提取面片级基，另一个分支根据稀疏系数推断像素级的变分分布。通过重复采样系数，我们可以得到无限的稀疏表示，从而产生不同的HR图像。根据NTIRE 2021学习SR空间挑战赛的初步结果，我们团队（FUDANMIC21）的发布分数排名第7位。VSpSR的实现发布于https://zmiclab.github.io/。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 72、提高VVC质量和超分辨率的多任务学习方法
- [ ] Multitask Learning for VVC Quality Enhancement and Super-Resolution 
时间：2021年04月16日                         第一作者：Charles Bonnineau                        [链接](https://arxiv.org/abs/2104.08319).                     
## 摘要：最新的视频编码标准，称为多功能视频编码（VVC），在编码链的不同层次上包含了几种新颖而精细的编码工具。与以前的高效视频编码（HEVC）标准相比，这些工具带来了显著的编码增益。然而，编码器仍然可以引入可见的编码伪影，主要由应用于将比特率调整到可用带宽的编码决策引起。因此，通常将预处理和后处理技术添加到编码管道以提高解码视频的质量。由于近年来在深度学习方面的进步，与传统方法相比，这些方法最近显示出了突出的效果。通常，多个神经网络被独立地训练来执行不同的任务，因此忽略了模型之间存在的冗余。在本文中，我们研究了一种基于学习的解决方案作为后处理步骤，以提高解码的VVC视频质量。我们的方法依赖于多任务学习来执行质量增强和超分辨率使用一个单一的共享网络优化多个退化水平。与传统的专用体系结构相比，该方案在减少编码伪影和超分辨率方面具有良好的性能，且网络参数较少。
<details>	<summary>英文摘要</summary>	The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 71、基于互Dirichlet网的无监督无配准高光谱图像超分辨率
- [ ] Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net 
时间：2021年04月15日                         第一作者：Ying Qu                        [链接](https://arxiv.org/abs/1904.12175).                     
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Remote Sensing and Geoscience </details>
<details>	<summary>邮件日期</summary>	2021年04月19日</details>

# 70、缩放SlowMo：一种高效的单级时空视频超分辨率框架
- [ ] Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution 
时间：2021年04月15日                         第一作者：Xiaoyu Xiang                       [链接](https://arxiv.org/abs/2104.07473).                     
## 摘要：本文提出了一种基于低分辨率（LR）和低帧速率（LFR）视频序列的高分辨率（HR）慢动作视频超分辨率算法。一种简单的方法是将其分解为两个子任务：视频帧插值（VFI）和视频超分辨率（VSR）。然而，在这个问题中，时间插值和空间上缩放是相互关联的。两阶段方法不能充分利用这一自然属性。另外，现有的VFI或VSR深度网络为了获得高质量的真实感视频帧，通常需要一个较大的帧重建模块，这使得两阶段的方法具有较大的模型，因而相对耗时。为了克服这一问题，我们提出了一种单级时空视频超分辨率框架，该框架可以直接从输入的LR和LFR视频中重建HR慢动作视频序列。我们不象VFI模型那样重建丢失的LR中间帧，而是通过特征时间插值模块对捕获局部时间上下文的丢失LR帧的LR帧特征进行时间插值。在广泛使用的基准测试上的大量实验表明，该框架不仅在干净和有噪声的LR帧上实现了更好的定性和定量性能，而且比最新的两级网络快数倍。源代码在中发布https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.
<details>	<summary>英文摘要</summary>	In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na\"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 . </details>
<details>	<summary>注释</summary>	Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 69、BAM：一种轻量级高效的单图像超分辨率均衡注意机制
- [ ] BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution 
时间：2021年04月15日                         第一作者：Fanyi Wang                       [链接](https://arxiv.org/abs/2104.07566).                     
## 摘要：单图像超分辨率（SISR）是计算机视觉领域中最具挑战性的问题之一。在基于深度卷积神经网络的方法中，注意机制显示出巨大的潜力。然而，由于网络结构的多样性，SISR任务缺乏一种通用的注意机制。本文提出了一种轻量级、高效的平衡注意机制（BAM），该机制可广泛适用于不同的SISR网络。它由Avgpool通道注意模块（ACAM）和Maxpool空间注意模块（MSAM）组成。这两个模块是并联的，以尽量减少误差积累和串扰。为了减少冗余信息对注意力产生的不良影响，我们仅将Avgpool应用于通道注意，因为Maxpool可以在空间维度上提取特征图中的虚幻极值点，我们只将Maxpool应用于空间注意，因为通道维度上的有用特征通常以最大值的形式存在于SISR任务中。为了验证BAM的有效性和鲁棒性，我们将其应用于12个最先进的SISR网络，其中8个没有注意，因此我们插入了BAM，4个有注意，因此我们用BAM替换了原有的注意模块。我们在Set5、Set14和BSD100基准数据集上进行了实验，其标度因子为x2、x3和x4。实验结果表明，BAM可以普遍提高网络性能。此外，我们还进行了烧蚀实验来证明BAM的极简性。结果表明，BAM的并行结构能够更好地平衡信道和空间注意，从而优于传统卷积块注意模块（CBAM）的串行结构。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR) is one of the most challenging problems in the field of computer vision. Among the deep convolutional neural network based methods, attention mechanism has shown the enormous potential. However, due to the diverse network architectures, there is a lack of a universal attention mechanism for the SISR task. In this paper, we propose a lightweight and efficient Balanced Attention Mechanism (BAM), which can be generally applicable for different SISR networks. It consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM). These two modules are connected in parallel to minimize the error accumulation and the crosstalk. To reduce the undesirable effect of redundant information on the attention generation, we only apply Avgpool for channel attention because Maxpool could pick up the illusive extreme points in the feature map across the spatial dimensions, and we only apply Maxpool for spatial attention because the useful features along the channel dimension usually exist in the form of maximum values for SISR task. To verify the efficiency and robustness of BAM, we apply it to 12 state-of-the-art SISR networks, among which eight were without attention thus we plug BAM in and four were with attention thus we replace its original attention module with BAM. We experiment on Set5, Set14 and BSD100 benchmark datasets with the scale factor of x2 , x3 and x4 . The results demonstrate that BAM can generally improve the network performance. Moreover, we conduct the ablation experiments to prove the minimalism of BAM. Our results show that the parallel structure of BAM can better balance channel and spatial attentions, thus outperforming the series structure of prior Convolutional Block Attention Module (CBAM). </details>
<details>	<summary>注释</summary>	13 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 68、基于迭代细化的图像超分辨率方法
- [ ] Image Super-Resolution via Iterative Refinement 
时间：2021年04月15日                         第一作者：Chitwan Saharia                       [链接](https://arxiv.org/abs/2104.07636).                     
## 摘要：我们提出了SR3，一种通过重复细化实现图像超分辨率的方法。SR3采用去噪扩散概率模型生成条件图像，并通过随机去噪过程进行超分辨率处理。推理从纯高斯噪声开始，并使用在不同噪声水平下进行去噪训练的U网络模型迭代地细化噪声输出。SR3在不同放大倍数的超分辨率任务、人脸和自然图像上表现出很强的性能。我们在CelebA HQ上对标准8X人脸超分辨率任务进行了人体评估，并与SOTA-GAN方法进行了比较。SR3实现了接近50%的傻瓜率，这表明照片逼真的输出，而GANs不超过34%的傻瓜率。我们进一步证明了SR3在级联图像生成中的有效性，其中生成模型与超分辨率模型相链接，在ImageNet上产生了11.3的竞争性FID分数。
<details>	<summary>英文摘要</summary>	We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 67、用于制导深度图超分辨率的离散余弦变换网络
- [ ] Discrete Cosine Transform Network for Guided Depth Map Super-Resolution 
时间：2021年04月14日                         第一作者：Zixiang Zhao                       [链接](https://arxiv.org/abs/2104.06977).                     
## 摘要：制导深度超分辨率（GDSR）是多模图像处理中的一个热点问题。目标是使用高分辨率（HR）RGB图像提供关于边缘和对象轮廓的额外信息，以便低分辨率深度贴图可以向上采样到HR贴图。针对现有方法中存在的RGB纹理过度传输、跨模态特征提取困难、模块工作机制不明确等问题，提出了一种改进的离散余弦变换网络（DCTNet）。首先，将成对的RGB/深度图像输入到半耦合特征提取模块。共享卷积核分别提取跨模态的公共特征，私有核分别提取各自的独特特征。然后将RGB特征输入到边缘注意机制中，以突出显示对上采样有用的边缘。随后，在离散余弦变换（DCT）模块中，采用DCT来解决图像域GDSR的优化问题。将该方法推广到多通道RGB/depth特征上采样，提高了DCTNet的合理性，比传统方法更灵活有效。最终的深度预测由重建模块输出。大量的定性和定量实验证明了该方法的有效性，它可以生成准确的HR深度图，超过了现有的方法。同时，通过烧蚀实验验证了模块设计的合理性。
<details>	<summary>英文摘要</summary>	Guided depth super-resolution (GDSR) is a hot topic in multi-modal image processing. The goal is to use high-resolution (HR) RGB images to provide extra information on edges and object contours, so that low-resolution depth maps can be upsampled to HR ones. To solve the issues of RGB texture over-transferred, cross-modal feature extraction difficulty and unclear working mechanism of modules in existing methods, we propose an advanced Discrete Cosine Transform Network (DCTNet), which is composed of four components. Firstly, the paired RGB/depth images are input into the semi-coupled feature extraction module. The shared convolution kernels extract the cross-modal common features, and the private kernels extract their unique features, respectively. Then the RGB features are input into the edge attention mechanism to highlight the edges useful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT) module, where DCT is employed to solve the optimization problem designed for image domain GDSR. The solution is then extended to implement the multi-channel RGB/depth features upsampling, which increases the rationality of DCTNet, and is more flexible and effective than conventional methods. The final depth prediction is output by the reconstruction module. Numerous qualitative and quantitative experiments demonstrate the effectiveness of our method, which can generate accurate and HR depth maps, surpassing state-of-the-art methods. Meanwhile, the rationality of modules is also proved by ablation experiments. </details>
<details>	<summary>邮件日期</summary>	2021年04月15日</details>

# 66、SRR-Net：一种高分辨率MR图像的超分辨率重建方法
- [ ] SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging 
时间：2021年04月13日                         第一作者：Wenqi Huang                       [链接](https://arxiv.org/abs/2104.05901).                     
## 摘要：提高磁共振成像（MRI）的图像分辨率和采集速度是一个具有挑战性的问题。主要有两种策略来处理速度-分辨率的折衷：（1）$k$空间欠采样和高分辨率采集；（2）低分辨率图像重建和图像超分辨率流水线。然而，这些方法要么在某些高加速因子下性能有限，要么存在两级结构的误差积累。本文将MR重建和图像超分辨率的思想结合起来，直接从低分辨率的$k$空间采样数据中恢复HR图像。特别地，将SR重建问题描述为一个变分问题，并提出了一种从求解算法中展开的可学习网络。为了提高细节细化性能，引入了鉴别器。在体HR多线圈脑数据的实验结果表明，该SRR网络能够恢复高分辨率的脑图像，具有良好的视觉质量和感知质量。
<details>	<summary>英文摘要</summary>	Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 65、走向快速准确的真实世界深度超分辨率：基准数据集和基线
- [ ] Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline 
时间：2021年04月13日                         第一作者：Lingzhi He                       [链接](https://arxiv.org/abs/2104.06174).                     
## 摘要：商业深度传感器获取的深度图分辨率较低，难以用于各种计算机视觉任务。因此，深度图超分辨率（SR）是一项实用而有价值的工作，它可以将深度图提升到高分辨率（HR）空间。然而，由于缺乏真实世界的成对低分辨率（LR）和HR深度图，现有的方法大多采用降采样来获得成对的训练样本。为此，我们首先构建了一个名为RGB-D-D的大规模数据集，它可以极大地促进深度图SR的研究，甚至可以促进更多与深度相关的实际任务。数据集中的“D-D”表示从手机和Lucid Helios分别捕获的成对LR和HR深度图，范围从室内场景到具有挑战性的室外场景。此外，我们提供了一个快速的深度图超分辨率（FDSR）基线，其中高频分量从RGB图像自适应分解来指导深度图超分辨率。在现有公共数据集上的大量实验证明了我们的网络与现有方法相比的有效性和效率。此外，对于真实的LR深度图，我们的算法可以生成更精确的HR深度图，边界更清晰，并且在一定程度上修正了深度值误差。
<details>	<summary>英文摘要</summary>	Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which upscales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use downsampling to obtain paired training samples. To this end, we first construct a large-scale dataset named "RGB-D-D", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The "D-D" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 64、混叠是你的盟友：从原始图像突发端到端的超分辨率
- [ ] Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts 
时间：2021年04月13日                         第一作者：Bruno Lecouat                       [链接](https://arxiv.org/abs/2104.06191).                     
## 摘要：本演示解决了从空间和时间上稍微不同的视点捕获的多个低分辨率快照重建高分辨率图像的问题。解决这个问题的关键挑战包括（i）以亚像素精度对齐输入图片，（ii）处理原始（噪声）图像以最大程度地忠实于本地相机数据，以及（iii）设计/学习非常适合该任务的图像先验（正则化器）。基于Wronski等人的见解，我们采用一种混合算法来解决这三个难题。在这种情况下，混叠是一个盟友，参数可以端到端地学习，同时保留了反问题经典方法的可解释性。我们的方法在合成和真实图像突发上的有效性得到了证明，在几个基准上建立了一个新的技术状态，并在智能手机和prosumer相机捕获的真实原始突发上提供了极好的定性结果。
<details>	<summary>英文摘要</summary>	This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras. </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 63、利用低分辨率流和掩模上采样实现高效的时空视频超分辨率
- [ ] Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling 
时间：2021年04月12日                         第一作者：Saikat Dutta                       [链接](https://arxiv.org/abs/2104.05778).                     
## 摘要：针对低分辨率、低帧速率的慢动作视频，提出了一种高效的时空超分辨率解决方案。一个简单的解决方案是连续运行视频超分辨率和视频帧插值模型。然而，这类解的内存效率低，推理时间长，不能充分利用时空关系的性质。为此，我们首先利用二次模型在LR空间中进行插值。使用最先进的视频超分辨率方法对输入LR帧进行超分辨率。利用双线性上采样技术，在HR空间重用用于合成LR插值帧的流程图和混合模板。这导致HR中间帧的粗略估计，该中间帧通常包含沿运动边界的伪影。通过残差学习，利用细化网络提高HR中间帧的质量。我们的模型是轻量级的，在REDS-STSR验证集中的性能比目前最先进的模型要好。
<details>	<summary>英文摘要</summary>	This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set. </details>
<details>	<summary>注释</summary>	Accepted at NTIRE Workshop, CVPR 2021. Project page: https://github.com/saikatdutta/FMU_STSR </details>
<details>	<summary>邮件日期</summary>	2021年04月14日</details>

# 62、基于深度学习的超分辨率网络边缘感知图像压缩
- [ ] Edge-Aware Image Compression using Deep Learning-based Super-resolution Network 
时间：2021年04月11日                         第一作者：Dipti Mishra                       [链接](https://arxiv.org/abs/2104.04926).                     
## 摘要：我们提出了一种基于学习的压缩方案，在预处理和后处理的深度cnn之间封装一个标准的编解码器。具体地说，我们通过引入：（a）一个边缘感知损失函数来防止在以前的工作中经常出现的模糊&（b）一个用于后处理的超分辨率卷积神经网络（CNN）以及一个相应的预处理网络，来展示对使用压缩-解压缩网络的先前方法的改进在低速率下提高率失真性能。该算法在从低分辨率到高分辨率的各种数据集上进行评估，即Set 5、Set 7、Classic 5、Set 14、Live 1、Kodak、General 100、CLIC 2019。与JPEG、JPEG2000、BPG和最近的CNN方法相比，该算法在低码率和高码率下的峰值信噪比分别提高了20.75%、8.47%、3.22%、3.23%和24.59%、14.46%、10.14%和8.57%。同样，在低比特率和高比特率下，MS-SSIM的这种改进分别约为71.43%、50%、36.36%、23.08%、64.70%和64.47%、61.29%、47.06%、51.52%和16.28%。使用CLIC 2019数据集，在低比特率和高比特率下，PSNR分别约为16.67%、10.53%、6.78%和24.62%、17.39%和14.08%，优于JPEG2000、BPG和最近的CNN方法。同样，与相同的方法相比，MS-SSIM在低比特率和高比特率下的性能分别约为72%、45.45%、39.13%、18.52%和71.43%、50%、41.18%和17.07%。其他数据集也实现了类似的改进。
<details>	<summary>英文摘要</summary>	We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%, 61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%, 10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%, and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively, compared to the same approaches. A similar type of improvement is achieved with other datasets also. </details>
<details>	<summary>注释</summary>	13 pages, 9 figures, 16 tables </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 61、CoPE：使用多项式展开的条件图像生成
- [ ] CoPE: Conditional image generation using Polynomial Expansions 
时间：2021年04月11日                         第一作者：Grigorios G Chrysos                       [链接](https://arxiv.org/abs/2104.05077).                     
## 摘要：生成建模已经发展成为机器学习的一个重要领域。深度多项式神经网络（PNNs）在无监督图像生成中取得了令人印象深刻的结果，其任务是将输入向量（即噪声）映射到合成图像。然而，PNNs的成功还没有在超分辨率等条件生成任务中得到推广。现有的pnn主要集中在单变量多项式展开上，对于两个变量的输入，即噪声变量和条件变量，pnn表现不好。在这项工作中，我们引入了一个通用的框架，称为CoPE，它可以对两个输入变量进行多项式展开，并捕捉它们的自相关和互相关。我们展示了CoPE如何被简单地扩充以接受任意数量的输入变量。CoPE分为五个任务（类条件生成、反问题、边缘到图像的转换、图像到图像的转换、属性引导生成），涉及八个数据集。全面评估表明，CoPE可用于处理各种条件生成任务。
<details>	<summary>英文摘要</summary>	Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 60、作物类型语义切分的语境自对比预训练
- [ ] Context-self contrastive pretraining for crop type semantic segmentation 
时间：2021年04月09日                         第一作者：Michail Tarasiou                       [链接](https://arxiv.org/abs/2104.04310).                     
## 摘要：本文提出了一种基于对比学习的全监督预训练方案，特别适合于密集分类任务。提出的上下文自对比丢失（CSCL）算法利用训练样本中每个位置与其局部上下文之间的相似性度量，学习一个使语义边界弹出的嵌入空间。对于卫星图像中作物类型的语义分割，我们发现包裹边界的性能是一个关键的瓶颈，并解释了CSCL如何解决该问题的根本原因，从而提高该任务的最新性能。此外，利用Sentinel-2（S2）卫星任务的图像，我们编制了据我们所知最大的卫星图像时间序列数据集，这些数据集由作物类型和包裹标识密集标注，我们与数据生成管道一起公开。利用这些数据，我们发现CSCL，即使在最小的预训练下，也可以改善所有的基线，并提出了一个超分辨率的语义分割过程，以获得更细粒度的作物类。该方法在二维和三维立体图像的语义分割任务中得到了进一步的验证，结果表明，该方法在竞争性基线下的性能得到了一致的提高。
<details>	<summary>英文摘要</summary>	In this paper we propose a fully-supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in an training sample and its local context. For crop type semantic segmentation from satellite images we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, dataset of satellite image timeseries densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The proposed method is further validated on the task of semantic segmentation on 2D and 3D volumetric images showing consistent performance improvements upon competitive baselines. </details>
<details>	<summary>注释</summary>	11 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 59、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月09日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 58、加法器：迈向节能图像超分辨率
- [ ] AdderSR: Towards Energy Efficient Image Super-Resolution 
时间：2021年04月09日                         第一作者：Dehua Song                       [链接](https://arxiv.org/abs/2009.08891).                     
<details>	<summary>邮件日期</summary>	2021年04月12日</details>

# 57、基于条件元网络的多重退化盲超分辨算法
- [ ] Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations 
时间：2021年04月08日                         第一作者：Guanghao Yin                       [链接](https://arxiv.org/abs/2104.03926).                     
## 摘要：虽然单图像超分辨率（single-image super-resolution，SISR）方法在单次退化方面取得了很大的成功，但在实际应用中仍存在性能下降和多重退化的问题。近年来，人们对多重降解的盲模型和非盲模型进行了研究。然而，由于训练数据和测试数据之间的分布变化，这些方法通常会显著降低性能。为此，我们首次提出了一个条件元网络框架（CMDSR），帮助SR框架学习如何适应输入分布的变化。我们利用所提出的条件网在任务级提取退化先验，以适应基本SR网络（BaseNet）的参数。具体地说，我们框架的ConditionNet首先从一个支持集学习退化先验知识，该支持集由来自同一任务的一系列退化图像块组成。然后自适应基网根据条件特征快速地改变其参数。此外，为了更好地提取退化先验信息，我们提出了一种任务对比损失的方法来减小任务内部的距离，增加任务级特征之间的跨任务距离。在不预先定义退化映射的情况下，我们的盲框架可以进行单参数更新，从而产生可观的SR结果。大量的实验证明了CMDSR在各种盲甚至非盲方法中的有效性。灵活的基本网结构也表明CMDSR可以作为一个大系列SISR模型的通用框架。
<details>	<summary>英文摘要</summary>	Although single-image super-resolution (SISR) methods have achieved great success on single degradation, they still suffer performance drop with multiple degrading effects in real scenarios. Recently, some blind and non-blind models for multiple degradations have been explored. However, those methods usually degrade significantly for distribution shifts between the training and test data. Towards this end, we propose a conditional meta-network framework (named CMDSR) for the first time, which helps SR framework learn how to adapt to changes in input distribution. We extract degradation prior at task-level with the proposed ConditionNet, which will be used to adapt the parameters of the basic SR network (BaseNet). Specifically, the ConditionNet of our framework first learns the degradation prior from a support set, which is composed of a series of degraded image patches from the same task. Then the adaptive BaseNet rapidly shifts its parameters according to the conditional features. Moreover, in order to better extract degradation prior, we propose a task contrastive loss to decrease the inner-task distance and increase the cross-task distance between task-level features. Without predefining degradation maps, our blind framework can conduct one single parameter update to yield considerable SR results. Extensive experiments demonstrate the effectiveness of CMDSR over various blind, even non-blind methods. The flexible BaseNet structure also reveals that CMDSR can be a general framework for large series of SISR models. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 56、太阳电池检测的联合超分辨与校正
- [ ] Joint Super-Resolution and Rectification for Solar Cell Inspection 
时间：2021年04月07日                         第一作者：Mathis Hoffmann                       [链接](https://arxiv.org/abs/2011.05003).                     
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 55、BasicVSR：寻找视频超分辨率及更高分辨率的关键组件
- [ ] BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond 
时间：2021年04月07日                         第一作者：Kelvin C.K. Chan                       [链接](https://arxiv.org/abs/2012.02181).                     
<details>	<summary>注释</summary>	CVPR 2021 camera-ready </details>
<details>	<summary>邮件日期</summary>	2021年04月08日</details>

# 54、基于内容自适应超分辨率的高效视频压缩
- [ ] Efficient Video Compression via Content-Adaptive Super-Resolution 
时间：2021年04月06日                         第一作者：Mehrdad Khani                       [链接](https://arxiv.org/abs/2104.02322).                     
## 摘要：视频压缩是互联网视频传输的重要组成部分。最近的研究表明，深度学习技术可以与人类设计的算法相媲美或优于人类设计的算法，但这些方法的计算效率和功耗明显低于现有的编解码器。本文提出了一种新的方法，通过一个小的、内容自适应的超分辨率模型来增强现有的编解码器，从而显著提高视频质量。我们的方法，SRVC，将视频编码成两个比特流：（i）内容流，通过使用现有的编解码器压缩下采样的低分辨率视频产生，（ii）模型流，对为视频的短片段定制的轻量级超分辨率神经网络的周期性更新进行编码。SRVC通过将解压缩后的低分辨率视频帧通过（时变）超分辨率模型来重构高分辨率视频帧，从而对视频进行解码。我们的结果表明，为了获得相同的PSNR，SRVC在慢模式下需要H.265每像素16%的比特，而在DVC（一种新的基于深度学习的视频压缩方案）中需要2%的每像素比特。SRVC在nvidiav100gpu上以每秒90帧的速度运行。
<details>	<summary>英文摘要</summary>	Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 53、超分辨率的测试时间调整：你只需要在更多的图像上过度调整
- [ ] Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a Few More Images 
时间：2021年04月06日                         第一作者：Mohammad Saeed Rad                       [链接](https://arxiv.org/abs/2104.02663).                     
## 摘要：现有的基于参考（RF）的超分辨率（SR）模型试图在高分辨率RF图像与低分辨率（LR）输入匹配的假设下提高SR的感知质量。由于射频图像在内容、颜色、对比度等方面应与测试图像相似，这妨碍了其在实际场景中的适用性。其他提高图像感知质量的方法，包括感知损失和对抗损失，往往通过显著降低PSNR/SSIM来显著降低对地面真实感的保真度。针对这两个问题，我们提出了一种简单而通用的方法，通过进一步微调训练数据集中具有与初始HR预测相似激活模式的图像子集上的SR网络，来提高预先训练的SR网络对给定LR输入的HR预测的感知质量，关于特征提取器的过滤器。特别地，我们从感知质量和PSNR/SSIM值方面展示了微调对这些图像的影响。与感知驱动的方法相反，我们证明了微调网络产生的HR预测具有更高的感知质量和相对于初始HR预测的PSNR/SSIM的最小变化。此外，我们还提出了有关随机共振网络滤波器的新的数值实验，通过滤波器相关性，我们表明，与基线网络或随机图像上微调的网络相比，我们方法中微调网络的滤波器更接近“理想”滤波器。
<details>	<summary>英文摘要</summary>	Existing reference (RF)-based super-resolution (SR) models try to improve perceptual quality in SR under the assumption of the availability of high-resolution RF images paired with low-resolution (LR) inputs at testing. As the RF images should be similar in terms of content, colors, contrast, etc. to the test image, this hinders the applicability in a real scenario. Other approaches to increase the perceptual quality of images, including perceptual loss and adversarial losses, tend to dramatically decrease fidelity to the ground-truth through significant decreases in PSNR/SSIM. Addressing both issues, we propose a simple yet universal approach to improve the perceptual quality of the HR prediction from a pre-trained SR network on a given LR input by further fine-tuning the SR network on a subset of images from the training dataset with similar patterns of activation as the initial HR prediction, with respect to the filters of a feature extractor. In particular, we show the effects of fine-tuning on these images in terms of the perceptual quality and PSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate that the fine-tuned network produces a HR prediction with both greater perceptual quality and minimal changes to the PSNR/SSIM with respect to the initial HR prediction. Further, we present novel numerical experiments concerning the filters of SR networks, where we show through filter correlation, that the filters of the fine-tuned network from our method are closer to "ideal" filters, than those of the baseline network or a network fine-tuned on random images. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 52、深脉冲超分辨率
- [ ] Deep Burst Super-Resolution 
时间：2021年04月06日                         第一作者：Goutam Bhat                        [链接](https://arxiv.org/abs/2101.10997).                     
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 51、基于注意的层次多模态融合高分辨率深度图成像
- [ ] High-resolution Depth Maps Imaging via Attention-based Hierarchical Multi-modal Fusion 
时间：2021年04月04日                         第一作者：Zhiwei Zhong                       [链接](https://arxiv.org/abs/2104.01530).                     
## 摘要：深度图记录了场景中视点和物体之间的距离，在许多实际应用中起着至关重要的作用。然而，消费者级RGB-D相机拍摄的深度图空间分辨率较低。引导深度图超分辨率（DSR）是解决这一问题的一种常用方法，它试图从输入的低分辨率（LR）深度及其作为引导的耦合HR-RGB图像恢复高分辨率（HR）深度图。如何正确地选择和传播一致性结构，正确地处理不一致性结构是指导DSR最具挑战性的问题。本文提出了一种新的基于注意的分层多模态融合（AHMF）网络。具体来说，为了有效地从LR深度和HR引导中提取和组合相关信息，我们提出了一种分层卷积层的多模式基于注意的融合（MMAF）策略，包括一个特征增强块，用于选择有价值的特征；一个特征重新校准块，用于统一具有不同外观特征的模式的相似性度量。在此基础上，提出了一种双向分层特征协作（BHFC）模型，充分利用多尺度特征间的低层空间信息和高层结构信息。实验结果表明，该方法在重建精度、运行速度和存储效率等方面均优于现有方法。
<details>	<summary>英文摘要</summary>	Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance. The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 50、具有光谱混合和异构数据集的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution with Spectral Mixup and Heterogeneous Datasets 
时间：2021年04月03日                         第一作者：Ke Li                       [链接](https://arxiv.org/abs/2101.07589).                     
<details>	<summary>注释</summary>	16 pages, 14 tables, 5 figures; Code available at https://github.com/kli8996/HSISR </details>
<details>	<summary>邮件日期</summary>	2021年04月06日</details>

# 49、用于学习失调光学变焦的平方变形对准网络
- [ ] SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom 
时间：2021年04月02日                         第一作者：Kangfu Mei                       [链接](https://arxiv.org/abs/2104.00848).                     
## 摘要：基于深度神经网络（DNN）的超分辨率算法大大提高了生成图像的质量。然而，由于学习失调光学变焦的困难，这些算法在处理真实世界的超分辨率问题时往往会产生明显的伪影。为了解决这一问题，本文提出了一种平方变形对准网络（SDAN）。我们的网络学习卷积核的每点平方偏移量，然后根据偏移量在校正的卷积窗口中对齐特征。因此，通过提取对齐的特征，可以最大限度地减少不对齐。与普通可变形卷积网络（DCN）中的逐点偏移不同，本文提出的平方偏移不仅加快了偏移学习，而且在参数较少的情况下提高了生成质量。此外，我们进一步提出一个有效的交叉堆积注意层来提高学习偏移量的准确性。它利用打包和解包操作来扩大偏移量学习的接受域，增强低分辨率图像与参考图像之间空间联系的提取能力。综合实验表明，该方法在计算效率和真实感细节方面均优于其他先进方法。
<details>	<summary>英文摘要</summary>	Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details. </details>
<details>	<summary>注释</summary>	ICME21. Code is available at https://github.com/MKFMIKU/SDAN </details>
<details>	<summary>邮件日期</summary>	2021年04月05日</details>

# 48、盲超分辨的无监督退化表示学习
- [ ] Unsupervised Degradation Representation Learning for Blind Super-Resolution 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2104.00416).                     
## 摘要：大多数现有的基于CNN的超分辨率（SR）方法都是在假设退化是固定的和已知的（例如双三次下采样）的基础上发展起来的。然而，当实际性能下降与假设不同时，这些方法的性能会严重下降。在实际应用中，为了处理各种未知的退化，以往的方法都是依靠退化估计来重建SR图像。然而，退化估计方法通常是耗时的，并且可能由于较大的估计误差而导致SR失效。本文提出了一种无监督退化表示学习方案，用于盲随机共振，无需显式退化估计。具体来说，我们学习抽象表示来区分表示空间中的各种退化，而不是像素空间中的显式估计。此外，我们还提出了一种基于学习表示的退化感知SR（DASR）网络，该网络能够灵活地适应各种退化。实验结果表明，我们的退化表征学习方法可以提取有区别的表征来获得精确的退化信息。在合成图像和真实图像上的实验表明，我们的网络对于盲SR任务达到了最先进的性能。代码位于：https://github.com/LongguangWang/DASR。
<details>	<summary>英文摘要</summary>	Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To handle various unknown degradations in real-world applications, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation estimation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this paper, we propose an unsupervised degradation representation learning scheme for blind SR without explicit degradation estimation. Specifically, we learn abstract representations to distinguish various degradations in the representation space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 47、探索图像超分辨率中的稀疏性实现高效推理
- [ ] Exploring Sparsity in Image Super-Resolution for Efficient Inference 
时间：2021年04月01日                         第一作者：Longguang Wang                       [链接](https://arxiv.org/abs/2006.09603).                     
<details>	<summary>注释</summary>	Accepted by CVPR 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 46、通过学习匹配内部斑块分布来估计MR切片轮廓
- [ ] MR Slice Profile Estimation by Learning to Match Internal Patch Distributions 
时间：2021年03月31日                         第一作者：Shuo Han                       [链接](https://arxiv.org/abs/2104.00100).                     
## 摘要：为了超分辨多层面二维磁共振（MR）图像的通平面方向，在训练监督算法时，可以将其切片选择剖面作为高分辨率（HR）到低分辨率（LR）的退化模型来生成成对数据。现有的超分辨率算法对切片选择轮廓进行了假设，因为给定的图像不容易知道它。在这项工作中，我们通过学习匹配其内部补丁分布来估计给定特定图像的切片选择剖面。具体来说，我们假设在应用正确的切片选择轮廓之后，沿着HR平面内方向的图像面片分布应该与沿着LR平面内方向的分布相匹配。因此，我们将切片选择轮廓的估计作为生成对抗网络（GAN）中学习生成器的一部分。这样，就可以在没有任何外部数据的情况下学习切片选择轮廓。我们的算法通过各向同性MR图像的模拟进行了测试，并将其与一种通过平面的超分辨率算法结合起来，以证明其优越性，同时也被用作测量图像分辨率的工具。我们的密码是https://github.com/shuohan/espreso2。
<details>	<summary>英文摘要</summary>	To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, accepted by Information Processing in Medical Imaging (IPMI) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月02日</details>

# 45、视频探索通过视频特定的自动编码器
- [ ] Video Exploration via Video-Specific Autoencoders 
时间：2021年03月31日                         第一作者：Kevin Wang                        [链接](https://arxiv.org/abs/2103.17261).                     
## 摘要：我们提出了简单的视频特定的自动编码器，使人类可控的视频探索。这包括各种各样的分析任务，例如（但不限于）空间和时间超分辨率、空间和时间编辑、对象移除、视频纹理、平均视频探索以及视频内部和跨视频的对应估计。以前的工作已经独立地研究了这些问题，并提出了不同的公式。在这项工作中，我们观察到一个简单的自动编码器训练（从头开始）对多个帧的特定视频，使一个人能够执行各种各样的视频处理和编辑任务。我们的任务是通过两个关键的观察来实现的：（1）由自动编码器学习的潜在代码捕获视频的空间和时间特性；（2）自动编码器可以将样本输入投射到视频特定的流形上。例如：（1）内插潜在代码实现了时间超分辨率和用户可控的视频纹理；（2）流形重投影实现了空间超分辨率、对象移除和去噪，而无需对任何任务进行训练。重要的是，通过主成分分析的潜在代码的二维可视化可以作为用户可视化和直观控制视频编辑的工具。最后，我们将我们的方法与现有技术进行定量对比，发现在没有任何监督和任务特定知识的情况下，我们的方法可以与专门为任务训练的监督方法进行比较。
<details>	<summary>英文摘要</summary>	We present simple video-specific autoencoders that enables human-controllable video exploration. This includes a wide variety of analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, object removal, video textures, average video exploration, and correspondence estimation within and across videos. Prior work has independently looked at each of these problems and proposed different formulations. In this work, we observe that a simple autoencoder trained (from scratch) on multiple frames of a specific video enables one to perform a large variety of video processing and editing tasks. Our tasks are enabled by two key observations: (1) latent codes learned by the autoencoder capture spatial and temporal properties of that video and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. For e.g. (1) interpolating latent codes enables temporal super-resolution and user-controllable video textures; (2) manifold reprojection enables spatial super-resolution, object removal, and denoising without training for any of the tasks. Importantly, a two-dimensional visualization of latent codes via principal component analysis acts as a tool for users to both visualize and intuitively control video edits. Finally, we quantitatively contrast our approach with the prior art and found that without any supervision and task-specific knowledge, our approach can perform comparably to supervised approaches specifically trained for a task. </details>
<details>	<summary>注释</summary>	Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/ </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 44、用于人脸超分辨率的边缘和身份保持网络
- [ ] Edge and Identity Preserving Network for Face Super-Resolution 
时间：2021年03月31日                         第一作者：Jonghyun Kim                       [链接](https://arxiv.org/abs/2008.11977).                     
<details>	<summary>注释</summary>	Neurocomputing'2021 DOI: 10.1016/j.neucom.2021.03.048 </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 43、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月31日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	The first two authors contributed equally to this work. Accepted to CVPR 2021 (camera-ready version) </details>
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 42、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月30日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 41、传递学习：探索盲超分辨退化的传递性
- [ ] Transitive Learning: Exploring the Transitivity of Degradations for Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Yuanfei Huang                       [链接](https://arxiv.org/abs/2103.15290).                     
## 摘要：现有的盲超分辨率（SR）方法由于对数据或模型的迭代估计和校正的依赖性，通常耗时且效率较低。针对这一问题，本文提出了一种基于端到端网络的盲随机共振传递学习方法。首先，我们分析并论证了退化的传递性，包括广泛使用的加法退化和卷积退化。在此基础上，提出了一种新的传递学习方法，通过自适应地推导传递变换函数来求解未知退化问题，而不需要任何迭代操作。具体而言，端到端TLSR网络由传递度（DoT）估计网络、齐次特征提取网络和传递学习模块组成。对盲SR任务的定量和定性评价表明，与现有的盲SR方法相比，本文提出的TLSR具有更好的性能和更少的时间消耗。代码可在https://github.com/YuanfeiHuang/TLSR。
<details>	<summary>英文摘要</summary>	Being extremely dependent on the iterative estimation and correction of data or models, the existing blind super-resolution (SR) methods are generally time-consuming and less effective. To address it, this paper proposes a transitive learning method for blind SR using an end-to-end network without any additional iterations in inference. To begin with, we analyze and demonstrate the transitivity of degradations, including the widely used additive and convolutive degradations. We then propose a novel Transitive Learning method for blind Super-Resolution on transitive degradations (TLSR), by adaptively inferring a transitive transformation function to solve the unknown degradations without any iterative operations in inference. Specifically, the end-to-end TLSR network consists of a degree of transitivity (DoT) estimation network, a homogeneous feature extraction network, and a transitive learning module. Quantitative and qualitative evaluations on blind SR tasks demonstrate that the proposed TLSR achieves superior performance and consumes less time against the state-of-the-art blind SR methods. The code is available at https://github.com/YuanfeiHuang/TLSR. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 40、高细节图像超分辨率的最佳伙伴
- [ ] Best-Buddy GANs for Highly Detailed Image Super-Resolution 
时间：2021年03月29日                         第一作者：Wenbo Li                       [链接](https://arxiv.org/abs/2103.15295).                     
## 摘要：我们考虑了单图像超分辨率（SISR）问题，即基于低分辨率（LR）输入生成高分辨率（HR）图像。近年来，生成性对抗网络（generativediscountarialnetworks，GANs）开始流行于制造幻觉。沿着这条路线的大多数方法依赖于预定义的单个LR-single-HR映射，这对于SISR任务来说不够灵活。而且，甘生成的虚假细节往往会破坏整个图像的真实感。我们通过为丰富的细节SISR提出bestbuddygan（bebygan）来解决这些问题。放松了不可变的一对一约束，使得估计出的补丁在训练过程中动态地寻求最佳监督，有利于产生更合理的细节。此外，我们提出了一种区域感知的对抗式学习策略，使我们的模型能够自适应地生成纹理区域的细节。大量的实验证明了我们方法的有效性。同时还构建了一个超高分辨率4K数据集，为今后的超分辨率研究提供了便利。
<details>	<summary>英文摘要</summary>	We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the immutable one-to-one constraint, we allow the estimated patches to dynamically seek the best supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 39、全知视频超分辨率
- [ ] Omniscient Video Super-Resolution 
时间：2021年03月29日                         第一作者：Peng Yi                        [链接](https://arxiv.org/abs/2103.15683).                     
## 摘要：最新的视频超分辨率（SR）方法要么采用迭代的方式来处理来自时间滑动窗口的低分辨率（LR）帧，要么利用先前估计的SR输出来帮助循环地重构当前帧。一些研究试图将这两种结构结合起来形成一个混合框架，但未能充分发挥其作用。在本文中，我们提出了一个全知的框架，不仅可以利用前面的SR输出，还可以利用现在和将来的SR输出。全知框架更具通用性，因为迭代框架、循环框架和混合框架可以看作它的特例。提出的全知框架使得生成器比其他框架下的生成器表现得更好。在公共数据集上的大量实验表明，该方法在客观度量、主观视觉效果和复杂度方面均优于现有的方法。我们的代码将会公开。
<details>	<summary>英文摘要</summary>	Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity. Our code will be made public. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 38、交叉MPI：利用多平面图像实现图像超分辨率的交叉尺度立体成像
- [ ] Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images 
时间：2021年03月29日                         第一作者：Yuemei Zhou                       [链接](https://arxiv.org/abs/2011.14631).                     
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 37、基于隐式场景表示的现场场景标注与理解
- [ ] In-Place Scene Labelling and Understanding with Implicit Scene Representation 
时间：2021年03月29日                         第一作者：Shuaifeng Zhi                       [链接](https://arxiv.org/abs/2103.15875).                     
## 摘要：语义标注与几何和辐射重建高度相关，因为形状和外观相似的场景实体更可能来自相似的类。最近的隐式神经重建技术是有吸引力的，因为他们不需要事先训练数据，但同样的完全自我监督的方法是不可能的语义，因为标签是人类定义的属性。我们扩展了神经辐射场（NeRF）技术，将语义与外观和几何信息进行联合编码，从而使用少量的特定场景的就地标注就可以得到完整、准确的二维语义标注。NeRF内在的多视图一致性和平滑性使得稀疏标签能够有效地传播，从而有利于语义。我们展示了这种方法的好处，当标签是稀疏或非常嘈杂的房间规模的场景。在视觉语义映射系统中，我们展示了它在各种有趣的应用中的优势，如高效的场景标注工具、新颖的语义视图合成、标签去噪、超分辨率、标签插值和多视图语义标签融合。
<details>	<summary>英文摘要</summary>	Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems. </details>
<details>	<summary>注释</summary>	Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/ </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 36、基于流的核先验算法及其在盲超分辨中的应用
- [ ] Flow-based Kernel Prior with Application to Blind Super-Resolution 
时间：2021年03月29日                         第一作者：Jingyun Liang                       [链接](https://arxiv.org/abs/2103.15977).                     
## 摘要：核估计通常是盲图像超分辨率（SR）的关键问题之一。最近，Double-DIP提出通过网络结构对核进行建模，KernelGAN则采用深度线性网络和一些正则化损失来约束核空间。然而，他们没有充分利用一般的SR核假设，即各向异性高斯核对图像SR是足够的。为了解决这个问题，本文提出了一种基于归一化流的核先验（FKP）核建模方法。通过学习各向异性高斯核分布和可处理的潜在分布之间的可逆映射，FKP可以很容易地代替双倾角和KernelGAN的核建模模块。具体地说，FKP在隐空间而不是网络参数空间中对核进行优化，从而生成合理的核初始化，遍历学习到的核流形，提高优化的稳定性。在合成图像和真实图像上的大量实验表明，所提出的FKP算法可以在较少的参数、运行时间和内存使用的情况下显著提高核估计精度，从而得到最新的盲SR结果。
<details>	<summary>英文摘要</summary>	Kernel estimation is generally one of the key problems for blind image super-resolution (SR). Recently, Double-DIP proposes to model the kernel via a network architecture prior, while KernelGAN employs the deep linear network and several regularization losses to constrain the kernel space. However, they fail to fully exploit the general SR kernel assumption that anisotropic Gaussian kernels are sufficient for image SR. To address this issue, this paper proposes a normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an invertible mapping between the anisotropic Gaussian kernel distribution and a tractable latent distribution, FKP can be easily used to replace the kernel modeling modules of Double-DIP and KernelGAN. Specifically, FKP optimizes the kernel in the latent space rather than the network parameter space, which allows it to generate reasonable kernel initialization, traverse the learned kernel manifold and improve the optimization stability. Extensive experiments on synthetic and real-world images demonstrate that the proposed FKP can significantly improve the kernel estimation accuracy with less parameters, runtime and memory usage, leading to state-of-the-art blind SR results. </details>
<details>	<summary>注释</summary>	Accepted by CVPR2021. Code: https://github.com/JingyunLiang/FKP </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 35、去噪去噪超分辨率流水线的再思考
- [ ] Rethinking the Pipeline of Demosaicing, Denoising and Super-Resolution 
时间：2021年03月29日                         第一作者：Guocheng Qian                        [链接](https://arxiv.org/abs/1905.02538).                     
<details>	<summary>注释</summary>	Code is available at: https://github.com/guochengqian/TENet </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 34、批量归一化单幅图像超分辨率网络的快速贝叶斯不确定性估计与约简
- [ ] Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network 
时间：2021年03月28日                         第一作者：Aupendu Kar                        [链接](https://arxiv.org/abs/1903.09410).                     
<details>	<summary>注释</summary>	To appear in the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 33、D2C-SR：一种从散度到收敛的图像超分辨率方法
- [ ] D2C-SR: A Divergence to Convergence Approach for Image Super-Resolution 
时间：2021年03月26日                         第一作者：Youwei Li                       [链接](https://arxiv.org/abs/2103.14373).                     
## 摘要：本文提出了一种新的图像超分辨率（SR）框架D2C-SR。作为一个不适定问题，超分辨率相关任务的关键挑战是对于给定的低分辨率输入可以有多个预测。大多数经典的方法和早期的基于深度学习的方法忽略了这一基本事实，将这个问题建模为一个确定性的过程，这往往导致不满意的结果。受最近SRFlow等工作的启发，我们采用半概率的方法解决了这一问题，并提出了一种两阶段流水线：一个发散阶段用于学习离散形式的高分辨率输出的分布，然后一个收敛阶段用于将学习到的预测融合成最终的输出。更具体地说，我们提出了一种基于树结构的深度网络，其中每个分支被设计来学习可能的高分辨率预测。在发散阶段，对每个分支分别进行训练以拟合地面真值，并用三重损失来增强发散分支的输出。随后，我们添加一个保险丝模块来组合多个预测，因为第一阶段的输出可能是次优的。可以训练引信模块以端到端的方式将w.r.t收敛到最终的高分辨率图像。我们对几个基准进行了评估，包括一个新提出的具有8倍放大因子的数据集。我们的实验表明，D2C-SR可以在峰值信噪比和SSIM上实现最先进的性能，并且计算量显著减少。
<details>	<summary>英文摘要</summary>	In this paper, we present D2C-SR, a novel framework for the task of image super-resolution(SR). As an ill-posed problem, the key challenge for super-resolution related tasks is there can be multiple predictions for a given low-resolution input. Most classical methods and early deep learning based approaches ignored this fundamental fact and modeled this problem as a deterministic processing which often lead to unsatisfactory results. Inspired by recent works like SRFlow, we tackle this problem in a semi-probabilistic manner and propose a two-stage pipeline: a divergence stage is used to learn the distribution of underlying high-resolution outputs in a discrete form, and a convergence stage is followed to fuse the learned predictions into a final output. More specifically, we propose a tree-based structure deep network, where each branch is designed to learn a possible high-resolution prediction. At the divergence stage, each branch is trained separately to fit ground truth, and a triple loss is used to enforce the outputs from different branches divergent. Subsequently, we add a fuse module to combine the multiple predictions as the outputs from the first stage can be sub-optimal. The fuse module can be trained to converge w.r.t the final high-resolution image in an end-to-end manner. We conduct evaluations on several benchmarks, including a new proposed dataset with 8x upscaling factor. Our experiments demonstrate that D2C-SR can achieve state-of-the-art performance on PSNR and SSIM, with a significantly less computational cost. </details>
<details>	<summary>注释</summary>	14 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 32、非对称CNN图像超分辨率分析
- [ ] Asymmetric CNN for image super-resolution 
时间：2021年03月25日                         第一作者：Chunwei Tian                       [链接](https://arxiv.org/abs/2103.13634).                     
## 摘要：近五年来，深度卷积神经网络（CNNs）被广泛应用于低层视觉。根据不同应用的特点，设计合适的CNN结构。然而，定制的体系结构通过对所有像素点的等价处理来聚集不同的特征，从而提高了应用的性能，忽略了局部功率像素点的影响，导致训练效率低下。在本文中，我们提出了一个非对称CNN（ACNet）包括一个非对称块（AB），一个mem？图像超分辨率增强块（MEB）和高频特征增强块（HFFEB）。该算法利用一维非对称卷积，在水平方向和垂直方向上增强平方卷积核，以增强局部显著特征对SISR的影响。MEB通过残差学习（RL）技术融合AB的所有分层低频特征，以解决长期依赖问题，并转换得到的低频fea？转换成高频特性。HFFEB利用低频和高频特征来获得更健壮的超分辨率特征，并解决过多的特征增强问题。广告？另外，它还负责重建高分辨率（HR）图像。大量实验表明，该网络能有效地解决单图像超分辨率（SISR）、盲SISR和盲噪声的盲SISR问题。ACNet的代码如所示https://github.com/hellloxiaotian/ACNet。
<details>	<summary>英文摘要</summary>	Deep convolutional neural networks (CNNs) have been widely applied for low-level vision over the past five years. According to nature of different applications, designing appropriate CNN architectures is developed. However, customized architectures gather different features via treating all pixel points as equal to improve the performance of given application, which ignores the effects of local power pixel points and results in low training efficiency. In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric block (AB), a mem?ory enhancement block (MEB) and a high-frequency feature enhancement block (HFFEB) for image super-resolution. The AB utilizes one-dimensional asymmetric convolutions to intensify the square convolution kernels in horizontal and vertical directions for promoting the influences of local salient features for SISR. The MEB fuses all hierarchical low-frequency features from the AB via residual learning (RL) technique to resolve the long-term dependency problem and transforms obtained low-frequency fea?tures into high-frequency features. The HFFEB exploits low- and high-frequency features to obtain more robust super-resolution features and address excessive feature enhancement problem. Ad?ditionally, it also takes charge of reconstructing a high-resolution (HR) image. Extensive experiments show that our ACNet can effectively address single image super-resolution (SISR), blind SISR and blind SISR of blind noise problems. The code of the ACNet is shown at https://github.com/hellloxiaotian/ACNet. </details>
<details>	<summary>注释</summary>	Blind Super-resolution; Blind Super-resolution with unknown noise </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 31、JDSR-GAN：构建联合协作的蒙面超分辨率学习网络
- [ ] JDSR-GAN: Constructing A Joint and Collaborative Learning Network for Masked Face Super-Resolution 
时间：2021年03月25日                         第一作者：Guangwei Gao                       [链接](https://arxiv.org/abs/2103.13676).                     
## 摘要：随着预防COVID-19病毒的重要性日益增强，在大多数视频监控场景中获得的人脸图像都是低分辨率的。然而，以往的人脸超分辨率算法大多不能在一个模型中同时处理两个任务。本文将遮罩遮挡视为图像噪声，构建了一个联合协作学习网络JDSR-GAN，用于遮罩人脸的超分辨率识别。给定一幅以掩模为输入的低质量人脸图像，由去噪模块和超分辨率模块组成的发生器的作用是获取高质量的高分辨率人脸图像。鉴别器利用了一些精心设计的损失函数来保证恢复的人脸图像的质量。此外，我们将身份信息和注意机制融入到我们的网络中，以实现可行的相关特征表达和信息性特征学习。通过联合进行去噪和人脸超分辨率处理，这两个任务可以相互补充，获得良好的性能。大量的定性和定量结果表明，我们提出的JDSR-GAN方法优于一些分别执行前两个任务的可比较方法。
<details>	<summary>英文摘要</summary>	With the growing importance of preventing the COVID-19 virus, face images obtained in most video surveillance scenarios are low resolution with mask simultaneously. However, most of the previous face super-resolution solutions can not handle both tasks in one model. In this work, we treat the mask occlusion as image noise and construct a joint and collaborative learning network, called JDSR-GAN, for the masked face super-resolution task. Given a low-quality face image with the mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some comparable methods which perform the previous two tasks separately. </details>
<details>	<summary>注释</summary>	24 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 30、噪声数据的多帧超分辨率分析
- [ ] Multi-frame Super-resolution from Noisy Data 
时间：2021年03月25日                         第一作者：Kireeti Bodduna                        [链接](https://arxiv.org/abs/2103.13778).                     
## 摘要：由于问题的病态性，从低分辨率数据中获得高分辨率图像在算法上具有挑战性。到目前为止，这类问题几乎没有得到解决，现有的一些方法使用了简单的正则化方法。我们证明了两种基于各向异性扩散思想的自适应正则化方法的有效性：除了评估经典的边缘增强各向异性扩散正则化方法外，我们还引入了一种新的非局部正则化方法。这被称为部门扩散。我们将其与经典超分辨率观测模型的所有六种变体结合起来，这些变体是由其三种扭曲、模糊和下采样算子的排列产生的。令人惊讶的是，在实际相关的噪声场景中进行的评估产生的排名与我们之前工作（SSVM 2017）中在无噪声环境中的排名不同。
<details>	<summary>英文摘要</summary>	Obtaining high resolution images from low resolution data with clipped noise is algorithmically challenging due to the ill-posed nature of the problem. So far such problems have hardly been tackled, and the few existing approaches use simplistic regularisers. We show the usefulness of two adaptive regularisers based on anisotropic diffusion ideas: Apart from evaluating the classical edge-enhancing anisotropic diffusion regulariser, we introduce a novel non-local one with one-sided differences and superior performance. It is termed sector diffusion. We combine it with all six variants of the classical super-resolution observational model that arise from permutations of its three operators for warping, blurring, and downsampling. Surprisingly, the evaluation in a practically relevant noisy scenario produces a different ranking than the one in the noise-free setting in our previous work (SSVM 2017). </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 29、一种实用的深盲图像超分辨率退化模型的设计
- [ ] Designing a Practical Degradation Model for Deep Blind Image Super-Resolution 
时间：2021年03月25日                         第一作者：Kai Zhang                       [链接](https://arxiv.org/abs/2103.14006).                     
## 摘要：人们普遍认为，如果假设的退化模型与真实图像中的退化模型相背离，单图像超分辨率（SISR）方法将无法取得很好的效果。虽然有几种退化模型考虑了模糊等其他因素，但它们仍然不能有效地覆盖真实图像的各种退化。针对这一问题，本文提出了一种更为复杂但实用的退化模型，该模型由随机混洗模糊、下采样和噪声退化三部分组成。具体地说，模糊由两个具有各向同性和各向异性高斯核的卷积来逼近；下采样从最近点、双线性和双三次插值中随机选择；噪声由不同噪声级的高斯噪声叠加而成，采用不同质量因子的JPEG压缩，通过逆前向摄像机图像信号处理（ISP）流水线模型和原始图像噪声模型生成处理后的摄像机传感器噪声。为了验证新退化模型的有效性，我们训练了一个深度盲ESRGAN超分解器，并将其应用于不同退化程度的合成图像和真实图像的超分辨。实验结果表明，新的退化模型有助于提高深超旋转变压器的实用性，为实际SISR应用提供了一种强有力的替代方案。
<details>	<summary>英文摘要</summary>	It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ESRGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications. </details>
<details>	<summary>注释</summary>	Code: https://github.com/cszn/BSRGAN </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 28、基于物理激励下采样核的内窥镜零炮超分辨
- [ ] Zero-shot super-resolution with a physically-motivated downsampling kernel for endomicroscopy 
时间：2021年03月25日                         第一作者：Agnieszka Barbara Szczotka                       [链接](https://arxiv.org/abs/2103.14015).                     
## 摘要：随着卷积神经网络（CNNs）的发展，超分辨率（SR）方法得到了长足的发展。CNNs已被成功应用于提高内镜成像质量。然而，内窥镜下SR研究的固有局限性仍然是缺乏地面真实高分辨率（HR）图像，通常用于监督训练和基于参考的图像质量评估（IQA）。因此，替代方法，如无监督SR正在探索中。为了解决非参考图像质量改善的需要，我们设计了一种新的零炮超分辨率（ZSSR）方法，该方法仅依赖于内窥镜数据，不需要地面真实图像，而是以自我监督的方式进行处理。我们根据内窥镜的特殊性定制了建议的管道，引入了两种方法：一种物理激励的Voronoi降尺度核，用于解释内窥镜基于不规则纤维的采样模式和真实的噪声模式。我们还利用视频序列来开发一个图像序列，以提高自监督零拍图像的质量。我们进行了烧蚀研究，以评估我们在缩小核尺度和噪声模拟方面的贡献。我们在合成数据和原始数据上验证了我们的方法。合成实验用基于参考的IQA进行评估，而我们对原始图像的结果则在由专家和非专家观察者进行的用户研究中进行评估。结果表明，ZSSR重建的图像质量优于基线方法。与有监督的单幅图像SR相比，ZSSR具有很强的竞争力，尤其是作为专家们首选的重建技术。
<details>	<summary>英文摘要</summary>	Super-resolution (SR) methods have seen significant advances thanks to the development of convolutional neural networks (CNNs). CNNs have been successfully employed to improve the quality of endomicroscopy imaging. Yet, the inherent limitation of research on SR in endomicroscopy remains the lack of ground truth high-resolution (HR) images, commonly used for both supervised training and reference-based image quality assessment (IQA). Therefore, alternative methods, such as unsupervised SR are being explored. To address the need for non-reference image quality improvement, we designed a novel zero-shot super-resolution (ZSSR) approach that relies only on the endomicroscopy data to be processed in a self-supervised manner without the need for ground-truth HR images. We tailored the proposed pipeline to the idiosyncrasies of endomicroscopy by introducing both: a physically-motivated Voronoi downscaling kernel accounting for the endomicroscope's irregular fibre-based sampling pattern, and realistic noise patterns. We also took advantage of video sequences to exploit a sequence of images for self-supervised zero-shot image quality improvement. We run ablation studies to assess our contribution in regards to the downscaling kernel and noise simulation. We validate our methodology on both synthetic and original data. Synthetic experiments were assessed with reference-based IQA, while our results for original images were evaluated in a user study conducted with both expert and non-expert observers. The results demonstrated superior performance in image quality of ZSSR reconstructions in comparison to the baseline method. The ZSSR is also competitive when compared to supervised single-image SR, especially being the preferred reconstruction technique by experts. </details>
<details>	<summary>邮件日期</summary>	2021年03月26日</details>

# 27、基于跨任务知识转移的单深度超分辨率场景结构引导学习
- [ ] Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for Single Depth Super-Resolution 
时间：2021年03月24日                         第一作者：Baoli Sun                       [链接](https://arxiv.org/abs/2103.12955).                     
## 摘要：现有的颜色引导深度超分辨率（DSR）方法需要成对的RGB-D数据作为训练样本，利用RGB图像的几何相似性作为结构引导来恢复退化的深度图。然而，在实际测试环境中，成对数据的收集可能有限或昂贵。因此，我们第一次探索在训练阶段学习跨模态知识，在训练阶段RGB和深度模态都可用，但在目标数据集上测试，只有单一的深度模态存在。我们的核心思想是在不改变网络结构的前提下，将场景结构制导知识从RGB模态提取到单个DSR任务。具体地说，我们构造了一个以RGB图像为输入的辅助深度估计（DE）任务来估计深度图，并协同训练DSR任务和DE任务来提高DSR的性能。在此基础上，提出了一个跨任务交互模块来实现双边跨任务知识转移。首先，我们设计了一个跨任务的提炼方案，鼓励DSR和DE网络以师生角色交换的方式相互学习。然后，我们提出了一个结构预测（SP）任务，该任务提供额外的结构正则化，以帮助DSR和DE网络学习更多信息的结构表示，以便进行深度恢复。大量实验表明，与其他DSR方法相比，该方法具有更高的性能。
<details>	<summary>英文摘要</summary>	Existing color-guided depth super-resolution (DSR) approaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to recover the degraded depth map due to their geometrical similarity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the first time to learn the cross-modality knowledge at training stage, where both RGB and depth modalities are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network architecture. Specifically, we construct an auxiliary depth estimation (DE) task that takes an RGB image as input to estimate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bilateral cross task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure prediction (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive experiments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 26、基于多尺度特征交互网络的轻量级超分辨率图像
- [ ] Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network 
时间：2021年03月24日                         第一作者：Zhengxue Wang                       [链接](https://arxiv.org/abs/2103.13028).                     
## 摘要：近年来，采用深度复杂卷积神经网络结构的单图像超分辨率（SISR）方法取得了良好的效果。然而，这些方法以较高的内存消耗为代价来提高性能，难以应用于存储和计算资源有限的移动设备。为了解决这个问题，我们提出了一个轻量级的多尺度特征交互网络（MSFIN）。对于轻量级SISR，MSFIN扩展了接收域，充分利用了低分辨率观测图像的信息特征，这些特征来自不同的尺度和交互连接。此外，我们还设计了一个轻量级的循环剩余信道注意块（RRCAB），使得网络能够在充分轻量级的同时受益于信道注意机制。在一些基准上的大量实验已经证实，我们提出的MSFIN可以通过更轻量化的模型实现与现有技术相当的性能。
<details>	<summary>英文摘要</summary>	Recently, the single image super-resolution (SISR) approaches with deep and complex convolutional neural network structures have achieved promising performance. However, those methods improve the performance at the cost of higher memory consumption, which is difficult to be applied for some mobile devices with limited storage and computing resources. To solve this problem, we present a lightweight multi-scale feature interaction network (MSFIN). For lightweight SISR, MSFIN expands the receptive field and adequately exploits the informative features of the low-resolution observed images from various scales and interactive connections. In addition, we design a lightweight recurrent residual channel attention block (RRCAB) so that the network can benefit from the channel attention mechanism while being sufficiently lightweight. Extensive experiments on some benchmarks have confirmed that our proposed MSFIN can achieve comparable performance against the state-of-the-arts with a more lightweight model. </details>
<details>	<summary>注释</summary>	Accepted by ICME2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月25日</details>

# 25、UltraSR：空间编码是基于隐式图像函数的任意尺度超分辨率的关键
- [ ] UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution 
时间：2021年03月23日                         第一作者：Xingqian Xu                       [链接](https://arxiv.org/abs/2103.12716).                     
## 摘要：NeRF和其他相关隐式神经表示方法的成功为连续图像表示开辟了一条新的途径，即不再需要从存储的离散二维阵列中查找像素值，而是可以从连续空间域上的神经网络模型中推断像素值。尽管最近的研究表明，这种新的方法在任意尺度的超分辨率任务中都能取得很好的效果，但由于高频纹理的错误预测，放大后的图像往往会出现结构性失真。在这项工作中，我们提出了一种简单而有效的基于隐式图像函数的新网络设计方法UltraSR，其中空间坐标和周期编码与隐式神经表示深度结合。我们通过大量的实验和研究表明，空间编码确实是下一阶段高精度隐式图像函数的关键。与以前最先进的方法相比，我们的UltraSR在DIV2K基准上设置了所有超分辨率标度下的最新性能。UltraSR在其他标准基准数据集上也取得了优异的性能，在几乎所有的实验中都优于以前的工作。我们的代码将在https://github.com/SHI-Labs/UltraSR-arbitral-Scale-Super-Resolution。
<details>	<summary>英文摘要</summary>	The recent success of NeRF and other related implicit neural representation methods has opened a new path for continuous image representation, where pixel values no longer need to be looked up from stored discrete 2D arrays but can be inferred from neural network models on a continuous spatial domain. Although the recent work LIIF has demonstrated that such novel approach can achieve good performance on the arbitrary-scale super-resolution task, their upscaled images frequently show structural distortion due to the faulty prediction on high-frequency textures. In this work, we propose UltraSR, a simple yet effective new network design based on implicit image functions in which spatial coordinates and periodic encoding are deeply integrated with the implicit neural representation. We show that spatial encoding is indeed a missing key towards the next-stage high-accuracy implicit image function through extensive experiments and ablation studies. Our UltraSR sets new state-of-the-art performance on the DIV2K benchmark under all super-resolution scales comparing to previous state-of-the-art methods. UltraSR also achieves superior performance on other standard benchmark datasets in which it outperforms prior works in almost all experiments. Our code will be released at https://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution. </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 24、双子网多级通信上采样大运动视频超分辨率
- [ ] Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling 
时间：2021年03月22日                         第一作者：Hongying Liu                       [链接](https://arxiv.org/abs/2103.11744).                     
## 摘要：视频超分辨率（VSR）的目标是恢复低分辨率（LR）的视频，并将其提高到更高的分辨率（HR）。由于视频任务的特点，在VSR算法中，对帧间运动信息的关注、总结和利用是非常重要的。特别是当视频包含大运动时，传统的方法容易产生非相干的结果或伪影。提出了一种新的双子网多级通信上采样深度神经网络（DSMC），用于大运动视频的超分辨率处理。设计了一个新的三维卷积U形残差密集网络（U3D-RDN）模块，用于精细隐式运动估计和运动补偿（MEMC）以及粗空间特征提取。提出了一种新的多级通信上采样（MSCU）模块，充分利用上采样的中间结果指导VSR。此外，本文还设计了一种新的双子网来辅助DSMC的训练，它的双子网损失有助于减少解空间和提高泛化能力。实验结果表明，与现有的方法相比，该方法在大运动视频上具有更好的性能。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月23日</details>

# 23、一种新的单图像超分辨率公共Alsat-2B数据集
- [ ] A new public Alsat-2B dataset for single-image super-resolution 
时间：2021年03月21日                         第一作者：Achraf Djerida                       [链接](https://arxiv.org/abs/2103.12547).                     
## 摘要：目前，当有可靠的训练数据集可用时，深度学习方法是图像超分辨率的主要解决方案。然而，对于遥感基准来说，获取高空间分辨率的图像是非常昂贵的。大多数超分辨率方法都采用下采样技术来模拟低分辨率和高分辨率的空间对，并构造训练样本。为了解决这一问题，本文提出了一种新的低分辨率和高分辨率（分别为10m和2.5m）的公共遥感数据集（Alsat2B），用于单幅图像的超分辨率处理。通过平移锐化得到高分辨率图像。此外，基于通用准则对数据集上一些超分辨率方法的性能进行了评估。结果表明，该方法是有前途的，并突出了数据集的挑战，这表明需要先进的方法来掌握低分辨率和高分辨率斑块之间的关系。
<details>	<summary>英文摘要</summary>	Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches. </details>
<details>	<summary>注释</summary>	This paper has been Accepted for publication in the International Geoscience and Remote Sensing Symposium (IGARSS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 22、任意输入输出波段下的高光谱图像超分辨率
- [ ] Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings 
时间：2021年03月19日                         第一作者：Zhongyang Zhang                       [链接](https://arxiv.org/abs/2103.10614).                     
## 摘要：高光谱图像具有较窄的光谱波段，能够获取丰富的光谱信息，适合于许多计算机视觉任务。HSI的一个基本限制是它的低空间分辨率，最近一些关于超分辨率（SR）的工作被提出来解决这个问题。然而，由于HSI摄像机的多样性，不同的摄像机捕捉到的图像具有不同的光谱响应函数和总通道数。现有的HSI数据集通常很小，因此不足以建模。提出了一种基于元学习的超分辨率（MLSR）模型，该模型可以在任意多个输入波段的峰值波长处获取HSI图像，并生成任意多个输出波段的峰值波长的超分辨率HSI。我们通过对NTIRE2020和ICVL数据集的波段进行采样，人工创建子数据集，模拟交叉数据集设置，并对其进行谱内插和外推的HSI SR。我们为所有子数据集训练单个MLSR模型，并为每个子数据集训练专用的基线模型。结果表明，与现有的HSI-SR方法相比，该模型具有相同的水平或更好的性能。
<details>	<summary>英文摘要</summary>	Hyperspectral images (HSIs) with narrow spectral bands can capture rich spectral information, making them suitable for many computer vision tasks. One of the fundamental limitations of HSI is its low spatial resolution, and several recent works on super-resolution(SR) have been proposed to tackle this challenge. However, due to HSI cameras' diversity, different cameras capture images with different spectral response functions and the number of total channels. The existing HSI datasets are usually small and consequently insufficient for modeling. We propose a Meta-Learning-Based Super-Resolution(MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate super-resolved HSIs with an arbitrary number of output bands' peak wavelengths. We artificially create sub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate the cross-dataset settings and perform HSI SR with spectral interpolation and extrapolation on them. We train a single MLSR model for all sub-datasets and train dedicated baseline models for each sub-dataset. The results show the proposed model has the same level or better performance compared to the-state-of-the-art HSI SR methods. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 21、视频超分辨率的自监督自适应算法
- [ ] Self-Supervised Adaptation for Video Super-Resolution 
时间：2021年03月18日                         第一作者：Jinsu Yoo                        [链接](https://arxiv.org/abs/2103.10081).                     
## 摘要：近年来，单图像超分辨率（single-image super-resolution，SISR）网络通过利用输入数据中的信息和大量外部数据集，使网络参数适应特定的输入图像，取得了良好的效果。然而，这些自监督SISR方法在视频处理中的扩展还有待研究。因此，我们提出了一种新的学习算法，使得传统的视频超分辨率（VSR）网络能够在不使用地面真实数据集的情况下调整参数来测试视频帧。通过利用空间和时间上的许多自相似块，我们提高了完全预训练VSR网络的性能，并产生了时间一致的视频帧。此外，我们提出了一种测试时知识提取技术，以较少的硬件资源加快了自适应速度。在我们的实验中，我们证明了我们的新学习算法可以微调最先进的VSR网络，并在大量的基准数据集上显著提高性能。
<details>	<summary>英文摘要</summary>	Recent single-image super-resolution (SISR) networks, which can adapt their network parameters to specific input images, have shown promising results by exploiting the information available within the input data as well as large external datasets. However, the extension of these self-supervised SISR approaches to video handling has yet to be studied. Thus, we present a new learning algorithm that allows conventional video super-resolution (VSR) networks to adapt their parameters to test video frames without using the ground-truth datasets. By utilizing many self-similar patches across space and time, we improve the performance of fully pre-trained VSR networks and produce temporally consistent video frames. Moreover, we present a test-time knowledge distillation technique that accelerates the adaptation speed with less hardware resources. In our experiments, we demonstrate that our novel learning algorithm can fine-tune state-of-the-art VSR networks and substantially elevate performance on numerous benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年03月19日</details>

# 20、结构化输出依赖建模的一般感知损失
- [ ] Generic Perceptual Loss for Modeling Structured Output Dependencies 
时间：2021年03月18日                         第一作者：Yifan Liu                       [链接](https://arxiv.org/abs/2103.10571).                     
## 摘要：感知损失作为一个有效的损失项被广泛应用于图像合成任务中，包括图像超分辨率、风格转换等。人们认为，成功的关键在于从经过大量图像预训练的CNNs中提取高层次的感知特征表示。这里我们揭示了，重要的是网络结构，而不是训练的权重。在没有任何学习的情况下，深层网络的结构就足以利用多层cnn捕获多个层次的变量统计之间的依赖关系。这种洞察消除了预先训练和特定网络结构（通常是VGG）的要求，这些都是先前假设的感知损失，从而实现了更广泛的应用。为此，我们证明了一个随机加权的深度CNN可以用来模拟输出的结构化依赖关系。在语义分割、深度估计和实例分割等稠密的逐像素预测任务中，与单独使用逐像素丢失的基线相比，扩展的随机感知丢失方法得到了更好的结果。我们希望这种简单的，扩展的知觉损失可以作为一种通用的结构化输出损失，适用于大多数结构化输出学习任务。
<details>	<summary>英文摘要</summary>	The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution, and style transfer. It was believed that the success lies in the high-level perceptual feature representations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the network structure instead of the trained weights. Without any learning, the structure of a deep network is sufficient to capture the dependencies between multiple levels of variable statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously assumed for the perceptual loss, thus enabling a significantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks. </details>
<details>	<summary>注释</summary>	Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 19、视频流预测辅助帧超分辨率
- [ ] Prediction-assistant Frame Super-Resolution for Video Streaming 
时间：2021年03月17日                         第一作者：Wang Shen                       [链接](https://arxiv.org/abs/2103.09455).                     
## 摘要：在在线视频游戏、现场直播等实时应用中，视频帧的传输延迟是至关重要的，新帧的接收截止时间必须赶上帧的渲染时间。否则，系统会缓冲一段时间，用户会遇到冻结屏幕，导致用户体验不尽如人意。一种有效的方法是在较差的带宽条件下传输较低质量的帧，例如使用可伸缩视频编码。在本文中，我们提出在两种情况下使用有损帧来提高视频质量。首先，当当前帧在渲染截止时间之前太晚而无法接收时（即丢失），我们建议使用先前接收到的高分辨率图像来预测未来的帧。第二，当当前接收到的帧的质量较低（即有损）时，我们建议使用先前接收到的高分辨率帧来增强低质量的当前帧。对于第一种情况，我们提出了一个小而有效的视频帧预测网络。对于第二种情况，我们将视频预测网络改进为视频增强网络，将当前帧和前一帧关联起来，以恢复高质量的图像。大量的实验结果表明，我们的方法在有损视频流环境中的性能优于现有的算法。
<details>	<summary>英文摘要</summary>	Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment. </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 18、ShipSRDet：一种基于超分辨特征表示的端到端遥感舰船探测器
- [ ] ShipSRDet: An End-to-End Remote Sensing Ship Detector Using Super-Resolved Feature Representation 
时间：2021年03月17日                         第一作者：Shitian He                       [链接](https://arxiv.org/abs/2103.09699).                     
## 摘要：高分辨率遥感图像可以为船舶检测提供丰富的外观信息。虽然已有的一些方法采用图像超分辨率（SR）来提高检测性能，但它们将图像超分辨率和船舶检测视为两个独立的过程，忽略了这两个相关任务之间的内在一致性。在本文中，我们探讨了图像SR对船舶检测的潜在好处，并提出了一种端到端网络ShipSRDet。在我们的方法中，我们不仅将超分辨图像提供给检测器，而且将SR网络的中间特征与检测网络的中间特征结合起来。这样，SR网络提取的信息性特征表示就可以充分用于船舶检测。在HRSC数据集上的实验结果验证了该方法的有效性。我们的ShipSRDet可以从输入图像中恢复丢失的细节，并取得了良好的船舶检测性能。
<details>	<summary>英文摘要</summary>	High-resolution remote sensing images can provide abundant appearance information for ship detection. Although several existing methods use image super-resolution (SR) approaches to improve the detection performance, they consider image SR and ship detection as two separate processes and overlook the internal coherence between these two correlated tasks. In this paper, we explore the potential benefits introduced by image SR to ship detection, and propose an end-to-end network named ShipSRDet. In our method, we not only feed the super-resolved images to the detector but also integrate the intermediate features of the SR network with those of the detection network. In this way, the informative feature representation extracted by the SR network can be fully used for ship detection. Experimental results on the HRSC dataset validate the effectiveness of our method. Our ShipSRDet can recover the missing details from the input image and achieves promising ship detection performance. </details>
<details>	<summary>注释</summary>	Accepted to IGARSS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月18日</details>

# 17、基于单镜头样本的超分辨跨域人脸模型
- [ ] Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar 
时间：2021年03月16日                         第一作者：Peike Li                       [链接](https://arxiv.org/abs/2103.08863).                     
## 摘要：传统的人脸超分辨率方法通常假设检测低分辨率（LR）图像与训练图像位于同一个域。由于光照条件和成像硬件的不同，在许多实际场景中，训练图像和测试图像之间不可避免地会出现域间隙。忽略这些域间隙将导致较低的人脸超分辨率（FSR）性能。然而，如何将训练好的FSR模型有效地转移到目标域中还没有被研究。为了解决这个问题，我们开发了一个基于域感知金字塔的人脸超分辨率网络，命名为DAP-FSR网络。我们的DAP-FSR是第一次尝试利用目标域中的一对高分辨率（HR）和LR样本从目标域超分辨LR人脸。具体来说，我们的DAP-FSR首先使用编码器来提取输入LR人脸的多尺度潜在表示。考虑到只有一个目标域的例子可用，我们建议通过混合目标域面和源域面的潜在表示来扩充目标域数据，然后将混合表示提供给我们的DAP-FSR解码器。解码器将生成与目标域图像样式相似的新人脸图像。生成的HR面依次用于优化我们的解码器以减少域间隙。通过迭代更新潜在的表示和我们的解码器，我们的DAP-FSR将适应目标域，从而实现真实和高质量的上采样HR人脸。在三个新构建的基准上的大量实验验证了我们的DAP-FSR的有效性和优越的性能。
<details>	<summary>英文摘要</summary>	Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art. </details>
<details>	<summary>邮件日期</summary>	2021年03月17日</details>

# 16、学习频率感知动态网络实现高效超分辨率
- [ ] Learning Frequency-aware Dynamic Network for Efficient Super-Resolution 
时间：2021年03月15日                         第一作者：Wenbin Xie                       [链接](https://arxiv.org/abs/2103.08357).                     
## 摘要：基于深度学习的方法，特别是卷积神经网络（CNNs）已经成功地应用于单幅图像超分辨率（SISR）领域。为了获得更好的逼真度和视觉质量，现有的网络大多采用繁重的设计和大量的计算。然而，现代移动设备的计算资源有限，难以承受昂贵的成本。为此，本文提出了一种基于离散余弦变换（DCT）域的频率感知动态网络，将输入信号按其系数分为多个部分。在实际应用中，高频部分采用昂贵的运算，低频部分采用廉价的运算，以减轻计算负担。由于像素或图像块属于低频区域，包含的纹理细节相对较少，因此这种动态网络不会影响生成的超分辨率图像的质量。此外，我们将预测器嵌入到所提出的动态网路中，以端到端微调手工制作的频率感知遮罩。在基准SISR模型和数据集上进行的大量实验表明，频率感知动态网络可以应用于各种SISR神经结构，在视觉质量和计算复杂度之间获得更好的折衷。例如，我们可以在保持最先进的SISR性能的同时，将EDSR模型的失败率降低大约50\%$。
<details>	<summary>英文摘要</summary>	Deep learning based methods, especially convolutional neural networks (CNNs) have been successfully applied in the field of single image super-resolution (SISR). To obtain better fidelity and visual quality, most of existing networks are of heavy design with massive computation. However, the computation resources of modern mobile devices are limited, which cannot easily support the expensive cost. To this end, this paper explores a novel frequency-aware dynamic network for dividing the input into multiple parts according to its coefficients in the discrete cosine transform (DCT) domain. In practice, the high-frequency part will be processed using expensive operations and the lower-frequency part is assigned with cheap operations to relieve the computation burden. Since pixels or image patches belong to low-frequency areas contain relatively few textural details, this dynamic network will not affect the quality of resulting super-resolution images. In addition, we embed predictors into the proposed dynamic network to end-to-end fine-tune the handcrafted frequency-aware masks. Extensive experiments conducted on benchmark SISR models and datasets show that the frequency-aware dynamic network can be employed for various SISR neural architectures to obtain the better tradeoff between visual quality and computational complexity. For instance, we can reduce the FLOPs of EDSR model by approximate $50\%$ while preserving state-of-the-art SISR performance. </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 15、低分辨率图像和视频中的三维人体姿势、形状和纹理
- [ ] 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos 
时间：2021年03月11日                         第一作者：Xiangyu Xu                       [链接](https://arxiv.org/abs/2103.06498).                     
## 摘要：基于单眼图像的三维人体姿态和形状估计一直是计算机视觉领域的一个研究热点。现有的深度学习方法依赖于高分辨率的输入，然而，在视频监控和体育广播等许多场景中并不总是可用的。处理低分辨率图像的两种常用方法是对输入应用超分辨率技术，这可能会导致不愉快的伪影，或者只是针对每个分辨率训练一个模型，这在许多实际应用中是不切实际的。针对上述问题，本文提出了一种新的算法RSC-Net，该算法由分辨率感知网络、自监督损失和对比学习机制组成。该方法能够在单个模型上学习不同分辨率下的三维人体姿态和形状。自我监督缺失加强了输出的尺度一致性，而对比学习方案加强了深层特征的尺度一致性。我们表明，这两个新的损失提供鲁棒性学习时，在弱监督的方式。此外，我们扩展了RSC网络来处理低分辨率的视频，并将其应用于从低分辨率输入中重建具有纹理的三维行人。大量的实验表明，RSC网络在处理低分辨率图像时，可以取得比现有方法更好的效果。
<details>	<summary>英文摘要</summary>	3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2007.13666 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 14、一种基于学习的轴向超分辨率视图外推方法
- [ ] A learning-based view extrapolation method for axial super-resolution 
时间：2021年03月11日                         第一作者：Zhaolin Xiao                       [链接](https://arxiv.org/abs/2103.06510).                     
## 摘要：轴向光场分辨率是指通过重新聚焦来区分不同深度特征的能力。轴向再聚焦精度相当于两个可分辨的再聚焦平面在轴向上的最小距离。高再聚焦精度对于一些光场应用（如显微镜）来说是必不可少的。在这篇论文中，我们提出了一种基于学习的方法来外推新的观点从轴向体积剪切极平面图像（EPIs）。与经典成像中的扩展数值孔径（NA）一样，外推光场可以获得具有较浅景深（DOF）的重聚焦图像，从而获得更精确的重聚焦结果。最重要的是，该方法不需要精确的深度估计。对合成光场和真实光场的实验结果表明，该方法不仅适用于全光相机（尤其是1.0型全光相机）拍摄的基线较小的光场，而且适用于基线较大的光场。
<details>	<summary>英文摘要</summary>	Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines. </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 13、使用真实退化图像的超分辨率卫星硬件
- [ ] Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images 
时间：2021年03月10日                         第一作者：Jack White                       [链接](https://arxiv.org/abs/2103.06270).                     
## 摘要：现代深超分辨率（SR）网络已成为图像重建和增强的重要技术。然而，这些网络通常是在缺乏真实图像中典型的图像降质噪声的基准图像数据上训练和测试的。在这篇论文中，我们通过评估SR在重建真实退化卫星图像中的性能，来测试在真实遥感有效载荷中使用深度SR的可行性。我们证明了一种称为增强深超分辨率网络（EDSR）的先进SR技术，在没有特定领域的预训练的情况下，只要地面分辨率足够远，就可以在地面采样距离较短的图像上恢复编码的像素数据。然而，这种恢复因所选地理类型而异。我们的结果表明，定制训练有可能进一步改善高架图像的重建，新的卫星硬件应优先考虑光学性能，而不是最小化像素大小，因为深SR可以克服后者的不足，但不能克服前者。
<details>	<summary>英文摘要</summary>	Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former. </details>
<details>	<summary>注释</summary>	6 pages, 6 figures, for supplementary results, see https://smpetrie.github.io/superres/ ACM-class: I.4.3 </details>
<details>	<summary>邮件日期</summary>	2021年03月12日</details>

# 12、高光谱图像超分辨率空间光谱反馈网络
- [ ] Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral Imagery 
时间：2021年03月07日                         第一作者：Enhai Liu                       [链接](https://arxiv.org/abs/2103.04354).                     
## 摘要：近年来，基于深度学习的单灰度/RGB图像超分辨率（SR）方法取得了很大的成功。然而，单幅高光谱图像的超分辨率处理存在两大障碍，限制了技术的发展。一是高光谱图像中高维复杂的光谱模式，使得空间信息和波段间的光谱信息难以同时获取。另一方面，高光谱训练样本的数目非常小，在训练深度神经网络时容易导致过度拟合。为了解决这些问题，本文提出了一种新的空间谱反馈网络（SSFN），利用全局谱带的高阶信息来细化局部谱带间的低阶表示。它不仅可以缓解高光谱数据高维性给特征提取带来的困难，而且可以使训练过程更加稳定。具体地说，我们在具有有限展开的RNN中使用隐藏状态来实现这种反馈方式。为了充分利用空间和光谱先验知识，设计了空间光谱反馈块（SSFB）来处理反馈连接并生成强大的高层表示。提出的SSFN具有早期预测能力，可以逐步重建最终的高分辨率高光谱图像。在三个基准数据集上的大量实验结果表明，与现有的方法相比，所提出的SSFN具有更好的性能。源代码位于https://github.com/tangzhenjie/SSFN。
<details>	<summary>英文摘要</summary>	Recently, single gray/RGB image super-resolution (SR) methods based on deep learning have achieved great success. However, there are two obstacles to limit technical development in the single hyperspectral image super-resolution. One is the high-dimensional and complex spectral patterns in hyperspectral image, which make it difficult to explore spatial information and spectral information among bands simultaneously. The other is that the number of available hyperspectral training samples is extremely small, which can easily lead to overfitting when training a deep neural network. To address these issues, in this paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to refine low-level representations among local spectral bands with high-level information from global spectral bands. It will not only alleviate the difficulty in feature extraction due to high dimensional of hyperspectral data, but also make the training process more stable. Specifically, we use hidden states in an RNN with finite unfoldings to achieve such feedback manner. To exploit the spatial and spectral prior, a Spatial-Spectral Feedback Block (SSFB) is designed to handle the feedback connections and generate powerful high-level representations. The proposed SSFN comes with a early predictions and can reconstruct the final high-resolution hyperspectral image step by step. Extensive experimental results on three benchmark datasets demonstrate that the proposed SSFN achieves superior performance in comparison with the state-of-the-art methods. The source code is available at https://github.com/tangzhenjie/SSFN. </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 11、基于深度学习的小数据集超分辨荧光显微镜
- [ ] Deep learning-based super-resolution fluorescence microscopy on small datasets 
时间：2021年03月07日                         第一作者：Varun Mannam                       [链接](https://arxiv.org/abs/2103.04989).                     
## 摘要：荧光显微术以微米级的分辨率显示生物有机体，使现代生物学有了巨大的发展。然而，由于衍射极限的限制，亚微米/纳米级的特征很难分辨。虽然各种超分辨率技术是为了达到纳米级的分辨率而发展起来的，但它们往往需要昂贵的光学装置或专门的荧光团。近年来，深度学习显示出减少技术障碍和从衍射限制图像获得超分辨率的潜力。为了得到准确的结果，传统的深度学习技术需要成千上万的图像作为训练数据集。由于荧光团的光漂白、光毒性和生物体内发生的动态过程，从生物样品中获取大量数据通常是不可行的。因此，利用小数据集实现基于深度学习的超分辨率具有挑战性。我们用一种新的基于卷积神经网络的方法来解决这一限制，这种方法成功地用小数据集训练并获得超分辨率图像。我们从15个不同的视场共采集了750张图像作为训练数据集来演示该技术。在每个视场中，采用超分辨率径向起伏方法生成单个目标图像。正如预期的那样，这个小数据集无法使用传统的超分辨率体系结构生成可用的模型。然而，使用新的方法，可以训练一个网络来从这个小数据集获得超分辨率图像。这种深度学习模型可应用于其他生物医学成像模式，如MRI和X射线成像，在这些模式中获取大型训练数据集是一项挑战。
<details>	<summary>英文摘要</summary>	Fluorescence microscopy has enabled a dramatic development in modern biology by visualizing biological organisms with micrometer scale resolution. However, due to the diffraction limit, sub-micron/nanometer features are difficult to resolve. While various super-resolution techniques are developed to achieve nanometer-scale resolution, they often either require expensive optical setup or specialized fluorophores. In recent years, deep learning has shown the potentials to reduce the technical barrier and obtain super-resolution from diffraction-limited images. For accurate results, conventional deep learning techniques require thousands of images as a training dataset. Obtaining large datasets from biological samples is not often feasible due to the photobleaching of fluorophores, phototoxicity, and dynamic processes occurring within the organism. Therefore, achieving deep learning-based super-resolution using small datasets is challenging. We address this limitation with a new convolutional neural network-based approach that is successfully trained with small datasets and achieves super-resolution images. We captured 750 images in total from 15 different field-of-views as the training dataset to demonstrate the technique. In each FOV, a single target image is generated using the super-resolution radial fluctuation method. As expected, this small dataset failed to produce a usable model using traditional super-resolution architecture. However, using the new approach, a network can be trained to achieve super-resolution images from this small dataset. This deep learning model can be applied to other biomedical imaging modalities such as MRI and X-ray imaging, where obtaining large training datasets is challenging. </details>
<details>	<summary>注释</summary>	SPIE Proceedings Volume 11650, Single Molecule Spectroscopy and Superresolution Imaging XIV; 116500O (2021) DOI: 10.1117/12.2578519 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 10、ClassSR：一种利用数据特征加速超分辨率网络的通用框架
- [ ] ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic 
时间：2021年03月06日                         第一作者：Xiangtao Kong                       [链接](https://arxiv.org/abs/2103.04039).                     
## 摘要：我们的目标是在大图像（2K-8K）上加速超分辨率（SR）网络。在实际应用中，大图像通常被分解成小的子图像。在此基础上，我们发现不同的图像区域具有不同的恢复难度，可以由不同容量的网络进行处理。直观地说，平滑区域比复杂纹理更容易超级求解。为了利用这一特性，我们可以采用适当的SR网络对分解后的不同子图像进行处理。在此基础上，我们提出了一种新的解决方案管道——ClassSR，它将分类和SR结合在一个统一的框架中。特别地，它首先使用一个类模块将子图像按恢复难度分为不同的类，然后应用一个SR模块对不同的类进行SR。类模块是一个传统的分类网络，而SR模块是一个由待加速SR网络及其简化版本组成的网络容器。我们进一步引入了一种新的两损失分类方法——类别损失和平均损失来产生分类结果。联合训练后，大部分子图像将通过较小的网络，从而大大降低了计算量。实验表明，我们的ClassSR可以帮助大多数现有方法（如FSRCNN、CARN、SRResNet、RCAN）在DIV8K数据集上节省高达50%的失败率。这个通用框架也可以应用于其他低层次的视觉任务。
<details>	<summary>英文摘要</summary>	We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks. </details>
<details>	<summary>注释</summary>	CVPR2021 paper + supplementary file </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 9、用稀疏表示生成图像
- [ ] Generating Images with Sparse Representations 
时间：2021年03月05日                         第一作者：Charlie Nash                       [链接](https://arxiv.org/abs/2103.03841).                     
## 摘要：图像的高维性对基于似然的生成模型的结构和采样效率提出了挑战。以前的方法，例如VQ-VAE，使用深度自动编码器来获得紧凑的表示，作为基于似然模型的输入更为实用。我们提出了另一种方法，受JPEG等常见图像压缩方法的启发，将图像转换为量化的离散余弦变换（DCT）块，这些块稀疏地表示为DCT通道、空间位置和DCT系数三元组的序列。我们提出了一种基于变换器的自回归结构，该结构被训练成序列预测下一个元素在这些序列中的条件分布，并有效地扩展到高分辨率图像。在一系列的图像数据集上，我们证明了我们的方法可以生成高质量、多样的图像，并且样本度量分数可以与最先进的方法相竞争。此外，我们还表明，简单的修改，我们的方法产生有效的图像着色和超分辨率模型。
<details>	<summary>英文摘要</summary>	The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 8、KOALAnet：基于核自适应局部调整的盲超分辨算法
- [ ] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment 
时间：2021年03月05日                         第一作者：Soo Ye Kim                       [链接](https://arxiv.org/abs/2012.08103).                     
<details>	<summary>注释</summary>	Accepted to CVPR 2021. The first two authors contributed equally to this work </details>
<details>	<summary>邮件日期</summary>	2021年03月08日</details>

# 7、真实世界的单图像超分辨率：简要回顾
- [ ] Real-World Single Image Super-Resolution: A Brief Review 
时间：2021年03月03日                         第一作者：Honggang Chen                       [链接](https://arxiv.org/abs/2103.02368).                     
## 摘要：单图像超分辨率（Single-image super-resolution，SISR）是近几十年来图像处理领域的一个研究热点，其目的是通过低分辨率（low-resolution，LR）观测重建高分辨率（high-resolution，HR）图像。特别是基于深度学习的超分辨率（SR）方法已经引起了人们的广泛关注，极大地提高了合成数据的重建性能。最近的研究表明，对合成数据的模拟结果通常高估了对真实世界图像的超分辨能力。在这样的背景下，越来越多的研究者致力于研究真实感图像的随机共振方法。本文对现实世界中的单幅图像超分辨率（RSISR）技术进行了综述。更具体地说，本综述涵盖了RSISR的关键公共可用数据集和评估指标，以及四大类RSISR方法，即基于退化建模的RSISR、基于图像对的RSISR、基于领域翻译的RSISR和基于自学习的RSISR。在基准数据集上比较了代表性RSISR方法的重建质量和计算效率。此外，我们还讨论了RSISR面临的挑战和未来的研究课题。
<details>	<summary>英文摘要</summary>	Single image super-resolution (SISR), which aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) observation, has been an active research topic in the area of image processing in recent decades. Particularly, deep learning-based super-resolution (SR) approaches have drawn much attention and have greatly improved the reconstruction performance on synthetic data. Recent studies show that simulation results on synthetic data usually overestimate the capacity to super-resolve real-world images. In this context, more and more researchers devote themselves to develop SR approaches for realistic images. This article aims to make a comprehensive review on real-world single image super-resolution (RSISR). More specifically, this review covers the critical publically available datasets and assessment metrics for RSISR, and four major categories of RSISR methods, namely the degradation modeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR, and self-learning-based RSISR. Comparisons are also made among representative RSISR methods on benchmark datasets, in terms of both reconstruction quality and computational efficiency. Besides, we discuss challenges and promising research topics on RSISR. </details>
<details>	<summary>注释</summary>	18 pages, 12 figure, 4 tables </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 6、无约束时空视频超分辨率学习
- [ ] Learning for Unconstrained Space-Time Video Super-Resolution 
时间：2021年02月25日                         第一作者：Zhihao Shi                       [链接](https://arxiv.org/abs/2102.13011).                     
## 摘要：近年来，大量的研究活动致力于视频增强，同时提高时间帧速率和空间分辨率。然而，现有的方法要么不能揭示时空信息之间的内在联系，要么在最终的时空分辨率的选择上缺乏灵活性。在这项工作中，我们提出了一个无约束的时空视频超分辨率网络，它可以有效地利用时空相关性来提高性能。此外，通过使用光流技术和广义像素混洗操作，它在调整时间帧速率和空间分辨率方面具有完全的自由度。实验结果表明，该方法不仅优于现有的算法，而且所需参数少，运行时间短。
<details>	<summary>英文摘要</summary>	Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 5、ShuffleUNet：基于深度学习的磁共振弥散加权成像的超分辨率
- [ ] ShuffleUNet: Super resolution of diffusion-weighted MRIs using deep learning 
时间：2021年02月25日                         第一作者：Soumick Chatterjee                       [链接](https://arxiv.org/abs/2102.12898).                     
## 摘要：弥散加权磁共振成像（DW-MRI）可用于表征神经组织的微观结构，例如通过纤维追踪以非侵入性方式描绘脑白质连接。高空间分辨率的磁共振成像（MRI）将在以更好的方式显示这些纤维束方面发挥重要作用。然而，获得这种分辨率的图像是以较长的扫描时间为代价的。由于患者的心理和身体状况，较长的扫描时间可能与运动伪影的增加有关。单图像超分辨率（Single-Image Super-Resolution，SISR）是一种通过深度学习从单个低分辨率（low-Resolution，LR）输入图像中获取高分辨率细节的技术，是本研究的重点。与插值技术或稀疏编码算法相比，深度学习算法从大数据集中提取先验知识，并从低分辨率图像中产生更优的MRI图像。本研究提出一种基于深度学习的超分辨技术，并将其应用于DW-MRI。从IXI数据集得到的图像被用作地面真值，并被人工降采样以模拟低分辨率图像。所提出的方法在统计学上比基线有了显著的改进，实现了0.913美元-0.045美元的SSIM。
<details>	<summary>英文摘要</summary>	Diffusion-weighted magnetic resonance imaging (DW-MRI) can be used to characterise the microstructure of the nervous tissue, e.g. to delineate brain white matter connections in a non-invasive manner via fibre tracking. Magnetic Resonance Imaging (MRI) in high spatial resolution would play an important role in visualising such fibre tracts in a superior manner. However, obtaining an image of such resolution comes at the expense of longer scan time. Longer scan time can be associated with the increase of motion artefacts, due to the patient's psychological and physical conditions. Single Image Super-Resolution (SISR), a technique aimed to obtain high-resolution (HR) details from one single low-resolution (LR) input image, achieved with Deep Learning, is the focus of this study. Compared to interpolation techniques or sparse-coding algorithms, deep learning extracts prior knowledge from big datasets and produces superior MRI images from the low-resolution counterparts. In this research, a deep learning based super-resolution technique is proposed and has been applied for DW-MRI. Images from the IXI dataset have been used as the ground-truth and were artificially downsampled to simulate the low-resolution images. The proposed method has shown statistically significant improvement over the baselines and achieved an SSIM of $0.913\pm0.045$. </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 4、用于视频超分辨率的深展开网络
- [ ] Deep Unrolled Network for Video Super-Resolution 
时间：2021年02月23日                         第一作者：Benjamin Naoto Chiche                       [链接](https://arxiv.org/abs/2102.11720).                     
## 摘要：视频超分辨率（VSR）的目的是从相应的低分辨率（LR）图像中重建一系列高分辨率（HR）图像。传统上，VSR问题的求解是基于迭代算法，该算法可以利用图像形成的先验知识和运动的假设。然而，这些经典的方法很难将自然图像中的复杂统计信息结合起来。此外，VSR最近受益于深度学习（DL）算法带来的改进。这些技术可以有效地从大量的图像集合中学习空间模式。然而，他们没有融入一些有关图像形成模型的知识，这限制了他们的灵活性。为解决反问题而开发的展开优化算法允许将先验信息纳入深度学习体系结构。它们主要用于单个图像恢复任务。采用展开的神经网络结构可以带来以下好处。首先，这可能会提高超分辨率任务的性能。这样，神经网络就具有更好的可解释性。最后，这允许灵活地学习单个模型，以非盲目地处理多个退化。本文提出了一种新的基于展开优化技术的VSR神经网络，并对其性能进行了讨论。
<details>	<summary>英文摘要</summary>	Video super-resolution (VSR) aims to reconstruct a sequence of high-resolution (HR) images from their corresponding low-resolution (LR) versions. Traditionally, solving a VSR problem has been based on iterative algorithms that can exploit prior knowledge on image formation and assumptions on the motion. However, these classical methods struggle at incorporating complex statistics from natural images. Furthermore, VSR has recently benefited from the improvement brought by deep learning (DL) algorithms. These techniques can efficiently learn spatial patterns from large collections of images. Yet, they fail to incorporate some knowledge about the image formation model, which limits their flexibility. Unrolled optimization algorithms, developed for inverse problems resolution, allow to include prior information into deep learning architectures. They have been used mainly for single image restoration tasks. Adapting an unrolled neural network structure can bring the following benefits. First, this may increase performance of the super-resolution task. Then, this gives neural networks better interpretability. Finally, this allows flexibility in learning a single model to nonblindly deal with multiple degradations. In this paper, we propose a new VSR neural network based on unrolled optimization techniques and discuss its performance. </details>
<details>	<summary>注释</summary>	6 pages. 3 figures. Published in: 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA) DOI: 10.1109/IPTA50016.2020.9286636 </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 3、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月23日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 2、基于切比雪夫变换域的图像超分辨率深度学习体系结构
- [ ] Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution 
时间：2021年02月21日                         第一作者：Ahlad Kumar                        [链接](https://arxiv.org/abs/2102.10640).                     
## 摘要：最近COVID-19的爆发促使研究人员在利用人工智能和深度学习的医学成像领域做出贡献。超分辨率（SR）在过去的几年中，利用深度学习方法取得了显著的效果。深度学习方法学习从低分辨率（LR）图像到相应的高分辨率（HR）图像的非线性映射的能力导致了SR在不同研究领域的引人注目的结果。本文提出了一种基于深度学习的切比切夫变换域图像超分辨率结构。这是通过将转换层通过定制的Tchebichef卷积层（$TCL$）集成到提议的体系结构中来实现的。TCL的作用是利用Tchebichef基函数将LR图像从空间域转换到正交变换域。使用称为逆切比雪夫卷积层（ITCL）的另一层实现上述变换的反演，该层将LR图像从变换域转换回空间域。研究表明，利用Tchebichef变换域进行超分辨率重建，利用了图像的高低频特征，简化了超分辨率重建任务。我们进一步引入转移学习方法来提高基于Covid的医学图像的质量。结果表明，我们的结构提高了COVID-19的X线和CT图像质量，提供了更好的图像质量，有助于临床诊断。与使用较少可训练参数的大多数深度学习方法相比，使用所提出的切比雪夫变换域超分辨率（TTDSR）结构获得的实验结果提供了具有竞争力的结果。
<details>	<summary>英文摘要</summary>	The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters. </details>
<details>	<summary>注释</summary>	11 pages, 12 figures, 53 references </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 1、用于原子分辨率图像的高精度原子分割、定位、去噪和超分辨率处理的TEMImageNet训练库和atomsenet深度学习模型
- [ ] TEMImageNet Training Library and AtomSegNet Deep-Learning Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-Resolution Processing of Atomic-Resolution Images 
时间：2021年02月20日                         第一作者：Ruoqian Lin                       [链接](https://arxiv.org/abs/2012.09093).                     
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

